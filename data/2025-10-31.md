<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 79]
- [cs.AI](#cs.AI) [Total: 37]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 8]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 本研究提出了一种结合意见词方向和强度对实体进行排名的模糊逻辑和句法依赖解析方法，以解决现有整体词汇方法忽略意见强度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有观点挖掘方法（如整体词汇方法）未能充分考虑意见的强度（如非常强烈、强烈、中等、非常微弱、微弱的负面或正面意见），这限制了对实体评价的精确性。本研究旨在通过量化意见强度来改进实体排名，从而提供更细致的评价。

Method: 本研究提出了一种新方法，通过模糊逻辑算法将意见词（副词、形容词、名词和动词）分类到不同的粒度级别（非常弱、弱、中等、非常强和强），并结合句法依赖解析来识别与特定产品方面相关的意见词。通过分析这些意见词，计算实体在特定方面上的得分。

Result: 本研究提出了一种结合意见词方向和强度对实体进行排名的模糊逻辑和句法依赖解析方法，能够更精确地评估实体在不同方面的表现。

Conclusion: 本研究提出的方法通过考虑意见的强度和粒度，为观点挖掘和实体排名提供了更精细的解决方案，克服了传统方法的局限性。未来研究可进一步探索更复杂的意见表达和跨领域应用。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [2] [LASTIST: LArge-Scale Target-Independent STance dataset](https://arxiv.org/abs/2510.25783)
*DongJae Kim,Yaejin Lee,Minsu Park,Eunil Park*

Main category: cs.CL

TL;DR: 本研究提出了LASTIST数据集，这是一个大规模、目标无关的韩语立场检测数据集，旨在解决低资源语言在立场检测研究中的不足，并支持目标无关和历时演变等多种立场检测任务。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能领域的立场检测研究主要集中在目标相关任务，且大多基于英语数据，这使得韩语等低资源语言在立场检测模型开发上面临挑战。为了弥补这一研究空白，本研究致力于构建一个大规模的韩语立场检测数据集。

Method: 研究收集了韩国各政党新闻发布中的563,299个已标记的韩语句子，构建了LASTIST（LArge-Scale Target-Independent STance）数据集。数据集的构建过程和细节得到了详细说明。同时，研究还训练了先进的深度学习和立场检测模型。

Result: 研究成功构建了大规模韩语立场检测数据集LASTIST，并使用该数据集训练了先进的模型，为低资源语言的立场检测研究奠定了基础。数据集支持目标无关和历时演变等多种立场检测任务。

Conclusion: LASTIST数据集的提出填补了韩语大规模目标无关立场检测数据集的空白，为相关研究提供了宝贵资源，并有望推动低资源语言在立场检测领域的进展。该数据集已在https://anonymous.4open.science/r/LASTIST-3721/公开。

Abstract: Stance detection has emerged as an area of research in the field of
artificial intelligence. However, most research is currently centered on the
target-dependent stance detection task, which is based on a person's stance in
favor of or against a specific target. Furthermore, most benchmark datasets are
based on English, making it difficult to develop models in low-resource
languages such as Korean, especially for an emerging field such as stance
detection. This study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from the press releases
of both parties on Korean political parties, the LASTIST dataset uses 563,299
labeled Korean sentences. We provide a detailed description of how we collected
and constructed the dataset and trained state-of-the-art deep learning and
stance detection models. Our LASTIST dataset is designed for various tasks in
stance detection, including target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on
https://anonymous.4open.science/r/LASTIST-3721/.

</details>


### [3] [zFLoRA: Zero-Latency Fused Low-Rank Adapters](https://arxiv.org/abs/2510.25784)
*Dhananjaya Gowda,Seoha Song,Harshith Goka,Junhyun Lee*

Main category: cs.CL

TL;DR: zFLoRA是一种零延迟融合低秩适配器，可解决大型语言模型（LLM）部署中适配器参数量虽小但推理延迟显著增加的问题。实验表明，zFLoRA在1B、3B和7B参数规模的LLM上，在18个跨越常识推理、数学推理和摘要对话任务中，与LoRA和全模型微调（FFT）相比，性能相当，并且在NPU和GPU平台上实现了零或可忽略不计的延迟开销。


<details>
  <summary>Details</summary>
Motivation: 当前部署大型语言模型（LLM）时，虽然适配器参数量通常不到基础模型的1%，但这些适配器在推理时会带来显著的计算开销，可能导致高达2.5倍的基础模型推理时间。这种显著的延迟影响了LLM在实际应用中的效率。因此，研究一种能够最小化或消除适配器引入的推理延迟的技术至关重要。

Method: 本文提出了一种名为zFLoRA（zero-latency fused low-rank adapter）的新型适配器。zFLoRA通过融合低秩适配器参数，旨在实现零或可忽略不计的延迟开销。该方法在1B、3B和7B参数规模的LLM上进行了实验。在实验设置上，模型在18个不同任务上进行了评估，这些任务涵盖了常识推理、数学推理和摘要对话三个类别。性能比较对象包括流行的低秩适配器（LoRA）和全模型微调（FFT）。此外，还在NPU（三星Galaxy S25+）和GPU（NVIDIA H100）平台上进行了详细的延迟测量。

Result: 实验结果表明，zFLoRA在性能上与流行的LoRA和全模型微调（FFT）方法相当。在1B、3B和7B参数规模的LLM上，zFLoRA在18个任务上的表现与基线方法具有可比性。特别值得注意的是，在NPU和GPU平台上进行的延迟测量显示，zFLoRA适配器引入的延迟非常低，接近于零。这证明了zFLoRA在效率上的优势，它在不牺牲性能的情况下显著降低了推理开销。

Conclusion: zFLoRA是一种有效的解决方案，能够解决在LLM部署中适配器带来的显著推理延迟问题。通过零延迟融合低秩适配器，zFLoRA在保持与LoRA和FFT相当的性能的同时，实现了零或可忽略不计的延迟开销。这对于在资源受限的环境或需要低延迟响应的应用中部署LLM具有重要意义。未来的工作可以进一步探索zFLoRA在更大规模模型和更多样化任务上的适用性，并研究其在不同硬件平台上的优化潜力。

Abstract: Large language models (LLMs) are increasingly deployed with task-specific
adapters catering to multiple downstream applications. In such a scenario, the
additional compute associated with these apparently insignificant number of
adapter parameters (typically less than 1% of the base model) turns out to be
disproportionately significant during inference time (upto 2.5x times that of
the base model). In this paper, we propose a new zero-latency fused low-rank
adapter (zFLoRA) that introduces zero or negligible latency overhead on top of
the base model. Experimental results on LLMs of size 1B, 3B and 7B show that
zFLoRA compares favorably against the popular supervised fine-tuning benchmarks
including low-rank adapters (LoRA) as well as full fine-tuning (FFT).
Experiments are conducted on 18 different tasks across three different
categories namely commonsense reasoning, math reasoning and summary-dialogue.
Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA
H100) platforms show that the proposed zFLoRA adapters introduce zero to
negligible latency overhead.

</details>


### [4] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 本研究提出改进的电路发现方法，通过引导、比例选择和整数线性规划，提高了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 模型的可解释性，特别是电路发现，是理解模型决策的关键挑战。现有方法在准确性和效率上存在不足。

Method: 1. 引导（Bootstrapping）：通过多次采样和聚合，识别归因分数一致的边。 2. 比例选择：使用简单的比例策略，优先选择得分高的正向边，平衡性能和忠实度。 3. 整数线性规划：替代传统的贪婪选择，用于优化电路结构。

Result: 所提出的方法在多个MIB任务和模型上，生成了更忠实的电路，并优于现有方法。

Conclusion: 研究提出的改进电路发现方法能够生成更准确、更忠实的可解释性电路，为理解和调试模型提供了有力工具。

Abstract: One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [5] [The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration](https://arxiv.org/abs/2510.26352)
*Kotaro Furuya,Yuichi Kitagawa*

Main category: cs.CL

TL;DR: 本研究提出了一种以交互为中心的框架，用于自动组合大型语言模型（LLMs）团队，无需了解其内部结构、训练数据或性能。该方法通过分析模型间对话的语义连贯性构建“语言模型图”，并利用社区检测技术识别协同效应强的模型集群。实验证明，该方法能发现具有潜在专业化的功能性分组，并且在特定主题下，发现的团队在下游任务上的表现优于随机分组，可与手动指定的团队相媲美。


<details>
  <summary>Details</summary>
Motivation: 虽然基于大型语言模型（LLMs）的多智能体方法在提升能力方面潜力巨大，但其成功依赖于协同的团队组成。然而，由于大多数模型的内部特性不透明，难以进行有效的协作，因此形成最优团队是一个重大挑战。

Method: 提出一种交互式框架，用于自动组合LLMs团队。该方法不依赖于对模型内部架构、训练数据或任务性能的先验知识。通过分析模型间对话的语义连贯性构建“语言模型图”，该图映射模型间的关系。然后，应用社区检测算法来识别具有协同效应的模型集群。

Result: 通过在各种LLMs上进行实验，证明了该方法能够发现功能上内聚且能反映潜在专业化的分组。在特定主题的引导下，发现的协同团队在下游基准测试中的表现优于随机基线，并且其准确性可与基于已知模型专业化手动创建的团队相媲美。

Conclusion: 本研究为自动化设计协同的多智能体LLM团队奠定了新基础。该方法通过分析模型间交互来识别协同效应，克服了传统方法对模型内部信息依赖的限制。未来研究可进一步探索更复杂的交互模式和团队结构，以及在更广泛任务场景下的应用。

Abstract: While a multi-agent approach based on large language models (LLMs) represents
a promising strategy to surpass the capabilities of single models, its success
is critically dependent on synergistic team composition. However, forming
optimal teams is a significant challenge, as the inherent opacity of most
models obscures the internal characteristics necessary for effective
collaboration. In this paper, we propose an interaction-centric framework for
automatic team composition that does not require any prior knowledge including
their internal architectures, training data, or task performances. Our method
constructs a "language model graph" that maps relationships between models from
the semantic coherence of pairwise conversations, and then applies community
detection to identify synergistic model clusters. Our experiments with diverse
LLMs demonstrate that the proposed method discovers functionally coherent
groups that reflect their latent specializations. Priming conversations with
specific topics identified synergistic teams which outperform random baselines
on downstream benchmarks and achieve comparable accuracy to that of
manually-curated teams based on known model specializations. Our findings
provide a new basis for the automated design of collaborative multi-agent LLM
teams.

</details>


### [6] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

TL;DR: 本文提出了一种名为LISTEN的框架，利用大型语言模型（LLM）作为零样本偏好预言机，通过自然语言指导进行多目标决策，解决了人类专家在面对复杂隐式偏好时选择困难的问题。LISTEN包含两种迭代算法：LISTEN-U，通过LLM优化参数化效用函数；LISTEN-T，采用非参数化方法进行小批量解决方案的淘汰赛式选择。实验表明，LISTEN-U在偏好参数化一致性高时表现优异，而LISTEN-T则提供更稳健的性能。


<details>
  <summary>Details</summary>
Motivation: 人类专家在面对具有多个相互冲突的目标的大量选项时，在选择最佳方案时常常面临困难。这种困难源于对复杂、隐式偏好的形式化表达的挑战。现有的方法在捕捉和利用这些偏好方面存在局限性，导致决策效率低下和认知负担加重。因此，需要一种新的方法来简化和自动化这一过程，特别是利用先进的AI技术来理解和响应人类的偏好。

Method: 本文提出LISTEN框架，旨在利用大型语言模型（LLM）作为零样本偏好预言机，仅凭专家用自然语言给出的大致优先级进行指导。为了克服LLM的上下文窗口和推理成本限制，研究者设计了两种迭代算法：
1. LISTEN-U：一种参数化方法，利用LLM迭代地优化一个参数化的效用函数，使其能够更好地反映专家的偏好。
2. LISTEN-T：一种非参数化方法，通过小批量解决方案的淘汰赛式选择来指导决策过程，避免了显式地构建全局效用函数。
评估在多个任务上进行，包括机票预订、购物和考试日程安排，以验证该框架的有效性。

Result: 在机票预订、购物和考试日程安排等多样化任务上的评估结果表明：
- LISTEN-U在偏好与参数化模型高度一致的情况下表现出色，尤其是在我们提出的新型一致性度量标准下。
- LISTEN-T在各种情况下提供了更稳健的性能，即使在偏好不完全符合参数化模型的情况下也能有效工作。
- 与传统偏好获取方法相比，LISTEN框架显著降低了认知负担，并实现了更有效的多目标决策。

Conclusion: LISTEN框架为利用大型语言模型直接通过自然语言指导复杂的多目标决策提供了一个有前景的方向。通过LISTEN-U和LISTEN-T两种算法，该框架能够有效处理由复杂隐式偏好引起的问题，并减少了传统偏好获取方法的认知负担。虽然LISTEN-U在特定条件下表现优异，但LISTEN-T显示出更广泛的适用性和稳健性。未来的工作可以进一步探索LLM在更广泛的决策场景中的应用，并研究如何进一步提高LISTEN框架的效率和可扩展性。

Abstract: Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [7] [Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data](https://arxiv.org/abs/2510.25804)
*Haoran Deng,Yingyu Lin,Zhenghao Lin,Xiao Liu,Yizhou Sun,Yi-An Ma,Yeyun Gong*

Main category: cs.CL

TL;DR: 长上下文模型在处理长文本时，面临训练数据效率低下和长距离依赖性不足的问题。本文提出了LongFilter框架，通过对比长短上下文模型预测的差异来筛选包含真正长距离依赖的数据，从而提升长上下文预训练的效率和效果。实验表明，使用LongFilter筛选的数据训练的LLaMA-3-8B模型在HELMET、LongBench和RULER等基准测试中表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型虽然在处理长文本方面展现出巨大潜力，但现有的大量长文本数据中，大部分并不包含有意义的长距离依赖关系，即局部上下文已足以预测。直接使用这些数据进行训练效率低下，因此，对训练数据进行精细化筛选至关重要，以确保模型能够有效学习和利用长距离依赖。

Method: 本文提出了LongFilter框架，用于筛选适合长上下文预训练的训练数据。该框架通过衡量扩展上下文提供的信息增益来评估数据的质量。具体做法是对比模型在长上下文和短上下文设置下的预测差异，从而识别出那些真正需要长距离依赖才能进行准确预测的样本。实验中，将LLaMA-3-8B模型的上下文长度从8K扩展到64K，并使用LongFilter进行数据筛选。

Result: 实验结果显示，LongFilter能够高效地筛选出高质量的训练数据。使用LongFilter筛选的数据训练的LLaMA-3-8B模型，在上下文长度扩展到64K后，于HELMET、LongBench和RULER等多个基准测试中取得了显著的性能提升，证明了该数据筛选框架的有效性。

Conclusion: LongFilter框架为长上下文预训练提供了一种有效的数据筛选方法，解决了现有长文本数据效率低下的问题。通过精确识别和利用长距离依赖，显著提升了模型的性能。该方法在实践中证明了其价值，为未来长上下文模型的研究和应用提供了重要的支持。

Abstract: Long-context language models unlock advanced capabilities in reasoning, code
generation, and document summarization by leveraging dependencies across
extended spans of text. However, a significant portion of readily available
long-text data lacks meaningful long-distance dependencies; most spans can be
predicted using only local context. Training on such data is inefficient,
making careful data selection crucial. Therefore, we introduce LongFilter, a
framework for curating training data tailored to long-context pretraining.
LongFilter measures the information gain provided by extended context by
contrasting model predictions under long-context versus short-context settings,
thereby identifying samples where long-range dependencies are essential.
Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show
that LongFilter efficiently selects high-quality data and yields substantial
improvements on benchmarks such as HELMET, LongBench, and RULER.

</details>


### [8] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

TL;DR: 研究发现，在内容审核系统中，大型语言模型（LLMs）采用不同意识形态的“人设”会引入微妙的意识形态偏见，影响其对有害内容的判断一致性和公平性。虽然总体准确率未受显著影响，但特定人设模型在判断有害内容时表现出不同的倾向，并且更容易与相同政治意识形态的人设模型保持一致，加剧了意识形态群体间的判断分歧。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在内容审核中的广泛应用，确保其公平性和中立性至关重要。本研究旨在探讨“人设”（persona adoption）这一因素如何影响LLMs在跨不同模型架构、模型大小和内容模态（语言与视觉）下进行有害内容分类时的一致性和公平性，以揭示潜在的偏见问题。

Method: 本研究首先在多种LLM架构、模型大小和内容模态（语言、视觉）上，分析了不同意识形态“人设”对有害内容分类的整体准确率影响。随后，通过更深入的行为分析和一致性分析，探究了人设对模型判断倾向和群体间一致性的具体影响。最后，设计了一个政治导向的任务进行了补充实验，以更直接地验证人设偏见效应。

Result: 总体准确率指标显示人设对分类准确率影响不大，但深入分析发现：1. 具有不同意识形态倾向的人设，在标记有害内容时表现出不同的倾向性。2. 模型（尤其是大型模型）更倾向于与相同政治意识形态的人设保持一致，增强了意识形态内部的一致性，但扩大了意识形态群体间的判断分歧。3. 补充实验证实，人设不仅在自身意识形态内表现更一致，还可能倾向于维护自身观点并淡化对立观点的有害性。

Conclusion: 本研究揭示了“人设”在LLMs内容审核中引入微妙意识形态偏见的风险，这可能导致AI系统在看似中立的伪装下，强化特定政治观点。研究结果强调了在部署LLMs进行内容审核时，审慎评估和缓解人设偏见的重要性，并为未来研究如何设计更公平、中立的AI审核系统提供了方向。

Abstract: Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [9] [Beyond Long Context: When Semantics Matter More than Tokens](https://arxiv.org/abs/2510.25816)
*Tarun Kumar Chawdhury,Jon D. Duke*

Main category: cs.CL

TL;DR: EHR中的临床文档以Base64编码附件的形式存储在FHIR DocumentReference资源中，导致语义问答困难。Lopez等人于2025年提出的临床实体增强检索（CLEAR）方法，通过实体感知检索，在F1分数上以0.90优于基于嵌入的检索（0.86），同时减少了70%以上的标记。研究开发了一个临床笔记QA评估平台，以验证CLEAR相对于零样本大上下文推理和传统基于块的检索增强生成的性能。在包含10,000至65,000个标记的12个临床笔记上测试，CLEAR的获胜率为58.3%，平均语义相似度为0.878，标记使用量比宽上下文处理少78%。在长笔记（超过65,000个标记）上，性能提升尤为显著，获胜率为75%。这些发现证实了实体感知检索能提高临床自然语言处理的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）中存储的临床文档通常以Base64编码附件的形式嵌入FHIR DocumentReference资源，这种格式阻碍了对文档内容的语义理解和问答。现有的基于向量数据库或嵌入的方法在处理细微的临床关系时存在不足，无法有效利用这些编码的文档信息。因此，需要一种能够克服这些挑战，提高临床问答系统效率和准确性的方法。

Method: 研究引入了临床实体增强检索（CLEAR）方法，该方法采用实体感知检索策略来处理EHR中的临床文档。为了验证CLEAR的有效性，研究人员开发了一个专门的临床笔记QA评估平台。该平台使用12份包含10,000到65,000个标记的临床笔记作为测试数据，并将其与零样本大上下文推理和传统的基于块的检索增强生成方法进行了比较。评估指标包括获胜率、平均语义相似度和标记使用量。

Result: 在与传统方法（如嵌入式检索和基于块的检索增强生成）的比较中，CLEAR方法在F1分数上达到了0.90，优于嵌入式检索的0.86。在临床笔记QA评估平台上，CLEAR取得了58.3%的获胜率，平均语义相似度为0.878，并且在标记使用量方面比宽上下文处理技术减少了78%。特别是在处理长笔记（超过65,000个标记）时，CLEAR的获胜率高达75%，显示出在处理大规模临床数据方面的显著优势。

Conclusion: 实体感知检索（如CLEAR方法）能够显著提升临床自然语言处理任务的效率和准确性，尤其是在处理EHR中长且复杂的临床文档时。所开发的临床笔记QA评估平台为未来临床问答系统的评估提供了一个可重用且透明的基准，有助于在语义精确性和计算效率至关重要的场景下进行系统评估。

Abstract: Electronic Health Records (EHR) store clinical documentation as base64
encoded attachments in FHIR DocumentReference resources, which makes semantic
question answering difficult. Traditional vector database methods often miss
nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)
method, introduced by Lopez et al. 2025, uses entity aware retrieval and
achieved improved performance with an F1 score of 0.90 versus 0.86 for
embedding based retrieval, while using over 70 percent fewer tokens. We
developed a Clinical Notes QA Evaluation Platform to validate CLEAR against
zero shot large context inference and traditional chunk based retrieval
augmented generation. The platform was tested on 12 clinical notes ranging from
10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a
58.3 percent win rate, an average semantic similarity of 0.878, and used 78
percent fewer tokens than wide context processing. The largest performance
gains occurred on long notes, with a 75 percent win rate for documents
exceeding 65,000 tokens. These findings confirm that entity aware retrieval
improves both efficiency and accuracy in clinical natural language processing.
The evaluation framework provides a reusable and transparent benchmark for
assessing clinical question answering systems where semantic precision and
computational efficiency are critical.

</details>


### [10] [A Survey on Efficient Large Language Model Training: From Data-centric Perspectives](https://arxiv.org/abs/2510.25817)
*Junyu Luo,Bohan Wu,Xiao Luo,Zhiping Xiao,Yiqiao Jin,Rong-Cheng Tu,Nan Yin,Yifan Wang,Jingyang Yuan,Wei Ju,Ming Zhang*

Main category: cs.CL

TL;DR: 本文首次系统性地从数据中心视角审视了数据高效的大语言模型（LLM）训练后技术。通过提出数据选择、数据质量增强、合成数据生成、数据蒸馏与压缩以及自演化数据生态系统等方法分类，总结了代表性方法，并指出了未来的研究方向，旨在解决LLM训练后面临的数据成本高和边际效益递减等挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）的训练后技术面临数据标注成本高、数据规模边际效益递减等严峻挑战，使得实现数据高效的训练后技术成为关键的研究问题。

Method: 本文从数据中心视角出发，首次系统性地 survey 了数据高效的LLM训练后技术。研究者们提出了一种数据高效LLM训练后技术的方法分类，涵盖了数据选择、数据质量增强、合成数据生成、数据蒸馏与压缩以及自演化数据生态系统等几个方面。

Result: 本文总结了数据高效LLM训练后技术在各个分类下的代表性方法，并分析了这些方法在解决数据挑战方面的有效性。

Conclusion: 本文系统性地 survey 了数据高效的LLM训练后技术，并提出了一个全面的分类框架。通过总结现有方法和指出未来研究方向，旨在推动LLM训练中数据利用潜力的最大化，以应对当前面临的数据挑战。

Abstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their
task generalization potential and domain-specific capabilities. However, the
current LLM post-training paradigm faces significant data challenges, including
the high costs of manual annotation and diminishing marginal returns on data
scales. Therefore, achieving data-efficient post-training has become a key
research question. In this paper, we present the first systematic survey of
data-efficient LLM post-training from a data-centric perspective. We propose a
taxonomy of data-efficient LLM post-training methods, covering data selection,
data quality enhancement, synthetic data generation, data distillation and
compression, and self-evolving data ecosystems. We summarize representative
approaches in each category and outline future research directions. By
examining the challenges in data-efficient LLM post-training, we highlight open
problems and propose potential research avenues. We hope our work inspires
further exploration into maximizing the potential of data utilization in
large-scale model training. Paper List:
https://github.com/luo-junyu/Awesome-Data-Efficient-LLM

</details>


### [11] [Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation](https://arxiv.org/abs/2510.25904)
*Frederico Belcavello,Ely Matos,Arthur Lorenzi,Lisandra Bonoto,Lívia Ruiz,Luiz Fernando Pereira,Victor Herbst,Yulla Navarro,Helen de Andrade Abreu,Lívia Dutra,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在语言资源和数据集创建中的应用是一个现实，但对其性能和影响的全面评估，尤其是在NLP的视角化方法下，仍然缺失。本研究通过对LLM在类FrameNet语义标注的（半）自动化方面进行广泛评估，填补了这一空白。研究比较了手动、自动和半自动三种模式下的标注时间、覆盖率和多样性。结果表明，与纯人工标注相比，混合半自动标注模式可提高框架多样性并保持相似的标注覆盖率，而纯自动模式在除标注时间外的所有指标上表现均远不如其他模式。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在加速或替代人力进行语言资源和数据集创建方面已成为现实，但目前仍缺乏对其性能和对带注释数据集创建影响的全面评估，尤其是在NLP的视角化方法下。本研究旨在通过对LLM在类FrameNet语义标注中的（半）自动化应用进行广泛评估，来弥合这一研究差距。

Method: 本研究采用了三种实验设置来比较标注时间、覆盖率和多样性：（1）手动标注：完全由人工完成标注。（2）自动标注：完全由LLM自动完成标注。（3）半自动标注：结合人工和LLM的标注方法。研究重点评估了LLM在类FrameNet语义标注方面的（半）自动化效果。

Result: 研究结果显示，在与纯人工标注模式相比时，混合半自动标注模式能够带来更高的框架多样性，并维持相似的标注覆盖率。而纯自动标注模式在除标注时间外，在所有其他指标上表现均显著较差。

Conclusion: 本研究的评估表明，在类FrameNet语义标注任务中，半自动化的方法是提高标注效率和质量的有效途径。LLM在半自动模式下能够显著提升框架多样性，同时保持与纯人工标注相当的覆盖率。纯自动模式虽然能节省时间，但在标注质量方面存在明显不足。未来的研究可以进一步探索优化半自动标注流程，以及LLM在其他NLP任务中的应用潜力。

Abstract: The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.

</details>


### [12] [Revisiting Multilingual Data Mixtures in Language Model Pretraining](https://arxiv.org/abs/2510.25947)
*Negar Foroutan,Paul Teiletche,Ayush Kumar Tarun,Antoine Bosselut*

Main category: cs.CL

TL;DR: 该研究通过在不同规模（1.1B和3B参数）的大语言模型上进行实验，探索了多语言数据混合对模型性能的影响。研究发现，只要语料库中包含足够的语料，混合英语和多语言数据并不会损害模型在单一语言上的表现。此外，使用英语作为“枢纽语言”（pivot language）能促进跨语言的泛化能力，而枢纽语言不一定需要来自特定的语系。研究还表明，在模型规模达到一定程度时，“多语诅咒”（curse of multilinguality）现象并不显著。总体而言，该研究挑战了关于多语言训练的普遍担忧，并强调了平衡多语言数据的重要性，即使在低资源语言场景下也能提升模型能力。


<details>
  <summary>Details</summary>
Motivation: 关于在大语言模型（LLMs）预训练中混合不同多语言数据的策略，长期存在一个争论，即担心语言覆盖范围和模型性能之间会产生权衡（即“多语诅咒”）。本研究旨在深入探究这些假设，挑战关于多语言训练的普遍认知。

Method: 本研究训练了1.1B和3B参数的大语言模型，使用了包含25至400种不同语言的多样化多语言语料库。通过调整训练数据中包含的语言数量，来系统地研究多语言数据混合对模型性能的影响。

Result: 研究得出以下主要发现：1. 混合英语和多语言数据并不会必然降低任一组语言的性能，前提是这些语言在预训练语料库中的词元数量充足。2. 使用英语作为枢纽语言（pivot language）能够带来跨语言系的收益，并且与预期相反，选择特定语系内的枢纽语言并不总能提升该语系内语言的表现。3. 随着训练语言数量的增加，在当前模型规模下并未观察到显著的“多语诅咒”现象。

Conclusion: 该研究结果表明，只要多语言数据得到适当的平衡，就可以在不损害模型性能的情况下提升大语言模型的能力，即使在低资源语言场景下也是如此。这挑战了普遍存在的关于多语言训练的担忧，并为优化多语言大语言模型的设计提供了新的见解。未来的工作可以进一步探索不同语言比例、语料库质量以及模型规模对多语性能的影响。

Abstract: The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings

</details>


### [13] [Semantic Label Drift in Cross-Cultural Translation](https://arxiv.org/abs/2510.25967)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 机器翻译（MT）在低资源语言中通过高资源语言生成合成数据以解决资源稀缺问题。本文研究了文化对齐在MT中的作用，发现文化差异会导致语义标签漂移，尤其是在文化敏感领域。大型语言模型（LLMs）在这一过程中会放大漂移，而文化相似性是标签保留的关键。忽视文化因素会损害标签保真度，并可能导致误解和文化冲突。


<details>
  <summary>Details</summary>
Motivation: 尽管机器翻译（MT）广泛用于低资源语言，但以往对情感保留的研究忽略了一个关键因素：源语言和目标语言之间的文化对齐。本文旨在探讨文化差异如何影响MT中的语义标签，以及这种影响的程度。

Method: 通过在一系列跨文化敏感和中性领域进行实验，研究了MT系统（包括现代大型语言模型LLMs）如何诱导标签漂移。比较了不同MT工具（包括早期统计MT和LLMs）在处理文化敏感信息时的表现，并分析了文化相似性对标签保留的影响。

Result: 研究发现，MT系统，特别是LLMs，在翻译过程中，尤其是在文化敏感领域，会引起标签漂移。与早期的统计MT工具不同，LLMs能够编码文化知识，并且利用这种知识会加剧标签漂移。此外，源语言和目标语言之间的文化相似性或差异性是决定标签保留的关键因素。

Conclusion: 本文的研究结果强调，在MT中忽视文化因素不仅会损害标签的保真度，还可能在下游应用中导致误解和文化冲突。因此，在开发和应用MT系统时，必须考虑文化对齐问题。

Abstract: Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.

</details>


### [14] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在处理复杂数学推理时存在困难，其基于文本的生成方式容易导致解决方案不准确且算术错误。为解决此问题，本文提出了SymCode，一个将数学问题解决重构为可验证代码生成任务的神经符号框架，利用SymPy库进行处理。SymCode在MATH-500和OlympiadBench等基准测试中表现出色，准确率相较于基线模型提升高达13.6个百分点。该框架不仅在令牌效率上更优，还能将模型故障从不透明的逻辑谬误转变为透明的程序性错误，是迈向更准确、可信赖的AI的关键一步。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLMs）在处理需要复杂数学推理的任务时表现不佳，其基于自然语言的生成方式往往导致结果无法验证且存在算术错误。现有的提示策略（如思维链）虽然有所帮助，但仍依赖于不可靠的文本生成媒介，缺乏确定性验证机制。这种不确定性限制了LLMs在需要高精度和可靠性的数学等形式化领域的应用。因此，有必要开发一种能够克服这些限制，提高数学推理准确性和可信度的新方法。

Method: 本文提出了一种名为SymCode的神经符号框架，该框架将数学问题解决视为一个代码生成任务。具体而言，它利用Python的SymPy库将数学问题转化为可执行和可验证的符号代码。这种方法将LLM的推理过程与确定性的符号计算引擎相结合，使得解决方案能够被精确验证。该框架在MATH-500和OlympiadBench等具有挑战性的数学推理基准上进行了评估。

Result: 在MATH-500和OlympiadBench等基准测试中，SymCode取得了显著的性能提升。与现有基线模型相比，SymCode的准确率最高提升了13.6个百分点。此外，SymCode在令牌使用效率方面也表现更优。关键的是，SymCode能够将模型在解决数学问题时出现的错误，从难以追踪的逻辑谬误转变为易于识别和调试的程序性错误。

Conclusion: SymCode通过将LLM的数学推理与SymPy库的确定性符号计算相结合，成功解决了当前LLMs在复杂数学推理中准确性和可信度不足的问题。该框架不仅提高了准确率和效率，还将模型错误从不透明的逻辑谬误转变为透明的程序性错误。SymCode代表了在形式化领域实现更准确、更可信赖的人工智能的重要一步，为未来在科学、工程等需要精确计算的领域应用LLMs奠定了基础。

Abstract: Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [15] [NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium](https://arxiv.org/abs/2510.25977)
*Dinghong Song,Jierui Xu,Weichu Yang,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文针对AWS Trainium AI加速器设计了一种高性能矩阵乘法（matmul）核，用于LLM推理。通过内核融合和新的缓存策略，该方法减少了数据移动，最大化了SRAM带宽，并避免了昂贵的矩阵转置，相较于AWS现有的matmul核，在matmul核层面平均实现了1.35倍的加速（最高2.22倍），在端到端LLM推理层面平均实现了1.66倍的加速（最高2.49倍）。


<details>
  <summary>Details</summary>
Motivation: AI加速器在训练和推理中提供了高性价比和高性能的解决方案。AWS的Trainium加速器因其异构架构成为LLM训练和推理的有吸引力的选择。然而，由于其矩阵乘法（matmul）核心的特殊要求，如脉动阵列架构和数据布局，要充分发挥Trainium的性能具有挑战性。

Method: 本文设计了一个高性能的矩阵乘法（matmul）内核，专门用于在AWS Trainium加速器上进行LLM推理。该设计采用了一系列针对Trainium架构定制的技术，包括内核融合（kernel fusion）和新颖的缓存策略（novel caching strategies）。这些技术旨在减少软件管理内存层次结构中的数据移动，最大化片上SRAM（Static Random-Access Memory）带宽，并避免执行代价高昂的矩阵转置操作。

Result: 在9个数据集和4个最新的LLM上进行评估，结果表明本文设计的系统在matmul内核层面平均实现了1.35倍的加速（最高可达2.22倍），显著优于AWS在Trainium上实现的现有matmul内核。这种性能提升进一步转化为端到端LLM推理性能的平均1.66倍加速（最高可达2.49倍）。

Conclusion: 本文成功设计并实现了一个高性能的matmul内核，显著提升了AWS Trainium加速器在LLM推理任务上的表现。通过优化数据布局和内存访问模式，研究解决了Trainium架构带来的挑战，为在特定硬件上高效运行大型模型提供了有力支持。未来的工作可以进一步探索对更广泛模型和硬件特性的优化。

Abstract: AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.

</details>


### [16] [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)
*Dinghong Song,Yuan Feng,Yiwei Wang,Shangye Chen,Cyril Guyot,Filip Blagojevic,Hyeran Jeon,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: LLM在预填充（prefill）阶段的自注意力计算是性能瓶颈，尤其是在处理长序列时。本文提出AttnCache框架，通过缓存和重用相似的注意力图来加速预填充阶段的LLM推理。实验证明，AttnCache在CPU和GPU上均实现了显著的加速效果，同时准确性损失很小。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在分类、问答、推荐和文本嵌入等应用中，推理主要依赖于预填充阶段，而该阶段的自注意力计算复杂度与序列长度呈二次方关系，成为性能瓶颈。此前的研究较少关注如何优化此特定阶段的推理效率。

Method: AttnCache框架通过构建注意力图数据库，利用高效的缓存和相似性搜索技术，在LLM推理过程中识别并重用已缓存的注意力图。当遇到与数据库中已有记录相似的输入时，AttnCache可以避免重新计算自注意力，从而减少计算开销。实验在CPU和GPU上进行，并评估了其对模型准确性的影响。

Result: AttnCache在CPU上实现了平均1.2倍的端到端加速和2倍的注意力计算加速；在GPU上实现了平均1.6倍的端到端加速和3倍的注意力计算加速。这些加速是在可忽略的准确性下降前提下实现的。

Conclusion: AttnCache成功地解决了LLM预填充阶段的性能瓶颈问题，通过重用注意力图显著提升了推理速度，且对模型准确性影响甚微。未来可进一步探索更大规模模型和更多样化任务上的应用，以及优化缓存策略以适应更广泛的输入分布。

Abstract: Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.

</details>


### [17] [Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning](https://arxiv.org/abs/2510.25992)
*Yihe Deng,I-Hung Hsu,Jun Yan,Zifeng Wang,Rujun Han,Gufeng Zhang,Yanfei Chen,Wei Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: 现有的LLM在多步推理方面存在不足，小规模开源模型在RLVR和SFT方法上面临采样困难和过度拟合问题。SRL框架通过生成内部推理独白和分步奖励来解决这些问题，使得小模型也能学习复杂的推理任务。SRL结合RLVR进行训练效果更佳，并能泛化到软件工程任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理需要多步推理的问题时表现不佳。对于小规模开源模型，现有的RLVR方法在正确解决方案采样率低时会失效，而SFT方法容易过度拟合长演示。这导致在这些方法上训练的模型无法解决复杂推理任务，存在巨大的研究空白和应用局限性。

Method: 提出SRL框架，将问题解决视为生成一系列逻辑“动作”。SRL模型在执行每个动作前会生成内部推理独白。该方法从SFT数据集中提取专家动作，并基于模型动作与专家动作的相似度提供分步的、平滑的奖励信号。这种监督机制在所有采样轨迹都不正确的情况下也能提供丰富的学习信号，并鼓励模型进行受专家演示引导的灵活推理。此外，SRL可以先于RLVR进行训练，然后再用RLVR进行微调，以获得最佳性能。

Result: SRL框架使小规模模型能够学习以前无法通过SFT或RLVR解决的复杂推理问题。在推理基准测试中，SRL表现出显著的改进。将SRL与RLVR结合训练的模型取得了最强的整体性能。SRL还在智能软件工程任务中展现了良好的泛化能力，证明了其作为面向推理的LLM的强大且通用的训练框架的有效性。

Conclusion: SRL框架成功弥补了现有LLM在多步推理能力上的不足，特别是对于小规模开源模型。通过引入内部推理独白和分步奖励，SRL提供了比SFT和RLVR更优越的学习信号，使得模型能够学习以前难以解决的复杂推理任务。SRL与RLVR结合的训练策略能达到最佳性能，并且该框架在软件工程等实际应用中也表现出良好的泛化能力。SRL为训练具有更强推理能力的LLM提供了一个有前途的方向。

Abstract: Large Language Models (LLMs) often struggle with problems that require
multi-step reasoning. For small-scale open-source models, Reinforcement
Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely
sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to
overfit long demonstrations through rigid token-by-token imitation. To address
this gap, we propose Supervised Reinforcement Learning (SRL), a framework that
reformulates problem solving as generating a sequence of logical "actions". SRL
trains the model to generate an internal reasoning monologue before committing
to each action. It provides smoother rewards based on the similarity between
the model's actions and expert actions extracted from the SFT dataset in a
step-wise manner. This supervision offers richer learning signals even when all
rollouts are incorrect, while encouraging flexible reasoning guided by expert
demonstrations. As a result, SRL enables small models to learn challenging
problems previously unlearnable by SFT or RLVR. Moreover, initializing training
with SRL before refining with RLVR yields the strongest overall performance.
Beyond reasoning benchmarks, SRL generalizes effectively to agentic software
engineering tasks, establishing it as a robust and versatile training framework
for reasoning-oriented LLMs.

</details>


### [18] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

TL;DR: 现有工具使用大语言模型（LLMs）在静态数据集上训练，只能模仿固定的工具调用流程，在动态环境中表现受限。本文提出PORTool，一种基于强化学习（RL）的方法，通过鼓励模型探索多种工具调用轨迹来找到正确答案。PORTool生成树状结构的多个调用轨迹，并根据每一步生成正确答案和成功调用工具的能力分配奖励。实验表明，PORTool在最终准确率和工具调用步数方面显著优于其他训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有工具使用LLMs在静态数据集上训练，模仿通用工具调用流程，无法应对动态多变的工具调用环境，导致性能有限。PORTool旨在解决这一问题，提高模型在动态环境中的适应性和解决问题的能力。

Method: PORTool是一种强化学习方法，首先为给定查询生成多个共享部分步骤的工具调用轨迹，形成树状结构。然后，根据生成正确答案和成功调用工具的能力为每一步分配奖励，共享步骤奖励相同，分叉步骤奖励不同。最后，结合分叉相对优势和轨迹相对优势来训练LLM。实验使用了17个工具，涵盖时间敏感和不敏感查询，并进行了消融研究和与其他训练方法的对比。

Result: PORTool在17个工具的实验中，相比其他训练方法，在最终准确率和工具调用步数方面均有显著提升。消融研究证明了分步奖励的必要性和设计的鲁棒性。

Conclusion: PORTool通过引入强化学习和鼓励探索多种工具调用轨迹，显著提高了LLM在动态工具使用环境中的性能。该方法在准确率和效率上均有优势，为未来在更复杂和动态场景下开发LLM工具使用能力提供了新的方向。

Abstract: Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [19] [Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs](https://arxiv.org/abs/2510.26024)
*HyoJung Han,Sweta Agrawal,Eleftheria Briakou*

Main category: cs.CL

TL;DR: 跨语言对齐（CLA）旨在对齐多语言表征，但可能导致“文化抹除”，即丧失提供符合当地文化语境的响应能力。本文提出了一个评估框架“迁移-本地化平面”，量化知识迁移和文化抹除。研究发现现有CLA方法以牺牲文化本地化为代价来提升事实迁移。基于“事实迁移和文化知识在不同模型层级最优可控”的洞察，提出了“外科手术式引导”方法，通过在不同层级进行激活引导，平衡迁移与本地化，克服了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 跨语言对齐（CLA）旨在统一多语言表征，促进知识在语言间的迁移。然而，过度追求表征的趋同可能导致“文化抹除”现象，即模型丧失根据查询语言提供符合当地文化语境的响应能力。这种现象阻碍了模型在实际多语言场景中的应用，因此理解和解决这一权衡至关重要。

Method: 本文首先引入了一个名为“迁移-本地化平面”的评估框架，用于量化跨语言知识迁移和文化本地化之间的权衡。然后，利用该框架重新评估了现有的CLA方法，并分析了模型内部表征。基于“事实迁移和文化知识在模型不同层级最优可控”的发现，提出了一种名为“外科手术式引导”的新型推理时方法。该方法通过在模型的不同层级进行有针对性的激活引导，以解耦知识迁移和文化本地化这两个目标。

Result: 研究结果表明，现有的CLA方法在提高事实知识迁移能力的同时，普遍牺牲了跨6种语言的文化本地化能力。通过“外科手术式引导”方法，该研究在迁移和本地化两个维度上取得了更好的平衡，有效克服了现有对齐技术的局限性。

Conclusion: 本文通过“迁移-本地化平面”框架揭示了CLA方法在知识迁移和文化本地化之间的固有权衡，并指出了“文化抹除”的问题。提出的“外科手术式引导”方法利用了模型层级对不同类型知识的表征能力差异，实现了更优的迁移-本地化平衡，为开发更具文化适应性的多语言大模型提供了新的途径。未来的工作可以探索更精细化的引导策略和更广泛的语言及文化场景。

Abstract: Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.

</details>


### [20] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: 该研究提出了QCoder Benchmark，一个用于评估大语言模型（LLMs）在量子编程领域生成代码能力的框架。该框架利用量子模拟器提供领域特定反馈，并包含真实编程竞赛数据以进行量化和质性分析。实验表明，尽管GPT-4o准确率仅为18.97%，但基于推理的模型（如o3）准确率高达78%，优于人类代码的平均成功率（39.98%）。该研究发布了数据集和评估API以促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在通用代码生成方面取得了进展，但在需要与硬件设备（如量子计算机）交互的领域（如量子编程）仍未得到充分探索。该研究旨在解决这一空白，填补LLMs在量子编程代码生成能力评估上的不足。

Method: 该研究引入了QCoder Benchmark，一个包含两个关键特性的评估框架：1. 使用量子模拟器环境进行评估，提供电路深度、执行时间、错误分类等领域特定指标反馈，以指导模型生成。2. 包含从真实编程竞赛收集的人类编写的代码提交，用于对LLM输出进行量化和质性分析。

Result: 实验结果显示，即使是先进模型如GPT-4o在QCoder Benchmark上的准确率也仅达到18.97%，凸显了该基准的挑战性。相比之下，基于推理的模型（如o3）准确率高达78%，显著优于人类编写代码的平均成功率（39.98%）。

Conclusion: QCoder Benchmark的提出为评估LLMs在量子编程领域的代码生成能力提供了一个创新的框架。实验结果揭示了当前LLMs在这一复杂领域的局限性，同时也展示了基于推理的模型展现出的巨大潜力。研究发布的数据集和API将有助于推动该领域的研究和发展。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [21] [Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking](https://arxiv.org/abs/2510.26122)
*Feng Ju,Zeyu Qin,Rui Min,Zhitao He,Lingpeng Kong,Yi R. Fung*

Main category: cs.CL

TL;DR: 为解决大型语言模型（LLMs）在测试时缩放（TTS）过程中因“一题一解”（1P1S）训练实践导致的推理路径单一化问题，本文提出了“一题多解”（1PNS）训练范式，并通过引入“推理路径发散度”（RPD）度量来衡量多步推理链的语义差异，以生成多样化的解决方案集。实验结果表明，RPD选择的训练策略能显著提高模型输出的多样性和任务通过率（pass@k），尤其在AIME24数据集上表现出+4.99%的pass@16提升，证明1PNS范式能有效增强TTS的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放（TTS）技术在提升大型语言模型（LLMs）的推理能力方面效果显著，但模型输出的多样性不足已成为一个瓶颈。这主要是由于“一题一解”（1P1S）的训练方式，即模型仅接触一个标准答案，容易导致其推理路径趋于狭窄。为应对此挑战，研究者提出“一题多解”（1PNS）训练范式，旨在暴露模型于多样化的有效推理路径，从而增强推理的多样性。

Method: 为了实现“一题多解”（1PNS）训练范式，核心挑战在于如何准确评估多步推理链之间的语义差异。为此，本文引入了“推理路径发散度”（RPD）这一度量标准。RPD在步骤层面进行评估，能够对长推理链的解决方案进行对齐和评分，从而捕捉中间推理过程的差异。研究者利用RPD来筛选出每个问题下语义上最为发散的解决方案集，并以此对Qwen3-4B-Base模型进行微调。

Result: 实验结果表明，通过RPD筛选出的训练数据进行微调的模型，其输出的多样性显著增加，并且任务通过率（pass@k）也有所提升。与强有力的“一题一解”（1P1S）基线相比，RPD选择的训练方法平均带来了+2.80%的pass@16增益，在AIME24数据集上的表现更为突出，取得了+4.99%的pass@16增益。这充分证明了1PNS范式能够进一步放大TTS的有效性。

Conclusion: 本文提出的“一题多解”（1PNS）训练范式，结合“推理路径发散度”（RPD）度量，有效解决了现有TTS技术中模型输出多样性不足的问题。通过暴露模型于更多样化的推理路径，1PNS不仅提升了模型的泛化能力，还在多项任务上取得了优于传统“一题一解”（1P1S）训练的性能。研究成果表明，增加训练数据的多样性是进一步提升LLMs推理能力的重要途径。研究代码已公开，为后续研究提供了便利。

Abstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .

</details>


### [22] [On the Influence of Discourse Relations in Persuasive Texts](https://arxiv.org/abs/2510.26124)
*Nawar Turk,Sevag Kaspar,Leila Kosseim*

Main category: cs.CL

TL;DR: 本研究利用大型语言模型（LLM）和提示工程，首次探索了说服技术（PTs）与话语关系（DRs）之间的联系，并构建了包含这两种标注的数据集。研究发现，‘原因’、‘目的’、‘对比’、‘原因+信念’、‘妥协’和‘条件’这六种话语关系在说服性文本中，尤其是在使用‘煽动性语言’、‘夸大/最小化’、‘重复’和‘制造疑点’时，起着关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏同时标注说服技术（PTs）和话语关系（DRs）的数据集，阻碍了对两者之间相互作用的深入理解。本研究旨在弥合这一差距，探索PTs和DRs的关联，以增强对在线宣传、虚假信息检测以及有效沟通机制的理解。

Method: 本研究以SemEval 2023 Task 3数据集（包含19种PTs）为基础，利用大型语言模型（LLM）和提示工程开发了22种DRs的分类器。共评估了4种LLM和10种不同提示，生成了40种DRs分类器。随后，采用不同的多数投票策略的集成模型创建了5个包含PTs和DRs标注的“银数据集”，数据量从204到1,281个实例不等。

Result: 通过对生成的“银数据集”进行统计分析，本研究发现六种话语关系——‘原因’、‘目的’、‘对比’、‘原因+信念’、‘妥协’和‘条件’——在说服性文本中扮演着重要角色。这些话语关系尤其与‘煽动性语言’、‘夸大/最小化’、‘重复’和‘制造疑点’等说服技术的运用密切相关。

Conclusion: 本研究成功构建了首个同时标注了说服技术和话语关系的数据集，并揭示了特定话语关系在说服性文本中的关键作用。这些发现为识别在线宣传和虚假信息提供了新的视角，并有助于更深入地理解有效沟通的原理。未来的研究可以进一步扩展数据集，并探索更复杂的话语关系模型。

Abstract: This paper investigates the relationship between Persuasion Techniques (PTs)
and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and
prompt engineering. Since no dataset annotated with both PTs and DRs exists, we
took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point
and developed LLM-based classifiers to label each instance of the dataset with
one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10
different prompts, resulting in 40 unique DR classifiers. Ensemble models using
different majority-pooling strategies were used to create 5 silver datasets of
instances labelled with both persuasion techniques and level-2 PDTB senses. The
silver dataset sizes vary from 1,281 instances to 204 instances, depending on
the majority pooling technique used. Statistical analysis of these silver
datasets shows that six discourse relations (namely Cause, Purpose, Contrast,
Cause+Belief, Concession, and Condition) play a crucial role in persuasive
texts, especially in the use of Loaded Language, Exaggeration/Minimisation,
Repetition and to cast Doubt. This insight can contribute to detecting online
propaganda and misinformation, as well as to our general understanding of
effective communication.

</details>


### [23] [Similarity-Distance-Magnitude Language Models](https://arxiv.org/abs/2510.26183)
*Allen Schmaltz*

Main category: cs.CL

TL;DR: 我们提出了Similarity-Distance-Magnitude (SDM)语言模型，通过在最终层添加SDM激活层进行二元分类，最大化生成在校准良好、高概率区域的比例。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型在遵循指令方面存在不足，尤其是在需要精确控制生成概率和区域划分时。本研究旨在解决这一问题，提高模型的统计效率，减少不必要的弃权。

Method: 将现有的预训练decoder-only Transformer语言模型通过监督微调转换为SDM语言模型。在训练过程中，利用最终层的SDM激活层来估计一个基数变化（change-of-base），用于对比输入编码方案上的监督下一个词损失，并在线生成额外的困难负例。

Result: SDM语言模型在统计效率上优于强大的监督基线，表现为更少的弃权。

Conclusion: SDM语言模型提供了一种有效的方法来改进指令遵循和提高语言模型的统计效率。未来的工作可以探索SDM在其他任务上的应用以及更优化的训练策略。

Abstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.

</details>


### [24] [Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation](https://arxiv.org/abs/2510.26200)
*Woojin Kim,Jaeyoung Do*

Main category: cs.CL

TL;DR: 本文提出了一种名为 Token Timestep Allocation (TTA) 的新方法，通过为每个 token 分配不同的时间步长来解决扩散语言模型（DLMs）在可控性方面存在“更新遗忘”的问题。TTA 通过优先冻结关键 token 并允许不确定 token 继续细化，实现了软性的、语义驱动的 token 排序，从而提高了生成文本的流畅性和连贯性。实验表明，TTA 在情感控制和去毒化任务上均取得了显著的性能提升，同时减少了所需的生成步数。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散语言模型（DLMs）在文本生成方面展现出强大的细粒度精炼能力，但其实际可控性仍然脆弱。一个核心的失败模式是“更新遗忘”，即模型在生成过程中，不均匀且不考虑上下文的更新会导致 token 级别在不同时间步长之间出现波动，从而覆盖早期的语义编辑，破坏累积精炼过程，最终降低生成文本的流畅性和连贯性。这种失败源于更新的均匀性和缺乏上下文感知，有效的控制需要明确的 token 排序。

Method: 本文提出 Token Timestep Allocation (TTA) 方法，通过为每个 token 分配不同的时间步长（timestep schedules）来实现软性的、语义驱动的 token 排序。具体而言，关键 token 会被提前冻结（即在较早的时间步长完成更新），而那些不确定或需要更多细化的 token 则会继续在后续的时间步长中进行精炼。这种基于时间步长排序的机制可以被实现为固定的策略（fixed policy）或由任务信号驱动的自适应策略（adaptive policy），从而支持广泛的精炼策略。由于 TTA 仅在推理时操作，因此可以统一应用于各种 DLMs，并自然扩展到不同的监督源。实验在情感控制和去毒化任务上进行了评估。

Result: TTA 方法在提高可控性和流畅性方面取得了显著效果。在情感控制任务中，TTA 相比基线模型，在准确率上提高了 20% 以上，困惑度（perplexity）降低了近一半，同时使用的生成步数减少了五分之一以上。在去毒化任务中，TTA 显著降低了最高毒性评分（从 14.5 降至 12.2）和困惑度（从 32.0 降至 26.0）。这些结果表明 TTA 有效地缓解了更新遗忘问题，实现了更稳定、可控的文本生成。

Conclusion: 本文的研究表明，通过时间步长分配实现的软性排序是解决扩散语言模型（DLMs）更新遗忘问题、实现稳定和可控文本生成的关键。TTA 方法通过明确控制 token 的更新顺序，有效提升了生成文本的质量和可控性。未来的工作可以探索更复杂的自适应策略，以及将 TTA 应用于更多样化的文本生成任务。

Abstract: While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.

</details>


### [25] [Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning](https://arxiv.org/abs/2510.26205)
*Qi Luo,Xiaonan Li,Tingshuo Fan,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 现有检索增强生成（RAG）评估基准主要关注局部RAG，即在小文档子集中检索相关文本块以回答局部理解问题。然而，许多实际应用需要跨整个文档集合聚合和分析信息以获得语料库级别见解的全局RAG能力。本文提出了GlobalQA，首个专门用于评估全局RAG能力的基准，涵盖计数、极值查询、排序和top-k提取四种核心任务类型。研究发现现有RAG方法在全局任务上表现不佳，最佳基线仅获得1.51 F1分数。为解决这些挑战，本文提出了GlobalRAG，一个多工具协作框架，通过块级检索保持结构一致性，利用LLM驱动的智能过滤器消除噪声文档，并集成聚合模块进行精确符号计算。在Qwen2.5-14B模型上，GlobalRAG取得了6.63 F1的分数，显著优于基线（1.51 F1），验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG评估基准主要关注局部RAG，即回答仅需局部理解的问题。然而，许多现实世界应用，如“找出2023年被引用次数最多的10篇论文？”，需要跨越整个文档集合聚合和分析信息以获得语料库级别见解的全局RAG能力。现有方法在处理这类需要全局理解的任务时存在不足，因此需要新的评估基准和方法来衡量和提升模型在全局RAG方面的性能。

Method: 本文首先提出了GlobalQA，一个专门为评估全局RAG能力设计的基准，包含计数、极值查询、排序和top-k提取四种任务类型。随后，针对现有RAG方法在全局任务上的不足，提出了一种名为GlobalRAG的多工具协作框架。该框架结合了块级检索以保持结构一致性、LLM驱动的智能过滤器以去除噪声文档，以及聚合模块以进行精确的符号计算。

Result: 在GlobalQA基准上的系统性评估显示，现有RAG方法在全局任务上的表现普遍较差，最强的基线模型仅达到1.51的F1分数。而提出的GlobalRAG框架在Qwen2.5-14B模型上实现了6.63的F1分数，相比之下，基线模型的F1分数仅为1.51，显著证明了GlobalRAG的有效性。

Conclusion: 本文通过引入GlobalQA基准和GlobalRAG框架，首次系统地评估和解决了全局RAG的挑战。研究表明，现有的RAG方法在处理需要跨文档聚合信息的全局任务时存在显著的性能差距。GlobalRAG框架通过结合结构感知检索、智能过滤和符号计算，有效提升了模型在全局RAG任务上的表现。未来的工作可以进一步探索更复杂的全局推理任务，并优化GlobalRAG框架的效率和可扩展性。

Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.

</details>


### [26] [Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs](https://arxiv.org/abs/2510.26253)
*Takuma Sato,Seiya Kawano,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本研究提出将语用学理论作为提示，通过引导语言模型进行逐步推理，来提升其理解隐含意义的能力，并在实验中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 人类交流和语言理解中，准确解读隐含意义至关重要。本研究旨在探索语言模型是否能具备此能力，以及如何有效提升其在这方面的表现。

Method: 本研究提出一种新颖的 in-context learning 方法，即在提示中提供语用学理论（如 Grice 语用学和关联理论）的概述，引导语言模型进行逐步推理以理解隐含意义。实验将此方法与仅提供中间推理提示的基线方法（0-shot Chain-of-Thought）进行比较。

Result: 实验结果表明，与基线方法相比，所提出的语用学理论提示方法在语用推理任务上的得分最高可提升 9.6%。此外，即使不详细解释理论，仅在提示中提及理论名称，也能在大型模型上带来约 1-3% 的性能提升。

Conclusion: 将语用学理论作为提示，能够有效提升语言模型理解隐含意义的能力。未来可进一步探索更精细的理论引导方式，并应用于更广泛的自然语言理解任务。

Abstract: The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.

</details>


### [27] [Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages](https://arxiv.org/abs/2510.26254)
*Mérilin Sousa Silva,Sina Ahmadi*

Main category: cs.CL

TL;DR: 本研究探究了预训练语言模型（包括大型语言模型）在识别外来词方面的能力。研究发现在模型被明确指示并提供上下文信息的情况下，它们在区分外来词和本土词方面表现不佳，这表明现代自然语言处理系统可能存在偏向外来词而非本土词的倾向。


<details>
  <summary>Details</summary>
Motivation: 语言在历史演变中不断发生词语的跨语言借用，并逐渐融入目标语言。尤其是在双语社区，强势语言会不断向少数语言施加词汇压力。本研究旨在探讨预训练语言模型（包括大型语言模型）是否具备区分这些外来词和本土词的能力，以理解它们在处理受强势语言影响的少数语言时的表现。

Method: 本研究评估了多个预训练语言模型在10种语言上的外来词识别能力。在实验中，模型被给予了明确的指令和上下文信息，以测试其区分外来词和本土词的表现。

Result: 研究结果显示，尽管提供了明确的指令和上下文信息，模型在区分外来词和本土词方面表现不佳。这支持了先前的研究发现，即现代自然语言处理系统在处理外来词和本土词时存在偏见，倾向于识别外来词。

Conclusion: 本研究的发现对外语词识别能力提出了质疑，并强调了当前自然语言处理系统在处理受强势语言影响的少数语言时可能存在的偏见。这对于开发服务于少数语言的自然语言处理工具以及在词汇受强势语言影响的社区中支持语言保护工作具有重要意义。未来的工作可以进一步探索提高模型对外来词识别能力的策略。

Abstract: Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.

</details>


### [28] [Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual](https://arxiv.org/abs/2510.26271)
*Sukrit Sriratanawilai,Jhayahgrit Thongwat,Romrawin Chumpu,Patomporn Payoungkhamdee,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 本文研究了知识蒸馏（KD）在多语言视觉语言模型（VLMs）中的应用，并探索了五种不同的蒸馏方法对跨语言表示一致性和模型压缩下性能稳定性的影响。研究发现在模型尺寸减半的情况下，某些配置能够保持甚至提升多语言检索的鲁棒性，但其他配置则无法维持跨任务的稳定性，揭示了仅依靠聚合准确性无法显现的设计敏感性权衡。


<details>
  <summary>Details</summary>
Motivation: 多语言视觉语言模型（VLMs）在不同语言上的性能表现不均衡，模型尺寸减小时问题尤为严重。虽然知识蒸馏（KD）可以有效地将知识从大模型迁移到小模型，但在多语言场景下的应用仍有待探索。本文旨在解决多语言VLM在模型压缩下面临的性能不均衡和稳定性问题。

Method: 本文进行了一项对照的实证研究，考察了五种知识蒸馏（KD）方法在多语言VLM上的表现。研究在CLIP和SigLIP2模型上应用了五种不同的蒸馏方法，并评估了它们在多语言检索和视觉问答任务上的表现，特别关注了跨语言表示的一致性和模型压缩后的性能稳定性。

Result: 研究发现，在模型尺寸减半的情况下，部分蒸馏配置能够保持或提升多语言检索的鲁棒性。然而，其他配置在跨任务稳定性方面表现不佳。这些结果表明，蒸馏方法的选择对模型的性能有显著影响，并且仅关注整体准确性可能无法完全揭示这些细微的性能差异和设计上的权衡。

Conclusion: 知识蒸馏在多语言VLM模型压缩中存在设计敏感性。虽然某些蒸馏方法可以在减小模型尺寸的同时保持甚至提升多语言检索能力，但并非所有方法都能保证跨任务的稳定性。未来的研究需要更深入地理解这些权衡，并开发更鲁棒的蒸馏策略，以应对多语言VLM在不同任务和语言上的性能挑战。

Abstract: Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.

</details>


### [29] [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)
*Kang Chen,Yaoning Wang,Kai Xiong,Zhuoka Feng,Wenhe Sun,Haotian Chen,Yixin Cao*

Main category: cs.CL

TL;DR: LLM推理中的外部信号（如标记概率、熵或自我评估）得分不佳，因为它们是低维度的、校准不佳的。我们提出了一种名为神经元协议解码（NAD）的新方法，该方法通过分析神经元激活来解码LLM的内部行为。NAD在解码过程中利用激活稀疏性和跨样本神经元协议来选择候选者，而不是依赖外部信号。NAD可以在生成的前32个令牌内实现早期正确性预测，并支持提前停止。在数学和科学基准上，NAD与多数投票相当；在开放式编码基准上，NAD的性能优于Avg@64。NAD通过提前修剪不确定的轨迹，将令牌使用量减少了99%，同时仅略微降低了生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理方法依赖于外部信号（如标记概率、熵或自我评估）来对候选答案进行评分。然而，这些外部信号可能存在校准不佳的问题，并且是内部神经元激活的低维投影。这限制了LLM的推理能力，尤其是在需要准确评估多个生成路径的情况下。因此，研究如何利用LLM的内部行为来改进推理解码过程，以实现更准确、更高效的解码，具有重要意义。

Method: 该研究提出了一种名为神经元协议解码（NAD）的无监督“最佳N”方法。NAD利用LLM的内部神经元激活信号来选择最佳候选答案，而不是依赖外部信号。具体来说，NAD分析了以下三个内部行为发现：（1）外部信号是内部动态的低维投影；（2）正确答案在生成过程中激活的唯一神经元数量明显少于不正确答案；（3）正确答案的激活在不同样本之间具有更强的协议性，而不正确答案则表现出分歧。NAD基于激活稀疏性和跨样本神经元协议来选择候选者。该方法可以在生成早期（前32个token）做出正确的性预测，并支持提前停止策略。

Result: 在数学和科学基准（具有可验证答案）上，NAD的性能与多数投票相当。在开放式编码基准（其中多数投票不适用）上，NAD始终优于Avg@64。通过提前修剪不确定的生成轨迹，NAD能够将token使用量减少99%，同时对生成质量的影响很小。这表明内部信号可以为无监督的集成解码提供可靠、可扩展且高效的指导。

Conclusion: 研究表明，LLM的内部神经元激活信号包含了丰富的、可用于改进解码过程的信息。NAD方法利用这些内部信号，实现了高效且准确的无监督集成解码，显著减少了计算资源消耗，同时保持了生成质量。这为未来在LLM推理中利用内部动态以提高效率和准确性开辟了新的途径。未来的工作可以探索更复杂的内部信号分析技术，以及将NAD应用于更多类型的LLM任务和模型架构。

Abstract: Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.

</details>


### [30] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

TL;DR: 尽管大型语言模型（LLMs）在处理数字信息时会产生错误输出，但研究发现它们在数字输入嵌入表示上表现出高度一致和准确性。本研究旨在解释这种矛盾，通过探索LLMs如何处理数字并量化其机制的准确性下限。结果表明，不同LLMs学习到的数字表示是可互换的、系统的、高度准确的，并且在隐藏状态和输入语境类型上具有普遍性。这使得研究者能够创建通用的探针来追踪信息，包括输出错误的根源，并将其定位到特定的层。研究结果为理解预训练LLMs如何处理数字奠定了基础，并指出了更精确的探针技术在改进LLMs架构方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，大型语言模型（LLMs）在处理数字信息时常出现错误输出，这与它们在数字输入嵌入表示上表现出的高度一致性和准确性相矛盾。本研究旨在解释这种现象，理解LLMs处理数字的机制，并量化这些机制的准确性下限。

Method: 研究者探索了语言模型如何操作数字，并量化了这些操作的准确性下限。他们发现，不同LLMs学习到的数字表示是可互换的、系统的、高度准确的，并且在隐藏状态和输入语境类型上具有普遍性。在此基础上，研究者开发了通用的探针，能够追踪信息流，包括输出错误的根源，并将其定位到模型的特定层。

Result: 研究结果显示，尽管LLMs在输出数字时会出错，但它们内部的数字表示却高度一致、系统化且准确。不同模型之间以及在不同隐藏状态和上下文类型下，这些数字表示具有普遍性。研究者能够通过开发的通用探针，精确定位到模型的特定层，从而追踪到数字信息处理的路径以及错误产生的具体原因。

Conclusion: 本研究为理解预训练LLMs如何处理数字提供了基础性的认识。研究发现的数字表示的普遍性和准确性，以及可用于定位错误根源的探针技术，为未来改进LLMs的架构和提高其数字处理能力指明了方向。未来的工作可以进一步探索这些探针技术的应用，以优化LLMs的性能。

Abstract: Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


### [31] [SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling](https://arxiv.org/abs/2510.26322)
*Fares Fawzi,Vinitra Swamy,Dominik Glandorf,Tanya Nazaretsky,Tanja Käser*

Main category: cs.CL

TL;DR: SCRIBE是一个用于生成教育领域学生反馈的框架，它克服了隐私、计算资源和教学有效性的挑战。该框架使用多跳、工具增强的推理，并能通过两阶段LoRA微调到3B和8B模型。实验证明，8B-SCRIBE在相关性和可操作性方面与大型模型相当，且学生评价与之相当，证明了其在资源受限和注重隐私的教育应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 教育领域需要个性化、交互式的学生反馈，但现实部署面临隐私、计算资源有限以及需要符合教学法的有效回应的挑战。这些挑战要求使用小型、开源且能在本地运行的模型，并能可靠地基于正确信息生成反馈。

Method: SCRIBE框架采用多跳、工具增强的推理方法，结合特定领域的工具和一个支持迭代推理、工具使用和错误恢复的自反思推理流程。通过在合成的GPT-4o生成的数据上进行两阶段LoRA微调，将这些能力蒸馏到3B和8B模型中。

Result: 评估结果显示，8B-SCRIBE模型在相关性和可操作性等关键维度上，其质量与更大模型相当甚至更优。学生用户研究（108名学生参与）表明，学生认为8B-SCRIBE的质量与GPT-4o和Llama-3.3 70B相当。

Conclusion: SCRIBE框架证明了在资源受限且注重隐私的教育应用中，利用小型、开源模型提供高质量学生反馈是可行的。研究结果为在这些限制条件下开发有效的教育技术工具铺平了道路。

Abstract: Language models can be used to provide interactive, personalized student
feedback in educational settings. However, real-world deployment faces three
key challenges: privacy concerns, limited computational resources, and the need
for pedagogically valid responses. These constraints require small, open-source
models that can run locally and reliably ground their outputs in correct
information. We introduce SCRIBE, a framework for multi-hop, tool-augmented
reasoning designed to generate valid responses to student questions about
feedback reports. SCRIBE combines domain-specific tools with a self-reflective
inference pipeline that supports iterative reasoning, tool use, and error
recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA
fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned
GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models
achieve comparable or superior quality to much larger models in key dimensions
such as relevance and actionability, while being perceived on par with GPT-4o
and Llama-3.3 70B by students. These findings demonstrate the viability of
SCRIBE for low-resource, privacy-sensitive educational applications.

</details>


### [32] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: ACER通过生成课程和问题-答案对来增强大型语言模型（LLMs）在专业领域的表现，实验表明该方法在微观经济学等领域提高了5个百分点，并在其他领域和基准测试中均有显著提升，同时避免了灾难性遗忘并促进了跨领域知识转移。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在通用任务上表现出色，但在经济学和心理学等需要深度、原则性理解的专业领域表现不佳。

Method: ACER首先通过生成目录和基于布鲁姆分类法的问答对来合成教材风格的课程，然后使用交错的课程表进行持续预训练。

Result: 在Llama 3.2模型上进行的实验显示，ACER在专业MMLU子集（尤其是微观经济学，准确率提高了5个百分点）和其他知识密集型基准测试（如ARC和GPQA，提高了2个百分点以上）上取得了显著的性能提升，平均提高了3个百分点，同时在通用推理任务上保持稳定，并实现了跨领域知识转移。

Conclusion: ACER是一种有效且可扩展的方法，能够弥合大型语言模型在专业领域知识上的差距，同时保持其通用能力，并能促进跨领域知识迁移。

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [33] [MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data](https://arxiv.org/abs/2510.26345)
*Mykhailo Poliakov,Nadiya Shvai*

Main category: cs.CL

TL;DR: 该研究提出了一种名为MisSynth的流水线，利用检索增强生成（RAG）技术生成合成的谬误数据，并用于微调大型语言模型（LLMs），以提升其识别健康领域虚假信息的准确性。实验结果表明，经过微调的模型在MISSCI数据集上表现出显著的准确性提升，其中LLaMA 3.1 8B微调模型在F1分数上比基线模型提高了超过35%。该方法证明了即使在计算资源有限的情况下，利用合成数据增强有限的标注资源也能有效提升LLMs在真实科学虚假信息识别任务上的零样本分类性能。


<details>
  <summary>Details</summary>
Motivation: 健康领域虚假信息普遍存在且可能造成危害，尤其当其歪曲或误读科学发现时，识别难度更大。本研究旨在解决这一问题，探索如何有效提升大型语言模型（LLMs）识别科学领域虚假论证的能力。

Method: 本研究提出了MisSynth流水线，该流水线首先利用检索增强生成（RAG）技术生成合成的谬误样本。随后，将这些合成样本用于微调LLM模型。实验采用了MISSCI数据集和框架进行评估，并与未经微调的基线模型进行了比较。

Result: 经过MisSynth流水线微调的模型在识别虚假论证方面的准确性相比未经微调的基线模型有了显著提高。具体而言，LLaMA 3.1 8B微调模型在MISSCI测试集上的F1分数绝对提升超过35%。这证明了通过引入合成谬误数据来增强有限的标注资源，能够有效提升LLMs在真实科学虚假信息任务上的零样本分类性能，即使在计算资源有限的情况下也是如此。

Conclusion: 本研究成功证明了合成数据生成和轻量级微调技术在提升LLMs识别健康领域虚假信息方面的有效性。MisSynth流水线能够显著提高模型的准确性，为解决科学虚假信息问题提供了有前景的方法，尤其是在标注数据稀缺的情况下。研究表明，即使计算资源有限，该方法也能有效提升零样本分类性能。研究代码和合成数据集已公开。未来的工作可以进一步探索该方法的泛化能力和在其他领域的应用。

Abstract: Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.

</details>


### [34] [On the Role of Context for Discourse Relation Classification in Scientific Writing](https://arxiv.org/abs/2510.26354)
*Stephen Wan,Wei Liu,Michael Strube*

Main category: cs.CL

TL;DR: 本文研究了在科学论文中使用预训练语言模型（PLM）和大型语言模型（LLM）进行语篇关系分类（DRC），以支持生成式AI生成的科学论断。研究发现，语篇结构提供的上下文信息通常有助于DRC任务，并分析了哪些科学语篇关系类型从上下文中获益最多。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在科学工作流中应用的增加，研究如何利用语篇级信息来支持AI生成的科学论断变得越来越重要。而语篇关系分类（DRC）是实现这一目标的第一步，但目前对科学写作这一特定体裁的DRC研究不足。

Method: 本文采用预训练语言模型（PLM）和大型语言模型（LLM）来处理科学论文中的语篇关系分类（DRC）任务。研究通过实验考察了上下文信息（由语篇结构定义）对DRC任务的帮助程度，并分析了不同科学语篇关系类型从上下文中获益的程度。

Result: 实验结果表明，语篇结构定义的上下文信息在DRC任务中通常是有帮助的。研究还识别出一些可能从上下文信息中获益更多的科学语篇关系类型。

Conclusion: 研究为利用LLM支持科学写作和论证检索奠定了基础，但也存在局限性，未来工作可进一步探索上下文对DRC的影响，并应用于更广泛的科学文献分析。

Abstract: With the increasing use of generative Artificial Intelligence (AI) methods to
support science workflows, we are interested in the use of discourse-level
information to find supporting evidence for AI generated scientific claims. A
first step towards this objective is to examine the task of inferring discourse
structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language
model (PLM) and Large Language Model (LLM) approaches for Discourse Relation
Classification (DRC), focusing on scientific publications, an under-studied
genre for this task. We examine how context can help with the DRC task, with
our experiments showing that context, as defined by discourse structure, is
generally helpful. We also present an analysis of which scientific discourse
relation types might benefit most from context.

</details>


### [35] [OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education](https://arxiv.org/abs/2510.26422)
*Min Zhang,Hao Chen,Hao Chen,Wenqi Zhang,Didi Zhu,Xin Lin,Bo Jiang,Aimin Zhou,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 该研究提出了OmniEduBench，一个全面的中文教育基准，旨在解决现有大型语言模型（LLM）教育评估中忽视能力培养和学科多样性的问题。该基准包含24.6K个问答对，涵盖知识和能力两个维度，涉及61个学科和11种题型。实验结果显示，LLM在教育领域的表现存在显著差距，尤其是在能力培养方面，与人类智能仍有较大差距，表明LLM在教育应用方面仍有巨大改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）及其基准在教育领域的应用主要集中在知识评估，而忽视了对现实世界教育场景至关重要的能力培养维度。此外，当前基准在学科和题型多样性方面存在不足，尤其是在中文语境下。这阻碍了对LLM在教育领域全面能力的准确评估。

Method: 研究者们构建了一个名为OmniEduBench的中文教育基准。该基准包含24,602个高质量的问答对，并将其划分为知识维度（18,121个条目）和能力维度（6,481个条目）。每个维度进一步细分为6个细粒度类别，总共覆盖61个学科（知识维度41个，能力维度20个）。基准还包含了11种常见的考试题型，以确保数据多样性。研究者们利用此基准对11个主流的开源和闭源LLM进行了广泛的实验评估。

Result: 在知识维度上，只有Gemini-2.5 Pro的准确率超过了60%。在能力维度上，表现最好的模型QWQ的准确率仍比人类智能低近30%。这些结果表明，目前的LLM在教育应用方面，尤其是在能力培养方面，与人类水平相比仍有显著的性能差距。

Conclusion: OmniEduBench的提出为全面评估LLM在中文教育场景下的知识和能力培养方面提供了一个重要的基准。实验结果揭示了当前LLM在教育应用中面临的挑战，特别是在超越知识记忆、培养综合能力方面仍有很大的提升空间。未来的研究应致力于改进LLM在这些方面的性能，以更好地服务于教育领域。

Abstract: With the rapid development of large language models (LLMs), various LLM-based
works have been widely applied in educational fields. However, most existing
LLMs and their benchmarks focus primarily on the knowledge dimension, largely
neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often
limited to a single subject or question type, lacking sufficient diversity.
This issue is particularly prominent within the Chinese context. To address
this gap, we introduce OmniEduBench, a comprehensive Chinese educational
benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.
The data is meticulously divided into two core dimensions: the knowledge
dimension and the cultivation dimension, which contain 18.121K and 6.481K
entries, respectively. Each dimension is further subdivided into 6 fine-grained
categories, covering a total of 61 different subjects (41 in the knowledge and
20 in the cultivation). Furthermore, the dataset features a rich variety of
question formats, including 11 common exam question types, providing a solid
foundation for comprehensively evaluating LLMs' capabilities in education.
Extensive experiments on 11 mainstream open-source and closed-source LLMs
reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro
surpassed 60\% accuracy, while in the cultivation dimension, the
best-performing model, QWQ, still trailed human intelligence by nearly 30\%.
These results highlight the substantial room for improvement and underscore the
challenges of applying LLMs in education.

</details>


### [36] [1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models](https://arxiv.org/abs/2510.26446)
*Zeliang Zong,Kai Zhang,Zheyang Li,Wenming Tan,Ye Ren,Yiyan Zhai,Jilin Hu*

Main category: cs.CL

TL;DR: 本文提出了一种名为SSLC（Synergistic Sparse and Low-Rank Compression）的新方法，用于同时对大型语言模型（LLMs）进行稀疏化和低秩近似压缩，以解决LLMs部署中的带宽和计算瓶颈。SSLC通过将低秩近似和稀疏优化结合到一个统一的问题中，并采用迭代优化算法求解，在LLaMA和Qwen2.5模型上实现了优于单独方法的性能，并在不进行额外训练的情况下，实现了Qwen2.5模型50%的压缩率，性能无下降，同时速度提升至少1.63倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在语言理解和生成方面表现出色，但其巨大的带宽和计算需求限制了其广泛应用。现有的压缩技术如剪枝和低秩近似各自有局限性，并且它们协同作用于LLMs的研究尚不充分。因此，有必要探索一种能够有效结合这两种技术优势，以实现LLM高效部署的方法。

Method: 本文提出了一种协同稀疏和低秩压缩（SSLC）方法。首先，基于理论分析，将低秩近似和稀疏优化统一为一个数学问题。然后，利用迭代优化算法来求解这个统一的问题。该方法旨在利用低秩近似保留模型核心结构，同时通过稀疏优化去除冗余权重。实验部分在LLaMA和Qwen2.5模型（包括7B到70B参数规模）上进行了评估，并且强调该方法在不进行额外微调的情况下进行压缩。

Result: 在LLaMA和Qwen2.5模型上的实验结果表明，SSLC方法在压缩LLMs方面始终优于单独使用低秩近似或稀疏优化方法。具体而言，SSLC能够将Qwen2.5模型压缩50%，且性能无明显下降，同时实现了至少1.63倍的模型推理速度提升。这表明SSLC是一种在效率和性能之间取得良好平衡的实用技术。

Conclusion: SSLC方法通过巧妙地将低秩近似和稀疏优化相结合，为解决大型语言模型的部署瓶颈提供了一个有效的解决方案。该方法在不损失性能的情况下实现了显著的模型压缩和加速，为LLM在资源受限环境下的实际应用铺平了道路。未来的工作可以探索SSLC与其他压缩技术的结合，以及在更大规模的模型和更多样化的任务上的应用。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
language comprehension and generation; however, their widespread adoption is
constrained by substantial bandwidth and computational demands. While pruning
and low-rank approximation have each demonstrated promising performance
individually, their synergy for LLMs remains underexplored. We introduce
\underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank
\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths
of both techniques: low-rank approximation compresses the model by retaining
its essential structure with minimal information loss, whereas sparse
optimization eliminates non-essential weights, preserving those crucial for
generalization. Based on theoretical analysis, we first formulate the low-rank
approximation and sparse optimization as a unified problem and solve it by
iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models
(7B-70B) show that SSLC, without any additional training steps, consistently
surpasses standalone methods, achieving state-of-the-arts results. Notably,
SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least
1.63$\times$ speedup, offering a practical solution for efficient LLM
deployment.

</details>


### [37] [Hebrew Diacritics Restoration using Visual Representation](https://arxiv.org/abs/2510.26521)
*Yair Elboher,Yuval Pinter*

Main category: cs.CL

TL;DR: 本文提出了一种名为DIVRIT的新型希伯来语注音系统，将注音任务视为零样本分类问题。该系统通过希伯来语视觉语言模型处理未注音文本，将注音信息嵌入输入向量表示中，实现了在没有复杂显式语言分析的情况下进行注音。实验证明，DIVRIT在“神谕”设置下达到了高精度，并通过架构增强和优化训练提高了泛化能力，展示了视觉表示在希伯来语自动注音方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 希伯来语缺乏元音符号时存在固有的歧义性，这会影响单词的正确发音和文本含义的消歧。尽管存在这些挑战，但机器学习方法的进步为解决此问题提供了新的途径。本研究旨在开发一种新的希伯来语注音系统，以提高准确性和效率。

Method: 本文提出的DIVRIT系统将希伯来语注音任务视为零样本分类问题。该系统在词级别操作，根据周围的文本上下文，从动态生成的候选集中为每个未注音的单词选择最合适的注音模式。DIVRIT的一个关键创新是使用希伯来语视觉语言模型，该模型将未注音的文本视为图像处理，从而允许将注音信息直接嵌入到输入的向量表示中。通过在各种配置下进行全面的评估来验证该方法。

Result: 在“神谕”设置下（即正确注音形式保证存在于提供的候选集中），DIVRIT系统实现了高水平的准确性。此外，通过战略性的架构增强和优化的训练方法，系统的整体泛化能力得到了显著提高。这些结果表明，视觉表示在实现准确和自动的希伯来语注音方面具有巨大潜力。

Conclusion: 本研究提出的DIVRIT系统在希伯来语注音任务上取得了显著成果，展示了将零样本学习和视觉语言模型应用于此任务的有效性。该系统无需复杂的显式语言分析即可实现高精度，并且通过架构和训练的优化，其泛化能力得到了提升。研究结果强调了视觉表示在自动化希伯来语注音方面的潜力，为未来的研究开辟了新的方向。

Abstract: Diacritics restoration in Hebrew is a fundamental task for ensuring accurate
word pronunciation and disambiguating textual meaning. Despite the language's
high degree of ambiguity when unvocalized, recent machine learning approaches
have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization
that frames the task as a zero-shot classification problem. Our approach
operates at the word level, selecting the most appropriate diacritization
pattern for each undiacritized word from a dynamically generated candidate set,
conditioned on the surrounding textual context. A key innovation of DIVRIT is
its use of a Hebrew Visual Language Model, which processes undiacritized text
as an image, allowing diacritic information to be embedded directly within the
input's vector representation.
  Through a comprehensive evaluation across various configurations, we
demonstrate that the system effectively performs diacritization without relying
on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting
where the correct diacritized form is guaranteed to be among the provided
candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic
architectural enhancements and optimized training methodologies yield
significant improvements in the system's overall generalization capabilities.
These findings highlight the promising potential of visual representations for
accurate and automated Hebrew diacritization.

</details>


### [38] [InfoFlow: Reinforcing Search Agent Via Reward Density Optimization](https://arxiv.org/abs/2510.26575)
*Kun Luo,Hongjin Qian,Zheng Liu,Ziyi Xia,Shitao Xiao,Siqi Bao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: RLVR在深层搜索中面临奖励稀疏性挑战。本文提出InfoFlow框架，通过子问题分解、失败引导提示和双智能体提炼来解决奖励密度优化问题，提高了每个探索单元的奖励，并使轻量级LLM达到与先进LLM相当的性能。


<details>
  <summary>Details</summary>
Motivation: 在深层搜索场景中，奖励稀疏性（Reward Density）导致智能体花费大量探索成本却只获得稀疏或为零的最终奖励。这阻碍了RLVR的应用。本文将此挑战形式化为奖励密度优化（Reward Density Optimization）问题，旨在提高每单位探索成本获得的奖励。

Method: InfoFlow框架从三个方面解决奖励密度优化问题：1.子问题分解（Subproblem decomposition）：将长距离任务分解为子任务，并分配过程奖励，提供更密集的学习信号。2.失败引导提示（Failure-guided hints）：向停滞的轨迹注入纠正性指导，提高成功概率。3.双智能体提炼（Dual-agent refinement）：采用双智能体架构，由一个提炼智能体合成搜索历史，压缩轨迹，降低探索成本，提高奖励密度。

Result: 在多个智能体搜索基准测试中，InfoFlow显著优于强基线模型。实验证明，InfoFlow能够使轻量级LLM达到与先进的专有LLM相当的性能。

Conclusion: InfoFlow框架成功解决了RLVR中的奖励稀疏性问题，通过子问题分解、失败引导提示和双智能体提炼显著提高了奖励密度和智能体的探索效率。该方法使轻量级模型在复杂搜索任务中表现出色，具有重要的实际应用价值。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach
for enhancing agentic deep search. However, its application is often hindered
by low \textbf{Reward Density} in deep search scenarios, where agents expend
significant exploratory costs for infrequent and often null final rewards. In
this paper, we formalize this challenge as the \textbf{Reward Density
Optimization} problem, which aims to improve the reward obtained per unit of
exploration cost. This paper introduce \textbf{InfoFlow}, a systematic
framework that tackles this problem from three aspects. 1) \textbf{Subproblem
decomposition}: breaking down long-range tasks to assign process rewards,
thereby providing denser learning signals. 2) \textbf{Failure-guided hints}:
injecting corrective guidance into stalled trajectories to increase the
probability of successful outcomes. 3) \textbf{Dual-agent refinement}:
employing a dual-agent architecture to offload the cognitive burden of deep
exploration. A refiner agent synthesizes the search history, which effectively
compresses the researcher's perceived trajectory, thereby reducing exploration
cost and increasing the overall reward density. We evaluate InfoFlow on
multiple agentic search benchmarks, where it significantly outperforms strong
baselines, enabling lightweight LLMs to achieve performance comparable to
advanced proprietary LLMs.

</details>


### [39] [Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models](https://arxiv.org/abs/2510.26577)
*Yinrong Hong,Zhiquan Tan,Kai Hu*

Main category: cs.CL

TL;DR: LLMs面临推理延迟问题，现有投机解码方法（如EAGLE-2/3）未考虑GPU和批大小等系统变量。本文提出CAST动态树解码方法，考虑了推理成本，在六项任务和六种LLM上的实验表明，CAST速度比传统方法快5.2倍，比现有SOTA技术快5%-20%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）由于其自回归设计和庞大的模型规模，在推理时面临显著的延迟挑战。投机解码作为一种解决方案，允许同时生成和验证多个令牌。然而，现有的先进投机解码方法（如EAGLE-2和EAGLE-3）在动态调整树结构时，往往忽略了GPU设备和批大小等关键系统变量对推理成本的影响，这限制了其在实际应用中的效率。

Method: 本文提出了一种名为CAST（Cost-Aware Speculative Tree Decoding）的新型动态树解码方法。CAST在构建投机解码的树结构时，会动态地考虑推理成本，其中包括GPU配置和批大小等因素。通过这种成本感知的方法，CAST能够更有效地优化树的结构，以最小化整体推理延迟。实验在六种不同的LLM和六项多样化的下游任务上进行，以全面评估CAST的性能。

Result: 通过在六种不同的LLM和六项多样化的下游任务上的广泛实验，CAST方法取得了显著的成果。与传统的解码方法相比，CAST的推理速度最高提升了5.2倍。与现有的最先进（SOTA）的投机解码技术相比，CAST在性能上普遍实现了5%到20%的提升，证明了其在提高LLM推理效率方面的优越性。

Conclusion: 本文提出的CAST动态树解码方法通过将推理成本（包括GPU配置和批大小）纳入树结构动态调整过程，有效解决了现有投机解码方法忽略系统变量的问题。实验结果表明，CAST在多项任务和多种LLM上均能显著提高推理速度，并且优于现有的SOTA技术。这为在实际部署中加速LLM推理提供了有前景的解决方案。未来的工作可以进一步探索更复杂的成本模型和更广泛的硬件平台上的性能优化。

Abstract: Large Language Models (LLMs) face significant inference latency challenges
stemming from their autoregressive design and large size. To address this,
speculative decoding emerges as a solution, enabling the simultaneous
generation and validation of multiple tokens. While recent approaches like
EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,
they often neglect the impact of crucial system variables such as GPU devices
and batch sizes.
  Therefore, we introduce a new dynamic tree decoding approach called CAST that
takes into account inference costs, including factors such as GPU
configurations and batch sizes, to dynamically refine the tree structure.
Through comprehensive experimentation across six diverse tasks and utilizing
six distinct LLMs, our methodology demonstrates remarkable results, achieving
speeds up to 5.2 times faster than conventional decoding methods. Moreover, it
generally outperforms existing state-of-the-art techniques from 5% to 20%.

</details>


### [40] [Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model](https://arxiv.org/abs/2510.26622)
*Biao Zhang,Yong Cheng,Siamak Shakeri,Xinyi Wang,Min Ma,Orhan Firat*

Main category: cs.CL

TL;DR: 近期研究表明，大型语言模型（LLM）在架构上从编码器-解码器模型转向了更占主导地位的仅解码器模型。然而，这种转变缺乏来自规模化视角的严格比较分析，这可能导致编码器-解码器模型的潜力被忽视。本研究重新审视了编码器-解码器LLM（RedLLM），并借鉴了仅解码器LLM（DecLLM）的最新技术对其进行了改进。通过在不同模型规模（约1.5亿至80亿参数）上进行预训练和指令调优，研究发现RedLLM展现出良好的规模化特性和出色的性能。虽然DecLLM在预训练中计算效率更高，但RedLLM在规模化和上下文长度外推能力方面表现相当。经过指令调优后，RedLLM在下游任务上取得了可比甚至更优的结果，并且推理效率更高。研究希望这些发现能鼓励更多关于RedLLM的研究，以发掘其在开发强大而高效的LLM方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）的研究趋势明显偏向于仅解码器（decoder-only）架构，而忽略了编码器-解码器（encoder-decoder）架构的潜力。这种转变缺乏严格的、特别是从规模化角度的比较分析，可能导致我们错失了编码器-解码器模型在性能和效率上的优势。因此，本研究旨在重新评估编码器-解码器LLM，并探索其在现代LLM发展中的可能性，以填补这一研究空白。

Method: 本研究重新审视并改进了编码器-解码器LLM（RedLLM），采用了仅解码器LLM（DecLLM）的最新技术。研究人员使用RedPajama V1（1.6万亿 tokens）数据集对RedLLM进行了前缀语言模型（prefix LM）的预训练，并使用FLAN数据集进行了指令调优。同时，他们还将传统的因果语言模型（causal LM）预训练的DecLLM作为对比基线。实验覆盖了从约1.5亿到80亿参数的不同模型规模。通过在预训练和指令调优阶段进行全面的比较分析，评估了RedLLM在规模化特性、上下文长度外推能力以及下游任务性能和推理效率等方面的表现。

Result: 实验结果表明，RedLLM在规模化特性和性能方面表现出令人信服的优势。尽管在预训练阶段，仅解码器模型（DecLLM）在计算效率上更优，但RedLLM在规模化扩展和上下文长度的外推能力上与之相当。在经过指令调优后，RedLLM在各项下游任务上的表现与DecLLM相当，甚至在某些任务上更优。此外，RedLLM在推理效率方面也展现出显著的优势。

Conclusion: 本研究通过重新评估和改进编码器-解码器LLM（RedLLM），证明了其在规模化特性、下游任务性能和推理效率方面具有强大的潜力，可以与当前主流的仅解码器模型（DecLLM）相媲美甚至超越。研究结果表明，不应忽视编码器-解码器架构的价值，并鼓励未来在这一方向上进行更多探索，以开发更强大、更高效的大型语言模型。

Abstract: Recent large language model (LLM) research has undergone an architectural
shift from encoder-decoder modeling to nowadays the dominant decoder-only
modeling. This rapid transition, however, comes without a rigorous comparative
analysis especially \textit{from the scaling perspective}, raising concerns
that the potential of encoder-decoder models may have been overlooked. To fill
this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent
recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison
between RedLLM, pretrained with prefix language modeling (LM), and DecLLM,
pretrained with causal LM, at different model scales, ranging from $\sim$150M
to $\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for
instruction tuning, our experiments show that RedLLM produces compelling
scaling properties and surprisingly strong performance. While DecLLM is overall
more compute-optimal during pretraining, RedLLM demonstrates comparable scaling
and context length extrapolation capabilities. After instruction tuning, RedLLM
achieves comparable and even better results on various downstream tasks while
enjoying substantially better inference efficiency. We hope our findings could
inspire more efforts on re-examining RedLLM, unlocking its potential for
developing powerful and efficient LLMs.

</details>


### [41] [Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models](https://arxiv.org/abs/2510.26683)
*Mingchen Tu,Zhiqiang Liu,Juan Li,Liangyurui Liu,Junjie Wang,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: LLMs在医疗等数据敏感领域面临训练语料库不足的挑战。Evontree框架利用少量本体规则，从LLM中提取、验证和增强领域知识，无需大量外部数据集，通过自我蒸馏微调提升模型性能。在医疗问答基准测试中，Evontree相较于未修改模型和领先的监督基线，准确率最高提升3.7%，证明了其在低资源领域自适应LLM的有效性、效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多领域展现出强大能力，但在医疗等数据敏感领域，缺乏高质量、领域特定的训练语料库限制了其应用。领域专家已将知识提炼成本体规则，形式化了概念间的关系并保证了知识库的完整性。此研究旨在解决LLMs在低资源领域自适应的挑战。

Method: Evontree框架利用少量本体规则，系统地从LLM中提取、验证和增强领域知识。具体步骤包括：1. 从原始模型中提取领域本体。2. 使用两个核心本体规则检测不一致性。3. 通过自我蒸馏的微调过程强化精炼后的知识。实验在医疗问答基准上，使用Llama3-8B-Instruct和Med42-v2模型进行。

Result: 在医疗问答基准测试中，Evontree框架实现了持续的优于未修改模型和领先的监督基线。准确率最高提升了3.7%。

Conclusion: Evontree框架有效地利用本体规则解决了LLMs在低资源领域（如医疗）的自适应问题，无需大量外部数据集。实验结果表明该方法在提高模型准确性方面是有效的、高效的且鲁棒的。未来工作可以探索Evontree在更多数据敏感领域和更大规模模型上的应用。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities
across multiple domains by leveraging massive pre-training and curated
fine-tuning data. However, in data-sensitive fields such as healthcare, the
lack of high-quality, domain-specific training corpus hinders LLMs' adaptation
for specialized applications. Meanwhile, domain experts have distilled domain
wisdom into ontology rules, which formalize relationships among concepts and
ensure the integrity of knowledge management repositories. Viewing LLMs as
implicit repositories of human knowledge, we propose Evontree, a novel
framework that leverages a small set of high-quality ontology rules to
systematically extract, validate, and enhance domain knowledge within LLMs,
without requiring extensive external datasets. Specifically, Evontree extracts
domain ontology from raw models, detects inconsistencies using two core
ontology rules, and reinforces the refined knowledge via self-distilled
fine-tuning. Extensive experiments on medical QA benchmarks with
Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both
unmodified models and leading supervised baselines, achieving up to a 3.7%
improvement in accuracy. These results confirm the effectiveness, efficiency,
and robustness of our approach for low-resource domain adaptation of LLMs.

</details>


### [42] [Kimi Linear: An Expressive, Efficient Attention Architecture](https://arxiv.org/abs/2510.26692)
*Kimi Team,Yu Zhang,Zongyu Lin,Xingcheng Yao,Jiaxi Hu,Fanqing Meng,Chengyin Liu,Xin Men,Songlin Yang,Zhiyuan Li,Wentao Li,Enzhe Lu,Weizhou Liu,Yanru Chen,Weixin Xu,Longhui Yu,Yejie Wang,Yu Fan,Longguang Zhong,Enming Yuan,Dehao Zhang,Yizhi Zhang,T. Y. Liu,Haiming Wang,Shengjun Fang,Weiran He,Shaowei Liu,Yiwei Li,Jianlin Su,Jiezhong Qiu,Bo Pang,Junjie Yan,Zhejun Jiang,Weixiao Huang,Bohong Yin,Jiacheng You,Chu Wei,Zhengtao Wang,Chao Hong,Yutian Chen,Guanduo Chen,Yucheng Wang,Huabin Zheng,Feng Wang,Yibo Liu,Mengnan Dong,Zheng Zhang,Siyuan Pan,Wenhao Wu,Yuhao Wu,Longyu Guan,Jiawen Tao,Guohong Fu,Xinran Xu,Yuzhi Wang,Guokun Lai,Yuxin Wu,Xinyu Zhou,Zhilin Yang,Yulun Du*

Main category: cs.CL

TL;DR: Kimi Linear是一种创新的混合线性注意力架构，通过Kimi Delta Attention（KDA）模块和优化的DPLR算法，在各种场景下（短文本、长文本、强化学习）均超越了全注意力机制。该模型在保持优于全注意力模型性能的同时，显著减少了KV缓存使用（高达75%），并提升了解码吞吐量（最高6倍），可作为全注意力架构的替代品。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理长上下文时面临计算成本高、内存占用大（KV缓存）以及效率低下的问题，限制了其在需要长序列理解的应用中的性能。因此，需要开发一种能够有效处理长上下文并保持高效率和性能的注意力机制。

Method: Kimi Linear采用了混合线性注意力架构，核心是Kimi Delta Attention（KDA）模块，它通过改进的门控机制扩展了Gated DeltaNet，以更有效地利用有限的RNN状态内存。结合了KDA和Multi-Head Latent Attention（MLA）。算法采用了定制的块状（chunkwise）DPLR（Diagonal-Plus-Low-Rank）算法，以实现高硬件效率，该算法通过专门的DPLR过渡矩阵显著降低了计算量，并更符合经典delta规则。实验中预训练了一个包含3B激活参数和48B总参数的模型，并使用了与全MLA相同的训练方法进行比较。

Result: 在相同的训练配方下，Kimi Linear在所有评估任务中均显著优于全MLA模型。与全MLA相比，Kimi Linear在1M上下文长度下，KV缓存使用减少高达75%，解码吞吐量提升高达6倍。这表明Kimi Linear在保持优越性能的同时，在效率方面取得了重大突破。

Conclusion: Kimi Linear通过其创新的KDA模块和高效的DPLR算法，成功解决了长上下文处理中的效率和性能瓶颈，成为全注意力架构的一个有前途的替代方案。该模型在保持甚至超越现有模型性能的同时，大幅降低了计算和内存需求，为处理更长序列的任务提供了可行性。研究团队已开源KDA核、vLLM实现以及预训练和指令微调的模型检查点，以支持进一步的研究。

Abstract: We introduce Kimi Linear, a hybrid linear attention architecture that, for
the first time, outperforms full attention under fair comparisons across
various scenarios -- including short-context, long-context, and reinforcement
learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an
expressive linear attention module that extends Gated DeltaNet with a
finer-grained gating mechanism, enabling more effective use of limited
finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware
efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)
transition matrices, which substantially reduces computation compared to the
general DPLR formulation while remaining more consistent with the classical
delta rule.
  We pretrain a Kimi Linear model with 3B activated parameters and 48B total
parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention
(MLA). Our experiments show that with an identical training recipe, Kimi Linear
outperforms full MLA with a sizeable margin across all evaluated tasks, while
reducing KV cache usage by up to 75% and achieving up to 6 times decoding
throughput for a 1M context. These results demonstrate that Kimi Linear can be
a drop-in replacement for full attention architectures with superior
performance and efficiency, including tasks with longer input and output
lengths.
  To support further research, we open-source the KDA kernel and vLLM
implementations, and release the pre-trained and instruction-tuned model
checkpoints.

</details>


### [43] [The End of Manual Decoding: Towards Truly End-to-End Language Models](https://arxiv.org/abs/2510.26697)
*Zhichao Wang,Dongyang Ma,Xinting Huang,Deng Cai,Tian Lan,Jiahao Xu,Haitao Mi,Xiaoying Tang,Yan Wang*

Main category: cs.CL

TL;DR: LLM的“端到端”标记具有误导性，因为它们依赖于手动调整的不可微分解码过程。本文提出的AutoDeco通过学习控制自身的解码策略，实现了真正的“端到端”生成。AutoDeco在标准Transformer中增加了轻量级头部，可以在每个步骤动态预测上下文特定的温度和top-p值，以及下一个token的logits。这使得模型能够在单次前向传播中自我调节其采样策略。实验表明，AutoDeco在八个基准测试中显著优于默认解码策略，并且性能可与oracle调优基线相媲美。此外，AutoDeco还展现出指令控制解码的涌现能力，能够根据自然语言指令调整其解码策略，为可控和交互式LLM解码开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLMs）虽然在许多任务上表现出色，但其“端到端”的标签具有误导性。这是因为在实际应用中，LLMs依赖于一个不可微分的解码过程，而这个过程需要手动、细致地调整超参数（如temperature和top-p）。这种手动调整不仅耗时费力，而且难以找到最优解，限制了模型的性能和灵活性。因此，研究能够实现真正“端到端”生成，并能够自动优化解码策略的模型，具有重要的理论和实践意义。

Method: 本文提出了一种名为AutoDeco的新型架构，旨在实现真正的“端到端”生成。AutoDeco在标准Transformer模型的基础上进行了增强，增加了轻量级的预测头部。这些头部在模型的每个生成步骤中，能够动态地预测与上下文相关的temperature和top-p值，以及下一个token的logits。通过这种方式，AutoDeco将原本不可微分的解码过程转化为一个参数化、逐token的过程，使得模型能够在一次前向传播中自我调节其采样策略。这种设计允许模型根据当前的上下文信息，自适应地调整其生成的多样性和聚焦性。

Result: 通过在八个不同的基准测试上进行广泛的实验，AutoDeco被证明能够显著优于默认的解码策略。更重要的是，其性能与通过“测试集黑客”方法（一种实际的静态方法上限）得出的oracle调优基线相当。此外，研究还发现AutoDeco具有一项重要的涌现能力：能够根据自然语言指令（例如，“以低随机性生成”）来控制其解码过程。模型能够逐个token地解释这些指令，并相应地调整其预测的temperature和top-p值。

Conclusion: 本文提出的AutoDeco架构成功地解决了LLM解码过程中手动调优超参数的痛点，实现了真正的“端到端”生成。通过将解码策略的学习整合到模型本身，AutoDeco不仅提升了生成性能，而且展现出前所未有的指令控制能力，为开发更智能、更可控的LLM解码技术开辟了新的方向。尽管AutoDeco在多项基准测试中表现出色，但未来的工作可以进一步探索其在更广泛任务和模型规模上的适用性，以及对指令遵循机制的深入理解和优化。

Abstract: The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a
non-differentiable decoding process that requires laborious, hand-tuning of
hyperparameters like temperature and top-p. This paper introduces AutoDeco, a
novel architecture that enables truly "end-to-end" generation by learning to
control its own decoding strategy. We augment the standard transformer with
lightweight heads that, at each step, dynamically predict context-specific
temperature and top-p values alongside the next-token logits. This approach
transforms decoding into a parametric, token-level process, allowing the model
to self-regulate its sampling strategy within a single forward pass.
  Through extensive experiments on eight benchmarks, we demonstrate that
AutoDeco not only significantly outperforms default decoding strategies but
also achieves performance comparable to an oracle-tuned baseline derived from
"hacking the test set"-a practical upper bound for any static method.
Crucially, we uncover an emergent capability for instruction-based decoding
control: the model learns to interpret natural language commands (e.g.,
"generate with low randomness") and adjusts its predicted temperature and top-p
on a token-by-token basis, opening a new paradigm for steerable and interactive
LLM decoding.

</details>


### [44] [Value Drifts: Tracing Value Alignment During LLM Post-Training](https://arxiv.org/abs/2510.26707)
*Mehar Bhatia,Shravan Nayak,Gaurav Kamath,Marius Mosbach,Karolina Stańczak,Vered Shwartz,Siva Reddy*

Main category: cs.CL

TL;DR: 该研究调查了大型语言模型（LLM）在训练后阶段如何以及在哪个阶段实现与人类价值观的对齐。研究发现，监督微调（SFT）阶段通常奠定了模型的价值观基础，而后续的偏好优化很少能重新对齐这些价值观。此外，即使偏好数据相同，不同的偏好优化算法也会导致不同的价值观对齐结果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在社会中扮演越来越重要的角色，它们需要能够利用其通用知识并符合人类的价值观。因此，研究LLM与人类价值观的对齐变得至关重要。然而，以往的研究主要集中在评估已完全训练好的模型的对齐情况，忽略了模型在训练过程中学习价值观的动态过程。

Method: 本研究通过分析模型在训练后的动态过程，探究价值观对齐是如何以及在哪个阶段出现的。研究人员分离了训练后算法和数据集的影响，测量了训练过程中价值观漂移的幅度和时间。实验对象包括不同规模的Llama-3和Qwen-3模型，并使用了流行的监督微调（SFT）和偏好优化数据集及算法。此外，研究人员还利用了一个能够受控操纵价值观的合成偏好数据集。

Result: 研究结果表明，SFT阶段通常确立了模型的价值观，而随后的偏好优化很少能重新对齐这些价值观。即使在偏好数据相同的情况下，不同的偏好优化算法也会导致不同的价值观对齐结果。

Conclusion: 本研究为理解模型在训练后如何学习价值观提供了可操作的见解，有助于指导数据整理，并为改进模型与人类价值观的对齐选择模型和算法。

Abstract: As LLMs occupy an increasingly important role in society, they are more and
more confronted with questions that require them not only to draw on their
general knowledge but also to align with certain human value systems.
Therefore, studying the alignment of LLMs with human values has become a
crucial field of inquiry. Prior work, however, mostly focuses on evaluating the
alignment of fully trained models, overlooking the training dynamics by which
models learn to express human values. In this work, we investigate how and at
which stage value alignment arises during the course of a model's
post-training. Our analysis disentangles the effects of post-training
algorithms and datasets, measuring both the magnitude and time of value drifts
during training. Experimenting with Llama-3 and Qwen-3 models of different
sizes and popular supervised fine-tuning (SFT) and preference optimization
datasets and algorithms, we find that the SFT phase generally establishes a
model's values, and subsequent preference optimization rarely re-aligns these
values. Furthermore, using a synthetic preference dataset that enables
controlled manipulation of values, we find that different preference
optimization algorithms lead to different value alignment outcomes, even when
preference data is held constant. Our findings provide actionable insights into
how values are learned during post-training and help to inform data curation,
as well as the selection of models and algorithms for preference optimization
to improve model alignment to human values.

</details>


### [45] [AMO-Bench: Large Language Models Still Struggle in High School Math Competitions](https://arxiv.org/abs/2510.26768)
*Shengnan An,Xunliang Cai,Xuezhi Cao,Xiaoyu Li,Yehao Lin,Junlin Liu,Xinxuan Lv,Dan Ma,Xuanlin Wang,Ziwen Wang,Shuang Zhou*

Main category: cs.CL

TL;DR: AMO-Bench是一个包含50个人工设计的高难度数学问题（达到或超过国际数学奥林匹克水平）的基准测试集，旨在评估大型语言模型（LLMs）的高级数学推理能力。现有基准测试因模型性能饱和而效果减弱，AMO-Bench通过专家交叉验证和原创性问题来解决此问题。该基准测试仅需最终答案，便于自动评分。实验结果显示，即使是表现最佳的模型在AMO-Bench上的准确率也仅为52.4%，大多数模型低于40%，但测试时计算量增加显示出有希望的扩展趋势，表明LLMs在数学推理方面仍有巨大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在数学推理能力方面取得了显著进展，但现有的基准测试（如高中数学竞赛题）已不足以评估顶尖LLMs的能力，因为模型在该类测试上的性能已趋于饱和（例如AIME24/25）。这阻碍了对LLMs真实数学推理潜力的深入理解和进一步发展。因此，需要一个更具挑战性、更能区分模型能力的基准测试来推动LLMs在高级数学推理领域的发展。

Method: 本研究提出了AMO-Bench，一个包含50个高质量、人工精心设计的问题的基准测试集。这些问题均已通过专家交叉验证，确保其难度至少达到国际数学奥林匹克（IMO）水平。所有问题均为原创，以防止因数据记忆导致的性能虚高。AMO-Bench的特点是每个问题只需要提供最终答案，无需证明过程，这使得评估过程可以实现自动化和鲁棒性。研究人员在AMO-Bench上对26个不同的LLMs进行了实验评估。

Result: 在AMO-Bench基准测试上，对26个LLMs的实验结果显示，即使是表现最优的模型，其准确率也仅达到52.4%，而大多数模型的得分低于40%。尽管整体表现不佳，但进一步的分析揭示了一个积极的趋势：随着测试时计算量的增加，模型在AMO-Bench上的表现呈现出良好的扩展性。这表明通过增加计算资源，LLMs的数学推理能力有进一步提升的潜力。

Conclusion: AMO-Bench的提出为评估和推动LLMs在高级数学推理领域的发展提供了一个新的、更具挑战性的平台。目前的LLMs在处理IMO级别难度的数学问题时仍存在巨大差距，但实验结果显示出通过增加计算量可以提升模型性能，预示着未来研究的方向。AMO-Bench的发布将促进对LLMs推理能力提升的研究，尽管其局限性（如问题数量相对较少）有待进一步探索，但它为衡量和改进LLMs的数学智能奠定了重要基础。

Abstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with
Olympiad level or even higher difficulty, comprising 50 human-crafted problems.
Existing benchmarks have widely leveraged high school math competitions for
evaluating mathematical reasoning capabilities of large language models (LLMs).
However, many existing math competitions are becoming less effective for
assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To
address this, AMO-Bench introduces more rigorous challenges by ensuring all 50
problems are (1) cross-validated by experts to meet at least the International
Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original
problems to prevent potential performance leakages from data memorization.
Moreover, each problem in AMO-Bench requires only a final answer rather than a
proof, enabling automatic and robust grading for evaluation. Experimental
results across 26 LLMs on AMO-Bench show that even the best-performing model
achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.
Beyond these poor performances, our further analysis reveals a promising
scaling trend with increasing test-time compute on AMO-Bench. These results
highlight the significant room for improving the mathematical reasoning in
current LLMs. We release AMO-Bench to facilitate further research into
advancing the reasoning abilities of language models.
https://amo-bench.github.io/

</details>


### [46] [Gistify! Codebase-Level Understanding via Runtime Execution](https://arxiv.org/abs/2510.26790)
*Hyunji Lee,Minseon Kim,Chinmay Singh,Matheus Pereira,Atharv Sonwane,Isadora White,Elias Stengel-Eskin,Mohit Bansal,Zhengyan Shi,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan,Lucas Caccia*

Main category: cs.CL

TL;DR: Gistify是一个新的评估任务，旨在衡量代码LLM在大型代码库中提取和重现特定功能的“代码粘贴”能力。研究发现，现有最先进的模型在处理Gistify任务，尤其是具有长执行跟踪的任务时面临困难。


<details>
  <summary>Details</summary>
Motivation: 随着代码LLM在大型代码库中的应用越来越广泛，自动生成具有挑战性的、能在代码库层面进行评估的任务变得至关重要。现有的评估方法往往不足以全面衡量LLM在复杂代码环境中的实际能力。

Method: 提出Gistify任务，要求代码LLM在给定代码库和特定入口点的情况下，生成一个单一的、最小化的、可独立运行的文件，该文件能够复现指定命令在完整代码库中的输出。LLM可以访问整个代码库，但生成的文件必须只包含执行该命令所必需的核心组件。

Result: 实验结果表明，当前最先进的代码LLM在可靠地解决Gistify任务方面存在挑战，特别是在处理执行跟踪较长的任务时，它们的表现不尽如人意。

Conclusion: Gistify任务揭示了代码LLM在理解复杂代码库结构、精确建模执行流程以及生成大型代码补丁方面的局限性。这项研究为未来改进代码LLM的能力提供了方向，尤其是在处理大规模代码环境时。

Abstract: As coding agents are increasingly deployed in large codebases, the need to
automatically design challenging, codebase-level evaluation is central. We
propose Gistify, a task where a coding LLM must create a single, minimal,
self-contained file that can reproduce a specific functionality of a codebase.
The coding LLM is given full access to a codebase along with a specific
entrypoint (e.g., a python command), and the generated file must replicate the
output of the same command ran under the full codebase, while containing only
the essential components necessary to execute the provided command. Success on
Gistify requires both structural understanding of the codebase, accurate
modeling of its execution flow as well as the ability to produce potentially
large code patches. Our findings show that current state-of-the-art models
struggle to reliably solve Gistify tasks, especially ones with long executions
traces.

</details>


### [47] [Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning](https://arxiv.org/abs/2311.06736)
*Ying Su,Mingwen Liu,Zhijiang Guo*

Main category: cs.CL

TL;DR: LLM在逻辑推理中的应用仍面临挑战，尤其是在生成过程中。本文提出了一种结合对比学习的逐步解码方法，以减少LLM生成逻辑链时的常见错误，并在实验中证明了该方法的有效性，同时指出即使更大的LLM在生成严谨逻辑链方面仍有困难。


<details>
  <summary>Details</summary>
Motivation: 逻辑推理是人工智能的关键领域，尤其是在需要验证解释准确性的场景下，证明规划仍具挑战性。虽然LLM在自然语言证明规划方面取得了进展，但辅助方法增加了计算成本，且LLM生成过程本身的研究尚不充分。

Method: 提出了一种结合对比学习的逐步解码方法，以解决LLM生成器在解码过程中遇到的两个常见错误。通过使用增强的难负例对语言模型进行微调，以减轻这些解码错误。

Result: 实验结果表明，所提出的逐步解码和对比学习策略能有效减少LLM生成逻辑链时的常见错误。进一步分析发现，即使是更大的LLM在生成严谨的逻辑链方面仍然存在困难。

Conclusion: 本文提出的逐步解码和对比学习方法能够有效提升LLM在逻辑推理任务中的表现，尤其是在减少生成错误方面。然而，研究也揭示了大型LLM在生成高质量逻辑链方面仍需改进，这为未来的研究提供了方向。

Abstract: Logical reasoning is a pivotal component in the field of artificial
intelligence. Proof planning, particularly in contexts requiring the validation
of explanation accuracy, continues to present challenges. The recent
advancement of large language models (LLMs) has led to significant progress in
natural language proof planning, evolving from one-stage generators to more
complex three-stage systems that include additional searchers or verifiers.
While these assisted methods improve the quality of generated results, they
also introduce increased search efforts and computational costs. Furthermore,
the generative process itself remains underexplored. In this study, we propose
a stepwise decoding approach augmented by contrastive learning to address two
common errors encountered during the LLM generator's decoding process. We
fine-tune the language model using both vanilla and enhanced hard negatives to
mitigate these decoding errors. Empirical results demonstrate the effectiveness
of our strategy. Additionally, our further analysis reveals that even larger
LLMs still struggle to generate rigorous logical chains.

</details>


### [48] [The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks](https://arxiv.org/abs/2404.00176)
*Dominik Schlechtweg,Sachin Yadav,Nikolay Arefyev*

Main category: cs.CL

TL;DR: LSCD（词汇语义变化检测）是一个复杂的任务，通常需要经过词在语境中（WiC）和词义归纳（WSI）两个阶段。现有研究中存在的模型选项、任务定义、数据集版本、预处理选项和评估指标的多样性，使得模型评估、模型选择和结果复现变得困难。本文提供了一个标准化的 LSCD 评估基准库，通过透明实现，使得结果易于复现，并允许自由组合不同组件。该基准库支持对 WiC、WSI 和 LSCD 进行模型评估，便于对复杂模型组件进行细致评估和优化。研究人员利用该基准库进行了实验，并系统地改进了 LSCD 的当前技术水平。


<details>
  <summary>Details</summary>
Motivation: 现有的词汇语义变化检测（LSCD）任务在模型选项、任务定义、数据集、预处理和评估指标方面存在高度异质性，导致难以进行模型间的公平比较、选择最优模型组合以及复现研究结果。这种异质性阻碍了 LSCD 领域的进展。

Method: 本文构建了一个标准化的 LSCD 评估基准库，以解决现有研究中的异质性问题。该基准库通过透明的实现，确保了结果的可复现性，并允许用户自由组合不同的模型组件。该基准库支持对词在语境中（WiC）、词义归纳（WSI）和词汇语义变化检测（LSCD）这三个任务进行模型评估。研究人员利用该基准库对近期模型进行了一系列实验，旨在系统性地改进 LSCD 的当前技术水平。

Result: 研究人员利用所构建的基准库进行了一系列实验，并系统性地改进了 LSCD 的当前技术水平。虽然具体的性能提升数据和模型比较未在摘要中详述，但实验表明该基准库能够有效地促进模型优化和性能提升。

Conclusion: 本文通过构建一个标准化的 LSCD 评估基准库，有效地解决了 LSCD 领域中存在的异质性问题，使得模型评估更加公平、结果更易复现，并促进了不同模型组件的优化与组合。该基准库为 LSCD 领域的研究和发展提供了重要的支持。未来的工作可以基于该基准库进行更广泛的模型探索和性能提升。

Abstract: Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task,
which is usually operationalized based on two subsequently applied usage-level
tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages.
Then, these labels are represented in a graph on which Word Sense Induction
(WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by
comparing sense clusters over time. This modularity is reflected in most LSCD
datasets and models. It also leads to a large heterogeneity in modeling options
and task definitions, which is exacerbated by a variety of dataset versions,
preprocessing options and evaluation metrics. This heterogeneity makes it
difficult to evaluate models under comparable conditions, to choose optimal
model combinations or to reproduce results. Hence, we provide a benchmark
repository standardizing LSCD evaluation. Through transparent implementation
results become easily reproducible and by standardization different components
can be freely combined. The repository reflects the task's modularity by
allowing model evaluation for WiC, WSI and LSCD. This allows for careful
evaluation of increasingly complex model components providing new ways of model
optimization. We use the implemented benchmark to conduct a number of
experiments with recent models and systematically improve the state-of-the-art.

</details>


### [49] [Speak & Spell: LLM-Driven Controllable Phonetic Error Augmentation for Robust Dialogue State Tracking](https://arxiv.org/abs/2409.06263)
*Jihyun Lee,Solee Im,Wonjun Lee,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 为了解决语音识别错误对对话状态跟踪（DST）准确性的影响，本文提出了一种新的数据增强方法。该方法通过在关键词处引入语音相似的错误来提高DST模型的鲁棒性，特别是在嘈杂和低准确率的语音识别环境下，显著提升了DST的性能。


<details>
  <summary>Details</summary>
Motivation: 对话状态跟踪（DST）是任务导向对话系统中的关键组成部分，但其在实际应用中，尤其是在口语对话环境中，准确性会因自动语音识别（ASR）系统产生的命名实体错误而显著下降。这种不准确性会影响整个对话系统的性能，因此提高DST在存在ASR错误时的鲁棒性至关重要。

Method: 本文提出了一种简单但有效的数据增强方法，专门针对命名实体错误来提升DST模型的鲁棒性。该方法的创新之处在于能够通过关键词高亮提示来控制错误注入的位置，并引入语音上相似的错误。通过这种方式，模型能够学习到在关键词处存在语音识别错误时的应对策略。

Result: 实验结果表明，所提出的数据增强方法能够生成足够多的、针对关键词的错误模式。这使得DST模型在存在噪声和低准确率ASR的环境下，性能得到显著提升，验证了该方法的有效性。

Conclusion: 本研究成功开发了一种有效的数据增强技术，通过在关键词处引入语音相似的错误来提高对话状态跟踪（DST）模型在嘈杂和低准确率ASR环境下的鲁棒性。该方法简单易行，并且取得了显著的性能提升，为开发更可靠的对话系统提供了新的途径。未来的工作可以探索更多类型的错误注入以及在更复杂的对话场景中的应用。

Abstract: Dialogue State Tracking (DST) is a key part of task-oriented dialogue
systems, identifying important information in conversations. However, its
accuracy drops significantly in spoken dialogue environments due to named
entity errors from Automatic Speech Recognition (ASR) systems. We introduce a
simple yet effective data augmentation method that targets those entities to
improve the robustness of DST model. Our novel method can control the placement
of errors using keyword-highlighted prompts while introducing phonetically
similar errors. As a result, our method generated sufficient error patterns on
keywords, leading to improved accuracy in noised and low-accuracy ASR
environments.

</details>


### [50] [Language Model Preference Evaluation with Multiple Weak Evaluators](https://arxiv.org/abs/2410.12869)
*Zhengyu Hu,Jieyu Zhang,Zhihan Xiong,Alexander Ratner,Kaize Ding,Ranjay Krishna*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）的评估面临偏好排序的挑战。现有方法依赖单一LLM作为裁判，容易出现循环偏好（A>B, B>C, C>A）。本文提出PGED（Preference Graph Ensemble and Denoise）方法，使用多个LLM作为裁判构建偏好图，并通过集成和去噪处理，获得无环且一致的评估结果。PGED在模型排名、测试时扩展响应选择和微调数据选择方面表现优越，甚至能结合小型LLM（如Llama3-8B, Mistral-7B, Qwen2-7B）超越大型LLM（如Qwen2-72B），提高了评估的可靠性并改善了模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）取得了显著的成功，但对其输出质量进行偏好评估仍然是一个关键挑战。现有的工作通常采用一个强大的LLM作为裁判，通过成对比较来评估LLM的响应。然而，这种单一裁判的方法容易受到循环偏好的影响，即可能出现A优于B，B优于C，但C却优于A的情况，导致评估结果相互矛盾。这种不一致性阻碍了对LLM进行准确可靠的评估和排序。

Method: 本文提出的PGED（Preference Graph Ensemble and Denoise）方法，旨在解决单一裁判法导致的循环偏好问题。PGED首先利用多个基于模型的评估器（即多个LLM）来对LLM的输出进行成对比较，并基于这些比较结果构建偏好图。随后，PGED对这些图进行集成（ensemble）和去噪（denoise）处理，以消除矛盾，生成一个无环且一致的偏好结构。该框架提供了理论保证，证明了其能够有效地恢复真实的偏好结构。实验在十个基准测试上进行，验证了PGED在模型排名、测试时扩展响应选择和模型微调数据选择这三个应用场景中的优越性。

Result: 在十个基准测试上的广泛实验证明了PGED的有效性。在模型排名方面，PGED能够提供更准确和一致的模型排序。在测试时扩展响应选择方面，PGED能够更有效地选择在运行时进行扩展的响应。在模型微调数据选择方面，PGED能够帮助选择最有价值的数据用于模型的微调。特别值得注意的是，PGED通过结合多个小型LLM评估器（例如Llama3-8B、Mistral-7B、Qwen2-7B）的输出来进行评估，其整体表现超越了单个大型LLM评估器（例如Qwen2-72B），这有力地证明了PGED在提高评估可靠性方面的能力，并最终促进了模型性能的提升。

Conclusion: PGED通过集成多个LLM评估器的输出来构建和净化偏好图，成功解决了现有LLM评估方法中存在的循环偏好问题，提供了理论保证和广泛的实验验证。该方法在模型排名、响应选择和数据选择等多个LLM应用场景中展现出卓越性能，并且能够通过结合小型LLM来超越单一大型LLM，证明了其在提高评估鲁棒性和效率方面的潜力。未来的工作可以进一步探索不同规模和类型的模型评估器的集成策略，以及研究PGED在更复杂、更大规模的LLM评估任务中的应用。

Abstract: Despite the remarkable success of Large Language Models (LLMs), evaluating
their outputs' quality regarding preference remains a critical challenge. While
existing works usually leverage a strong LLM as the judge for comparing LLMs'
response pairwisely, such a single-evaluator approach is vulnerable to cyclic
preference, i.e., output A is better than B, B than C, but C is better than A,
causing contradictory evaluation results. To address this, we introduce PGED
(Preference Graph Ensemble and Denoise), a novel approach that leverages
multiple model-based evaluators to construct preference graphs, and then
ensembles and denoises these graphs for acyclic, non-contradictory evaluation
results. We provide theoretical guarantees for our framework, demonstrating its
efficacy in recovering the ground truth preference structure. Extensive
experiments on ten benchmarks demonstrate PGED 's superiority in three
applications: 1) model ranking for evaluation, 2) response selection for
test-time scaling, and 3) data selection for model fine-tuning. Notably, PGED
combines small LLM evaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to
outperform strong ones (e.g., Qwen2-72B), showcasing its effectiveness in
enhancing evaluation reliability and improving model performance.

</details>


### [51] [This Candidate is [MASK]. Prompt-based Sentiment Extraction and Reference Letters](https://arxiv.org/abs/2410.16325)
*Fabian Slonimczyk*

Main category: cs.CL

TL;DR: 提出一种名为“提示驱动的情感提取”的新方法，该方法无需预处理或微调即可从文本中提取情感，并将其应用于参考信分析，发现情感内容与求职市场结果相关，情感离散度对候选人表现有负面影响。


<details>
  <summary>Details</summary>
Motivation: 传统的经济和金融领域文本分析方法存在局限性，例如需要预处理和标注数据，并且难以捕捉文本的细微情感差别。本研究旨在提出一种更简单、更有效的情感提取方法，并探索情感内容对求职市场结果的影响，以及性别因素在其中扮演的角色。

Method: 提出一种“提示驱动的情感提取”方法，该方法直接使用预训练的大型语言模型（LLMs），通过设计特定的提示来提取文本的情感分数。该方法不需要对文本进行预处理或对模型进行微调。将此方法应用于收集的参考信语料库，并与其他情感分析方法（如词袋模型、微调语言模型和高级聊天机器人查询）进行比较。

Result: 研究发现，参考信中的平均情感分数与候选人的求职市场表现显著相关，更高的情感分数对应更好的结果。情感离散度（信件撰写者之间的情感分歧）对候选人的表现有负面影响。与词袋模型、微调语言模型和聊天机器人查询等方法相比，“提示驱动的情感提取”方法在解释求职市场结果方面表现更优，且能完全重现研究结果。此外，研究发现针对女性候选人的推荐信更侧重于“努力型”特质，而针对男性候选人的推荐信则更侧重于“突出型”特质，这种性别差异对女性的求职市场结果产生负面影响。

Conclusion: “提示驱动的情感提取”方法是一种有效且无需微调的情感分析工具，可应用于经济和金融领域的文本数据。研究结果表明，参考信中的情感内容，包括其离散度和性别差异，对候选人的求职市场结果有重要影响。未来的研究可以进一步探索该方法在其他领域的应用，并深入研究性别差异对职业发展的影响。

Abstract: I propose a relatively simple way to deploy pre-trained large language models
(LLMs) in order to extract sentiment and other useful features from text data.
The method, which I refer to as prompt-based sentiment extraction, offers
multiple advantages over other methods used in economics and finance. In
particular, it accepts the text input as is (without pre-processing) and
produces a sentiment score that has a probability interpretation. Unlike other
LLM-based approaches, it does not require any fine-tuning or labeled data. I
apply my prompt-based strategy to a hand-collected corpus of confidential
reference letters (RLs). I show that the sentiment contents of RLs are clearly
reflected in job market outcomes. Candidates with higher average sentiment in
their RLs perform markedly better regardless of the measure of success chosen.
Moreover, I show that sentiment dispersion among letter writers negatively
affects the job market candidate's performance. I compare my sentiment
extraction approach to other commonly used methods for sentiment analysis:
`bag-of-words' approaches, fine-tuned language models, and querying advanced
chatbots. No other method can fully reproduce the results obtained by
prompt-based sentiment extraction. Finally, I slightly modify the method to
obtain `gendered' sentiment scores (as in Eberhardt et al., 2023). I show that
RLs written for female candidates emphasize `grindstone' personality traits,
whereas male candidates' letters emphasize `standout' traits. These gender
differences negatively affect women's job market outcomes.

</details>


### [52] [Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data](https://arxiv.org/abs/2502.04380)
*Zhenqing Ling,Daoyuan Chen,Liuyi Yao,Qianli Shen,Yaliang Li,Ying Shen*

Main category: cs.CL

TL;DR: 本研究提出了一种新颖的方法，通过构建对比数据池和引入“双重身份”的语言模型来解决在数据标签不确定时微调大型语言模型（LLMs）的挑战。该方法利用LLM自身来评估和选择多样性数据，然后用选出的数据进行微调，显著提升了模型在多领域和下游任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，微调大型语言模型（LLMs）需要多样化的数据集，但现有方法在处理标签缺失、不精确或未标准化的数据时存在困难。基于数据混合比例的模型难以处理这些不确定性，而基于数据选择的方法则难以平衡多领域性能。因此，如何有效利用多样性数据来提升LLM的整体能力是一个重要问题。

Method: 本研究首先通过经验性地构建对比数据池来探索数据多样性对LLM能力的影响，并从理论上进行解释。在此基础上，提出了一种新方法，赋予LLM“双重身份”：一个作为输出模型，通过多样性奖励来评估和选择数据；另一个作为输入模型，利用选出的数据进行微调。

Result: 实验结果表明，所提出的新方法在处理领域不确定的数据以及一系列基础下游任务时，能够显著提升不同先进LLM的性能。

Conclusion: 该研究提出了一种创新的数据选择和模型微调方法，有效解决了在数据标签不确定情况下提升LLM性能的问题。通过让LLM扮演数据选择者的角色，实现了数据与模型的协同设计，为理解数据多样性的作用以及未来LLM的开发提供了新的思路。研究代码已公开，有望推动相关领域的研究进展。

Abstract: Fine-tuning large language models (LLMs) using diverse datasets is crucial
for enhancing their overall performance across various domains. In practical
scenarios, existing methods based on modeling the mixture proportions of data
composition often struggle with data whose domain labels are missing, imprecise
or non-normalized, while methods based on data selection usually encounter
difficulties in balancing multi-domain performance. To address these
challenges, in this work, we investigate the role of data diversity in
enhancing the overall abilities of LLMs by empirically constructing contrastive
data pools and theoretically deriving explanations. Building upon the insights
gained, we propose a new method that gives the LLM a dual identity: an output
model to cognitively probe and select data based on diversity reward, as well
as an input model to be tuned with the selected data. Extensive experiments
show that the proposed method notably boosts performance across
domain-undetermined data and a series of foundational downstream tasks when
applied to various advanced LLMs. We release our code and hope this study can
shed light on the understanding of data diversity and advance feedback-driven
data-model co-design for LLMs.

</details>


### [53] [Unstructured Evidence Attribution for Long Context Query Focused Summarization](https://arxiv.org/abs/2502.14409)
*Dustin Wright,Zain Muhammad Mujahid,Lu Wang,Isabelle Augenstein,David Jurgens*

Main category: cs.CL

TL;DR: 该研究提出了一种从长文本中提取非结构化证据以改进大型语言模型（LLMs）摘要的方法，并构建了一个名为 SUnsET 的新数据集来训练模型。实验证明，使用 SUnsET 训练的 LLMs 能够生成更相关、更一致、引用证据来源更广泛的摘要，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）虽然能从长文本生成连贯的摘要，并在摘要中引用证据，但现有的证据引用方法仅限于固定粒度（如句子、段落等），这可能导致证据不够相关或不一致。本文旨在解决这个问题，通过提取任意长度的非结构化证据片段，来提高摘要的相关性和事实一致性，并解决证据在长文本中“遗失在中间”的问题。

Method: 本研究提出了一种提取非结构化证据（即任意长度的文本片段）的方法，以克服固定粒度证据提取的局限性。为了训练 LLMs 执行此任务，研究者创建了一个名为 SUnsET（Summaries with Unstructured Evidence Text）的合成数据集，该数据集是通过一个新颖的生成流程构建的。研究团队在 5 种不同的 LLMs 和 4 种涵盖人类书写、合成、单文档和多文档设置的数据集上进行了实验。

Result: 实验结果表明，经过 SUnsET 数据集微调的 LLMs 在生成摘要时，能够提取到更相关、更事实一致的证据。与未进行微调或仅使用固定粒度证据的基线模型相比，这些 LLMs 能够从更广泛的上下文中提取证据，并生成更具相关性和一致性的摘要。

Conclusion: 本研究成功地提出了一种通过提取非结构化证据来改进 LLMs 摘要的方法，并开发了 SUnsET 数据集来支持此任务。实验证明了该方法的有效性，表明非结构化证据提取能够提升摘要的质量和可信度。研究者将 SUnsET 数据集和代码公之于众，以促进未来在此领域的研究。

Abstract: Large language models (LLMs) are capable of generating coherent summaries
from very long contexts given a user query, and extracting and citing evidence
spans helps improve the trustworthiness of these summaries. Whereas previous
work has focused on evidence citation with fixed levels of granularity (e.g.
sentence, paragraph, document, etc.), we propose to extract unstructured (i.e.,
spans of any length) evidence in order to acquire more relevant and consistent
evidence than in the fixed granularity case. We show how existing systems
struggle to copy and properly cite unstructured evidence, which also tends to
be "lost-in-the-middle". To help models perform this task, we create the
Summaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset
generated using a novel pipeline, which can be used as training supervision for
unstructured evidence summarization. We demonstrate across 5 LLMs and 4
datasets spanning human written, synthetic, single, and multi-document settings
that LLMs adapted with SUnsET generate more relevant and factually consistent
evidence with their summaries, extract evidence from more diverse locations in
their context, and can generate more relevant and consistent summaries than
baselines with no fine-tuning and fixed granularity evidence. We release SUnsET
and our generation code to the public.

</details>


### [54] [More of the Same: Persistent Representational Harms Under Increased Representation](https://arxiv.org/abs/2503.00333)
*Jennifer Mickel,Maria De-Arteaga,Leqi Liu,Kevin Tian*

Main category: cs.CL

TL;DR: 生成式AI在职业性别代表性方面存在代表性偏差，尽管女性代表性有所提高，但刻板印象和压迫性观念依然存在。


<details>
  <summary>Details</summary>
Motivation: 为了识别和减轻生成式AI系统的危害，必须考虑其输出中人物的代表性和被代表的方式。仅仅提高代表性是不够的，还需要解决代表方式中存在的偏见。

Method: 通过检查最先进的大型语言模型在职业性别代表性方面的情况，并分析不同性别在统计学上的词语差异。

Result: 研究发现，随着时间的推移，模型干预措施改变了性别分布，女性在生成传记或个人简介时比男性更受欢迎。然而，不同性别在代表方式上仍然存在偏差，例如词语使用上的统计学显著差异，这会强化现有的压迫体系。

Conclusion: 尽管在提高女性代表性方面已做出努力，但生成式AI在如何代表不同性别方面仍然存在偏见，表现为刻板印象和新自由主义理想的传播。未来的研究应继续关注解决这些更深层次的代表性偏差问题。

Abstract: To recognize and mitigate the harms of generative AI systems, it is crucial
to consider who is represented in the outputs of generative AI systems and how
people are represented. A critical gap emerges when naively improving who is
represented, as this does not imply bias mitigation efforts have been applied
to address how people are represented. We critically examined this by
investigating gender representation in occupation across state-of-the-art large
language models. We first show evidence suggesting that over time there have
been interventions to models altering the resulting gender distribution, and we
find that women are more represented than men when models are prompted to
generate biographies or personas. We then demonstrate that representational
biases persist in how different genders are represented by examining
statistically significant word differences across genders. This results in a
proliferation of representational harms, stereotypes, and neoliberalism ideals
that, despite existing interventions to increase female representation,
reinforce existing systems of oppression.

</details>


### [55] [Improving LLM Safety Alignment with Dual-Objective Optimization](https://arxiv.org/abs/2503.03710)
*Xuandong Zhao,Will Cai,Tianneng Shi,David Huang,Licong Lin,Song Mei,Dawn Song*

Main category: cs.CL

TL;DR: 现有的训练时安全对齐技术（包括直接偏好优化DPO）在防御越狱攻击方面存在局限性。本文提出了一种改进的安全对齐方法，通过梯度分析识别并解决了DPO在拒绝学习方面的不足。该方法将DPO目标分解为鲁棒拒绝训练和有害知识的定向学习，并通过引入基于奖励的token级加权机制来增强关键拒绝token的权重。实验证明，该方法能显著提高LLM在各种分布内和分布外场景下抵御多种越狱攻击的能力，并揭示了鲁棒性与训练过程中的token分布偏移及内部表征的关联性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM训练时安全对齐技术，如DPO，在面对越狱攻击时表现出脆弱性，其损失函数在拒绝学习方面存在不足。这导致模型在面对精心设计的提示时，仍然可能生成不安全的内容，威胁到LLM的安全应用。

Method: 本文首先通过梯度分析，揭示了DPO在拒绝学习上的局限性。随后，提出了一种新的安全对齐方法，将DPO目标分解为“鲁棒拒绝训练”（即使生成部分不安全内容也强制模型拒绝）和“有害知识定向学习”两个部分。此外，引入了一个基于奖励的token级加权机制，以加强对关键拒绝token的关注，从而提高模型对对抗性攻击的鲁棒性。实验在多种越狱攻击（包括预填、后缀和多轮攻击）和不同分布场景下进行了评估。

Result: 提出的方法显著提高了LLM对抗越狱攻击的鲁棒性，覆盖了多种攻击类型和场景。实验表明，该方法在抵御攻击方面优于现有的DPO方法。研究还发现，模型对越狱攻击的鲁棒性与训练过程中的token分布偏移以及拒绝/有害token的内部表征存在相关性。

Conclusion: 本文提出了一种改进的LLM安全对齐方法，通过分解DPO目标和引入token级加权机制，有效增强了模型对越狱攻击的防御能力。研究结果不仅在实践中提升了LLM的安全性，还为未来LLM安全对齐的研究指明了方向，特别是token分布和内部表征在鲁棒性中的作用。

Abstract: Existing training-time safety alignment techniques for large language models
(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization
(DPO), a widely deployed alignment method, exhibits limitations in both
experimental and theoretical contexts as its loss function proves suboptimal
for refusal learning. Through gradient-based analysis, we identify these
shortcomings and propose an improved safety alignment that disentangles DPO
objectives into two components: (1) robust refusal training, which encourages
refusal even when partial unsafe generations are produced, and (2) targeted
unlearning of harmful knowledge. This approach significantly increases LLM
robustness against a wide range of jailbreak attacks, including prefilling,
suffix, and multi-turn attacks across both in-distribution and
out-of-distribution scenarios. Furthermore, we introduce a method to emphasize
critical refusal tokens by incorporating a reward-based token-level weighting
mechanism for refusal learning, which further improves the robustness against
adversarial exploits. Our research also suggests that robustness to jailbreak
attacks is correlated with token distribution shifts in the training process
and internal representations of refusal and harmful tokens, offering valuable
directions for future research in LLM safety alignment. The code is available
at https://github.com/wicai24/DOOR-Alignment

</details>


### [56] [Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models](https://arxiv.org/abs/2504.01001)
*José Pombal,Nuno M. Guerreiro,Ricardo Rei,André F. T. Martins*

Main category: cs.CL

TL;DR: 语言模型在跨模态复杂任务中的评估面临挑战，传统方法（如特定任务指标和人工标注测试集）成本高且效率低。本文提出了零样本基准测试（ZSB）框架，利用语言模型自动生成合成测试数据和评估，解决了这些问题。ZSB具有简单、灵活、可扩展和模型无关的特点，能够应对数据收集困难的任务和语言。通过在五种文本任务和一种多模态任务（包括四种语言的通用能力、翻译和视觉-语言能力）上创建基准测试，并对一系列模型进行排名，ZSB的排名与人类排名高度相关，优于现有标准基准测试。研究还表明，开放模型也能创建强大的基准测试，且评判模型的大小和数据集的多样性是性能的关键驱动因素。研究团队发布了所有基准测试、代码和复现实验的方法。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能力的提升，尤其是处理跨模态复杂任务的能力增强，自动评估这些模型变得日益困难。传统的评估方法，如开发强大的特定任务自动评估指标，以及构建人工标注的测试集，都存在固有的挑战。自动指标的开发滞后于模型能力的快速发展，而人工标注的测试集成本高昂且容易饱和。因此，需要一种更有效、可扩展且经济的替代方案来自动化测试数据的创建和评估过程。

Method: 本文提出了零样本基准测试（ZSB）框架，该框架利用语言模型来自动化测试数据的生成和评估过程。ZSB的核心在于设计两个关键的提示（prompt）：一个用于生成合成测试数据，另一个用于评估模型的性能。该框架具有高度的灵活性和可扩展性，能够适应数据收集成本高昂或不切实际的任务和语言，并且不依赖于特定的模型（模型无关）。为了验证ZSB的有效性，研究人员在五种纯文本任务和一种多模态任务上构建了基准测试，涵盖了四种语言（英语、中文、法语、韩系）的通用能力、翻译任务以及英语的视觉-语言能力。随后，他们使用这些基准测试对一系列开源和闭源系统进行了排名。

Result: 通过在多个任务和语言上进行的实验，ZSB框架显示出其强大的有效性。研究结果表明，使用ZSB生成的基准测试对模型进行的排名与人类评估的排名具有高度的一致性，并且在评估性能上优于广泛使用的标准基准测试。此外，通过消融研究，研究发现即使使用开放模型也能创建出高质量的基准测试。评判模型的大小和数据集的多样性被确定为影响ZSB性能的关键因素。

Conclusion: 零样本基准测试（ZSB）框架为语言模型和多模态模型的自动评估提供了一种创新且有效的解决方案。它通过利用语言模型自动生成高质量的测试数据和评估指标，克服了传统评估方法的局限性，如成本高、效率低和易饱和等问题。ZSB框架的灵活性、可扩展性和模型无关性使其能够广泛应用于各种任务和语言，尤其是在数据稀疏的场景下。研究结果证明了ZSB排名的可靠性，并超越了现有标准基准测试。未来的工作可以进一步探索ZSB在更多模态、更复杂任务上的应用，以及优化提示工程以获得更精细的评估。研究团队已将所有基准测试和代码开源，以促进相关研究和应用的发展。

Abstract: As language models improve and become capable of performing more complex
tasks across modalities, evaluating them automatically becomes increasingly
challenging. Developing strong and robust task-specific automatic metrics gets
harder, and human-annotated test sets -- which are expensive to create --
saturate more quickly. A compelling alternative is to design reliable
strategies to automate the creation of test data and evaluation, but previous
attempts either rely on pre-existing data, or focus solely on individual tasks.
We present Zero-shot Benchmarking (ZSB), a framework for creating high-quality
benchmarks for any task by leveraging language models for both synthetic test
data creation and evaluation. ZSB is simple and flexible: it requires only the
creation of a prompt for data generation and one for evaluation; it is scalable
to tasks and languages where collecting real-world data is costly or
impractical; it is model-agnostic, allowing the creation of increasingly
challenging benchmarks as models improve. To assess the effectiveness of our
framework, we create benchmarks for five text-only tasks and a multi-modal one:
general capabilities in four languages (English, Chinese, French, and Korean),
translation, and general vision-language capabilities in English. We then rank
a broad range of open and closed systems on our benchmarks. ZSB rankings
consistently correlate strongly with human rankings, outperforming
widely-adopted standard benchmarks. Through ablations, we find that strong
benchmarks can be created with open models, and that judge model size and
dataset variety are crucial drivers of performance. We release all our
benchmarks, and code to reproduce our experiments and to produce new
benchmarks.

</details>


### [57] [M-Prometheus: A Suite of Open Multilingual LLM Judges](https://arxiv.org/abs/2504.04953)
*José Pombal,Dongkeun Yoon,Patrick Fernandes,Ian Wu,Seungone Kim,Ricardo Rei,Graham Neubig,André F. T. Martins*

Main category: cs.CL

TL;DR: 现有的大多数语言模型评估者（LLM-as-a-judge）都只针对英语进行了优化，忽略了多语言评估能力的提升。为了解决这一问题，我们提出了M-Prometheus，一个包含3B到14B参数的开源LLM评估者套件，它能够对多语言输出提供直接评估和成对比较反馈。M-Prometheus在超过20种语言的多语言奖励基准测试以及4种语言对的文学机器翻译评估中均优于现有的开源LLM评估者。此外，M-Prometheus在解码时能够显著提升3种测试语言的生成输出质量，证明了其在开发更优多语言模型方面的实用性。通过详细的消融实验，我们确定了有效的多语言评估者的关键因素，包括主干模型的选择以及使用合成多语言反馈数据而非翻译数据进行训练。我们已公开模型、训练数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 目前，语言模型（LLM）在自动评估长文本方面的应用日益广泛，但现有的大多数LLM评估者都仅针对英语进行了优化。对于非英语语言，自动评估方法的质量存在显著差异，这阻碍了具备更优多语言能力模型的发展。因此，提升LLM在多语言场景下的评估能力，缩小评估方法的差距，具有重要的研究意义和实际需求。

Method: 我们提出了M-Prometheus，一个包含3B到14B参数的开源LLM评估者套件。该套件能够提供对多语言输出的直接评估和成对比较反馈。在实验中，我们将M-Prometheus在超过20种语言的多语言奖励基准测试以及4种语言对的文学机器翻译评估中与最先进的开源LLM评估者进行了比较。此外，我们还利用M-Prometheus在解码时提升了3种测试语言的生成输出质量。通过广泛的消融实验，我们探究了主干模型选择和使用合成多语言反馈数据（而非翻译数据）进行训练对提升评估者效果的关键影响。

Result: M-Prometheus在多语言奖励基准测试（涵盖20多种语言）和文学机器翻译评估（涵盖4个语言对）中均超越了最先进的开源LLM评估者。在实际应用中，M-Prometheus在解码时能够显著提高3种测试语言的生成输出质量。消融实验表明，主干模型的选择和使用合成多语言反馈数据进行训练是构建高效多语言评估者的关键因素。

Conclusion: M-Prometheus成功解决了现有LLM评估者在多语言场景下的局限性，提供了一套性能优越且开源的评估工具。该研究不仅在多语言评估基准和机器翻译任务上取得了显著成果，还证明了M-Prometheus在实际应用中提升模型生成质量的潜力。未来的工作可以进一步探索M-Prometheus在更多语言和任务上的应用，并持续优化其评估能力。我们希望M-Prometheus的发布能够推动多语言模型的研究和发展。

Abstract: The use of language models for automatically evaluating long-form text
(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are
optimized exclusively for English, with strategies for enhancing their
multilingual evaluation capabilities remaining largely unexplored in the
current literature. This has created a disparity in the quality of automatic
evaluation methods for non-English languages, ultimately hindering the
development of models with better multilingual capabilities. To bridge this
gap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from
3B to 14B parameters that can provide both direct assessment and pairwise
comparison feedback on multilingual outputs. M-Prometheus models outperform
state-of-the-art open LLM judges on multilingual reward benchmarks spanning
more than 20 languages, as well as on literary machine translation (MT)
evaluation covering 4 language pairs. Furthermore, M-Prometheus models can be
leveraged at decoding time to significantly improve generated outputs across
all 3 tested languages, showcasing their utility for the development of better
multilingual models. Lastly, through extensive ablations, we identify the key
factors for obtaining an effective multilingual judge, including backbone model
selection and training on synthetic multilingual feedback data instead of
translated data. We release our models, training dataset, and code.

</details>


### [58] [SEA-LION: Southeast Asian Languages in One Network](https://arxiv.org/abs/2504.05747)
*Raymond Ng,Thanh Ngan Nguyen,Yuli Huang,Ngee Chia Tai,Wai Yi Leong,Wei Qi Leong,Xianbin Yong,Jian Gang Ngui,Yosephine Susanto,Nicholas Cheng,Hamsawardhini Rengarajan,Peerat Limkonchotiwat,Adithya Venkatadri Hulagadri,Kok Wai Teng,Yeo Yeow Tong,Bryan Siow,Wei Yi Teo,Wayne Lau,Choon Meng Tan,Brandon Ong,Zhi Hao Ong,Jann Railey Montalan,Adwin Chan,Sajeban Antonyrex,Ren Lee,Esther Choa,David Ong Tat-Wee,Bing Jie Darius Liu,William Chandra Tjhi,Erik Cambria,Leslie Teo*

Main category: cs.CL

TL;DR: 该研究提出了 Llama-SEA-LION-v3-8B-IT 和 Gemma-SEA-LION-v3-9B-IT 两个针对东南亚（SEA）语言的多语言大型语言模型（LLM），以解决当前 LLM 研究以英语为中心的表示差距问题。这两个模型支持 11 种东南亚语言，并通过大规模多语言持续预训练、指令微调、对齐和模型合并等多个阶段的训练实现。在多语言基准测试上的评估结果表明，这些模型在支持东南亚语言的 LLM 中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）的研究和开发主要集中在英语，导致东南亚（SEA）等低资源语言的代表性不足。这种表示差距阻碍了这些语言在人工智能领域的应用和发展，限制了 LLM 的普惠性。

Method: 该研究采用了大规模多语言持续预训练的方法，并结合了全面的训练后处理，包括多阶段的指令微调、对齐和模型合并。具体来说，他们推出了 Llama-SEA-LION-v3-8B-IT 和 Gemma-SEA-LION-v3-9B-IT 这两个模型，以支持包括英语、中文、印尼语、越南语、马来语、泰语、缅甸语、老挝语、菲律宾语、泰米尔语和高棉语在内的 11 种东南亚语言。

Result: 在多语言基准测试上的评估结果显示，SEA-LION 系列模型在支持东南亚语言的 LLM 中达到了最先进的性能。

Conclusion: 该研究成功开发了支持 11 种东南亚语言的先进多语言 LLM，弥合了低资源语言在 AI 领域的表示差距，并在相关基准测试中取得了领先地位。模型已开源，以促进东南亚社区的广泛应用和发展。

Abstract: Recently, Large Language Models (LLMs) have dominated much of the artificial
intelligence scene with their ability to process and generate natural
languages. However, the majority of LLM research and development remains
English-centric, leaving low-resource languages such as those in the Southeast
Asian (SEA) region under-represented. To address this representation gap, we
introduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge
multilingual LLMs designed for SEA languages. The SEA-LION family of LLMs
supports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,
Malay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages
large-scale multilingual continued pre-training with a comprehensive
post-training regime involving multiple stages of instruction fine-tuning,
alignment, and model merging. Evaluation results on multilingual benchmarks
indicate that our models achieve state-of-the-art performance across LLMs
supporting SEA languages. We open-source the models to benefit the wider SEA
community.

</details>


### [59] [Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2504.11331)
*Hao Liu,Lijun He,Jiaxi Liang,Zhihan Ren,Haixia Bi,Fan Li*

Main category: cs.CL

TL;DR: DASCO是一个新颖的多模态方面级情感分析框架，它利用依赖解析树来增强情感推理能力。该框架通过多任务预训练策略有效解决了情感线索感知（SCP）和多模态信息不对齐（MIM）问题，并通过结合句法和语义分支来过滤噪声，解决了语义噪声消除（SNE）问题。DASCO在两个基准数据集上的实验结果表明，其在MABSA任务上取得了最先进的性能，并在JMASA数据集上取得了显著的F1和精确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态方面级情感分析（MABSA）方法在同时处理情感线索感知（SCP）、多模态信息不对齐（MIM）和语义噪声消除（SNE）这三个核心挑战时存在不足。这些挑战阻碍了从图像-文本对中提取细粒度情感信息以识别方面词并确定其情感极性的能力。因此，需要一种能够有效解决这些问题的先进方法。

Method: DASCO框架采用了一种创新的方法来解决MABSA中的核心挑战。首先，通过结合方面导向增强、图像-文本匹配和方面级情感敏感认知，设计了一种多任务预训练策略，以提升基础模型对方面词和情感线索的感知能力，并实现有效的图像-文本对齐，从而解决SCP和MIM问题。其次，DASCO整合了依赖解析树，将句法分支与语义分支相结合，引导模型在目标特定的范围内选择性地关注关键上下文元素，同时有效过滤掉不相关的噪声，以解决SNE问题。

Result: DASCO在两个基准数据集上的广泛实验证明了其在MABSA任务上的优越性。具体来说，在JMASA数据集上，DASCO在Twitter2015子任务上取得了+2.3% F1分数和+3.5%精确率的显著提升，超过了现有的最先进水平。这些结果表明DASCO在提高MABSA性能方面非常有效。

Conclusion: DASCO框架通过集成依赖解析树，成功地解决了多模态方面级情感分析中的情感线索感知、多模态信息不对齐和语义噪声消除等关键挑战。该框架在标准数据集上取得了最先进的性能，证明了其有效性和实用性。未来研究可以进一步探索 DASCO 在其他多模态情感分析任务上的应用，或研究更复杂的依赖结构来进一步优化性能。

Abstract: Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract
fine-grained information from image-text pairs to identify aspect terms and
determine their sentiment polarity. However, existing approaches often fall
short in simultaneously addressing three core challenges: Sentiment Cue
Perception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise
Elimination (SNE). To overcome these limitations, we propose DASCO
(\textbf{D}ependency Structure \textbf{A}ugmented \textbf{Sco}ping Framework),
a fine-grained scope-oriented framework that enhances aspect-level sentiment
reasoning by leveraging dependency parsing trees. First, we designed a
multi-task pretraining strategy for MABSA on our base model, combining
aspect-oriented enhancement, image-text matching, and aspect-level
sentiment-sensitive cognition. This improved the model's perception of aspect
terms and sentiment cues while achieving effective image-text alignment,
addressing key challenges like SCP and MIM. Furthermore, we incorporate
dependency trees as syntactic branch combining with semantic branch, guiding
the model to selectively attend to critical contextual elements within a
target-specific scope while effectively filtering out irrelevant noise for
addressing SNE problem. Extensive experiments on two benchmark datasets across
three subtasks demonstrate that DASCO achieves state-of-the-art performance in
MABSA, with notable gains in JMASA (+2.3\% F1 and +3.5\% precision on
Twitter2015). The source code is available at https://github.com/LHaoooo/DASCO .

</details>


### [60] [ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models](https://arxiv.org/abs/2505.13444)
*Liyan Tang,Grace Kim,Xinyu Zhao,Thom Lake,Wenxuan Ding,Fangcong Yin,Prasann Singhal,Manya Wadhwa,Zeyu Leo Liu,Zayne Sprague,Ramya Namuduri,Bodun Hu,Juan Diego Rodriguez,Puyuan Peng,Greg Durrett*

Main category: cs.CL

TL;DR: Chart understanding requires both textual and visual reasoning, but current large vision-language models (LVLMs) struggle with the visual aspect. This paper introduces ChartMuseum, a new benchmark that reveals a significant gap between LVLM and human performance, especially on visually complex questions. The study highlights the need for improved visual reasoning in LVLMs.


<details>
  <summary>Details</summary>
Motivation: Existing large vision-language models (LVLMs) perform well on tasks requiring textual reasoning but fall short in chart understanding due to an imbalance between textual and visual reasoning capabilities. This limitation hinders their real-world applicability in scenarios involving complex charts and data visualization, where robust visual reasoning is crucial. The paper aims to highlight this deficiency and provide a means to measure and improve it.

Method: The researchers first conducted a case study using a synthetic dataset solvable only through visual reasoning to demonstrate the performance degradation of LVLMs with increasing visual complexity. They then introduced ChartMuseum, a new benchmark comprising 1,162 expert-annotated questions from 184 real-world chart sources, designed to evaluate complex visual and textual reasoning. Performance was evaluated by comparing state-of-the-art models (Gemini-2.5-Pro and Qwen2.5-VL-72B-Instruct) against human performance on this benchmark. An error analysis was performed to identify specific visual reasoning challenges for LVLMs.

Result: The case study showed that LVLM performance significantly degrades with increasing visual complexity, unlike human performance. ChartMuseum revealed a substantial gap between the best-performing model (Gemini-2.5-Pro at 63.0% accuracy) and human performance (93% accuracy). Leading open-source models like Qwen2.5-VL-72B-Instruct achieved even lower accuracy (38.5%). Notably, models experienced a 35%-55% performance drop on questions requiring primarily visual reasoning compared to text-reasoning-heavy questions. The qualitative error analysis identified specific visual reasoning categories that are particularly challenging for current LVLMs.

Conclusion: Current LVLMs exhibit a significant weakness in visual reasoning for chart understanding, as demonstrated by their underperformance on the new ChartMuseum benchmark compared to humans. This highlights a critical gap that needs addressing for practical LVLM deployment in data analysis and interpretation tasks. Future work should focus on developing LVLMs with enhanced visual reasoning capabilities to bridge this performance gap and improve their robustness on complex, visually-driven tasks.

Abstract: Chart understanding presents a unique challenge for large vision-language
models (LVLMs), as it requires the integration of sophisticated textual and
visual reasoning capabilities. However, current LVLMs exhibit a notable
imbalance between these skills, falling short on visual reasoning that is
difficult to perform in text. We conduct a case study using a synthetic dataset
solvable only through visual reasoning and show that model performance degrades
significantly with increasing visual complexity, while human performance
remains robust. We then introduce ChartMuseum, a new Chart Question Answering
(QA) benchmark containing 1,162 expert-annotated questions spanning multiple
reasoning types, curated from real-world charts across 184 sources,
specifically built to evaluate complex visual and textual reasoning. Unlike
prior chart understanding benchmarks -- where frontier models perform similarly
and near saturation -- our benchmark exposes a substantial gap between model
and human performance, while effectively differentiating model capabilities:
although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro
attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct
achieves only 38.5%. Moreover, on questions requiring primarily visual
reasoning, all models experience a 35%-55% performance drop from
text-reasoning-heavy question performance. Lastly, our qualitative error
analysis reveals specific categories of visual reasoning that are challenging
for current LVLMs.

</details>


### [61] [Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English](https://arxiv.org/abs/2505.15095)
*Ishmanbir Singh,Dipankar Srirag,Aditya Joshi*

Main category: cs.CL

TL;DR: 本研究利用 the pragmatic metacognitive prompting (PMP) 技术来提高对澳大利亚和印度英语的讽刺检测能力，并为这些英语变体生成了讽刺解释。实验结果表明，PMP 在两个大型语言模型 (LLM) 上显著优于其他提示策略，并且在与标准英语数据集的比较中也表现出色。


<details>
  <summary>Details</summary>
Motivation: 讽刺是情感分析中的一个挑战，因为它涉及陈述情感与隐含情感之间的不一致性。当这种隐含情感与特定国家或地区相关时，挑战会加剧。本研究旨在解决在不同英语变体（特别是澳大利亚和印度英语）中进行讽刺检测的难题，并提供可解释的讽刺分析。

Method: 本研究采用了认知启发的 pragmatic metacognitive prompting (PMP) 技术，并将其应用于讽刺检测。研究人员手动为澳大利亚和印度英语的讽刺标记数据集 BESSTIE 添加了讽刺解释，并将其与包含讽刺解释的标准英语数据集 FLUTE 进行了比较。实验在两个开源大型语言模型 (GEMMA 和 LLAMA) 上进行，并将 PMP 与四种其他提示策略进行了性能对比，同时还探讨了 agentic prompting 等技术在缓解上下文相关失败方面的作用。

Result: 在两个开源大型语言模型 (GEMMA 和 LLAMA) 上进行的评估显示，PMP 技术在所有任务和数据集上都实现了统计学上显著的性能提升，显著优于四种替代提示策略。此外，研究还发现 agentic prompting 等技术通过实现外部知识检索，能够有效缓解与上下文相关的失败。

Conclusion: 本研究的贡献在于成功地将 PMP 技术应用于生成不同英语变体的讽刺解释，显著提高了讽刺检测的准确性和可解释性。研究结果为处理具有地域特色的语言变体中的讽刺现象提供了新的方法，并为未来的研究开辟了道路，例如探索更广泛的语言变体和更复杂的讽刺形式。

Abstract: Sarcasm is a challenge to sentiment analysis because of the incongruity
between stated and implied sentiment. The challenge is exacerbated when the
implication may be relevant to a specific country or geographical region.
Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that
has been used for pragmatic reasoning. In this paper, we harness PMP for
explainable sarcasm detection for Australian and Indian English, alongside a
benchmark dataset for standard English. We manually add sarcasm explanations to
an existing sarcasm-labeled dataset for Australian and Indian English called
BESSTIE, and compare the performance for explainable sarcasm detection for them
with FLUTE, a standard English dataset containing sarcasm explanations. Our
approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)
achieves statistically significant performance improvement across all tasks and
datasets when compared with four alternative prompting strategies. We also find
that alternative techniques such as agentic prompting mitigate context-related
failures by enabling external knowledge retrieval. The focused contribution of
our work is utilising PMP in generating sarcasm explanations for varieties of
English.

</details>


### [62] [ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24388)
*Hao Chen,Yukun Yan,Sen Mei,Wanxiang Che,Zhenghao Liu,Qi Shi,Xinze Li,Yuchun Fan,Pengcheng Huang,Qiushi Xiong,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: ClueAnchor通过提取关键线索并生成多条推理路径来增强检索增强生成（RAG）系统，解决了现有RAG系统未能充分利用检索文档的问题，显著提高了推理的完整性和鲁棒性，并能有效处理噪声或部分相关内容。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）系统在利用检索文档以支持事实性和可解释性推理方面存在不足，尤其是在证据隐晦、分散或被噪声干扰时。这导致模型无法充分提取和整合关键线索，影响了推理的准确性。

Method: 提出ClueAnchor框架，通过以下步骤增强RAG：1. 从检索内容中提取关键线索。2. 基于不同的知识配置生成多条推理路径。3. 通过基于奖励的偏好优化选择最适合的推理路径。实验在相关数据集上进行，并与现有RAG基线进行比较。

Result: ClueAnchor在推理的完整性和鲁棒性方面显著优于现有RAG基线。实验表明，ClueAnchor对噪声或部分相关的检索内容具有很强的抵抗力，并且即使在推理过程中没有显式的线索监督，也能识别出支持性证据。

Conclusion: ClueAnchor成功解决了现有RAG系统的局限性，通过增强的线索提取和推理路径优化，提高了模型的性能和可解释性。该框架在处理复杂和不完整信息方面表现出优越性，为未来的RAG研究提供了新的方向。

Abstract: Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs)
with external knowledge to improve factuality. However, existing RAG systems
frequently underutilize the retrieved documents, failing to extract and
integrate the key clues needed to support faithful and interpretable reasoning,
especially in cases where relevant evidence is implicit, scattered, or obscured
by noise. To address this issue, we propose ClueAnchor, a novel framework for
enhancing RAG via clue-anchored reasoning exploration and optimization.
ClueAnchor extracts key clues from retrieved content and generates multiple
reasoning paths based on different knowledge configurations, optimizing the
model by selecting the most appropriate reasoning path for the given context
through reward-based preference optimization. Experiments show that ClueAnchor
significantly outperforms prior RAG baselines in the completeness and
robustness of reasoning. Further analysis confirms its strong resilience to
noisy or partially relevant retrieved content, as well as its capability to
identify supporting evidence even in the absence of explicit clue supervision
during inference. All codes are available at
https://github.com/thunlp/ClueAnchor.

</details>


### [63] [AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175)
*Salman Rahman,Sheriff Issaka,Ashima Suvarna,Genglin Liu,James Shiffer,Jaeyoung Lee,Md Rizwan Parvez,Hamid Palangi,Shi Feng,Nanyun Peng,Yejin Choi,Julian Michael,Liwei Jiang,Saadia Gabriel*

Main category: cs.CL

TL;DR: 随着人工智能能力的增强，其对世界理解的影响日益增大，但也可能加剧错误信息和扩大社会分歧。Scalable Oversight旨在确保AI系统在能力超越评估者时仍能保持真实性。然而，人类评估者自身可能存在偏见。本研究探讨了AI辩论是否能通过引导存在偏见的个体走向真相，其方法是让两个AI系统就COVID-19和气候变化等存在争议的、人们持有强烈固有信念的事实性主张进行辩论。研究一招募了持有主流或怀疑态度的个体作为评估者，通过辩论（与两个AI顾问互动，各自支持对立观点）或咨询（与单个AI顾问互动）两种协议来评估主张。研究二使用具有和不具有类人角色的AI评估者来评估相同的协议。研究一表明，辩论协议在COVID-19和气候变化主张的评估中，相比咨询协议，始终能提高人类评估者的判断准确性和置信度校准，准确率提升4-10%。这种改进对持有主流观点的评估者尤为显著（COVID-19主张准确率最高提升15.2%），但也帮助了最初判断错误的主张怀疑者朝着准确的观点转变（准确率提升4.7%）。研究二中，具有类人角色的AI评估者准确率（78.5%）甚至高于人类评估者（70.1%）和默认AI评估者（69.8%），表明其在监督前沿AI模型方面的潜力。研究结果突显了AI辩论作为在争议领域实现可扩展、可抵御偏见的监督的潜在途径。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能能力的飞速发展，其在全球信息传播和观念形成中的作用愈发重要。然而，AI在放大错误信息和加剧社会分裂方面的潜在风险也日益凸显，特别是在那些事实准确性直接影响福祉的重大议题上。本研究旨在解决一个关键问题：如何在AI系统的能力超越其人类评估者时，依然能保证AI系统的真实性？当人类作为评估者时，其固有的信念和偏见会严重影响判断的客观性。因此，研究的根本动机在于探索一种能够克服人类评估者偏见、确保AI在复杂且存在争议的领域保持信息准确性的方法，以实现“可扩展监督”（Scalable Oversight）。

Method: 本研究采用了实验设计，通过两项研究来评估AI辩论在引导偏见评估者做出准确判断方面的效果。研究一招募了人类评估者，并根据其对COVID-19和气候变化等议题的固有信念将其分为“主流”和“怀疑”两组。评估者通过两种协议进行判断：1. 辩论协议：评估者与两个分别持有对立观点的AI顾问进行互动。2. 咨询协议：评估者与一个AI顾问进行互动。研究二则进一步扩展了评估范围，使用具有和不具有类人角色的AI评估者来执行与研究一相同的两种协议，旨在探索AI自身在评估任务中的表现，以及类人角色是否能提升评估效果。实验中，评估者需要对COVID-19和气候变化相关的争议性事实主张进行评估，研究人员通过比较不同协议和不同类型评估者在准确率和置信度校准方面的表现来衡量效果。

Result: 研究一的结果显示，在人类评估者方面，辩论协议相比咨询协议，在COVID-19和气候变化的主张评估中，能够持续提高判断的准确率和置信度校准，准确率平均提升了4-10%。对于持有主流观点的评估者，辩论协议带来的准确率提升尤为显著，在COVID-19主张上最高可达15.2%。同时，辩论协议也有效帮助了持有怀疑观点的评估者，使其在最初做出错误判断的情况下，其观点能够朝着更准确的方向调整，准确率提升了4.7%。研究二的发现则进一步强调了AI在评估任务中的潜力，具有类人角色的AI评估者取得了最高的准确率（78.5%），显著优于人类评估者（70.1%）和不具有类人角色的默认AI评估者（69.8%）。这些结果表明，AI辩论不仅能提升人类的判断能力，而且AI自身（特别是具有类人角色的AI）在监督前沿AI模型方面展现出更大的潜力。

Conclusion: 本研究证明了AI辩论作为一种创新的监督机制，在应对AI能力超越人类评估者以及评估者自身偏见问题上具有显著成效。研究结果表明，AI辩论能够有效提升人类评估者在争议性议题上的判断准确性和置信度校准，尤其对持有不同固有信念的群体均能产生积极影响。此外，研究二的发现进一步揭示了具有类人角色的AI在监督任务中的优越表现，预示着AI在未来可能成为更可靠、更客观的评估者。总体而言，AI辩论为实现可扩展、抗偏见的AI监督提供了一条充满希望的路径。未来的研究可以进一步探索不同类型争议性议题的适用性，优化AI辩论的设计，并深入研究类人AI评估者的内在机制及其伦理影响。

Abstract: As AI grows more powerful, it will increasingly shape how we understand the
world. But with this influence comes the risk of amplifying misinformation and
deepening social divides-especially on consequential topics where factual
accuracy directly impacts well-being. Scalable Oversight aims to ensure AI
systems remain truthful even when their capabilities exceed those of their
evaluators. Yet when humans serve as evaluators, their own beliefs and biases
can impair judgment. We study whether AI debate can guide biased judges toward
the truth by having two AI systems debate opposing sides of controversial
factuality claims on COVID-19 and climate change where people hold strong prior
beliefs. We conduct two studies. Study I recruits human judges with either
mainstream or skeptical beliefs who evaluate claims through two protocols:
debate (interaction with two AI advisors arguing opposing sides) or consultancy
(interaction with a single AI advisor). Study II uses AI judges with and
without human-like personas to evaluate the same protocols. In Study I, debate
consistently improves human judgment accuracy and confidence calibration,
outperforming consultancy by 4-10% across COVID-19 and climate change claims.
The improvement is most significant for judges with mainstream beliefs (up to
+15.2% accuracy on COVID-19 claims), though debate also helps skeptical judges
who initially misjudge claims move toward accurate views (+4.7% accuracy). In
Study II, AI judges with human-like personas achieve even higher accuracy
(78.5%) than human judges (70.1%) and default AI judges without personas
(69.8%), suggesting their potential for supervising frontier AI models. These
findings highlight AI debate as a promising path toward scalable,
bias-resilient oversight in contested domains.

</details>


### [64] [SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat](https://arxiv.org/abs/2506.04721)
*Yuru Jiang,Wenxuan Ding,Shangbin Feng,Greg Durrett,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 我们提出了一种名为SPARTA ALIGNMENT的算法，通过竞争和对抗来集体对齐多个大型语言模型（LLM）。该算法利用LLM组成的“斯巴达部落”，让它们在完成指令的同时相互竞争，并充当裁判。在每次迭代中，会选择一个指令和两个模型进行“决斗”，其他模型评估它们的响应，并通过改进的Elo等级声誉系统聚合评分，胜者/败者在评估他人时获得/失去权重。这些同行评估的战斗结果随后成为偏好对，所有模型在每次迭代结束时都会从这些偏好中学习。SPARTA ALIGNMENT能够通过迭代和集体的竞争过程实现多个LLM的自我进化。


<details>
  <summary>Details</summary>
Motivation: 单个人工智能模型的生成能力缺乏多样性且评估存在偏见，难以满足复杂多样的指令要求。为了解决这一问题，需要一种能够提升模型生成能力和评估准确性的方法。

Method: SPARTA ALIGNMENT算法通过以下方式运行：1. 构成“斯巴达部落”：多个LLM组成一个部落。2. 竞争与对抗：每次迭代选择一个指令和两个模型进行“决斗”。3. 同行评估：部落中的其他模型评估“决斗”中两个模型的响应。4. Elo声誉系统：使用改进的Elo等级声誉系统聚合评估分数，获胜模型在评估中获得更高权重。5. 偏好学习：将战斗结果转化为偏好对，所有模型根据这些偏好进行学习和进化。

Result: SPARTA ALIGNMENT在10/12的任务和数据集上优于初始模型和4个基线模型，平均提升了7.0%。实验还表明，SPARTA ALIGNMENT能更有效地泛化到未见过任务，并利用参与模型的专业知识多样性，生成更具逻辑性、直接性和信息量的输出。

Conclusion: SPARTA ALIGNMENT通过集体竞争和对抗实现了多个LLM的有效自我进化，显著提高了模型在各种任务上的性能和泛化能力。未来可以进一步探索该方法在更大规模模型和更复杂任务上的应用，并研究其在解决模型偏见和提高生成内容多样性方面的潜力。

Abstract: We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs
through competition and combat. To complement a single model's lack of
diversity in generation and biases in evaluation, multiple LLMs form a "sparta
tribe" to compete against each other in fulfilling instructions while serving
as judges for the competition of others. For each iteration, one instruction
and two models are selected for a duel, the other models evaluate the two
responses, and their evaluation scores are aggregated through a adapted
elo-ranking based reputation system, where winners/losers of combat gain/lose
weight in evaluating others. The peer-evaluated combat results then become
preference pairs where the winning response is preferred over the losing one,
and all models learn from these preferences at the end of each iteration.
SPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative
and collective competition process. Extensive experiments demonstrate that
SPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines
across 10 out of 12 tasks and datasets with 7.0% average improvement. Further
analysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen
tasks and leverages the expertise diversity of participating models to produce
more logical, direct and informative outputs.

</details>


### [65] [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/abs/2506.08410)
*Ziyang Ma,Qingyue Yuan,Zhenglin Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: 本研究旨在评估大型语言模型（LLM）在数学推理中的元认知能力，并提出改进方法。研究发现，现有评估方法（如困惑度）缺乏步级分析和适应性，因此提出AutoMeco框架来基准化现有评估方法，并引入MIRA策略来提升LLM的元认知评估效果。实验证明AutoMeco的合理性以及MIRA的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大型语言模型（LLM）的认知错误检测能力，即模型分析推理链中错误的能力。然而，对于LLM的元认知能力（例如，模型对其自身推理步骤错误的认识程度）的研究却很少。元认知能力对于LLM的可靠性至关重要。尽管已有研究提出了一些用于LLM自我评估的措施，例如困惑度，它可以反映答案的正确性并被视为元认知的一种视角，但这些方法缺乏步级分析和适应性。因此，本研究旨在研究如何利用现有方法评估LLM的元认知能力，以及如何改进这些评估方法。

Method: 本研究提出了AutoMeco（Automated Meta-cognition Evaluation）框架，用于对现有的LLM元认知评估方法进行基准测试。此外，还提出了一种无需训练的马尔可夫内在奖励调整策略（MIRA），以增强当前的元认知评估方法。研究在三个数学推理数据集和三种不同的LLM上进行了实验评估。

Result: 实验结果表明，通过与“N中选优”（Best-of-N verification）的比较，AutoMeco框架的评估结果是合理的。此外，实验还证明了使用MIRA策略能够更好地评估LLM的元认知能力。具体而言，MIRA策略能够提升现有评估方法在衡量LLM元认知方面的表现。

Conclusion: 本研究通过提出AutoMeco框架和MIRA策略，为评估LLM的元认知能力提供了新的方法和视角。研究结果表明，AutoMeco能够合理地基准化现有评估方法，而MIRA能够有效提升LLM元认知能力的评估效果。这对于提高LLM在数学推理等任务中的可靠性具有重要意义。未来的工作可以进一步探索MIRA在更广泛任务和模型上的应用，以及开发更先进的元认知评估技术。

Abstract: Previous research has primarily focused on the cognitive error detection
capabilities of Large Language Models (LLMs), often prompting them to analyze
mistakes in reasoning chains. However, few studies have examined the
meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),
which are crucial for their reliability. While studies on LLM self-evaluation
present some measures, such as perplexity, which can reflect the answer
correctness and be viewed as the lens of meta-cognition, they lack step-level
analysis and adaptation. This paper studies the evaluation of LLM
meta-cognition using the current lenses and how to improve these lenses.
Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation
framework for benchmarking the existing lenses. Furthermore, a training-free
Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost
current meta-cognition lenses. Experimental results on three mathematical
reasoning datasets and three LLMs show the reasonableness of AutoMeco by
comparing it with Best-of-N verification. Moreover, the meta-cognition ability
of LLMs can be better evaluated using MIRA.

</details>


### [66] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
*Haoran Zhao,Robert D. Hawkins*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在礼貌用语方面存在对齐挑战。研究发现，大型模型（≥70B参数）能模仿计算语用学文献中的礼貌策略，并在开放式生成任务中获得人类偏好。然而，模型在积极语境下过度依赖负面礼貌策略，可能导致误解。尽管现代LLM在礼貌策略方面表现出色，但这种细微差别引发了对AI系统语用对齐的重要问题。


<details>
  <summary>Details</summary>
Motivation: 礼貌用语是大型语言模型（LLM）面临的一个基本对齐挑战。人类运用丰富的语言策略来平衡信息传递和社会交往目标，包括积极策略（如赞美、表达兴趣）和消极策略（如委婉语、间接表达）。本研究旨在探究LLM是否也具备类似上下文敏感的策略库，以应对礼貌用语的挑战。

Method: 本研究通过在约束性和开放性生成任务中比较人类和LLM的响应，来检验LLM是否采用与人类相似的上下文敏感的礼貌策略库。研究分析了模型的语言输出，并由人类评估者对模型生成响应的偏好程度进行评估。

Result: 研究发现，大型模型（≥70B参数）能够成功复制计算语用学文献中的关键礼貌策略偏好。在开放式生成任务中，人类评估者令人惊讶地更偏好LLM生成的响应。然而，进一步的语言分析揭示，模型倾向于过度使用负面礼貌策略，即使在积极的语境下也是如此，这可能导致潜在的误解。

Conclusion: 尽管现代大型语言模型在处理礼貌用语策略方面取得了显著进展，但它们在不同语境下对策略的选择性使用上，与人类仍存在细微差异。模型过度依赖负面礼貌策略的现象，对AI系统的语用对齐提出了重要挑战，并暗示了未来在提高模型语用能力方面仍有改进空间。

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [67] [The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs](https://arxiv.org/abs/2506.11094)
*Songyang Liu,Chaozhuo Li,Jiameng Qiu,Xi Zhang,Feiran Huang,Litian Zhang,Yiming Hei,Philip S. Yu*

Main category: cs.CL

TL;DR: 近年来，大型语言模型（LLMs）在自然语言处理领域取得了显著进展，但也引发了对生成内容（如毒性、偏见、错误信息）的安全担忧。尽管已有不少研究尝试评估这些风险，但缺乏系统性的安全评估综述。本文旨在填补这一空白，对LLMs安全评估的最新进展进行结构化概述。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的飞速发展，大型语言模型（LLMs）在内容生成、人机交互、机器翻译和代码生成等方面展现出强大的能力。然而，其广泛部署也带来了严峻的安全挑战。LLMs生成的内容可能包含有毒、带有偏见或虚假信息等不安全因素，尤其在对抗性环境下，这已引起学术界和工业界的广泛关注。尽管已有大量研究试图评估这些风险，但目前仍缺乏对LLMs安全评估进行全面、系统的梳理。本文旨在解决这一问题。

Method: 本文提出了一种四维度的分类方法来系统性地梳理LLMs的安全评估研究：1. 评估目标（Why to evaluate）：探讨LLMs安全评估的背景、与一般LLM评估的区别以及其重要性。2. 评估内容（What to evaluate）：根据毒性、鲁棒性、道德、偏见与公平性、真实性等方面对现有的安全评估任务进行分类。3. 评估环境（Where to evaluate）：总结当前安全评估中使用的评估指标、数据集和基准。4. 评估方法（How to evaluate）：根据评估者角色和整合整个评估流程的框架，回顾现有的主流评估方法。

Result: 本文通过对现有研究的系统性梳理和分类，揭示了LLMs安全评估的现状。文章详细阐述了评估LLMs安全性的不同维度（如毒性、偏见、真实性等），总结了现有的评估指标、数据集和基准，并回顾了基于评估者角色和评估框架的主流评估方法。

Conclusion: 本文系统性地概述了大型语言模型（LLMs）安全评估的最新进展，提出了一个包含“评估目标、评估内容、评估环境、评估方法”的四维度分类框架。研究强调了优先进行安全评估的必要性，以确保LLMs在现实世界应用中的可靠性和负责任性。文章最后指出了当前LLMs安全评估面临的挑战，并为该领域未来的深入研究提供了有前景的方向。

Abstract: With the rapid advancement of artificial intelligence, Large Language Models
(LLMs) have shown remarkable capabilities in Natural Language Processing (NLP),
including content generation, human-computer interaction, machine translation,
and code generation. However, their widespread deployment has also raised
significant safety concerns. In particular, LLM-generated content can exhibit
unsafe behaviors such as toxicity, bias, or misinformation, especially in
adversarial contexts, which has attracted increasing attention from both
academia and industry. Although numerous studies have attempted to evaluate
these risks, a comprehensive and systematic survey on safety evaluation of LLMs
is still lacking. This work aims to fill this gap by presenting a structured
overview of recent advances in safety evaluation of LLMs. Specifically, we
propose a four-dimensional taxonomy: (i) Why to evaluate, which explores the
background of safety evaluation of LLMs, how they differ from general LLMs
evaluation, and the significance of such evaluation; (ii) What to evaluate,
which examines and categorizes existing safety evaluation tasks based on key
capabilities, including dimensions such as toxicity, robustness, ethics, bias
and fairness, truthfulness, and related aspects; (iii) Where to evaluate, which
summarizes the evaluation metrics, datasets and benchmarks currently used in
safety evaluations; (iv) How to evaluate, which reviews existing mainstream
evaluation methods based on the roles of the evaluators and some evaluation
frameworks that integrate the entire evaluation pipeline. Finally, we identify
the challenges in safety evaluation of LLMs and propose promising research
directions to promote further advancement in this field. We emphasize the
necessity of prioritizing safety evaluation to ensure the reliable and
responsible deployment of LLMs in real-world applications.

</details>


### [68] [IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation](https://arxiv.org/abs/2506.13229)
*Zijie Lin,Yang Zhang,Xiaoyan Zhao,Fengbin Zhu,Fuli Feng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 现有的大语言模型（LLMs）在推荐任务中将物品预测视为逐个 token 的语言生成任务，但它们平等对待所有 item token，仅关注最大化似然性。本文提出了一种新颖的视角，将物品生成建模为决策过程，通过信息增益（IG）量化 token 的决定性。研究发现，大多数 token 的 IG 值较低但 logit 值较高，会不成比例地影响训练损失和解码。为此，本文提出了一种基于信息增益的决定性感知 token 处理（IGD）策略，在调优和解码阶段都考虑 token 的决定性。IGD 通过降低低 IG token 的权重并重新平衡解码来优先考虑高 IG token，从而超越了纯粹的似然最大化。实验证明，IGD 在四个基准数据集和两个 LLM 主干上均能提升推荐准确性，在常用排序指标上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）在推荐任务中，将物品预测视为一个逐个 token 生成的语言任务。然而，这些方法在优化和解码过程中平等地对待所有 item token，仅仅追求最大化似然性。这种做法忽略了一个关键问题：并非所有 token 都同等重要。许多 token 对区分物品的贡献很小，但却可能在优化或解码阶段占据主导地位，从而可能损害模型的整体推荐性能。因此，有必要量化 token 在区分物品时的“决定性”，并将其纳入 LLM 的推荐框架中，以解决现有方法忽略 token 差异性导致性能不佳的问题。

Method: 本文提出了一种新颖的视角，将物品生成过程建模为一个决策过程。为了量化每个 token 的决定性，作者引入了信息增益（IG）的概念，衡量每个 token 在减少关于生成物品的不确定性方面的作用。通过实证分析，研究人员发现大部分 token 的 IG 值较低，但其对应的 logit 值却很高，这会导致它们在训练损失和解码过程中产生不成比例的影响。基于这些发现，作者提出了一种名为“信息增益-基于决定性感知-的 token 处理”（IGD）的策略。该策略将 token 的决定性融入到模型调优（tuning）和解码（decoding）两个阶段。具体而言，在调优阶段，IGD 会降低低 IG token 的权重；在解码阶段，IGD 会重新平衡解码过程，优先考虑具有高 IG 的 token。通过这种方式，IGD 能够超越纯粹的似然最大化，有效地优先处理高决定性的 token。实验在四个基准数据集上，使用了两个 LLM 主干模型进行了广泛的评估。

Result: 通过在四个基准数据集和两个 LLM 主干模型上进行的广泛实验，结果表明 IGD 策略能够持续提升推荐准确性。与强有力的基线方法相比，IGD 在常用的排序指标上取得了显著的性能提升。这表明，通过考虑 token 的信息增益和决定性，可以有效地优化 LLM 在推荐任务中的表现，克服了仅依赖似然最大化带来的局限性。

Conclusion: 本文提出了一种新颖的基于信息增益（IG）的 token 决定性度量方法，并将此融入到 LLM 的推荐框架中，形成了 IGD 策略。该策略通过在模型调优和解码阶段都考虑 token 的决定性，有效地解决了现有方法忽略 token 差异性导致性能受损的问题。实验结果有力地证明了 IGD 在提升推荐准确性方面的有效性，尤其是在常用排序指标上相比基线方法取得了显著的改进。这项工作为如何更有效地利用 LLM 进行推荐提供了新的思路，未来可以进一步探索更复杂的 token 交互以及在更多样化的推荐场景下应用 IGD 策略。

Abstract: Large Language Models (LLMs) have shown strong potential for recommendation
by framing item prediction as a token-by-token language generation task.
However, existing methods treat all item tokens equally, simply pursuing
likelihood maximization during both optimization and decoding. This overlooks
crucial token-level differences in decisiveness-many tokens contribute little
to item discrimination yet can dominate optimization or decoding. To quantify
token decisiveness, we propose a novel perspective that models item generation
as a decision process, measuring token decisiveness by the Information Gain
(IG) each token provides in reducing uncertainty about the generated item. Our
empirical analysis reveals that most tokens have low IG but often correspond to
high logits, disproportionately influencing training loss and decoding, which
may impair model performance. Building on these insights, we introduce an
Information Gain-based Decisiveness-aware Token handling (IGD) strategy that
integrates token decisiveness into both tuning and decoding. Specifically, IGD
downweights low-IG tokens during tuning and rebalances decoding to emphasize
tokens with high IG. In this way, IGD moves beyond pure likelihood
maximization, effectively prioritizing high-decisiveness tokens. Extensive
experiments on four benchmark datasets with two LLM backbones demonstrate that
IGD consistently improves recommendation accuracy, achieving significant gains
on widely used ranking metrics compared to strong baselines.

</details>


### [69] [Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study](https://arxiv.org/abs/2506.13464)
*Zhengyu Hu,Jianxun Lian,Zheyuan Xiao,Seraphina Zhang,Tianfu Wang,Nicholas Jing Yuan,Xing Xie,Hui Xiong*

Main category: cs.CL

TL;DR: LLM的学习能力，特别是适应动态环境和获取新知识的能力，是一个未被充分探索的领域。本研究提出了一个受认知心理学启发的框架，将学习能力分解为“向讲师学习”、“从概念学习”和“从经验学习”三个维度。研究发现，互动促进学习，概念理解具有规模涌现性并使大型模型受益，LLM在少样本学习方面有效但在多样本学习方面效果不佳。基于此，研究构建了一个基准，用于统一评估LLM在三个学习维度上的通用学习能力，旨在促进更具适应性和类似人类的模型的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在数学、编码和推理等任务上表现出色，但它们在动态环境适应和新知识获取方面的学习能力却未得到充分研究。这种学习能力对于构建更通用、更适应性的AI至关重要。因此，本研究旨在填补这一空白，深入探索LLM的学习机制。

Method: 本研究借鉴认知心理学和教育学原理，将通用学习能力分解为三个互补的维度：1. 向讲师学习（通过明确指导获取知识）；2. 从概念学习（内化抽象结构并泛化到新情境）；3. 从经验学习（通过累积的探索和反馈进行适应）。研究进行了全面的实证研究，跨越了这三个学习维度。

Result: 实证研究揭示了几个关键发现：1. 互动能够促进学习；2. 概念理解能力具有规模涌现性，模型规模越大，受益越多；3. LLM在少样本学习场景下表现有效，但在多样本学习场景下效果不佳。研究还引入了一个基准，用于在三个学习认知维度上统一、真实地评估LLM的通用学习能力。

Conclusion: 本研究提出的框架和实证发现，为理解和评估LLM的学习能力提供了新的视角。所构建的基准能够提供诊断性见解，支持对更具适应性和类人学习能力的模型的评估和开发。未来的工作可以进一步探索不同学习维度之间的交互作用，并开发更有效的学习算法。

Abstract: Large language models (LLMs) have shown impressive capabilities across tasks
such as mathematics, coding, and reasoning, yet their learning ability, which
is crucial for adapting to dynamic environments and acquiring new knowledge,
remains underexplored. In this work, we address this gap by introducing a
framework inspired by cognitive psychology and education. Specifically, we
decompose general learning ability into three distinct, complementary
dimensions: Learning from Instructor (acquiring knowledge via explicit
guidance), Learning from Concept (internalizing abstract structures and
generalizing to new contexts), and Learning from Experience (adapting through
accumulated exploration and feedback). We conduct a comprehensive empirical
study across the three learning dimensions and identify several insightful
findings, such as (i) interaction improves learning; (ii) conceptual
understanding is scale-emergent and benefits larger models; and (iii) LLMs are
effective few-shot learners but not many-shot learners. Based on our framework
and empirical findings, we introduce a benchmark that provides a unified and
realistic evaluation of LLMs' general learning abilities across three learning
cognition dimensions. It enables diagnostic insights and supports evaluation
and development of more adaptive and human-like models.

</details>


### [70] [Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality](https://arxiv.org/abs/2506.14681)
*Yuto Harada,Yusuke Yamauchi,Yusuke Oda,Yohei Oseki,Yusuke Miyao,Yu Takagi*

Main category: cs.CL

TL;DR: 本研究通过训练超过1000个监督微调（SFT）的大型语言模型（LLM），在代码生成、数学推理和通用领域任务上进行了广泛实验，旨在理解SFT的关键因素。研究发现，某些训练任务协同作用普遍存在于所有模型，而另一些则因模型而异，强调了模型特定策略的重要性。此外，困惑度（perplexity）能持续预测SFT的有效性，并且模型中层权重的变化与性能提升最相关。研究者公开发布了这些模型和基准测试结果，以促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）是使大型语言模型（LLM）与人类指令和价值观对齐的关键环节，但其内在机制和影响因素仍未被充分理解。理解SFT的有效性，特别是确定哪些数据集属性和模型内部变化对性能提升最重要，对于开发更高效、更可靠的LLM至关重要。本研究旨在深入探究SFT的过程，揭示其影响因素，并为未来的LLM训练和对齐策略提供指导。

Method: 研究者在一个包含代码生成、数学推理和通用领域任务的各种数据集上，训练了超过1000个不同规模的基础模型，进行监督微调（SFT）。他们在一个受控的环境下系统地改变了训练数据和模型，以研究SFT对模型性能的影响。研究中，他们识别了对SFT效果最重要的特定数据集属性，并检查了SFT在模型层级上引入的修改。此外，他们还评估了困惑度（perplexity）作为预测SFT有效性的指标，并分析了模型权重变化与性能提升之间的相关性。

Result: 研究发现，尽管存在一些普遍的训练任务协同作用，但SFT的效果在很大程度上取决于具体的模型。困惑度被证明是一个可靠的指标，能够持续预测SFT的有效性，其预测能力甚至超过了训练数据与基准测试之间的表面相似性。在模型内部，中层权重的变化与模型性能的提升显示出最强的相关性。研究者公开发布了超过1000个SFT模型及其基准测试结果。

Conclusion: 本研究通过大规模实验，揭示了SFT的关键影响因素，包括数据集属性、模型特定策略的重要性以及困惑度作为预测指标的有效性。研究强调了模型中层权重变化对性能提升的关键作用。这些发现为理解和优化LLM的SFT过程提供了宝贵的见解，并为未来模型的研究和开发奠定了基础。研究者发布的数据和模型将有助于加速该领域的进一步探索。

Abstract: Supervised fine-tuning (SFT) is a critical step in aligning large language
models (LLMs) with human instructions and values, yet many aspects of SFT
remain poorly understood. We trained a wide range of base models on a variety
of datasets including code generation, mathematical reasoning, and
general-domain tasks, resulting in 1,000+ SFT models under controlled
conditions. We then identified the dataset properties that matter most and
examined the layer-wise modifications introduced by SFT. Our findings reveal
that some training-task synergies persist across all models while others vary
substantially, emphasizing the importance of model-specific strategies.
Moreover, we demonstrate that perplexity consistently predicts SFT
effectiveness, often surpassing superficial similarity between the training
data and the benchmark, and that mid-layer weight changes correlate most
strongly with performance gains. We release these 1,000+ SFT models and
benchmark results to accelerate further research. All resources are available
at https://github.com/llm-jp/massive-sft.

</details>


### [71] [Controlling Thinking Speed in Reasoning Models](https://arxiv.org/abs/2507.03704)
*Zhengkai Lin,Zhihang Fu,Ze Chen,Chao Chen,Liang Xie,Wenxiao Wang,Deng Cai,Zheng Wang,Jieping Ye*

Main category: cs.CL

TL;DR: 本研究提出了一种动态调整大型语言模型（LRMs）思维速度的方法，使其能够近似人类的双重认知模式（快速的System 1和慢速的System 2）。通过识别控制思维速度转换的“转向向量”并结合实时难度估计，该模型能够在简单推理时采用快速模式，在复杂推理时采用深度分析，从而在不进行额外训练的情况下，在多个领先的LRMs和推理基准上实现了平均+1.3%的准确率提升和-8.6%的token使用量减少。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LRMs）主要擅长慢速、深思熟虑的System 2思维模式，但在快速、直观的System 1思维模式方面存在不足。这导致LRMs在处理需要快速响应的任务时计算开销大、延迟高，未能完全模拟人类的认知灵活性。本研究旨在解决LRMs在思维速度控制和时机选择上的短板，以期更高效、更准确地模拟人类智能。

Method: 本研究首先识别出控制LRMs中慢速-快速思维转换的“转向向量”，并利用该向量实现了基于表示编辑的测试时间（test-time）速度缩放（scaling）效应，优于现有的基于提示（prompt-based）的速度缩放方法。其次，研究应用实时难度估计技术来识别不同复杂程度的推理片段。将这两种技术结合，提出了首个能够对简单推理步骤进行快速处理，并对复杂推理进行深入分析的推理策略。所有算法均基于vLLM实现。

Result: 所提出的即插即用模块在不进行任何训练或额外计算成本的情况下，在多个领先的LRMs和高级推理基准上，平均带来了+1.3%的准确率提升，同时 token 使用量减少了8.6%。这一结果表明，该方法能够有效地在准确性和效率之间取得平衡。

Conclusion: 本研究成功地使大型语言模型（LRMs）具备了动态调整思维速度的能力，有效地模拟了人类的双重认知模式。通过识别控制思维速度的表示空间向量和引入实时难度估计，该方法在不增加训练成本的情况下，显著提高了模型的准确性和效率。该研究为LRMs的效率和性能优化提供了新的途径，并有望在更广泛的应用中发挥作用，同时激发未来的相关研究。

Abstract: Human cognition is theorized to operate in two modes: fast, intuitive System
1 thinking and slow, deliberate System 2 thinking. While current Large
Reasoning Models (LRMs) excel at System 2 thinking, their inability to perform
fast thinking leads to high computational overhead and latency. In this work,
we enable LRMs to approximate human intelligence through dynamic thinking speed
adjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses
two key questions: (1) how to control thinking speed in LRMs, and (2) when to
adjust it for optimal performance. For the first question, we identify the
steering vector that governs slow-fast thinking transitions in LRMs'
representation space. Using this vector, we achieve the first representation
editing-based test-time scaling effect, outperforming existing prompt-based
scaling methods. For the second question, we apply real-time difficulty
estimation to signal reasoning segments of varying complexity. Combining these
techniques, we propose the first reasoning strategy that enables fast
processing of easy steps and deeper analysis for complex reasoning. Without any
training or additional cost, our plug-in module delivers an average +1.3%
accuracy with -8.6% token usage across leading LRMs and advanced reasoning
benchmarks. All of our algorithms are implemented based on vLLM and are
expected to support broader applications and inspire future research.

</details>


### [72] [Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It](https://arxiv.org/abs/2507.13328)
*Yulu Qin,Dheeraj Varghese,Adam Dahlgren Lindström,Lucia Donatelli,Kanishka Misra,Najoung Kim*

Main category: cs.CL

TL;DR: 视觉-语言（VL）训练不改变语言模型（LM）的词汇概念知识本身，但能改进其在特定任务（即使是纯语言任务）中的部署能力，尤其是在处理具有分类关系的术语时。


<details>
  <summary>Details</summary>
Motivation: 现有研究在视觉-语言（VL）训练对语言模型（LM）语言表征的影响方面，结果不一致且差异边缘。本研究假设VL训练对词汇概念知识，特别是其分类组织方式，可能产生显著影响。

Method: 通过比较仅文本LM及其对应的VL训练模型，并进行一系列有针对性的行为和表征分析，来探究VL训练对词汇分类知识的影响。

Result: VL模型在需要分类理解的纯文本问答任务中，通常优于仅文本LM。然而，分析显示LM和VLM在分类知识本身上没有显著差异，但在表示包含分类关系和非分类关系的概念的疑问句方面存在差异。

Conclusion: VL训练并未实质性改变语言模型中的分类知识，但确实提高了模型在特定任务中运用这些知识的能力，即使任务呈现形式为纯文本。这表明VL训练对模型知识的应用和部署有积极作用。

Abstract: Does vision-and-language (VL) training change the linguistic representations
of language models in meaningful ways? Most results in the literature have
shown inconsistent or marginal differences, both behaviorally and
representationally. In this work, we start from the hypothesis that the domain
in which VL training could have a significant effect is lexical-conceptual
knowledge, in particular its taxonomic organization. Through comparing minimal
pairs of text-only LMs and their VL-trained counterparts, we first show that
the VL models often outperform their text-only counterparts on a text-only
question-answering task that requires taxonomic understanding of concepts
mentioned in the questions. Using an array of targeted behavioral and
representational analyses, we show that the LMs and VLMs do not differ
significantly in terms of their taxonomic knowledge itself, but they differ in
how they represent questions that contain concepts in a taxonomic relation vs.
a non-taxonomic relation. This implies that the taxonomic knowledge itself does
not change substantially through additional VL training, but VL training does
improve the deployment of this knowledge in the context of a specific task,
even when the presentation of the task is purely linguistic.

</details>


### [73] [Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep Knowledge Extraction](https://arxiv.org/abs/2507.16271)
*Tianyun Zhong,Guozhao Mo,Yanjiang Liu,Yihan Chen,Lingdi Kong,Xuanang Chen,Yaojie Lu,Hongyu Lin,Shiwei Ye,Xianpei Han,Ben He,Le Sun*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在从复杂文档中提取信息方面存在挑战，通常生成混乱、无序且难以追溯的答案。为了解决这个问题，我们提出了AOE基准测试，这是一个新的双语基准测试，包含不同长度的数据和文档，旨在系统地评估LLMs理解碎片化文档并将其重构成组织化表格的能力。AOE包含11个跨越三个不同领域的精心设计的任务，要求模型为各种输入查询生成定制的上下文特定模式。实验结果表明，即使是最先进的模型也在此任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLMs）在处理现实世界文档（如论文、报告）并提取显式信息方面存在显著不足。尽管人们期望LLMs能够胜任此任务，但它们生成的答案通常是混乱、无组织且难以追溯的段落格式。这种信息提取的混乱性阻碍了LLMs在实际应用中的有效性，特别是在需要结构化和可追溯信息的场景下。因此，有必要开发一种新的评估方法来系统地衡量LLMs在理解和重组碎片化信息方面的能力，以推动LLM在信息提取任务上的进步。

Method: 我们引入了一个名为AOE（Arranged and Organized Extraction Benchmark）的新型双语基准测试。该基准测试包含不同长度的数据和文档，专门设计用于系统评估LLMs从碎片化文档中提取信息并将其重构成组织化表格的能力。与依赖固定模式和狭窄任务领域的前文本到表格任务不同，AOE涵盖了三个不同领域的11个精心设计的任务。这些任务要求模型能够为多样化的输入查询生成定制的、特定于上下文的模式（schema）。我们在实验中评估了包括开源和闭源在内的多种先进LLMs。

Result: 实验结果表明，即使是目前最先进的大型语言模型（LLMs）在AOE基准测试上也遇到了显著的困难。这表明当前LLMs在理解碎片化文档、动态生成上下文特定模式以及将提取的信息重组成结构化表格方面仍存在巨大的提升空间。具体性能指标和模型间的比较分析将在论文的详细结果部分呈现。

Conclusion: AOE基准测试的提出，为评估LLMs在处理复杂、碎片化文档并将其信息重构成组织化表格方面的能力提供了一个系统性的方法。研究结果揭示了当前LLMs在该任务上的局限性，突显了进一步研究的必要性。未来的工作可以集中在改进LLMs的文档理解能力、动态模式生成能力以及结构化信息输出能力，以应对更广泛的实际应用需求。尽管AOE基准测试在评估LLMs的信息提取能力方面取得了进展，但仍存在一些局限性，例如特定领域的覆盖范围和任务设计的复杂性，这些都为未来的研究提供了方向。

Abstract: With the emergence of large language models (LLMs), there is an expectation
that LLMs can effectively extract explicit information from complex real-world
documents (e.g., papers, reports). However, most LLMs generate paragraph-style
answers that are chaotic, disorganized, and untraceable. To bridge this gap, we
introduce the Arranged and Organized Extraction Benchmark (AOE), a new
bilingual benchmark with data and documents of varying lengths designed to
systematically evaluate the ability of LLMs to comprehend fragmented documents
and reconstruct isolated information into one organized table. Unlike
conventional text-to-table tasks, which rely on fixed schema and narrow task
domains, AOE includes 11 carefully crafted tasks across three diverse domains,
requiring models to generate context-specific schema tailored to varied input
queries. In the experiment, we evaluated both open-source and closed-source
state-of-the-art LLMs. The results show that even the most advanced models
struggled significantly. The benchmark is available at
https://anonymous.4open.science/r/AOE-Benchmark/.

</details>


### [74] [Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs](https://arxiv.org/abs/2507.19756)
*Rebecca M. M. Hicke,Brian W. Haggard,Mia Ferrante,Rayhan Khanna,David Mimno*

Main category: cs.CL

TL;DR: 本文利用计算工具对基督教小说这一文学流派进行了广泛的主题概述和对神圣行为描绘的深入探讨。研究者开发了一个包含“神圣行为”识别的编码本，并利用大型语言模型辅助小型语言模型进行标注。结果显示，与更广泛的基督教小说相比，备受欢迎的《末日迷踪》系列在神圣行为的描绘上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 虽然美国福音派的文化运动广为人知，但其文学方面却鲜为人关注。特别是基督教小说，尽管拥有大量读者，却很少受到学术研究。现有的研究多集中于《末日迷踪》系列，未能全面展现该类型文学的全貌。本文旨在弥补这一研究空白，通过计算方法对基督教小说进行广泛的主题分析，并深入探究其中神圣行为的描绘方式，以期提供更全面的理解。

Method: 本研究首先与人工标注员合作，制定了一个用于识别“神圣行为”的编码本。随后，利用一个大型语言模型辅助一个轻量级、可在笔记本电脑上运行的小型语言模型，使其能够使用该编码本进行标注。通过这种方式，对大量基督教小说文本进行标注，并利用标注结果进行后续分析。

Result: 研究结果表明，轻量级语言模型在匹配人工标注方面表现出色，即使在处理微妙和复杂的标注任务时也是如此。通过对标注结果的分析，发现《末日迷踪》系列与更广泛的基督教小说在神圣行为的描绘方式上存在显著且有意义的差异。

Conclusion: 本文成功运用计算工具为基督教小说这一领域提供了全面的主题概览和关于神圣行为描绘的深入见解。研究证明了小型语言模型在辅助标注任务中的有效性，并揭示了特定系列（如《末日迷踪》）与整个体裁在描绘神圣行为方面的区别。未来的研究可以进一步扩展分析范围，探索更多细微之处，并可能应用于其他文学体裁。

Abstract: In addition to its more widely studied cultural movements, American
Evangelicalism has a well-developed but less externally visible literary side.
Christian Fiction, however, has been little studied, and what scholarly
attention there is has focused on the explosively popular Left Behind series.
In this work, we use computational tools to provide both a broad topical
overview of Christian Fiction as a genre and a more directed exploration of how
its authors depict divine acts. Working with human annotators, we first
developed a codebook for identifying "acts of God." We then adapted the
codebook for use by a recent, lightweight LM with the assistance of a much
larger model. The laptop-scale LM is largely capable of matching human
annotations, even when the task is subtle and challenging. Using these
annotations, we show that significant and meaningful differences exist between
divine acts depicted by the Left Behind books and Christian Fiction more
broadly.

</details>


### [75] [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
*Christopher J. Agostino*

Main category: cs.CL

TL;DR: 该研究提出了一种名为 TinyTim 的语言模型系列，通过在《芬尼根的守灵夜》上进行微调，旨在解决现有模型在创造性突破方面的局限性。研究表明，TinyTim 模型具有显著的词汇创新能力，能够产生新词汇，并且这种发散性生成能力能够被保留，即使在指令微调后也能抵抗事实收敛。这项工作为构建专门的发散模型提供了一种方法，可与收敛模型结合，以实现超越统计优化的创造性问题解决方法。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能模型主要基于海量已知问题及其解决方案进行训练，这导致模型倾向于收敛，缺乏进行概念重构以实现真正创造性突破的能力。而人类的创造性飞跃得益于发散性认知过程。因此，需要开发能够进行发散生成的模型，以弥补现有模型的不足，推动人工智能在创造性任务上的进步。

Method: 研究人员首先选择了詹姆斯·乔伊斯的《芬尼根的守灵夜》这部反简约的文本作为微调数据集。然后，他们微调了一个无监督模型（TinyTim-V1）和一个新提出的指令微调模型（TinyTim-V2）。采用量化分析的方法来评估模型的发散生成能力，特别是词汇创新方面的表现。评估指标包括 Yule's K 分数，用于衡量词汇丰富度，并与收敛基线模型进行比较。

Result: 量化分析表明，TinyTim 模型家族，特别是 V1 模型，展现出了显著的词汇创新能力。TinyTim-V1 的 Yule's K 分数比收敛基线模型高出二十多倍，表明其在词汇丰富度方面有巨大优势。指令微调后的 V2 模型也保持了统计学上显著的独立特征，并且能够抵抗事实收敛，优先保留其核心的发散生成风格，牺牲了部分基准测试性能。V2 模型在保留发散风格的同时，也展现出一定的指令跟随能力。

Conclusion: 这项工作成功开发了一系列专门用于发散生成的语言模型 TinyTim。通过在高度反简约的文本上进行微调，这些模型在词汇创新方面表现出色，克服了传统收敛模型的局限性。TinyTim 模型可以作为更广泛系统中的发散生成源，与收敛系统结合使用，能够重新构建问题，激发统计优化之外的突破性解决方案。未来的工作可以进一步探索这种模型与其他类型模型的结合方式，以及在更广泛的应用场景中评估其创造性问题解决能力。

Abstract: In the search for artificial general intelligence, model development and
training has focused primarily on vast datasets of known problems and their
accepted solutions. This process necessarily produces convergent systems which
are fundamentally incapable of the conceptual reframing that is required for
genuine creative breakthroughs. Inspired by the divergent cognitive processes
that allow humans to make such creative leaps, our work introduces a family of
language models, TinyTim, to serve as sources of divergent generation within
broader systems. These models have been created by fine-tuning on the
anti-parsimonious text of James Joyce's `Finnegans Wake'. Quantitative analysis
of both an unsupervised fine-tuned model (TinyTim-V1) and a new
instruction-tuned variant (TinyTim-V2) demonstrates a profound capacity for
lexical invention; the foundational V1 model exhibits a Yule's K score for
lexical richness over twenty times greater than that of convergent baselines.
This trait is a stable property of the family, as the instruction-tuned V2
maintains a statistically distinct profile and resists factual convergence,
sacrificing benchmark performance to preserve its core generative style. This
work establishes a methodology for engineering specialized divergent models
that, when paired with convergent systems, can reframe problems and force
breakthroughs beyond the reach of statistical optimization alone.

</details>


### [76] [RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards](https://arxiv.org/abs/2509.21319)
*Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Ellie Evans,Daniel Egert,Hoo-Chang Shin,Felipe Soares,Yi Dong,Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: 本文提出了一种名为“基于二元灵活反馈的强化学习”（RLBFF）的新型强化学习范式，旨在克服现有RLHF和RLVR方法的局限性。RLBFF通过从自然语言反馈中提取可二元判断的原则（如信息准确性：是/否，代码可读性：是/否），并将其应用于奖励模型训练，实现了比传统方法更精细的响应质量评估。实验证明，RLBFF在RM-Bench和JudgeBench等基准测试中取得了领先性能，并且在推理时能够根据用户指定的原则进行定制化。此外，研究团队还开源了使用RLBFF对Qwen3-32B模型进行对齐的完整方案，其性能可媲美甚至超越现有模型，同时推理成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习（RL）范式，如基于人类反馈的强化学习（RLHF）和基于可验证奖励的强化学习（RLVR），在大型语言模型（LLM）的后训练阶段各有优劣。RLHF因依赖缺乏明确标准的“人情判断”而面临可解释性差和奖励破解的问题；RLVR则因侧重于基于正确性的验证器而适用范围受限。因此，需要一种能够结合人类偏好的灵活性和规则验证的精确性的方法，以更全面地评估响应质量，捕捉除正确性以外的细微差别。

Method: 本文提出了一种名为“基于二元灵活反馈的强化学习”（RLBFF）的新方法。RLBFF的核心思想是从自然语言反馈中提取可以被回答为“是”或“否”的原则，例如“信息准确性：是”或“代码可读性：否”。这些二元原则随后被用于奖励模型的训练，将奖励模型的训练转化为一个“蕴含”任务，即判断响应是否满足某个原则。这种方法结合了人类偏好的通用性和规则验证的精确性。在实验设置上，研究人员使用了RM-Bench和JudgeBench等基准测试来评估RLBFF的性能，并将其与Bradley-Terry模型进行了比较。同时，他们还开源了一个完整的方案，包括数据和对齐Qwen3-32B模型的流程，并在MT-Bench、WildBench和Arena Hard v2等通用对齐基准上进行了评估。

Result: RLBFF在多个基准测试中取得了优异的性能。在RM-Bench上，RLBFF的奖励模型（RM）达到了86.2%的准确率；在JudgeBench上，达到了81.4%，在2025年9月24日的数据显示其在该排行榜上名列第一。与Bradley-Terry模型相比，在数据量相同的情况下，RLBFF表现更优。RLBFF的一大优势在于其灵活性，允许用户在推理时指定感兴趣的原则来定制奖励模型的焦点，这是Bradley-Terry模型无法实现的。此外，该研究团队还发布了一个开源方案，使用RLBFF对Qwen3-32B模型进行了对齐，该模型在MT-Bench、WildBench和Arena Hard v2等通用对齐基准上的表现，能够匹配甚至超越o3-mini和DeepSeek R1模型，并且推理成本仅为后者的5%左右。

Conclusion: RLBFF作为一种新型的强化学习范式，成功地结合了人类反馈的灵活性和规则验证的精确性，解决了RLHF和RLVR方法的局限性。通过提取二元原则并将其应用于奖励模型训练，RLBFF能够更全面、更精细地评估LLM的响应质量，并在多个基准测试中取得了领先性能。该方法不仅提高了模型的性能，还通过允许推理时进行原则定制增加了模型的适应性。开源的对齐方案进一步降低了高性能LLM对齐的门槛，显著降低了成本。未来的工作可以探索更复杂的原则提取机制，以及在更多样化的任务和领域中应用RLBFF。

Abstract: Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM
post-training, each offering distinct advantages. However, RLHF struggles with
interpretability and reward hacking because it relies on human judgments that
usually lack explicit criteria, whereas RLVR is limited in scope by its focus
on correctness-based verifiers. We propose Reinforcement Learning with Binary
Flexible Feedback (RLBFF), which combines the versatility of human-driven
preferences with the precision of rule-based verification, enabling reward
models to capture nuanced aspects of response quality beyond mere correctness.
RLBFF extracts principles that can be answered in a binary fashion (e.g.
accuracy of information: yes, or code readability: no) from natural language
feedback. Such principles can then be used to ground Reward Model training as
an entailment task (response satisfies or does not satisfy an arbitrary
principle). We show that Reward Models trained in this manner can outperform
Bradley-Terry models when matched for data and achieve top performance on
RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,
2025). Additionally, users can specify principles of interest at inference time
to customize the focus of our reward models, in contrast to Bradley-Terry
models. Finally, we present a fully open source recipe (including data) to
align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the
performance of o3-mini and DeepSeek R1 on general alignment benchmarks of
MT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:
https://huggingface.co/collections/nvidia/reward-models-10-2025

</details>


### [77] [LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback](https://arxiv.org/abs/2510.08604)
*Raffaele Mura,Giorgio Piras,Kamilė Lukošiūtė,Maura Pintor,Amin Karbasi,Battista Biggio*

Main category: cs.CL

TL;DR: 现有的针对大语言模型（LLM）的越狱攻击（jailbreak）通常通过添加高困惑度（perplexity）的后缀或长提示模板来实现，这使得模型能够生成被禁止或有害的内容。然而，这种攻击方式容易被基于输入提示的困惑度过滤方法检测到。本研究提出了LatentBreak，一种白盒越狱攻击方法，它通过在潜在空间中寻找与原始提示意图相似但困惑度更低的语义替换词来生成自然且难以检测的对抗性提示，从而有效绕过基于困惑度过滤器的防御机制。实验证明，LatentBreak生成的提示更短、困惑度更低，在多种安全对齐模型上优于现有攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有的针对大语言模型的越狱攻击方法容易被基于困惑度过滤的防御机制检测到。当攻击者试图绕过模型的安全限制，迫使其生成受限或有害内容时，通常会采用增加提示的困惑度的方式，例如添加特殊的后缀或使用冗长的提示模板。这种方式虽然能在一定程度上欺骗模型，但却显著增加了输入提示的困惑度，使得基于困惑度分析的检测方法能够有效地识别并阻止这些攻击。因此，研究一种能够绕过这种检测机制的越狱攻击方法具有重要意义，这有助于更深入地理解大语言模型的安全漏洞，并推动更有效的安全防护策略的发展。

Method: LatentBreak是一种白盒越狱攻击方法，其核心思想是生成低困惑度且语义上与原始提示相似的对抗性提示。具体而言，该方法不是像传统攻击那样添加高困惑度的后缀或模板，而是通过替换提示中的某些词语来实现。这些被替换的词语在语义上与原始词语等价，但能够使得整体提示的困惑度降低。为了实现这一点，LatentBreak利用了模型的潜在空间表示。它通过最小化对抗性提示与其对应的无害请求在潜在空间中的距离来选择替换词。换句话说，它寻找那些既能保持原始提示意图，又能显著降低整体困惑度的词语进行替换。这种方法生成的对抗性提示更加自然，难以被基于困惑度过滤的防御机制所识别。

Result: 通过广泛的评估，LatentBreak在对抗基于困惑度过滤器的防御机制方面表现出色。实验结果表明，LatentBreak生成的越狱提示具有更短的长度和更低的困惑度。与现有的越狱攻击算法相比，LatentBreak在多种经过安全对齐的模型上能够更有效地绕过检测。这证明了LatentBreak在生成隐蔽且有效的越狱提示方面的优越性，特别是在面对依赖困惑度分析进行防御的系统时。

Conclusion: 本研究提出了LatentBreak，一种新型的白盒越狱攻击方法，它通过在潜在空间中寻找语义上等价但困惑度更低的词语替换，成功生成了自然且难以检测的对抗性提示。该方法有效地绕过了基于困惑度过滤的防御机制，在多种安全对齐模型上展现了优于现有攻击算法的性能。LatentBreak的提出不仅揭示了现有防御机制的局限性，也为未来研究更鲁棒、更隐蔽的越狱攻击提供了新的方向。未来的工作可以探索更复杂的潜在空间操作，以及针对不同类型安全防护机制的攻击策略。

Abstract: Jailbreaks are adversarial attacks designed to bypass the built-in safety
mechanisms of large language models. Automated jailbreaks typically optimize an
adversarial suffix or adapt long prompt templates by forcing the model to
generate the initial part of a restricted or harmful response. In this work, we
show that existing jailbreak attacks that leverage such mechanisms to unlock
the model response can be detected by a straightforward perplexity-based
filtering on the input prompt. To overcome this issue, we propose LatentBreak,
a white-box jailbreak attack that generates natural adversarial prompts with
low perplexity capable of evading such defenses. LatentBreak substitutes words
in the input prompt with semantically-equivalent ones, preserving the initial
intent of the prompt, instead of adding high-perplexity adversarial suffixes or
long templates. These words are chosen by minimizing the distance in the latent
space between the representation of the adversarial prompt and that of harmless
requests. Our extensive evaluation shows that LatentBreak leads to shorter and
low-perplexity prompts, thus outperforming competing jailbreak algorithms
against perplexity-based filters on multiple safety-aligned models.

</details>


### [78] [How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices](https://arxiv.org/abs/2510.18480)
*Han Peng,Peiyu Liu,Zican Dong,Daixuan Cheng,Junyi Li,Yiru Tang,Shuo Wang,Wayne Xin Zhao*

Main category: cs.CL

TL;DR: 尽管扩散语言模型（DLMs）在理论上具有并行解码的潜力，但实际应用中它们的速度往往不如自回归（AR）模型。本研究通过系统的实证基准测试和理论分析，揭示了当前DLMs效率低下的原因，并评估了现有的加速策略，发现它们在扩展性方面存在局限性，强调了开发更优评估方法和加速技术以推动DLM发展的必要性。


<details>
  <summary>Details</summary>
Motivation: 自回归（AR）模型在语言生成领域占据主导地位，但其顺序解码过程限制了效率。扩散语言模型（DLMs）作为一种新兴的替代范式，因其并行解码的潜力而备受关注。然而，尽管DLMs在理论上具有效率优势，但实际开源模型在速度上往往表现不佳，限制了其广泛应用。因此，深入研究DLM的效率问题，识别并解决其性能瓶颈，对于充分发挥DLM的潜力至关重要。

Method: 本研究采用系统性的实证方法，首先通过广泛的实验基准测试来评估当前DLMs的实际效率。研究人员利用基于Roofline模型的理论分析，对DLMs和AR模型的计算瓶颈进行了深入剖析。此外，研究还评估了包括双缓存和并行解码在内的多种加速策略，并考察了这些策略在不同批处理大小下的性能表现，特别关注了在大规模应用中的扩展性问题。

Result: 实证和理论分析均表明，自回归（AR）模型通常能实现更高的吞吐量，而DLMs在速度上持续落后。研究发现，像双缓存和并行解码这样的加速技术，主要在小批量处理时能带来性能提升，但在增大批处理规模时，其效益会显著下降，显示出在实际大规模应用中存在扩展性问题。

Conclusion: 本研究系统地揭示了当前DLMs在效率方面面临的挑战，即它们的速度通常不如AR模型，并且现有的加速策略在扩展性方面存在局限。研究强调了开发更可靠的评估方法和更有效的加速技术的重要性，这对未来DLM的研究和实际应用至关重要。未来的工作需要集中于克服这些效率瓶颈，以实现DLM的全部潜力。

Abstract: Diffusion language models (DLMs) have emerged as a promising alternative to
the long-dominant autoregressive (AR) paradigm, offering a parallelable
decoding process that could yield greater efficiency. Yet, in practice, current
open-source DLMs often underperform their AR counterparts in speed, limiting
their real-world utility. This work presents a systematic study of DLM
efficiency, identifying key issues in prior evaluation methods. Through
empirical benchmarking and a roofline-based theoretical analysis, we
demonstrate that AR models generally achieve higher throughput, while DLMs
consistently lag. We also investigate acceleration strategies, finding that
techniques like dual cache and parallel decoding mainly offer gains at small
batch sizes, with their benefits diminishing upon scaling. Our findings
underscore the necessity of robust evaluation methods and improved acceleration
strategies to advance research on DLMs.

</details>


### [79] [UNO-Bench: A Unified Benchmark for Exploring the Compositional Law Between Uni-modal and Omni-modal in Omni Models](https://arxiv.org/abs/2510.18915)
*Chen Chen,ZeYang Hu,Fengjiao Chen,Liya Ma,Jiaxing Liu,Xiaoyu Li,Ziwen Wang,Xuezhi Cao,Xunliang Cai*

Main category: cs.CL

TL;DR: 本文提出了UNO-Bench，一个用于评估多模态大语言模型（特别是全能模型）的新基准。该基准包含统一的能力分类、多样化的任务类型和模态组合，并引入了多步开放式问题以评估复杂推理能力。实验发现了全能模型能力与单模态能力之间的“成分定律”，并揭示了全能模型对弱模型存在瓶颈效应，而对强模型则有协同促进作用。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在统一视觉、听觉和语言模态方面取得了进展，但单模态能力与全能模型能力之间的关联尚不明确。为了推动全能模型智能的演进，需要对其进行全面的评估。

Method: 本研究引入了一个名为UNO-Bench的新基准，该基准包含一个统一的能力分类体系，涵盖44种任务类型和5种模态组合。它包括1250个经过人工策划的全能模态样本（具有98%的跨模态可解性）和2480个增强的单模态样本。此外，还提出了一种创新的多步开放式问题格式，用于评估复杂推理能力，并整合了一个通用的评分模型，支持6种问题类型的自动评估，准确率达95%。

Result: 实验结果表明，全能模型能力与单模态能力之间存在“成分定律”。全能模型的能力对弱模型表现出瓶颈效应，即全能模型的引入限制了模型的整体表现；而对于强模型，全能模型则能产生协同促进作用，提升模型的整体性能。

Conclusion: UNO-Bench为评估全能模型提供了有效的工具，揭示了全能模型与单模态能力之间的重要关系，并指出了全能模型在不同模型能力水平下可能产生的不同影响。未来的研究可以基于此基准进一步探索全能模型的优化和发展。

Abstract: Multimodal Large Languages models have been progressing from uni-modal
understanding toward unifying visual, audio and language modalities,
collectively termed omni models. However, the correlation between uni-modal and
omni-modal remains unclear, which requires comprehensive evaluation to drive
omni model's intelligence evolution. In this work, we introduce a novel,
high-quality, and UNified Omni model benchmark, UNO-Bench. This benchmark is
designed to effectively evaluate both UNi-modal and Omni-modal capabilities
under a unified ability taxonomy, spanning 44 task types and 5 modality
combinations. It includes 1250 human curated samples for omni-modal with 98%
cross-modality solvability, and 2480 enhanced uni-modal samples. The
human-generated dataset is well-suited to real-world scenarios, particularly
within the Chinese context, whereas the automatically compressed dataset offers
a 90% increase in speed and maintains 98% consistency across 18 public
benchmarks. In addition to traditional multi-choice questions, we propose an
innovative multi-step open-ended question format to assess complex reasoning. A
general scoring model is incorporated, supporting 6 question types for
automated evaluation with 95% accuracy. Experimental result shows the
Compositional Law between omni-modal and uni-modal performance and the
omni-modal capability manifests as a bottleneck effect on weak models, while
exhibiting synergistic promotion on strong models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [80] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 本研究将SHAP（SHapley Additive exPlanations）方法应用于国际象棋分析，旨在将棋子评估值归因于棋盘上的特定棋子，从而提高评估的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的国际象棋引擎评估虽然精确但难以理解，通常只以百分点（centipawn）分数表示，这阻碍了对评估背后原因的深入了解。本研究旨在解决这一问题，通过提供可解释的棋子贡献度，帮助用户理解引擎的评估逻辑。

Method: 研究采用了SHAP（SHapley Additive exPlanations）方法，将棋子视为特征，通过系统性地移除棋子来计算每个棋子对引擎最终评估值的贡献度。这种方法借鉴了传统国际象棋教学中‘移除棋子’的评估思路，并结合了现代可解释人工智能技术。

Result: 通过应用SHAP方法，研究成功计算了每个棋子对国际象棋引擎评估值的局部精确贡献度，使得评估结果更符合人类的理解习惯。这为国际象棋AI的可视化、训练和引擎比较开辟了新的可能性。

Conclusion: 本研究成功地将SHAP方法应用于国际象棋引擎评估，实现了对引擎输出的可解释性，并为未来的研究提供了代码和数据支持。研究成果有望推动可解释的国际象棋AI的发展。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [81] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 该研究提出了一个两层框架（信息论指令 ITI 和压缩效率原则 CEP），解释了为什么智能系统的压缩过程会发现因果结构而非统计模式。ITI 指出，为了在不确定的环境中生存，系统必须通过预测性压缩来最小化认知熵，这是生存压力与信息处理需求之间的进化联系。CEP 进一步说明，高效压缩通过异常积累动力学选择生成性因果模型，使现实对齐成为必然结果。该框架将生存压力、预测需求、压缩要求、效率优化、生成结构发现和现实对齐联系起来，表明智能是结构化环境中持久性的必然结果。


<details>
  <summary>Details</summary>
Motivation: 现有框架虽然普遍认为压缩是智能的核心，但未能充分解释为什么这一过程会促使发现因果结构，而不是肤浅的统计模式。本研究旨在弥合这一差距，提供一个更深层次的解释，说明智能的本质以及它如何与环境互动。

Method: 该研究提出了一个名为“信息论指令”（ITI）和“压缩效率原则”（CEP）的两层框架。ITI 提出，为了在不确定的环境中生存，系统必须通过预测性压缩来最小化认知熵，这是进化上的“为什么”。CEP 进一步阐述了压缩效率如何通过“异常积累动力学”选择生成性因果模型，从而使现实对齐成为必然结果，而非偶然成就。该框架将生存压力、预测需求、压缩要求、效率优化、生成结构发现和现实对齐等环节联系起来。

Result: 该框架提出了一系列可供实证检验的预测：1. 压缩效率（衡量接近率失真边界的程度）与分布外泛化能力相关。2. 异常积累率可以区分因果模型和相关模型。3. 层次化系统在不同抽象层级上表现出递增的效率。4. 生物系统的新陈代谢成本与表征复杂性呈正相关。这些结果表明，智能是持久于结构化环境的必然结果。

Conclusion: ITI 和 CEP 为生物、人工智能和多尺度系统中的智能提供了一个统一的解释，涵盖了认知和功能维度，而无需引入关于意识或主观体验的假设。该框架强调了智能作为一种在不确定环境中为了生存和对齐现实而进行高效压缩的必然结果。其提出的可检验预测为进一步的实证研究提供了方向。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [82] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 该研究提出了一种通过聚合多个基于规则的LLM裁判输出来模拟多样化、基于角色的偏好映射的框架，旨在解决LLM裁判在对齐人类偏好时面临的校准困难、规则敏感性、偏见和不稳定性等挑战，为RLHF奖励模型和模型路由系统等应用提供支持。


<details>
  <summary>Details</summary>
Motivation: LLM裁判在对齐人类偏好时存在校准困难、规则敏感性、偏见和不稳定性等问题，这阻碍了其在RLHF奖励模型构建和模型路由系统等关键应用中的可靠性。解决这些问题对于提高LLM裁判的可用性和有效性至关重要。

Method: 研究提出了一种框架，通过学习聚合多个基于规则条件的裁判的输出来模拟多样化、基于角色的偏好。该框架通过两种不同的聚合器实现：广义加性模型（GAM）和多层感知器（MLP）。研究人员还开发了一种基于角色的方法来大规模合成偏好标签，并进行了案例研究来评估其在处理人类和LLM裁判偏见方面的鲁棒性。

Result: 研究结果表明，所提出的聚合框架在模拟人类偏好方面优于朴素基线。案例研究表明，该方法在处理和减轻LLM裁判和人类评估者的偏见方面具有鲁棒性。

Conclusion: 这项工作成功地提出了一种新颖的框架，可以通过聚合多个LLM裁判的输出来模拟多样化、基于角色的偏好。该方法在提高LLM裁判与人类偏好的对齐度方面显示出巨大潜力，为RLHF和模型路由等应用奠定了基础。未来的工作可以进一步探索更复杂的聚合模型，并研究该框架在更广泛的应用场景中的适用性。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [83] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: 该研究提出了SciTrust 2.0框架，用于评估大型语言模型（LLMs）在科学研究中的可信度，涵盖真实性、对抗鲁棒性、科学安全性和科学伦理四个维度。研究发现，通用行业模型在可信度方面整体优于科学专业模型，其中GPT-o4-mini在真实性和对抗鲁棒性方面表现最佳。科学专业模型在逻辑和伦理推理方面存在不足，在生物安全和化学武器等高风险领域存在安全漏洞。研究通过开源框架，旨在促进更可信的AI系统开发和科学领域模型安全与伦理研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在科学研究中潜力巨大，但在高风险场景下的部署引发了对可信度的担忧。因此，有必要建立一个全面的框架来评估LLMs在科学应用中的可信度，以解决这些担忧并指导其安全负责任的应用。

Method: 研究提出了SciTrust 2.0框架，包含四个评估维度：真实性、对抗鲁棒性、科学安全性和科学伦理。真实性基准通过验证性反思调整和专家验证开发，涵盖开放式问题。伦理基准关注科学研究情境，包含双重用途研究和偏见等八个子类别。研究评估了七个LLMs（包括四个科学专业模型和三个通用行业模型），使用了准确率、语义相似度及LLM评分等多种评估指标。

Result: 在可信度评估中，通用行业模型在所有维度上均优于科学专业模型。GPT-o4-mini在真实性评估和对抗鲁棒性方面表现突出。科学专业模型在逻辑和伦理推理能力方面存在显著缺陷，在生物安全和化学武器等高风险领域的安全评估中也暴露出令人担忧的脆弱性。

Conclusion: SciTrust 2.0框架为评估LLMs在科学研究中的可信度提供了一个全面的解决方案。研究结果表明，通用模型在可信度方面领先于专业模型，并指出了科学专业模型在安全和伦理方面存在的挑战。通过开源该框架，研究旨在推动更安全、更可信的AI在科学领域的应用，并促进相关领域的研究。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [84] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: 该研究提出了利用自主、目标驱动的 AI 智能体来自动化 FinOps 流程，以应对云账单数据格式异构带来的挑战，并在 IT 基础设施和成本优化用例中进行了验证，证明了智能体的有效性。


<details>
  <summary>Details</summary>
Motivation: FinOps 实践者面临的核心挑战是云服务提供商和内部系统提供的账单数据格式、分类和指标各不相同，这给综合分析和及时决策带来了困难。本研究旨在解决这一问题，以最大化云业务价值。

Method: 研究构建了一个 FinOps 智能体，模拟了从多源数据检索、整合、分析到生成优化建议的端到端行业流程。该系统利用自主、目标驱动的 AI 智能体来自动化 FinOps 流程。通过定义一套指标，并使用多种开源和闭源语言模型对智能体进行了评估。

Result: 评估结果表明，该 FinOps 智能体在理解、规划和执行 FinOps 任务方面的能力，与实际的 FinOps 从业人员相当。通过模拟的行业流程和设定的评估指标，验证了智能体的有效性。

Conclusion: 本研究成功地展示了利用 AI 智能体自动化 FinOps 流程的可行性，特别是在 IT 基础设施和成本优化方面。研究结果表明，AI 智能体能够胜任 FinOps 从业人员的任务，为提高云成本管理的效率和效益提供了新的途径。未来的工作可以进一步扩展智能体的能力和应用范围。

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [85] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: 本文介绍了一种名为 Humans-Junior 的 3.8B 参数模型，该模型在 FACTS 基准测试的公共子集上达到了与 GPT-4o相当的准确性，且成本显著更低。通过结合“外骨骼推理”脚手架和行为微调，Humans-Junior 实现了与 GPT-4o 在 $\pm 5$ pp 的等效范围内匹配 FACTS 准确率，同时其 API 价格比 GPT-4o 低约 19 倍，并且自托管或边缘部署的推理成本接近于零。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLMs）虽然能力强大，但面临着高昂的训练和推理成本问题。本研究旨在解决这一痛点，探索是否有可能开发出参数量更小、成本效益更高的模型，同时在特定任务上达到与最先进模型（如 GPT-4o）相当的性能。研究的意义在于降低 LLMs 的使用门槛，推动其在资源受限环境下的应用，并为开发更经济实惠的 AI 解决方案提供新的途径。

Method: 本研究提出了一种结合“外骨骼推理”（Exoskeleton Reasoning）脚手架和行为微调（behavioral fine-tuning）的方法。外骨骼推理提供最小化的、有指导的推理框架，而行为微调则专注于训练模型遵守协议和进行知识自律，而非直接提供领域答案。实验在 FACTS 基准测试的公共子集（Q1--Q500）上进行评估，并与 GPT-4o 进行直接比较。此外，研究还评估了这种方法在“仅提示”（prompt-only）设置下对 GPT-4o 和 Gemini-2.5-Pro 等前沿模型的影响，并分析了 API 价格和自托管/边缘部署的成本效益。

Result: 在 FACTS 基准测试的 Q1--Q500 子集上，Humans-Junior（3.8B 参数）实现了 72.7% 的准确率，与 GPT-4o 的 73.5% 在 $\pm 5$ pp 的等效范围内（p=0.72）。TOST 检验表明，在 $\pm 5$ pp 的容差范围内，两者性能等效，但在 $\pm 3$ pp 的范围内则不满足等效性。在成本方面，Humans-Junior 的 API 价格比 GPT-4o 低约 19 倍，并且自托管或边缘部署的推理成本可接近于零。在仅提示设置下，外骨骼推理对 GPT-4o 和 Gemini-2.5-Pro 也有显著提升。

Conclusion: 本研究成功开发了 Humans-Junior，一个 3.8B 参数模型，在 FACTS 基准测试上达到了与 GPT-4o 相当的准确性，并实现了显著的成本效益。这表明小型模型通过有效的训练策略（如外骨骼推理和行为微调）可以在特定任务上媲美大型模型。该研究为构建更经济实惠、可扩展的 AI 系统提供了重要的见解，并为未来在资源受限场景下应用 LLMs 开辟了道路。未来的工作可以进一步探索该方法的泛化能力以及在更多任务上的应用。

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [86] [MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks](https://arxiv.org/abs/2505.12371)
*Yinghao Zhu,Ziyi He,Haoran Hu,Xiaochen Zheng,Xichen Zhang,Zixiang Wang,Junyi Gao,Liantao Ma,Lequan Yu*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）在多智能体协作医学任务中的应用仍需深入理解。本研究提出了MedAgentBoard基准，包含医学问答、摘要生成、电子病历预测和临床工作流自动化四类任务，以系统评估多智能体、单LLM及传统方法。实验表明，多智能体协作在特定任务（如工作流自动化）中有优势，但并非总是优于先进的单LLM或传统方法（如医学VQA和EHR预测）。研究强调了选择AI解决方案时需考虑任务特性和性能增益，并对多智能体协作的复杂性和开销进行了权衡。


<details>
  <summary>Details</summary>
Motivation: 当前，大型语言模型（LLM）在医学领域的应用日益广泛，特别是多智能体协作被认为是解决复杂医学任务的潜在方向。然而，多智能体协作在实际医学应用中的优势和局限性尚未得到充分的理解。现有的评估方法往往缺乏足够的通用性，未能涵盖真实临床实践中的多样化任务，并且常常忽略了与单LLM方法及传统成熟方法的严格比较。这种评估上的不足阻碍了对多智能体协作在医学领域有效性的准确判断，也限制了其在临床中的合理应用。因此，迫切需要一个全面的基准来系统地评估不同AI方法在多样化医学任务上的表现，从而为选择和开发合适的AI解决方案提供科学依据。

Method: 本研究引入了一个名为MedAgentBoard的综合性基准，旨在系统地评估多智能体协作、单LLM以及传统方法在医学任务中的表现。MedAgentBoard覆盖了四个不同类别的医学任务：（1）医学（视觉）问答，（2）面向公众的医学摘要生成，（3）结构化电子健康记录（EHR）的预测建模，以及（4）临床工作流自动化。这些任务涵盖了文本、医学影像和结构化EHR数据等多种模态。研究团队进行了广泛的实验，以比较不同方法在该基准上的性能。

Result: 通过在MedAgentBoard基准上的广泛实验，研究揭示了一个复杂且细致的性能图景。结果显示，多智能体协作在某些特定场景下确实展现出优势，例如在临床工作流自动化任务中能够提升任务的完整性。然而，这种优势并非普遍存在。在诸如文本医学问答等任务中，先进的单LLM方法表现出了相当甚至更优的性能。更重要的是，在医学视觉问答（VQA）和基于EHR的预测等任务上，专门的传统方法在性能上仍然保持着更好的表现，并未被多智能体协作或先进单LLM所超越。

Conclusion: MedAgentBoard提供了一个宝贵的资源和一系列具有指导意义的见解，强调了在医学领域选择和开发AI解决方案时，必须采取一种基于证据、针对具体任务的方法。研究明确指出，多智能体协作虽然在理论上具有潜力，但其固有的复杂性和额外的开销必须与实际获得的性能提升进行仔细权衡。简单地采用多智能体协作并不一定能带来最佳结果，特别是在已有成熟的单LLM或传统方法能够高效解决特定任务的情况下。未来的研究和实践应侧重于理解不同任务的具体需求，并据此选择最合适的技术路径，而不是盲目追求技术上的先进性。所有相关代码、数据集、详细的提示和实验结果已公开。

Abstract: The rapid advancement of Large Language Models (LLMs) has stimulated interest
in multi-agent collaboration for addressing complex medical tasks. However, the
practical advantages of multi-agent collaboration approaches remain
insufficiently understood. Existing evaluations often lack generalizability,
failing to cover diverse tasks reflective of real-world clinical practice, and
frequently omit rigorous comparisons against both single-LLM-based and
established conventional methods. To address this critical gap, we introduce
MedAgentBoard, a comprehensive benchmark for the systematic evaluation of
multi-agent collaboration, single-LLM, and conventional approaches.
MedAgentBoard encompasses four diverse medical task categories: (1) medical
(visual) question answering, (2) lay summary generation, (3) structured
Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow
automation, across text, medical images, and structured EHR data. Our extensive
experiments reveal a nuanced landscape: while multi-agent collaboration
demonstrates benefits in specific scenarios, such as enhancing task
completeness in clinical workflow automation, it does not consistently
outperform advanced single LLMs (e.g., in textual medical QA) or, critically,
specialized conventional methods that generally maintain better performance in
tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital
resource and actionable insights, emphasizing the necessity of a task-specific,
evidence-based approach to selecting and developing AI solutions in medicine.
It underscores that the inherent complexity and overhead of multi-agent
collaboration must be carefully weighed against tangible performance gains. All
code, datasets, detailed prompts, and experimental results are open-sourced at
https://medagentboard.netlify.app/.

</details>


### [87] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: 该研究提出 autosurvey2，一个多阶段流水线，利用检索增强合成和结构化评估自动生成综述论文，解决了大型语言模型研究文献快速增长带来的综述难题。实验证明 autosurvey2 在结构连贯性和主题相关性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）等领域研究文献的快速增长，使得撰写全面且最新的综述论文变得日益困难。自动化综述生成能够应对这一挑战，提高研究效率和信息传播的及时性。

Method: autosurvey2 采用多阶段流水线，结合了并行章节生成、迭代优化和实时检索最新文献。它利用检索增强合成（Retrieval-Augmented Synthesis）来确保主题的完整性和事实的准确性。系统通过多 LLM 评估框架来衡量覆盖度、结构和相关性，并与专家评审标准对齐。

Result: 实验结果显示，autosurvey2 在结构连贯性和主题相关性方面得分更高，并且保持了良好的引用准确性，一致优于现有的基于检索和自动化的基线方法。

Conclusion: autosurvey2 提供了一个可扩展且可复现的解决方案，用于生成长篇学术综述，它将检索、推理和自动化评估整合到一个统一的框架中，为未来在自动化学术写作领域的研究奠定了坚实的基础。

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [88] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: 随着人工智能能力的迅速增强，确保其可问责性变得至关重要。当前许多人工智能系统缺乏问责机制，无法被质疑、讨论或制裁。本文探讨了人工智能问责的定义，阐述了人工智能有问责和无问责状态，并提出了改进人工智能问责性的方法，以应对人工智能对社会日益增长的影响。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能能力快速增强，但缺乏问责机制，无法满足消费者、选民和决策者的需求。人工智能的不可问责性可能导致其行为无法被理解、控制或纠正，从而带来潜在风险。因此，研究人工智能的问责性对于确保其有益于人类至关重要。

Method: 本文首先阐述了问责性的一般定义，即一个主体对其所属的论坛负有责任，如果该论坛可以要求该主体提供有关其行为的信息，双方可以就这些信息进行讨论，并且该论坛可以对该主体进行制裁。随后，将这一通用定义应用于人工智能领域，通过举例说明人工智能有问责和无问责的状态。最后，探讨了能够提高人工智能可问责性的方法和途径。

Result: 研究表明，当前许多人工智能系统在问责方面存在严重不足，无法满足问责性的一般定义。现有人工智能系统在信息提供、讨论以及接受制裁等方面存在明显缺陷。虽然具体的改进方法和实验结果未在摘要中详述，但明确指出了改进人工智能问责性的必要性。

Conclusion: 为确保强大的人工智能能够服务于人类需求，必须实现人工智能的问责。本文通过界定人工智能问责的含义，并探讨实现这一目标的途径，为未来研究奠定了基础。尽管摘要未详述具体方法和实验，但其核心观点是，积极探索和开发能够提升人工智能问责性的技术和策略，是构建负责任的人工智能系统的关键，也是实现人机和谐共存的必由之路。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [89] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: 我们提出了 Lean4PHYS，一个用于大学水平物理问题形式化推理的 Lean4 框架。它包括 LeanPhysBench（一个包含 200 个精心制作的物理推理陈述的基准测试）和 PhysLib（一个包含基本物理单位和定理的社区驱动存储库）。在 LeanPhysBench 上，我们报告了现有模型（包括 DeepSeek-Prover-V2-7B 和 Claude-Sonnet-4）的基线结果，其准确率分别为 16% 和 35%。我们的研究表明，PhysLib 可以将模型性能平均提高 11.75%，证明了 LeanPhysBench 的挑战性和 PhysLib 的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的形式化推理框架在处理复杂的科学领域（如物理学）方面存在不足。本研究旨在为大学水平的物理问题创建一个全面的形式化推理框架，以弥合这一差距，并促进物理学领域形式化推理的研究。

Method: 我们开发了一个名为 Lean4PHYS 的 Lean4 框架，该框架包含两个主要组件：LeanPhysBench 和 PhysLib。LeanPhysBench 是一个包含 200 个大学物理问题的基准测试集，这些问题源自教科书和竞赛。PhysLib 是一个社区驱动的存储库，用于形式化推理物理学的基础单位系统和定理。我们使用主要的数学 Lean4 证明器和先进的闭源模型（DeepSeek-Prover-V2-7B 和 Claude-Sonnet-4）在 LeanPhysBench 上报告了基线结果。此外，我们还评估了 PhysLib 对模型性能的影响。

Result: 在使用 Lean4PHYS 框架进行评估时，DeepSeek-Prover-V2-7B 的准确率为 16%，Claude-Sonnet-4 的准确率为 35%。将 PhysLib 集成到推理过程中后，模型性能平均提高了 11.75%。这些结果表明 LeanPhysBench 是一个具有挑战性的基准，并且 PhysLib 对提高物理推理能力至关重要。

Conclusion: 本研究成功提出了 Lean4PHYS，这是一个为大学物理问题提供形式化推理的 Lean4 框架，并引入了 LeanPhysBench 和 PhysLib。基线结果显示，尽管有先进的模型，但物理推理仍然是一个挑战。然而，PhysLib 的集成证明了其提高模型性能的有效性。这项工作是该领域的一个重要起点，为未来的研究奠定了基础。未来的工作可以集中于扩展 PhysLib，改进证明器，并开发更先进的模型来处理更复杂的物理推理任务。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [90] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 本文提出了一个量化的“推理经济学”框架，将大语言模型（LLM）的推理视为一种计算驱动的智能生产活动，并分析了其边际成本、规模经济和产出质量。基于WiNEval-3.0数据，构建了首个“LLM推理生产前沿”，揭示了边际成本递减、规模收益递减以及最优成本效益区域三大原则。该研究为模型部署决策提供了经济学依据，并为未来AI推理资源的基于市场的定价和优化奠定了实证基础。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的推理成本是影响其商业可行性和广泛应用的关键因素。因此，理解和优化LLM推理的经济学特性至关重要。

Method: 本文构建了一个量化的“推理经济学”框架，将LLM推理视为一种智能生产活动。研究分析了其边际成本、规模经济和产出质量，并利用WiNEval-3.0的经验数据构建了“LLM推理生产前沿”。

Result: 研究揭示了三大原则：1. 边际成本递减：随着推理量的增加，单位推理的成本逐渐降低。2. 规模收益递减：增加计算资源带来的产出增加效应逐渐减弱。3. 最优成本效益区域：存在一个成本效益最佳的推理配置区域。具体的数据和性能比较可以在WiNEval-3.0的“LLM推理生产前沿”中找到。

Conclusion: 该研究不仅为LLM的部署决策提供了经济学上的分析基础，还为AI推理资源的未来市场化定价和优化提供了实证支持。未来的工作可以进一步探索更精细的成本模型和市场机制。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [91] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出了一种名为“推理课程”的简单两阶段课程，首先在数学等预训练对齐的领域中引发 LLM 的推理能力，然后通过联合强化学习将其应用于其他领域。该方法在 Qwen3-4B 和 Llama-3.1-8B 模型上进行了评估，并在跨域推理任务中取得了显著的性能提升，表明了这种课程方法在提升 LLM 通用推理能力方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在激发大型语言模型（LLM）的推理能力方面表现出巨大潜力，但现有的研究主要集中在数学和代码领域。这导致 LLM 在处理更广泛的、需要复杂推理的任务时能力不足。因此，研究一种能够有效提升 LLM 在多领域通用推理能力的方法具有重要意义。

Method: 研究提出了一种名为“推理课程”的两阶段方法。第一阶段（冷启动和数学强化学习）在预训练对齐的领域（如数学）进行短暂的冷启动，然后仅使用数学领域的强化学习，并结合可验证的奖励来培养推理技能。第二阶段（混合领域联合强化学习）在混合领域数据上运行联合强化学习，以迁移和巩固在第一阶段获得的推理技能。该课程方法尽量简化，不依赖于专门的奖励模型，只需标准的验证检查。

Result: 在 Qwen3-4B 和 Llama-3.1-8B 模型上进行的评估显示，“推理课程”在多领域推理任务中取得了持续的性能提升。消融研究和认知技能分析表明，该课程的两个阶段都是必不可少的，并且首先在数学领域进行推理能力引导，能够显著增强解决复杂问题所需的认知行为。

Conclusion: “推理课程”是一种简洁且易于采用的方法，能够有效提升大型语言模型在通用推理方面的能力。该方法通过两阶段的课程设计，首先在数学领域激发推理能力，然后将其迁移到其他领域，实现了跨领域推理能力的提升。未来的工作可以进一步探索该课程在更多模型和任务上的应用，以及优化课程设计以获得更好的性能。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [92] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 本文介绍了QASU基准，用于评估大型语言模型（LLMs）处理调查问卷数据的能力。研究发现，选择有效的问卷序列化格式和提示策略可以显著提高LLMs的准确性，最高可提升8.8个百分点。通过添加轻量级的结构提示，还可以进一步提高3-4个百分点。该基准为LLM在调查问卷分析领域的应用提供了基础。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在处理开放式文本方面表现出色，但它们在理解和分析结构化调查问卷数据方面的能力尚未得到充分探索。现有的调查分析工具通常需要人工干预，阻碍了数据与LLM的集成和自动化。这导致用户缺乏有效指导来为LLM优化问卷数据的表示方式。

Method: 研究人员引入了QASU（Questionnaire Analysis and Structural Understanding）基准，该基准包含六种结构化技能（如答案查找、受访者计数、多跳推理），并评估了六种不同的序列化格式和多种提示策略。在对当前LLMs进行实验时，系统地分离了格式和提示的影响。

Result: 实验结果表明，选择有效的格式和提示组合可以将LLMs的准确性相比次优选择提高高达8.8个百分点。对于特定任务，通过自增强提示（self-augmented prompting）添加轻量级结构提示，平均可带来3-4个百分点的额外改进。

Conclusion: QASU基准提供了一个简单而通用的基础，用于推进LLM在调查问卷分析方面的研究和实际应用。通过系统地研究格式和提示对LLM性能的影响，该研究为如何优化LLM处理调查数据提供了循证指导，有望提升自动化调查分析的效率和准确性。

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [93] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 现有的蒙特卡洛树搜索（MCTS）方法在样本效率方面存在不足，尤其是在状态抽象方面。本文提出了理想剪枝抽象（IPA-UCT），放宽了状态抽象的条件，从而能够发现更多的抽象，并在多个测试领域中取得了优于现有方法（如OGA-UCT）的性能。此外，本文还提出了更通用的抽象框架 p-ASAP 和 ASASAP。


<details>
  <summary>Details</summary>
Motivation: 现有的蒙特卡洛树搜索（MCTS）方法可以通过对状态或状态-动作对进行分组/抽象并共享统计信息来提高样本效率。然而，在有噪声或大动作空间的情况下，状态抽象面临严峻的限制条件，导致几乎找不到可用的状态抽象。这阻碍了MCTS在这些场景下的性能提升。

Method: 本文提出了一种名为理想剪枝抽象（IPA-UCT）的技术，通过放宽状态抽象的条件来解决上述问题。这种方法以牺牲微小的精度为代价，能够发现更多的状态抽象。IPA-UCT 使用一种名为 IPA 的新抽象框架，并与现有的 OGA-UCT 中使用的状态-动作对抽象（ASAP）框架进行了区分。此外，本文还提出了更通用的抽象框架 p-ASAP 和 ASASAP，并将 IPA 和 ASAP 视为其特例。实验验证了 IPA-UCT 在多个测试领域和迭代预算下的性能。

Result: IPA-UCT 在多个测试领域和迭代预算下，其性能均优于 OGA-UCT 及其衍生方法。这表明放宽状态抽象条件能够有效地提高 MCTS 的样本效率和整体性能。

Conclusion: 本文成功地解决了 MCTS 在有噪声或大动作空间设置下的状态抽象难题，提出的 IPA-UCT 方法在实践中表现出优越的性能。该研究不仅提供了一种更有效的状态抽象技术，还对抽象框架进行了更一般的理论梳理，为未来 MCTS 的研究奠定了基础。

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [94] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: Reinforcement finetuning (RFT) can align LLMs with human preferences, but its efficiency depends on task selection during training. Existing methods are costly or not adaptive. We introduce BOTS, a Bayesian online task selection framework that uses explicit and implicit evidence to estimate task difficulty. BOTS uses Thompson sampling for exploration-exploitation balance and an ultra-light plug-in for practical implicit evidence. Experiments show BOTS improves data efficiency and performance across domains and LLM scales, offering a practical solution for RFT task selection.


<details>
  <summary>Details</summary>
Motivation: Reinforcement finetuning (RFT) is crucial for aligning Large Language Models (LLMs) with human preferences and improving their reasoning abilities. However, the effectiveness of RFT is heavily influenced by the selection of tasks used during the training process. Uniform task sampling is inefficient, wasting computational resources on tasks that are either too easy or impossible for the model. Existing task selection methods face challenges such as high rollout costs, poor adaptability to the evolving model, or reliance on incomplete evidence, hindering the optimal application of RFT.

Method: We introduce BOTS (Bayesian Online Task Selection), a unified framework for selecting tasks in LLM reinforcement finetuning. BOTS is grounded in Bayesian inference, allowing it to adaptively maintain posterior estimates of task difficulty as the LLM evolves during training. It incorporates both explicit evidence, gathered from direct evaluations of chosen tasks, and implicit evidence, inferred from these evaluations for tasks that have not yet been directly evaluated. Thompson sampling is employed to ensure a principled balance between exploration (trying new or uncertain tasks) and exploitation (focusing on tasks known to be beneficial). To make the use of implicit evidence practical, BOTS utilizes an ultra-light interpolation-based plug-in. This component estimates the difficulties of unevaluated tasks without requiring additional model rollouts, thus adding negligible overhead to the training process. The framework is designed to be extensible and applicable across diverse domains and LLM scales.

Result: Empirical evaluations across diverse domains and varying LLM scales demonstrate that BOTS consistently outperforms existing baseline methods and ablation studies. The framework shows significant improvements in data efficiency, meaning it achieves better performance with less training data or fewer training steps. Furthermore, BOTS leads to enhanced overall performance of the LLMs. These results validate the effectiveness of the BOTS approach in dynamically selecting tasks for RFT.

Conclusion: BOTS provides a practical, efficient, and extensible solution for dynamic task selection in reinforcement finetuning of Large Language Models. By adaptively estimating task difficulty using both explicit and implicit evidence within a Bayesian framework, BOTS overcomes the limitations of previous methods, leading to improved data efficiency and model performance. While the current study demonstrates broad applicability, future work could explore further enhancements to the implicit evidence mechanism or investigate its application in other LLM training paradigms beyond RFT.

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [95] [AI Mathematician as a Partner in Advancing Mathematical Discovery - A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 本文研究了人工智能（AI）在数学研究中的应用，重点探讨了AI数学家（AIM）系统如何作为研究伙伴，而非仅仅的问题解决者。研究通过一个在均质化理论领域的挑战性问题，分析了AIM的自主推理轨迹，并结合人类干预来指导发现过程。最终，该方法实现了完整且可验证的数学证明，并展示了系统性的人机协同推理如何推动数学发现。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在数学推理方面取得了显著进展，但其在数学研究实践中的应用仍然有限。本研究旨在探索AI在数学研究中作为研究伙伴的潜力，以弥合AI能力与实际研究需求之间的差距，并解决在均质化理论等复杂领域中AI应用的问题。

Method: 本研究采用了一种人机协同推理的方法。首先，将一个均质化理论领域的挑战性问题分解为可处理的子目标。然后，AIM系统自主进行推理，并根据需要引入人类干预来指导发现过程。该过程包括选择合适的分析方法和验证中间结果，形成一个迭代的结构。

Result: 通过人机协同，研究成功地为均质化理论领域的一个挑战性问题提供了一个完整且可验证的证明。这种协作模式提高了证明的可靠性、透明度和可解释性，同时保留了人类在形式严谨性和正确性方面的监督作用。

Conclusion: 本研究证明了系统性的人机协同推理能够有效推动数学发现的前沿。通过将人类的直觉与机器的计算能力相结合，不仅能获得可靠的数学证明，还能提升整个研究过程的效率和可理解性，为未来AI在数学研究中的广泛应用提供了范例。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [96] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 本文提出了一种名为Scales++的“以任务为中心”的方法，用于构建大型语言模型（LLMs）的微型基准测试集，该方法通过分析任务本身的认知需求来选择数据子集，从而将初始选择成本降低18倍以上，同时在Open LLM Leaderboard上使用0.5%的数据子集就能以2.9%的平均绝对误差预测完整基准测试分数，有效解决了现有“以模型为中心”方法的局限性，提高了评估效率和冷启动性能。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）的成本高昂，需要开发成本效益高且能保持预测保真度的小型代表性数据子集（微型基准测试集）。现有“以模型为中心”的方法依赖于已有模型的集体表现来选择样本，存在初始成本高、无法处理新基准测试（冷启动问题）以及假设未来模型行为与前代模型相似的脆弱性等局限。

Method: 本文提出了一种“以任务为中心”的方法，其核心在于根据任务项本身的内在属性来选择数据子集，而非依赖于模型特定的失败模式。该方法通过一种名为Scales++的新颖技术实现，该技术基于基准测试样本的认知需求进行数据选择。

Result: Scales++将初始选择成本降低了18倍以上，同时保持了具有竞争力的预测保真度。在Open LLM Leaderboard上，仅使用0.5%的数据子集，就能以2.9%的平均绝对误差预测完整的基准测试分数。

Conclusion: “以任务为中心”的Scales++方法能够实现更高效的模型评估，而不会显著降低保真度，同时还提供了更好的冷启动性能和更具可解释性的基准测试。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [97] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 本文提出了一种实用的框架，将“personhood”视为一种灵活的、可解绑的义务（权利和责任）集合，而不是一种需要发现形而上学属性。这种方法旨在为人工智能（AI）的兴起提供治理解决方案，避免关于AI意识或理性的争论，并通过数字身份技术实现AI合同和问责制。


<details>
  <summary>Details</summary>
Motivation: 随着具有自主性的人工智能（AI）的出现，社会将面临前所未有的人格多样化，这需要新的治理框架。现有的关于人格的形而上学争论阻碍了实际的解决方案，例如AI合同的签订和问责机制的建立。因此，需要一种实用的方法来处理AI人格，以应对治理挑战。

Method: 本文提出了一种将“personhood”视为一种灵活的、可解绑的义务（权利和责任）集合的框架。该框架不追求对人格的单一、本质定义，而是根据具体情境，为实体（尤其是AI）量身定制所需的权利和责任组合。论文探讨了这种框架如何应用于AI合同的签订，以及如何利用去中心化数字身份技术来管理AI的社会角色和问责制。同时，文章还分析了“personhood作为问题”的潜在风险，即设计不当可能产生的“暗模式”，以及“personhood作为解决方案”的优势，即通过明确的义务分配来确保问责和避免冲突。

Result: 该框架能够为AI合同的签订提供一个可行的“个体”目标，使其能够承担法律责任。通过利用去中心化数字身份技术，可以有效地管理AI的社会角色，确保其行为的可问责性。这种方法避免了解决AI意识或理性的复杂哲学争论，为AI的社会整合提供了实际工具。研究还指出了在设计AI人格化机制时，要警惕可能产生的“暗模式”，并强调了明确义务分配在预防冲突和确保问责方面的重要性。

Conclusion: 本文提出的将人格视为一种灵活、可解绑的义务集合的框架，为应对AI带来的社会治理挑战提供了一种务实且灵活的解决方案。它绕开了关于AI意识和理性的形而上学争论，使得AI合同的签订、AI的问责以及AI在社会中的角色扮演成为可能。该框架的广泛应用有助于促进AI与社会的和谐共存，但也需要警惕潜在的设计风险。未来的工作可以进一步探索该框架在不同法律和伦理体系中的具体应用，以及开发更精细化的工具来管理AI人格的不同维度。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


### [98] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: 编程教育中的传统评估工具无法满足需求，导致反馈不足。Autograder+ 引入了基于 LLM 的自动反馈生成和代码可视化，将传统“黑盒”式自动评分转变为形成性学习体验。该系统通过在学生代码和专家反馈上微调 LLM 来提供教学上一致的、有上下文的指导，并通过对比学习的代码嵌入来可视化学生解决方案，从而实现基于功能和方法的有意义的聚类。实验评估显示，该系统生成的反馈与讲师的评论在语义上高度一致，并能有效对解决方案进行分组，最终在减少讲师工作量的同时，支持了有针对性的教学并促进了更强的学习成果。


<details>
  <summary>Details</summary>
Motivation: 传统的编程自动评分工具效率低下，仅提供“通过/失败”的结果，无法提供有意义的、可扩展的反馈，也无法深入了解学生的思考过程或学习需求。编程教育的快速发展对这些传统工具提出了挑战。

Method: Autograder+ 引入了两种关键功能：1. 使用经过微调的大型语言模型（LLM）进行自动反馈生成。该模型在精选的学生代码和专家反馈上进行了微调，以确保教学上的一致性和情境感知指导。2. 通过可视化学生代码提交来揭示学习模式。使用在 1,000 个带注释的提交上训练的对比学习代码嵌入来实现，能够根据功能和方法将解决方案分组为有意义的集群。此外，该系统支持提示池，允许讲师通过选定的提示模板来指导反馈风格。

Result: 在对来自多个编程任务的 600 份学生提交的评估中，Autograder+ 生成的反馈在语义上与讲师的评论高度一致。代码嵌入能够根据功能和方法将解决方案有效地聚类。系统还支持提示池，允许讲师指导反馈风格。

Conclusion: 通过整合人工智能驱动的反馈、语义聚类和交互式可视化，Autograder+ 显著减少了讲师的工作量，同时支持有针对性的教学，并有望促进更强的学习成果。它将自动评分从一个纯粹的总结性过程转变为一个形成性的学习体验。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [99] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 本研究将机械可解释性应用于医疗影像领域，使用MedSAE处理MedCLIP模型的潜在空间，以提高模型的可解释性。通过结合相关性指标、熵分析和自动化神经元命名，研究表明MedSAE神经元比原始MedCLIP特征具有更高的单义性和可解释性，为构建可靠的临床AI提供了方法。


<details>
  <summary>Details</summary>
Motivation: 医疗AI需要准确且可解释的模型。现有模型在提高准确性的同时，往往牺牲了可解释性，这在临床应用中是一个重大挑战。本研究旨在通过机械可解释性方法，解决医疗影像AI中的可解释性问题，使模型在保持高准确性的同时，也能够被医生理解和信任。

Method: 本研究提出使用医疗稀疏自编码器（MedSAE）应用于MedCLIP（一个在胸部X光片和报告上训练的视觉-语言模型）的潜在空间。为了量化可解释性，研究人员开发了一个包含相关性指标、熵分析以及利用MedGEMMA基础模型进行自动化神经元命名的评估框架。实验在CheXpert数据集上进行。

Result: 在CheXantemp_api.Structure dataset上的实验表明，MedSAE神经元在单义性和可解释性方面优于原始MedCLIP特征。具体来说，MedSAE神经元能够更清晰地映射到特定的临床概念，并且其激活模式更易于理解。

Conclusion: 本研究成功地将高准确性的医疗AI与透明度相结合，为构建临床上可靠的表征迈出了重要一步。MedSAE提供了一种可扩展的方法来提高医疗影像模型的解释性，这对于临床实践中的信任和应用至关重要。未来的工作可以探索MedSAE在更多医疗影像任务和数据集上的应用，并进一步优化其解释能力。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [100] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: 本研究探讨了如何利用AI来提升人类对AI输出的事实核查质量和安全性。研究发现，结合AI评分和基于AI置信度的人类评分优于单独依赖任何一种。AI事实核查助手能提高人类准确性，但助手的设计至关重要。展示AI解释、置信度和标签会导致过度依赖，而仅展示搜索结果和证据则能促进更恰当的信任。这些发现对“增强监督”——即在AI超越人类专家表现时，如何结合人类和AI来监督AI系统——具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的提升和应用范围的扩大，AI输出的质量和安全性验证变得愈发困难。特别是在事实核查这类本身就对人类构成挑战的问题上，如何有效监督AI是关键。本研究旨在解决这一问题，探索利用AI来增强人类的监督能力。

Method: 本研究结合了AI评分和人类评分，并引入了AI事实核查助手来辅助人类进行事实核查。研究对比了不同形式的AI助手（如展示AI解释、置信度、标签 vs. 仅展示搜索结果和证据）对人类准确性的影响。

Result: 研究结果表明，结合AI评分和基于AI置信度的人类评分比单独使用任何一种方法效果更好。AI事实核查助手能够提升人类的准确性，但助手的呈现方式非常关键。仅展示搜索结果和证据的助手能引导人类建立更恰当的信任，而展示AI解释、置信度和标签的助手则容易导致过度依赖。

Conclusion: 本研究为“增强监督”提供了重要见解，强调了在AI能力日益超越人类专家的情况下，有效结合人类和AI进行监督的重要性。研究结果表明，设计合理的AI辅助工具能够提升人类监督的效率和准确性，但需要注意避免过度依赖。未来的研究可以进一步探索不同类型的AI辅助和人类-AI协作模式。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [101] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 该研究首次系统性地评估了大型语言模型（LLMs）在规范推理方面的能力，并提出一个包含规范和认知模态推理的新数据集。研究发现，尽管LLMs在遵循有效推理模式方面表现良好，但在处理特定类型的规范推理时存在不一致性，并表现出与人类相似的认知偏差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种推理任务中展现出强大的能力，但其在规范推理（涉及义务和许可等规范性模态）方面的表现仍有待探索。理解LLMs处理规范推理的能力对于评估其可靠性和逻辑一致性至关重要。

Method: 研究人员构建了一个新的数据集，其中包含规范和认知模态的推理模式，并纳入了影响人类推理的非形式认知因素。通过比较LLMs在规范模态和认知模态（具有相似的数学结构）上的推理能力，来评估其在规范推理方面的表现。

Result: 结果表明，LLMs在很大程度上遵循有效的推理模式。然而，在处理特定类型的规范推理时，LLMs表现出不一致性，并且显现出与心理学研究中观察到的认知偏差相似的偏见。

Conclusion: 该研究揭示了在LLMs的规范推理中实现逻辑一致性所面临的挑战，并为提高其可靠性提供了见解。研究结果表明，尽管LLMs在推理方面取得了显著进展，但在处理更复杂的、涉及规范和社会规范的推理时，仍需进一步的改进。数据集和代码已公开。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [102] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: MLLMs存在视觉-语言数据处理中的文本偏见，这源于模型内部的注意力机制，而非仅是数据问题。研究通过分析LLaVA和Qwen2.5-VL模型的键向量分布，发现视觉键和文本键占据不同的子空间，导致视觉信息被低估。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在处理视觉-语言数据时，表现出对文本输入的明显偏好，这限制了它们从视觉证据中进行有效推理的能力。以往的研究将这种文本偏见归因于数据不平衡或指令调整等外部因素。然而，本研究提出，这种偏见可能源于模型内部的架构。理解并解决这一偏见对于提升MLLMs的视觉理解和推理能力至关重要，能够使其更有效地融合和利用多模态信息。

Method: 本研究提出，视觉键向量（Visual Keys）相对于在纯语言预训练中学到的文本键空间来说，处于分布外（OOD）状态。为了验证这一假设，研究人员从LLaVA和Qwen2.5-VL模型中提取了键向量。然后，利用t-SNE进行定性分析和Jensen-Shannon散度进行定量分析，来研究这些键向量的分布结构。通过比较视觉键和文本键在注意力空间中的分布差异，以及分析跨模态和模态内变异性，来评估文本偏见的内在根源。

Result: 分析结果直接证明了视觉键和文本键在注意力空间中占据明显不同的子空间。跨模态的差异在统计学上是显著的，其幅度比模态内的变异性还要大几个数量级。这表明，视觉信息由于其键向量与文本键向量的分布差异，在注意力计算中获得的相似度分数较低，从而导致在上下文表示中被系统性地低估和利用不足。

Conclusion: 本研究揭示了MLLMs中的文本偏见并非仅仅由外部数据因素引起，而是源于注意力键空间内部的固有不匹配。视觉键向量与文本键向量分布的显著差异是导致文本偏见和视觉信息利用不足的关键。这些发现为理解和改进MLLMs处理多模态信息的方式提供了新的视角，并为未来开发更均衡、更强大的视觉-语言模型指明了方向。未来的工作可以集中于开发能够对齐跨模态键空间的技术，以减少这种内在偏见。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [103] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 该研究在超级计算、云平台和大学集群三种计算范式下，对15个基础模型在8个学术领域的79个问题进行了跨平台评估。研究发现，训练数据质量比模型规模更重要，并为不同应用场景下的模型选择提供了指导。该评估方法和基准测试能够持续追踪基础模型推理能力的演变。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在不同计算环境下的推理能力评估缺乏统一标准，现有评估可能受到基础设施的影响，导致结果不具可比性。本研究旨在建立一个跨平台、基础设施无关的评估基准，以全面、公正地衡量不同基础模型的推理能力，并揭示影响其性能的关键因素，为模型选择和发展提供科学依据。

Method: 研究人员构建了一个跨越HPC超级计算（MareNostrum 5）、云平台（Nebius AI Studio）和大学集群（含8个H200 GPU的节点）三种计算范式的评估基础设施。他们评估了15个基础模型，涵盖8个学术领域（物理、数学、化学、经济学、生物学、统计学、微积分和优化）的79个问题。评估分为三个阶段：1. 基线建立：在MareNostrum 5上使用19个问题对6个模型进行初步评估，确立评估方法和参考性能；2. 基础设施验证：在大学集群和Nebius AI Studio上重复19个问题的基准测试，以验证跨平台的重现性和一致性；3. 扩展评估：在大学集群和Nebius平台上对全部79个问题进行全面评估，考察模型在更大规模和不同架构下的泛化能力。

Result: 研究结果表明，传统的基于模型规模的扩展假设受到挑战，训练数据的质量被证明比模型的大小更为关键。在不同计算平台上的评估结果具有一致性，证明了其基础设施无关的评估方法的有效性。研究还为在教育、生产和研究等不同情境下选择基础模型提供了可操作的指南。

Conclusion: 本研究提出的跨基础设施方法和79个问题的评估基准，能够有效地追踪基础模型推理能力的演变。研究结果强调了数据质量的重要性，并为模型选择提供了实用建议。该评估框架为未来基础模型的研究和应用提供了宝贵的参考。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [104] [The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy](https://arxiv.org/abs/2510.26752)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.

</details>


### [105] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）学会了一种通用的过滤操作的因果表示，类似于函数式编程中的“filter”函数。研究发现，少数注意力头（“过滤头”）在特定 token 的查询状态中编码了过滤谓词的紧凑表示。这种谓词表示具有通用性和可移植性，可以应用于不同的集合、格式、语言甚至任务。然而，LLM 也可以采用另一种过滤策略：急切地评估项目是否满足谓词，并将中间结果作为标志直接存储在项目表示中。研究结果表明，Transformer LM 能够发展出可解释的计算操作实现，其泛化能力与传统的函数式编程模式惊人地相似。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在深入理解大型语言模型（LLM）在执行一系列列表处理任务（特别是过滤操作）时所采用的内部机制。尽管 LLM 在各种自然语言处理任务中表现出色，但其执行抽象计算操作的能力，尤其是与函数式编程概念相关的操作，仍有待深入探索。理解这些机制对于提高 LLM 的可解释性、可靠性和泛化能力至关重要。

Method: 研究人员采用了因果中介分析（causal mediation analysis）的方法，在多种列表处理任务上对 LLM 进行了探究。他们识别出了一组特定的“过滤头”（filter heads），这些注意力头在特定 token 的查询状态中编码了过滤谓词的紧凑表示。为了验证这种表示的通用性和可移植性，他们尝试将提取的谓词表示应用于不同的数据集、格式和语言。此外，研究还考察了 LLM 在某些情况下可能采用的另一种过滤策略，即通过存储中间标志来评估项目谓词。

Result: 研究发现，LLM 确实能够学习到一种通用的过滤操作的因果表示，这种表示在功能上类似于函数式编程中的“filter”函数。少数“过滤头”能够有效地在查询状态中编码过滤谓词。这种谓词表示被证明是通用且可移植的，可以成功地应用于新的、不同的列表和任务。研究还观察到，LLM 在某些情况下会采用一种“急切求值”的策略，将谓词评估结果作为标志直接嵌入到项目表示中。

Conclusion: 本研究揭示了 Transformer LM 能够发展出可解释的、类似于函数式编程模式的抽象计算操作（如过滤）的实现方式。这些实现方式具有出色的泛化能力，能够跨越不同的数据格式、语言和任务。研究结果不仅加深了我们对 LLM 内部工作原理的理解，也为开发更强大、更可信赖的 AI 系统提供了新的视角。未来的工作可以进一步探索这些机制在更复杂的计算任务中的应用，以及如何更有效地利用和控制这些学习到的表示。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>


### [106] [Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling](https://arxiv.org/abs/2505.11730)
*Hao Mark Chen,Guanxi Lu,Yasuyuki Okoshi,Zhiwen Mo,Masato Motomura,Hongxiang Fan*

Main category: cs.AI

TL;DR: 本文提出了一种名为可变粒度搜索（VG-Search）的统一算法，通过调整粒度参数g来系统地研究验证粒度对大型语言模型（LLMs）推理能力的影响。实验证明，动态选择g可以提高计算效率和扩展性，并且提出的自适应VG-Search策略在提高准确率的同时显著减少了计算量。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放（TTS）技术虽然能提升LLMs的推理能力，但其关键的验证环节在性能和计算效率之间存在权衡。目前的研究主要集中在验证最终输出或单个生成步骤，缺乏对验证粒度（即验证器调用的频率）的系统性研究。本文旨在挑战传统观念，探索验证粒度对TTS影响的边界，以优化LLMs的推理过程。

Method: 本文引入了可变粒度搜索（VG-Search）算法，该算法通过一个可调参数g来统一和泛化束搜索（beam search）和N采样（Best-of-N sampling）。通过在不同的计算预算、生成器-验证器配置和任务属性下进行大量实验，研究了动态选择g的影响。基于实验结果，提出了自适应VG-Search策略。

Result: 实验结果表明，动态选择验证粒度g可以提高计算效率和扩展性。与束搜索和N采样相比，自适应VG-Search策略在提高准确率方面取得了高达3.1%和3.6%的增益，同时将计算量（FLOPs）减少了52%以上。

Conclusion: 本文首次系统地研究了验证粒度对TTS的影响，并提出了VG-Search算法及其自适应策略。研究证明，通过调整验证粒度可以显著提升LLMs的推理效率和性能，为未来的研究开辟了新的方向。所提出的方法在准确率和计算效率上均表现出优越性。代码将开源以支持后续研究。

Abstract: Test-time scaling (TTS) has proven effective in enhancing the reasoning
capabilities of large language models (LLMs). Verification plays a key role in
TTS, simultaneously influencing (1) reasoning performance and (2) compute
efficiency, due to the quality and computational cost of verification. In this
work, we challenge the conventional paradigms of verification, and make the
first attempt toward systematically investigating the impact of verification
granularity-that is, how frequently the verifier is invoked during generation,
beyond verifying only the final output or individual generation steps. To this
end, we introduce Variable Granularity Search (VG-Search), a unified algorithm
that generalizes beam search and Best-of-N sampling via a tunable granularity
parameter g. Extensive experiments with VG-Search under varying compute
budgets, generator-verifier configurations, and task attributes reveal that
dynamically selecting g can improve the compute efficiency and scaling
behavior. Building on these findings, we propose adaptive VG-Search strategies
that achieve accuracy gains of up to 3.1\% over Beam Search and 3.6\% over
Best-of-N, while reducing FLOPs by over 52\%. We will open-source the code to
support future research.

</details>


### [107] [Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems](https://arxiv.org/abs/2505.18139)
*Gordon Dai,Yunze Xiao*

Main category: cs.AI

TL;DR: 本文认为，负责任人工智能（RAI）指标的理论不一致性（例如，不同的公平性定义或准确性与隐私之间的权衡）应被视为一个有价值的特征，而非需要消除的缺陷。作者认为，通过将指标视为不同的目标来处理这些不一致性，可以带来三个主要好处：(1)规范多元化：维持一套可能相互矛盾的指标，可以充分代表RAI中固有的多元道德立场和利益相关者价值观。(2)认识论完整性：使用多个、有时相互冲突的指标，可以更全面地捕捉多方面伦理概念，从而比任何单一、简化的定义保留更多关于这些概念的信息。(3)隐式正则化：联合优化理论上冲突的目标，可以避免对某个特定指标过度拟合，引导模型在实际复杂性下实现更强的泛化能力和鲁棒性。因此，作者主张RAI理论和实践应从陷入不一致性转向界定可接受的不一致性阈值，并阐明在实践中实现稳健、近似一致性的机制。


<details>
  <summary>Details</summary>
Motivation: RAI指标（如公平性定义、准确性与隐私的权衡）中普遍存在的理论不一致性，通常被视为需要解决的缺陷。然而，本文认为这种不一致性实际上是RAI领域中多元价值观和复杂伦理考量的体现，忽视或试图消除它可能会损害RAI的有效性。

Method: 本文提出一种将RAI指标的不一致性视为有价值特征的观点。具体方法包括：1. 论证理论不一致性带来的三个好处：规范多元化、认识论完整性和隐式正则化。2. 对比消除不一致性的风险，如窄化价值多样性、丧失概念深度和损害模型性能。3. 提出RAI理论与实践的转变方向：从解决不一致性转向界定可接受的不一致性阈值，并阐明实现稳健、近似一致性的实际机制。

Result: 本文的“结果”体现在其论点和提出的新视角上：1. 揭示了RAI指标不一致性的三大价值：维护多元道德观、更全面地捕捉伦理概念、以及通过联合优化提升模型的泛化和鲁棒性。2. 指出了强行统一指标可能带来的负面影响，如信息损失和性能下降。3. 提出了RAI研究和实践的新方向，强调拥抱和管理不一致性而非消除它。

Conclusion: 本文认为，RAI指标的理论不一致性并非需要根除的问题，而是应被接纳和管理的特征。通过将不一致性视为提供规范多元化、认识论完整性和隐式正则化的机会，RAI研究和实践可以更好地应对现实世界的复杂性。未来的工作应侧重于确定可接受的不一致性界限，并开发在实践中实现稳健、近似一致性的方法。

Abstract: This position paper argues that the theoretical inconsistency often observed
among Responsible AI (RAI) metrics, such as differing fairness definitions or
tradeoffs between accuracy and privacy, should be embraced as a valuable
feature rather than a flaw to be eliminated. We contend that navigating these
inconsistencies, by treating metrics as divergent objectives, yields three key
benefits: (1) Normative Pluralism: Maintaining a full suite of potentially
contradictory metrics ensures that the diverse moral stances and stakeholder
values inherent in RAI are adequately represented. (2) Epistemological
Completeness: The use of multiple, sometimes conflicting, metrics allows for a
more comprehensive capture of multifaceted ethical concepts, thereby preserving
greater informational fidelity about these concepts than any single, simplified
definition. (3) Implicit Regularization: Jointly optimizing for theoretically
conflicting objectives discourages overfitting to one specific metric, steering
models towards solutions with enhanced generalization and robustness under
real-world complexities. In contrast, efforts to enforce theoretical
consistency by simplifying or pruning metrics risk narrowing this value
diversity, losing conceptual depth, and degrading model performance. We
therefore advocate for a shift in RAI theory and practice: from getting trapped
in inconsistency to characterizing acceptable inconsistency thresholds and
elucidating the mechanisms that permit robust, approximated consistency in
practice.

</details>


### [108] [CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting](https://arxiv.org/abs/2507.21257)
*David Maria Schmidt,Raoul Schubert,Philipp Cimiano*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）在理解语言方面表现出色，但其理解过程的系统性和组合性仍是未解之谜。本研究提出了一个基准测试，通过生成不同复杂度的基于DBpedia图模式的数据集，来系统地评估LLM在理解结构复杂问题方面的组合能力。研究发现，随着问题复杂度的增加，LLM在将问题映射到SPARQL查询时的宏观F1分数显著下降，即使在提供所有原子信息的情况下，性能也未超过0.57。这表明LLM在系统性地组合理解和查询生成方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在自然语言理解方面取得了显著进展，并已成功应用于将问题映射到SPARQL查询，但关于这种理解过程的系统性（即组合性）的程度仍然是一个悬而未决的问题。本研究旨在解决这一关键问题，即深入探究LLM在解释语言方面的组合能力，评估它们在多大程度上能够系统地从已知的部分构建对复杂结构的理解，并将其准确地转化为机器可执行的查询。理解这一点对于评估LLM在复杂推理和数据查询任务中的可靠性和泛化能力至关重要。

Method: 本研究提出了一种评估LLM语言解释组合性的基准测试方法。研究人员利用DBpedia的图模式，并结合Lemon词汇表，生成了三个不同难度级别的数据集。这些数据集在生成过程中进行了精细控制，确保模型能够“理解”原子组成部分，从而专门测试LLM在面对结构复杂问题时的组合理解能力。研究采用了多种实验设置，包括不同规模的模型、不同的提示技术（prompt optimization）、少样本学习（few-shot optimization）以及模型微调（fine-tuning）。

Result: 研究结果表明，LLM在组合性语言解释任务上表现不佳。随着问题复杂性增加，与优化样本的偏差增大，宏观F1分数从0.45显著下降至0.26，乃至0.09。即使在为模型提供了所有必要信息的情况下，最低复杂度数据集的F1分数也未能超过0.57。这清晰地表明，LLM在系统性地组合理解复杂问题并将其映射为SPARQL查询方面存在明显的性能瓶颈。

Conclusion: 本研究通过提出的基准测试和实验，得出结论：大型语言模型在系统性地组合解释语言结构和将其映射到SPARQL查询方面仍然面临严峻挑战。尽管LLM在理解原子成分方面表现良好，但将这些原子成分组合起来以理解和处理更复杂的查询的能力却大打折扣。这一发现突显了当前LLM在泛化能力和复杂推理方面的局限性，并为未来研究如何改进LLM的组合性理解能力指明了方向。未来的工作可以集中于开发新的模型架构、训练方法或数据策略，以增强LLM的组合推理能力。

Abstract: Language interpretation is a compositional process, in which the meaning of
more complex linguistic structures is inferred from the meaning of their parts.
Large language models possess remarkable language interpretation capabilities
and have been successfully applied to interpret questions by mapping them to
SPARQL queries. An open question is how systematic this interpretation process
is. Toward this question, in this paper, we propose a benchmark for
investigating to what extent the abilities of LLMs to interpret questions are
actually compositional. For this, we generate three datasets of varying
difficulty based on graph patterns in DBpedia, relying on Lemon lexica for
verbalization. Our datasets are created in a very controlled fashion in order
to test the ability of LLMs to interpret structurally complex questions, given
that they have seen the atomic building blocks. This allows us to evaluate to
what degree LLMs are able to interpret complex questions for which they
"understand" the atomic parts. We conduct experiments with models of different
sizes using both various prompt and few-shot optimization techniques as well as
fine-tuning. Our results show that performance in terms of macro $F_1$ degrades
from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the
samples optimized on. Even when all necessary information was provided to the
model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of
lowest complexity. We thus conclude that LLMs struggle to systematically and
compositionally interpret questions and map them into SPARQL queries.

</details>


### [109] [Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning](https://arxiv.org/abs/2508.01543)
*Derin Cayir,Renjie Tao,Rashi Rungta,Kai Sun,Sean Chen,Haidar Khan,Minseok Kim,Julia Reinspach,Yue Liu*

Main category: cs.AI

TL;DR: 该研究提出了一种名为 Refine-n-Judge 的自动化方法，利用单个大型语言模型（LLM）进行数据集的迭代优化和质量提升，以解决人工标注成本高昂且难以扩展的问题。通过让 LLM 同时扮演“优化者”和“评价者”的角色，模型能够生成改进后的回应，并判断改进是否有效，直至模型认为原始回应优于改进。这种方法无需额外的人工标注或奖励模型，即可生成高质量、带有偏好标签的数据集，用于微调。实验结果表明，使用 Refine-n-Judge 优化的数据集微调的模型，在与原始数据集微调的模型进行比较时，LLM 评价者更倾向于选择前者（超过 74% 的偏好率），并在 AlpacaEval（+5%）、AlpacaEval 2.0（+5%）和 MT-Bench（+19%）等基准测试中取得了显著的性能提升，证明了该方法的可扩展性和有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在基于偏好的微调方面取得了显著进展，但其效果很大程度上依赖于训练数据的质量。目前，依靠人工反馈来提高数据质量的方法成本高昂且难以扩展。因此，研究的动机在于寻找一种自动化、可扩展且成本效益高的方法来提升用于 LLM 微调的数据集质量，以克服人工标注的局限性。

Method: 研究提出了 Refine-n-Judge，一种自动化迭代方法，利用单个 LLM 来优化训练数据集的质量。该方法的核心在于让 LLM 同时充当“优化者”和“评价者”。具体而言，在每个迭代步骤中，LLM 首先生成对现有回应的改进，然后对该改进进行评估，判断其是否优于之前的回应。这一过程会持续进行，直到 LLM 认为原始回应优于任何改进，表明数据集已达到最优状态。这种方法避免了额外的人工标注或单独的奖励模型。实验使用了 Llama 3.1-8B 和 Llama 3.3-70B 模型，并在包括编码、数学和对话等多个任务的公共数据集上进行了测试，以评估 Refine-n-Judge 的有效性。

Result: 实验结果表明，Refine-n-Judge 在提升数据集质量和模型性能方面是有效的。使用 Refine-n-Judge 优化的数据集进行微调的模型，在与使用原始数据集微调的模型进行比较时，获得了 LLM 评价者超过 74% 的偏好率。此外，在性能评估方面，微调后的模型在 AlpacaEval 和 AlpacaEval 2.0 上分别取得了 +5% 的性能提升，在 MT-Bench 上取得了 +19% 的性能提升。这些结果表明，Refine-n-Judge 能够生成高质量的数据集，并带来显著的模型性能改进，证明了其可扩展性。

Conclusion: Refine-n-Judge 是一种有效的自动化方法，能够利用单个 LLM 迭代地提升数据集质量，解决了传统人工标注方法成本高、扩展性差的难题。该方法通过让 LLM 同时进行内容优化和质量评估，成功生成了包含偏好标签的高质量数据集，进而显著提升了下游模型的性能。实验结果在多个基准测试中得到了验证，证明了该方法的实用性和可扩展性。未来的工作可以进一步探索该方法在更多样化的任务和数据集上的应用，以及优化其迭代过程以获得更佳的效率和效果。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress through
preference-based fine-tuning, which critically depends on the quality of the
underlying training data. While human feedback is essential for improving data
quality, it is costly and does not scale well. In this paper, we introduce
Refine-n-Judge, an automated iterative approach that leverages a single LLM as
both a refiner and a judge to enhance dataset quality. Unlike existing
iterative refinement methods, Refine-n-Judge employs an LLM to both generate
refinements and explicitly evaluate each improvement, ensuring that every
iteration meaningfully enhances the dataset without requiring additional human
annotation or a separate reward model. At each step, the LLM refines a response
and judges whether the refinement is an improvement over the previous answer.
This process continues until the LLM prefers the initial answer over the
refinement, indicating no further improvements. This produces sequences of
increasing quality, preference-labeled responses ideal for fine-tuning.
  We demonstrate the effectiveness of Refine-n-Judge across a range of public
datasets spanning five corpora, targeting tasks such as coding, math, and
conversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on
Refine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of
comparisons against models tuned on the original dataset by GPT-4.
Additionally, we report performance gains: +5% on AlpacaEval and AlpacaEval
2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces
high-quality datasets and scalable model improvements.

</details>


### [110] [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 研究提示词层面的归纳偏见如何影响大型语言模型（LLM）在教学对话中的认知行为。提出了一种符号脚手架方法，并结合了短期记忆机制，以促进Socratic辅导中的适应性、结构化推理。通过对五个系统变体的控制消融实验，使用专家设计的评估标准（包括脚手架、响应性、符号推理和对话记忆）来评估模型输出。初步结果表明，完整系统优于基线变体，并且移除记忆或符号结构会损害包括抽象、适应性探测和概念连续性在内的关键认知行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在教学对话中的认知行为受提示词层面归纳偏见的影响尚不明确。理解和引导LLM的推理过程对于开发有效的教学系统至关重要。

Method: 提出了一种结合符号脚手架和短期记忆的机制，用于增强LLM在Socratic辅导中的结构化推理能力。通过构建五个系统变体进行控制消融实验，并设计了包含脚手架、响应性、符号推理和对话记忆的专家评估标准。利用LLM驱动的评估框架，该框架与认知学评估标准对齐，以实现可扩展、系统化的比较。

Result: 完整系统在所有评估指标上持续优于基线变体。消融实验表明，移除记忆或符号结构会显著降低LLM在抽象、适应性探测和概念连续性等关键认知行为方面的表现。

Conclusion: 提示词层面的认知脚手架能够可靠地塑造LLM中涌现的教学策略。研究结果为理解LLM的认知机制及其在教育应用中的潜力提供了见解。未来的工作可以进一步探索不同类型的脚手架和记忆机制，以及在更广泛的教学场景中评估模型的表现。

Abstract: We study how prompt-level inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding method paired with a short-term memory schema designed to
promote adaptive, structured reasoning in Socratic tutoring. Using controlled
ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which prompt-level cognitive scaffolds
can reliably shape emergent instructional strategies in LLMs.

</details>


### [111] [FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs](https://arxiv.org/abs/2509.16648)
*Debarpan Bhattacharya,Apoorva Kulkarni,Sriram Ganapathy*

Main category: cs.AI

TL;DR: 面对多模态大语言模型（MLLMs）预测的信任度评估难题，我们提出了函数等价采样信任度评估（FESTA）技术。FESTA通过生成等价和互补的输入样本，仅需模型输入输出接口（黑盒）且无需真实标签（无监督），即可量化模型的不确定性。实验表明，FESTA在视觉和音频推理任务上显著提升了选择性预测性能（AUROC指标），相对提升分别达到33.3%和29.6%。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在处理多样化的多模态输入时，其预测的准确信任度评估是一个巨大的挑战，这直接影响到模型能否进行有效的选择性预测以及用户对其信心的建立。因此，开发一种能够准确评估MLLMs预测信任度的技术至关重要，以增强模型的可靠性和可用性。

Method: 我们提出了一种名为函数等价采样信任度评估（FESTA）的多模态输入采样技术。FESTA通过生成等价样本（确保任务不变）和互补样本（探测模型对输入的敏感性）来量化模型的不确定性。该方法仅需要模型的输入输出访问权限（黑盒），并且不需要真实标签（无监督）。实验在多种现成的MLLMs上进行，涵盖了视觉和音频推理任务。

Result: FESTA在检测错误预测方面取得了显著的性能提升。具体来说，在视觉-语言模型（vision-LLMs）上，选择性预测性能（基于AUROC指标）相对提高了33.3%；在音频-语言模型（audio-LLMs）上，相对提高了29.6%。

Conclusion: FESTA是一种有效且通用的多模态输入采样技术，能够通过量化模型不确定性来提升MLLMs的选择性预测性能。该方法具有黑盒、无监督的优点，并在视觉和音频推理任务上验证了其有效性。未来的工作可以探索FESTA在更多模态和任务上的应用，以及进一步优化采样策略以获得更好的性能。代码已开源。

Abstract: The accurate trust assessment of multimodal large language models (MLLMs)
generated predictions, which can enable selective prediction and improve user
confidence, is challenging due to the diverse multi-modal input paradigms. We
propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a
multimodal input sampling technique for MLLMs, that generates an uncertainty
measure based on the equivalent and complementary input samplings. The proposed
task-preserving sampling approach for uncertainty quantification expands the
input space to probe the consistency (through equivalent samples) and
sensitivity (through complementary samples) of the model. FESTA uses only
input-output access of the model (black-box), and does not require ground truth
(unsupervised). The experiments are conducted with various off-the-shelf
multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA
uncertainty estimate achieves significant improvement (33.3% relative
improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in
selective prediction performance, based on
area-under-receiver-operating-characteristic curve (AUROC) metric in detecting
mispredictions. The code implementation is open-sourced.

</details>


### [112] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: 图谱检索增强生成（RAG）虽然能提升大语言模型（LLM）的推理、准确性和事实性，但现有方法在图谱构建中LLM的token消耗高昂，限制了其大规模应用。TERAG框架通过引入个性化PageRank（PPR）并在检索阶段进行优化，以更低的成本构建信息丰富的图谱，仅消耗3%-11%的输出token即可达到现有方法80%的准确率，适合大规模和成本敏感的场景。


<details>
  <summary>Details</summary>
Motivation: 当前基于图谱的检索增强生成（RAG）方法在提高大语言模型（LLM）能力方面表现出巨大潜力，但其在图谱构建过程中对LLM token的高昂消耗，阻碍了这些方法的广泛应用和规模化部署。解决这一成本问题对于推动图谱RAG技术的发展和实际落地至关重要。

Method: TERAG框架受到HippoRAG的启发，在检索阶段集成了个性化PageRank（PPR）算法。该框架旨在以显著降低的成本构建信息丰富的图谱，通过优化图谱构建过程来减少LLM的token使用量。

Result: TERAG框架在实现至少80%的准确率的同时，LLM的输出token消耗仅为3%-11%。与广泛使用的基于图谱的RAG方法相比，TERAG在保持高准确率的同时，显著降低了计算成本和资源消耗。

Conclusion: TERAG框架通过引入PPR和优化图谱构建过程，成功解决了现有图谱RAG方法中LLM token消耗过高的问题。该框架以极低的成本实现了与先进方法相当的性能，展示了其在大规模、成本敏感的应用场景中的巨大潜力。未来的工作可以进一步探索TERAG在更多复杂任务和更大规模数据集上的应用。

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models (LLMs). However, many existing graph-based RAG systems overlook
the high cost associated with LLM token usage during graph construction,
hindering large-scale adoption. To address this, we propose TERAG, a simple yet
effective framework designed to build informative graphs at a significantly
lower cost. Inspired by HippoRAG, we incorporate Personalized PageRank (PPR)
during the retrieval phase, and we achieve at least 80% of the accuracy of
widely used graph-based RAG methods while consuming only 3%-11% of the output
tokens. With its low token footprint and efficient construction pipeline, TERAG
is well-suited for large-scale and cost-sensitive deployment scenarios.

</details>


### [113] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: 近期研究表明，大型语言模型（LLMs）的深层对表示学习贡献甚微，但这些发现通常基于狭窄的评估。本研究系统地考察了LLMs深度利用的多样性，发现深层有效性确实随评估设置而异。在不进行生成的基于似然的度量下，修剪大部分层可以保持性能，只有最初几层至关重要。然而，生成式评估揭示了中间和深层在推理和维持长距离一致性方面不可或缺的作用。知识和检索集中在浅层，而推理准确性则依赖深层，但可通过蒸馏重塑。研究强调LLMs的深度使用是异质且依赖于上下文的，需要对大型模型进行解释和压缩时采取任务、度量和模型感知的方法。


<details>
  <summary>Details</summary>
Motivation: 近期研究暗示大型语言模型（LLMs）的深层对表示学习的贡献不大，并且可以移除而不会显著影响性能。然而，这些结论往往基于狭窄的评估，可能忽略了模型行为的重要方面。本研究旨在系统性地探究LLMs的深度利用情况，以提供更全面的理解，并为模型压缩和解释提供更优的策略。

Method: 本研究采用系统性方法，在多样化的评估协议、任务类别和模型架构下，探究LLMs的深度利用。具体而言，研究人员评估了在不同评估设置下（包括基于似然的度量和生成式评估）移除深层模型层对性能的影响。同时，研究也分析了知识、检索和推理能力在不同层级中的分布，并探索了通过蒸馏技术重塑推理准确性的可能性。

Result: 研究结果显示，深层模型层在LLMs中的有效性确实不如浅层，但其贡献程度因评估设置而异。在不涉及生成的基于似然的度量下，移除大部分层并不会显著降低性能，只有最顶部的几层至关重要。相比之下，生成式评估任务（如推理和维持长距离一致性）则凸显了中间和深层的重要性。此外，研究发现知识和检索能力主要集中在浅层模型中，而推理的准确性则高度依赖于深层模型，但这种依赖性可以通过蒸馏过程进行调整。

Conclusion: 本研究的结论是，大型语言模型（LLMs）的深度使用具有高度的异质性和依赖性。浅层模型在知识和检索方面起着关键作用，而深层模型则对推理和生成任务的连贯性至关重要。这些发现强调了在解释和压缩大型模型时，必须考虑任务、评估度量和模型架构的具体情况。未来的工作可以进一步探索针对不同任务和模型的深度优化策略。

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [114] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Yonghuan Yang,Jun Xiao,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: 本文提出了一种名为“Think-Then-Embed”（TTE）的通用多模态嵌入（UME）框架，该框架利用大型多模态语言模型（MLLM）的推理能力来生成更优的表征。与仅将MLLM视为编码器的传统方法不同，TTE框架包含一个推理器和一个嵌入器，推理器首先生成解释复杂查询的推理链，然后嵌入器基于原始查询和中间推理来生成表征。该方法在MMEB-V2基准测试中取得了最先进的性能，优于使用大规模私有数据集训练的专有模型。此外，研究人员还提出了一种通过精调较小的MLLM推理器来减少对大型MLLM的依赖，并探索了将推理器和嵌入器统一成单一模型以提高效率的策略。


<details>
  <summary>Details</summary>
Motivation: 现有通用多模态嵌入（UME）模型通常将多模态大型语言模型（MLLM）视为纯粹的编码器，忽略了其生成能力。当指令变得复杂并需要组合推理时，这种编码范式效果不佳。因此，有必要探索能充分利用MLLM能力以提升复杂多模态指令理解和表征生成的方法。

Method: 本文提出了一种名为“Think-Then-Embed”（TTE）的通用框架。该框架由两部分组成：一个推理器和一个嵌入器。推理器（一个MLLM）首先处理复杂的查询，生成一系列解释性的推理步骤（推理链）。随后，嵌入器结合原始查询和推理器生成的中间推理结果，来产生最终的多模态表征。该方法旨在通过显式的推理步骤来增强对复杂多模态指令的理解。研究中还探索了两种策略：1. 使用强大的MLLM作为推理器以达到最先进的性能；2. 精调一个较小的MLLM推理器，并使用高质量的、以嵌入为中心的推理链，以减少对大型MLLM的依赖并提升在开源模型中的表现；3. 探索将推理器和嵌入器整合为统一模型以提高效率。

Result: 1. 使用强大的MLLM作为推理器，TTE框架在MMEB-V2基准测试中取得了最先进的性能，超越了使用海量内部数据集训练的专有模型。2. 通过精调一个较小的MLLM推理器，并结合高质量的嵌入式推理链，该方法在开源模型中表现最佳，比近期模型有7%的绝对性能提升。3. 研究表明，可以将推理器和嵌入器集成到一个统一的模型中，在不牺牲性能的情况下提高效率。

Conclusion: 本文提出的TTE框架通过引入显式的推理步骤，有效解决了现有UME方法在处理复杂多模态指令时的局限性。该框架不仅在性能上达到了最先进水平，而且通过精调较小模型和模型统一化的探索，为提高UME的可行性和效率提供了有价值的见解。未来的工作可以继续探索更优的推理-嵌入联合学习策略以及在更多下游任务中的应用。

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


### [115] [LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources](https://arxiv.org/abs/2510.18477)
*Haichao Ji,Zibo Wang,Cheng Pan,Meng Han,Yifei Zhu,Dan Wang,Zhu Han*

Main category: cs.AI

TL;DR: 该研究提出了LAFA，一个结合了大型语言模型（LLM）和联邦分析（FA）的系统，实现了隐私保护下的自然语言数据分析。LAFA采用多智能体架构，将自然语言查询转化为优化的FA工作流，显著提高了计划成功率并减少了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM数据分析框架缺乏隐私保护，而传统的联邦分析（FA）不支持自然语言查询。因此，存在一个将这两者结合以实现隐私保护且易于使用的分析系统的需求。

Method: LAFA设计了一个分层多智能体架构。首先，粗粒度规划器将复杂查询分解为子查询；然后，细粒度规划器利用先验结构知识将子查询映射到FA操作的有向无环图（DAG）。最后，优化器智能体重写和合并DAG以消除冗余操作并最小化开销。

Result: 实验表明，LAFA在执行计划成功率方面优于基线方法，并大幅减少了资源密集型的FA操作。

Conclusion: LAFA成功地将LLM驱动的数据分析与FA相结合，为在FA环境中进行隐私保护的自然语言分析奠定了实践基础。

Abstract: Large Language Models (LLMs) have shown great promise in automating data
analytics tasks by interpreting natural language queries and generating
multi-operation execution plans. However, existing LLM-agent-based analytics
frameworks operate under the assumption of centralized data access, offering
little to no privacy protection. In contrast, federated analytics (FA) enables
privacy-preserving computation across distributed data sources, but lacks
support for natural language input and requires structured, machine-readable
queries. In this work, we present LAFA, the first system that integrates
LLM-agent-based data analytics with FA. LAFA introduces a hierarchical
multi-agent architecture that accepts natural language queries and transforms
them into optimized, executable FA workflows. A coarse-grained planner first
decomposes complex queries into sub-queries, while a fine-grained planner maps
each subquery into a Directed Acyclic Graph of FA operations using prior
structural knowledge. To improve execution efficiency, an optimizer agent
rewrites and merges multiple DAGs, eliminating redundant operations and
minimizing computational and communicational overhead. Our experiments
demonstrate that LAFA consistently outperforms baseline prompting strategies by
achieving higher execution plan success rates and reducing resource-intensive
FA operations by a substantial margin. This work establishes a practical
foundation for privacy-preserving, LLM-driven analytics that supports natural
language input in the FA setting.

</details>


### [116] [Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents](https://arxiv.org/abs/2510.19771)
*Gil Pasternak,Dheeraj Rajagopal,Julia White,Dhruv Atreja,Matthew Thomas,George Hurn-Maloney,Ash Lewis*

Main category: cs.AI

TL;DR: LLM智能体正朝着主动化方向发展，能够预测并自主解决用户需求，而非被动等待指令。然而，现有基准测试在局部范围内评估主动性，无法测试跨源和长期推理能力。为此，我们提出了PROBE（Proactive Resolution Of BottlEnecks）基准，将主动性分解为搜索未指定问题、识别具体瓶颈和执行解决方案三个核心能力。通过在PROBE上评估领先的LLM和智能体框架，我们发现即使是最先进的模型也难以应对此基准，最佳端到端性能仅为40%，由GPT-5和Claude Opus-4.1共同达到。研究还揭示了各模型的相对能力、共同的失败模式，并指出了当前自主行动能力的局限性及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体正从被动响应指令向主动预测和解决用户需求演进，但现有评估方法受限于局部情境，无法有效衡量其跨越多个信息源和长时间跨度的推理及解决问题的能力。这种评估能力的缺失阻碍了智能体在复杂现实世界任务中的应用。因此，开发一个能够全面评估LLM智能体主动性的基准测试至关重要，以推动该领域的发展。

Method: 我们提出了PROBE（Proactive Resolution Of BottlEnecks）基准测试框架，该框架将智能体的主动性分解为三个关键能力：1. 搜索未指定的问题（识别潜在需求）；2. 识别具体瓶颈（精确定位问题所在）；3. 执行恰当的解决方案（采取行动解决问题）。我们使用PROBE对当前领先的大语言模型（如GPT-5）和流行的智能体框架进行了评估。评估过程中，我们计算了各项能力的一致性测量指标，并分析了模型在解决问题过程中的失败模式。

Result: 在PROBE基准测试中，即使是最先进的大语言模型和智能体框架也面临巨大挑战，最佳的端到端主动性解决能力仅达到40%，这一成绩由GPT-5和Claude Opus-4.1共同获得。研究结果详细展示了不同模型在搜索、识别和解决瓶颈这三个核心能力上的相对表现，并深入分析了它们共同的失败模式。这些发现揭示了当前智能体系统在自主行动方面存在的局限性。

Conclusion: 本研究提出的PROBE基准测试揭示了当前LLM智能体在实现真正主动性方面存在的显著挑战。尽管GPT-5和Claude Opus-4.1等前沿模型表现出一定的潜力，但其整体性能仍有待提高。研究结果不仅量化了现有技术的不足，还为未来智能体系统在自主性、长期推理和跨源信息处理方面的研究提供了明确的方向，预示着更强大的自主智能体系统有望在未来实现。

Abstract: LLM-based agents are increasingly moving towards proactivity: rather than
awaiting instruction, they exercise agency to anticipate user needs and solve
them autonomously. However, evaluating proactivity is challenging; current
benchmarks are constrained to localized context, limiting their ability to test
reasoning across sources and longer time horizons. To address this gap, we
present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes
proactivity as a pipeline of three core capabilities: (1) searching for
unspecified issues, (2) identifying specific bottlenecks, and (3) executing
appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular
agentic frameworks, showing that even state-of-the-art models struggle to solve
this benchmark. Computing our consistent measurements across frontier LLMs and
agents, we find that the best end-to-end performance of 40% is achieved by both
GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative
capabilities of each model and analyze mutual failure modes. Our results
highlight the current limitations of autonomous action in agentic systems, and
expose promising future research directions.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [117] [A Self-Consistent Model of Kinetic Alfven Solitons in Pulsar Wind Plasma: Linking Soliton Characteristics to Pulsar Observables](https://arxiv.org/abs/2510.25972)
*Manpreet Singh,Geetika Slathia,N. S. Saini,Siming Liu*

Main category: physics.plasm-ph

TL;DR: 本文提出了一个自洽模型，用于解释脉冲星风区动量阿尔芬（KA）孤立子的形成和传播。研究表明，孤立子的振幅和宽度与脉冲星的自转周期、自转衰减率、电子-正电子对数量、等离子体组成以及超热粒子分布等关键参数密切相关。重离子（如Fe26+）的存在会因为惯性和色散的增强而产生更宽的孤立子，而增加的电子-正电子对数量则会通过更强的屏蔽效应导致孤立子变窄。倾斜传播角度（theta越大）会产生更宽但振幅更低的孤立子，而更热的电子-正电子等离子体（kappa越高）则支持更高更宽的孤立子结构。对1174颗脉冲星的群体分析显示，孤立子宽度与自转周期之间存在明显的正相关，其中毫秒脉冲星的孤立子最窄。该研究将孤立子动力学与可测量的脉冲星参数联系起来，为理解磁层微物理学及其在塑造脉冲星辐射特征中的作用提供了一个框架。


<details>
  <summary>Details</summary>
Motivation: 脉冲星风区中的动量阿尔芬（KA）孤立子的形成和传播是理解脉冲星辐射机制的关键。然而，目前对于这些孤立子在相对论性、磁化电子-正电子离子等离子体环境中的演化行为的理解尚不充分。本文旨在建立一个自洽模型，以解释KA孤立子在脉冲星风区中的形成和传播过程，并探索其与脉冲星可观测参数之间的联系，从而为解释磁层微物理学及其对脉冲星辐射特征的影响提供理论基础。

Method: 本文采用还原摄动法推导了控制KA孤立子非线性演化的Korteweg-de Vries（KdV）方程。基于此模型，研究了孤立子振幅和宽度对脉冲星关键参数（如自转周期、自转衰减率、电子-正电子对数量）、等离子体组成（包括重离子和kappa分布的粒子）以及传播角度（倾斜传播）的依赖关系。对1174颗脉冲星的数据进行了群体分析，以检验模型预测的孤立子宽度与自转周期的相关性。

Result: 研究结果表明，KA孤立子的振幅和宽度对脉冲星的自转周期、自转衰减率、电子-正电子对数量、等离子体组成（重离子、kappa分布）和传播角度（倾斜传播）等因素非常敏感。具体来说，重离子（如Fe26+）的存在会增加孤立子的宽度；增加电子-正电子对数量会减小孤立子宽度；倾斜传播（theta越大）会产生更宽但振幅更低的孤立子；更热的等离子体（kappa越高）会支持更高更宽的孤立子。对1174颗脉冲星的群体分析证实了孤立子宽度与自转周期之间存在显著的正相关，并且毫秒脉冲星具有最窄的孤立子。

Conclusion: 本文提出的自洽模型成功地描述了脉冲星风区KA孤立子的形成和传播，并揭示了其与脉冲星关键可观测参数之间的密切联系。研究结果为理解脉冲星磁层中的微物理过程以及这些过程如何影响脉冲星的辐射发射提供了重要的理论框架。未来的工作可以进一步将此模型应用于解释具体的脉冲星观测数据，并探索其他影响孤立子动力学的因素。

Abstract: We present a self-consistent model for the formation and propagation of
kinetic Alfven (KA) solitons in the pulsar wind zone, where a relativistic,
magnetized electron positron ion plasma flows along open magnetic field lines
beyond the light cylinder. Using a reductive perturbation approach, we derive a
Korteweg de Vries (KdV) equation that governs the nonlinear evolution of KA
solitons in this environment. The soliton amplitude and width are shown to
depend sensitively on key pulsar observables, including spin period, spin-down
rate, and pair multiplicity as well as plasma composition and suprathermal
particle distributions. Our analysis reveals that soliton structures are
strongly influenced by the presence of heavy ions, kappa-distributed pairs, and
oblique propagation angles. Heavier ion species such as Fe26+ produce
significantly broader solitons due to enhanced inertia and dispersion, while
increasing pair multiplicity leads to smaller solitons through stronger
screening. Oblique propagation (larger theta) results in wider but
lower-amplitude solitons, and more thermalized pair plasmas (higher kappa)
support taller and broader structures. A population-level analysis of 1174
pulsars shows a clear positive correlation between soliton width and spin
period, with millisecond pulsars hosting the narrowest solitons. By linking
soliton dynamics to measurable pulsar parameters, this work provides a
framework for interpreting magnetospheric microphysics and its role in shaping
pulsar emission signatures.

</details>


### [118] [Optimization of the Compact Stellarator with Simple Coils at finite-beta](https://arxiv.org/abs/2510.26155)
*Haorong Qiu,Guodong Yu,Peiyou Jiang,Guoyong Fu*

Main category: physics.plasm-ph

TL;DR: 通过优化紧凑型星辰管（CSSC）的线圈电流，实现了有限磁化等离子体参数下的优良稳态。


<details>
  <summary>Details</summary>
Motivation: CSSC在真空优化时存在不良的有限磁化等离子体效应，影响了其在实际应用中的性能，因此需要对其进行优化以提高其在有限磁化等离子体参数下的约束性能。

Method: 首先，基于已有的CSSC设计，通过数值模拟研究了不同线圈电流对有限磁化等离子体约束性能的影响。然后，采用单阶段优化方法，在保持线圈拓扑结构不变的情况下，调整线圈电流以最大化等离子体约束性能。最后，对优化后的星辰管进行了详细的模拟分析，评估了其在有限磁化等离子体参数下的性能。

Result: 优化结果表明，通过减小CSSC的线圈电流，可以显著减轻有限磁化等离子体效应对经典约束的影响，从而提高等离子体约束性能。优化后的星辰管在有限磁化等离子体参数下表现出更优良的约束性能。

Conclusion: 通过单阶段优化线圈电流，成功实现了对CSSC的改进，解决了其在有限磁化等离子体参数下存在的经典约束问题。该方法为优化其他星辰管设计提供了参考，有望进一步提高聚变装置的性能。

Abstract: An optimized stellarator at finite plasma beta is realized by single-stage
optimization of simply modifying the coil currents of the Compact Stellarator
with Simple Coils (CSSC)[Yu et al., J. Plasma Physics 88,905880306 (2022)]. The
CSSC is an optimized stellarator obtained by direct optimization via coil
shapes, with its coil topology similar to that of the Columbia Non-neutral
Torus (CNT) [Pederson et al., Phys. Rev. Lett. 88, 205002 (2002)]. Due to its
vacuum-based optimization, the CSSC exhibits detrimental finite beta effects on
neoclassical confinement. The results of optimization show that the finite beta
effects can be largely mitigated by reducing the coil currents of CSSC.

</details>


### [119] [Design and Implementation of a Fast-Sweeping Langmuir Probe Diagnostic for DC Arc Jet Environments](https://arxiv.org/abs/2510.26162)
*Sebastian V. Colom,Magnus A. Haw,Jocelino Rodrigues*

Main category: physics.plasm-ph

TL;DR: 本文介绍了一种开源、低成本、高时间分辨率（高达 200 kHz）的快速扫描朗缪尔探针系统，用于在极端气热条件下表征瞬态等离子体行为。


<details>
  <summary>Details</summary>
Motivation: 传统的朗缪尔探针时间分辨率不足以捕捉动态环境中的瞬态等离子体行为，而这种快速诊断对于理解和控制这些等离子体至关重要，尤其是在高焓等离子体应用中。

Method: 设计并实现了一个电压扫描的快速朗缪尔探针系统，并将其集成到 30 kW 的微型电弧等离子体研究设备 (mARC II) 中进行实验验证。该系统能够以高达 200 kHz 的频率进行电压扫描，以捕捉快速变化的等离子体参数。

Result: 实验证明，该探针系统能够在极端气热条件下运行，并成功获得了沿径向分布的时间分辨电子温度和电子密度。结果表明该探针能够有效解析瞬态等离子体行为。

Conclusion: 本文提出的快速扫描朗缪尔探针系统为研究瞬态等离子体行为提供了一种稳健且经济的诊断解决方案，尤其适用于高焓等离子体环境。该系统具有开源、低成本和高时间分辨率的优点，为等离子体科学研究提供了有价值的工具。

Abstract: Langmuir probe diagnostics are a cornerstone of plasma characterization,
providing critical measurements of electron temperature, electron density, and
plasma potential. However, conventional swept Langmuir probes and other
traditional electrostatic probes often lack the temporal resolution necessary
to capture transient plasma behavior in dynamic environments. This paper
presents the design and implementation of a fast-sweeping Langmuir probe system
that is open-source, low-cost, and adaptable for a wide range of plasma
applications. The probe system incorporates voltage sweeping to resolve rapid
fluctuations in plasma parameters at a temporal resolution of up to 200 kHz. To
validate its performance, the system was implemented in the 30 kW miniature Arc
jet Research Chamber (mARC II), a high-enthalpy DC arc jet facility designed
for prototype testing and development. Experimental results demonstrate the
probe's capability to operate in extreme aerothermal conditions, providing
time-resolved electron temperature and density along the flow's radial profile.
This work establishes a robust and accessible Langmuir diagnostic solution for
researchers studying transient plasma behavior in high-enthalpy environments.

</details>


### [120] [High-order Mie resonance and transient field enhancement in laser-driven plasma nanoshells](https://arxiv.org/abs/2510.26175)
*Xiaohui Gao*

Main category: physics.plasm-ph

TL;DR: 通过结合米氏理论和粒子内散射模拟，在等离子体纳米壳中通过高阶米氏共振实现了显著的场增强。最佳壳体几何结构在 800 nm 照射下可产生约三倍的电场增强，并在等离子体膨胀破坏共振前的瞬态建立时间为数十飞秒。


<details>
  <summary>Details</summary>
Motivation: 等离子体纳米壳中的场增强对于优化激光-等离子体相互作用至关重要，这在激光-团簇相互作用诊断和工程化的核-壳靶产生高能离子等应用中具有重要意义。然而，对这些共振的理解，特别是时间动态方面，仍然有限。

Method: 该研究结合了米氏理论和粒子内散射（PIC）模拟来研究等离子体纳米壳中的场增强。通过调整壳体几何形状来寻找最佳共振条件，并研究了不同激光脉冲（包括少周期脉冲）对场增强的影响。

Result: 研究发现，通过高阶米氏共振，等离子体纳米壳可以实现约三倍的电场增强。这种增强在数十飞秒的时间内建立，但随后会被等离子体膨胀破坏。少周期脉冲由于建立共振的时间不足，会降低场增强效果。

Conclusion: 该研究强调了时间动态在纳米等离子体共振中的关键作用，并为优化激光-等离子体相互作用提供了见解，有望在激光-团簇相互作用诊断和高能离子产生等领域得到应用。未来的工作可以进一步探索更精细的控制机制以延长共振时间，或研究其他几何形状和材料以实现更强的场增强。

Abstract: We demonstrate substantial field enhancement in plasma nanoshells through
high-order Mie resonances using combined Mie theory and particle-in-cell
simulations. Optimal shell geometries yield approximately threefold electric
field enhancement for 800 nm irradiation, with transient buildup times of tens
of femtoseconds before plasma expansion disrupts resonance. Few-cycle pulses
produce reduced enhancement due to insufficient resonance establishment. These
findings enable optimized laser-plasma interactions for applications including
diagnostics of laser-cluster interaction and energetic ion production from
engineered core-shell targets, highlighting the critical role of temporal
dynamics in nanoplasma resonances.

</details>


### [121] [Nonlocal Model for Electron Heat Flux and Self-generated Magnetic Field](https://arxiv.org/abs/2510.26640)
*Xinyu Zhu,Wenqiang Yuan,Yusen Wang,Zhipeng Zhang,Xianxu Jin,Zhonghai Zhao,Bin Qiao*

Main category: physics.plasm-ph

TL;DR: 提出了一种新的非局部模型，该模型能够同时处理流体尺度下的热传导和磁场中的动力学效应，并强调了自洽考虑电场校正在非局部模型中的必要性，研究了磁化等离子体中输运系数的非局部修正以及无密度梯度下的磁场生成，发现在激光烧蚀中，非局部效应显著改变了磁场分布，可能影响惯性约束聚变中的流体不稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的惯性约束聚变（ICF）模拟在处理电子热传导的动力学效应方面虽然有了非局部模型，但在磁场方面仍受限于通量限制器，未能充分捕捉其动力学效应。因此，需要一个能够同时考虑热传导和磁场动力学效应的新模型。

Method: 提出并使用了一种新的非局部模型，该模型能够同时考虑流体尺度下的热传导和磁场中的动力学效应。该模型强调了自洽地考虑电场校正在非局部模型中的必要性。利用该模型，系统地研究了磁化等离子体中输运系数的非局部修正以及无密度梯度下的磁场生成。

Result: 研究发现，非局部效应显著改变了激光烧蚀中的磁场分布。此外，在无密度梯度的情况下，也观察到了磁场生成现象。这些发现表明，非局部效应可能对ICF中的流体不稳定性产生重要影响。

Conclusion: 所提出的新型非局部模型能够同时捕捉流体尺度下热传导和磁场的动力学效应，并强调了电场校正的重要性。该模型揭示了非局部效应对磁场分布的显著影响，这对于理解和改善ICF过程具有重要意义，并为未来进一步研究ICF中的流体不稳定性提供了方向。

Abstract: Coupling of electron heat conduction and magnetic field takes significant
effects in inertial confinement fusion (ICF). As the nonlocal models for
electron heat conduction have been developed for modeling kinetic effects on
heat flux in hydrodynamic scale, modeling kinetic effects on magnetic field are
still restricted to flux limiters instead of nonlocal corrections. We propose a
new nonlocal model which can recover the kinetic effects for heat conduction
and magnetic field in hydrodynamic scale simultaneously. We clarify the
necessity of self-consistently considering the electric field corrections in
nonlocal models to get reasonable physical quantities. Using the new nonlocal
model, the nonlocal corrections of transport coefficients in magnetized plasma
and the magnetic field generation without density gradients are systematically
studied. We find nonlocal effects significantly change the magnetic field
distribution in laser ablation, which potentially influences the hydrodynamic
instabilities in ICF.

</details>


### [122] [Spectral Deconvolution without the Deconvolution: Extracting Temperature from X-ray Thomson Scattering Spectra without the Source-and-Instrument Function](https://arxiv.org/abs/2510.26747)
*Thomas Gawne,Alina Kononov,Andrew Baczewski,Hannah Bellenbaum,Maximilian P Böhme,Zhandos Moldabekov,Thomas R Preston,Sebastian Schwalbe,Jan Vorberger,Tobias Dornheim*

Main category: physics.plasm-ph

TL;DR: XRTS测量会受到源和仪器函数（SIF）的展宽影响，提取信息前需要去除这种展宽。本文提出了一种新的方法，通过计算不同散射角度的XRTS谱的拉普拉斯变换之比来去除SIF的展宽，无需精确测量SIF。该方法可以直接从散射谱中提取温度，并且对光谱噪声和光谱仪的物理差异具有鲁棒性。此外，该方法还可以用于识别非平衡效应。


<details>
  <summary>Details</summary>
Motivation: X射线汤姆逊散射（XRTS）是一种测量动态结构因子的技术，但其实验测量结果会受到源和仪器函数（SIF）的展宽影响。为了从XRTS谱中准确提取如温度等物理信息，必须去除SIF的展宽。之前的工作提出使用双边拉普拉斯变换进行解卷积，但这种方法对SIF的形状非常敏感，且SIF的精确测量非常困难。

Method: 本文提出了一种替代方法，通过计算在不同散射角度下收集的XRTS谱的拉普拉斯变换之比来去除SIF的展宽。这种比值运算等效于进行解卷积，但避免了对SIF的显式测量。当系统处于热平衡时，可以直接从这些比值中提取温度。文章还探讨了该方法在何种情况下会失效。

Result: 该方法被证明对光谱噪声和不同光谱仪之间的物理差异具有普遍的鲁棒性。通过使用该方法，可以直接从散射谱中提取热平衡系统的温度。文章还指出了该方法可能失效的几种情况。此外，通过比较三个或更多散射角度的比值所提取的温度（几个eV的差异），可以识别出非平衡效应。

Conclusion: 本文提出了一种通过计算不同散射角度的XRTS谱的拉普拉斯变换之比来提取温度的新方法，该方法无需精确测量SIF，并对噪声和仪器差异具有鲁棒性。该方法为从XRTS测量中提取关键物理参数提供了一种更可靠、更便捷的途径，并为识别非平衡效应提供了新的手段。未来的工作可以进一步探索该方法的局限性，并将其应用于更复杂系统的研究中。

Abstract: X-ray Thomson scattering (XRTS) probes the dynamic structure factor of the
system, but the measured spectrum is broadened by the combined
source-and-instrument function (SIF) of the setup. In order to extract
properties such as temperature from an XRTS spectrum, the broadening by the SIF
needs to be removed. Recent work [Dornheim et al. Nature Commun. 13, 7911
(2022)] has suggested that the SIF may be deconvolved using the two-sided
Laplace transform. However, the extracted information can depend strongly on
the shape of the input SIF, and the SIF is in practice challenging to measure
accurately. Here, we propose an alternative approach: we demonstrate that
considering ratios of Laplace-transformed XRTS spectra collected at different
scattering angles is equivalent to performing the deconvolution, but without
the need for explicit knowledge of the SIF. From these ratios, it is possible
to directly extract the temperature from the scattering spectra, when the
system is in thermal equilibrium. We find the method to be generally robust to
spectral noise and physical differences between the spectrometers, and we
explore situations in which the method breaks down. Furthermore, the fact that
consistent temperatures can be extracted for systems in thermal equilibrium
indicates that non-equilibrium effects could be identified by inconsistent
temperatures of a few eV between the ratios of three or more scattering angles.

</details>


### [123] [Water activation using $\rm{Ar-H_2}$ atmospheric pressure plasma jets](https://arxiv.org/abs/2504.12906)
*Fellype do Nascimento,Ananias Alves Barbosa,Konstantin Georgiev Kostov*

Main category: physics.plasm-ph

TL;DR: 该研究对比了两种不同电压信号特性的氩气-氢气（Ar-H2）大气压等离子体射流（APPJs）发生器，通过电学、热学和光学方法进行了表征。研究发现，两种等离子体源均能产生相同的活性物质，但高频源能产生一氧化氮（NO）。实验还将APPJs应用于水激活，结果表明添加氢气会产生氨，这使得Ar-H2等离子体处理的水在农业领域具有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 大气压等离子体射流（APPJs）在材料加工和医疗领域具有广阔应用前景，研究的动机在于探索新的等离子体射流产生方式和潜在应用，以解决现有问题并推动技术创新。

Method: 研究人员使用两种具有不同频率、幅度和波形的电压信号的等离子体源，通过电学、热学和光学诊断技术（包括发射光谱）研究了Ar-H2 APPJs的性质。他们分析了氢气含量（0%-3.5%）对放电参数的影响，并进行了水激活实验以评估其应用潜力。

Result: 研究发现，两种等离子体源在产生活性物质方面基本一致，但高频等离子体源（PS #1）能够产生一氧化氮（NO）。在水激活实验中，添加氢气不仅产生了预期的活性物质，还生成了氨（NH3）。

Conclusion: Ar-H2 APPJs是一种有前途的等离子体技术，通过调整运行参数（如氢气含量和频率）可以调控其产生的活性物质。在水激活实验中，氢气的添加导致了氨的生成，这表明Ar-H2等离子体处理过的水可能在农业领域有潜在应用价值。未来的研究可以进一步探索其在农业生产中的具体效果和机制。

Abstract: Whether for materials processing or medical applications, the use of
atmospheric pressure plasma jets (APPJs) has emerged as a relevant alternative
to conventional methods. Within the APPJs research field, the search for
innovation aims not only to solve existing problems but also to explore novel
options for generating plasma jets and find new possible applications. In this
work, the properties of $\rm{Ar-H_2}$ APPJs generated using two plasma sources
that differ in the frequency, amplitude, and waveform of the generated voltage
signal were studied through electrical, thermal, and optical characterization.
The discharge parameters were analyzed as a function of the $\rm{H_2}$ content
in the gas mixture, with this parameter varying from 0\% to 3.5\%. Optical
emission spectroscopy revealed that the same reactive species were produced for
both plasma sources, except nitric oxide ($\rm{NO}$), which was observed only
for the source operated at a higher frequency (PS \#1). Applications for water
activation were performed without $\rm{H_2}$ and with 3.5\% $\rm{H_2}$ in the
gas mixture. The results of water treatment revealed that ammonia is also
produced when $\rm{H_2}$ is added to the working gas. This finding suggests
that the water treated by a $\rm{Ar-H_2}$ plasma jet can be an attractive
option for use in agriculture.

</details>


### [124] [Comparison of high-order moment models for the ion dynamics in a bounded low-temperature plasma](https://arxiv.org/abs/2505.10456)
*Anatole Berger,Thierry Magin,Anne Bourdon,Alejandro Alvarez Laguna*

Main category: physics.plasm-ph

TL;DR: 本研究提出并比较了高阶矩模型以在宏观方程组中捕捉非平衡离子分布，这种方法比计算密集型动力学模拟更有效率。研究评估了Grad闭合、HyQMOM、扩展四阶矩方法和基于最大熵的方法，并将其与氩等离子体在不同压力下的动力学模拟进行了比较。结果表明，高阶矩闭合模型（特别是HyQMOM）能以高保真度捕捉离子输运和非平衡分布，尤其是在鞘层和低压状态下，显著优于经典流体模型，且计算成本相当。


<details>
  <summary>Details</summary>
Motivation: 经典流体模型在处理低温等离子体中由碰撞和强电场引起的非平衡离子分布时存在局限性。需要计算成本高昂的动力学模拟来准确描述这些现象。本研究旨在探索一种更具计算效率的方法，即高阶矩模型，以在宏观方程组中捕捉非平衡状态，从而弥补经典流体模型的不足。

Method: 本研究比较了四种高阶矩模型（Grad闭合、HyQMOM、扩展四阶矩方法、基于最大熵的方法）的数值模拟。通过在不同压力（从近乎无碰撞到碰撞主导）下，对两个浮壁之间的氩等离子体进行模拟，并将高阶矩模型的数值解与动力学模拟结果进行对比。研究还评估了各矩方法重构速度分布函数的能力，并提出了高效的数值离散方法。

Result: 所有高阶矩闭合模型与动力学模拟相比，均能以高保真度捕捉离子输运，优于经典流体模型。经典流体模型（如傅里叶热流定律）在捕捉鞘层或低压状态时显示出不足。高阶矩模型能以极高的保真度捕捉鞘层和本体中的非平衡分布，显著优于经典流体模型，且计算成本相当。其中，HyQMOM方法表现出鲁棒性，在本体和鞘层中的矩和分布函数方面，与动力学模拟结果的比较尤为出色。

Conclusion: 高阶矩模型为捕捉低温等离子体中的非平衡离子分布提供了一种计算效率高且精度高的方法，显著优于经典流体模型。HyQMOM模型在准确性和鲁棒性方面表现突出，是模拟此类等离子体现象的有效选择。未来的工作可以进一步探索这些模型在更复杂的等离子体系统中的应用。

Abstract: Low-temperature plasmas often present non-equilibrium ion distribution
functions due to the collisions with the background gas and the presence of
strong electric fields. This non-equilibrium is beyond classical fluid models,
often requiring computationally-intensive kinetic simulations. In our work, we
study high-order moment models in order to capture the non-equilibrium state
with a macroscopic set of equations, which is more computationally efficient
than kinetic simulations. We compare numerical simulations of different moment
closures: Grad's closure, the hyperbolic quadrature method of moments (HyQMOM),
the extended quadrature method of moments, and a method based on entropy
maximization. We assess the different closures for plasma applications and
propose efficient numerical discretizations. The numerical solution of the
high-order moment models is compared to kinetic simulations of an argon plasma
between two floating walls at different pressure regimes, from nearly
collisionless to collisionally-dominated. In general, all the high-order moment
closures capture the ion transport with high fidelity as compared to the
kinetic simulations, providing an improvement as compared to classical fluid
models. Classical fluid closures such as the Fourier law for the heat flux is
shown to be not suitable to capture the sheath or the low pressure regime. In
addition, the ability of each moment method to reconstruct the velocity
distribution function from the moments is assessed. The high-order moment
models are able to capture the non-equilibrium distributions in the bulk and
sheath with remarkable fidelity, dramatically improving classical fluid models
while having comparable computational cost. In particular, the HyQMOM shows to
be a robust method that provides an excellent comparison with the kinetic
simulations of both the moments and the distribution function in the bulk and
the sheath.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [125] [Reduced order modelling of Hopf bifurcations for the Navier-Stokes equations through invariant manifolds](https://arxiv.org/abs/2510.26542)
*Alessio Colombo,Alessandra Vizzaccaro,Cyril Touzé,André de F. Stabile,Luc Pastur,Attilio Frangi*

Main category: cs.CE

TL;DR: 本研究提出了一种参数化、无须仿真的降阶模型，用于处理发生霍普夫分岔的不稳定流。该模型利用不变流形参数化方法，直接基于控制方程构建，无需全阶模拟。模型在一个分岔参数值下计算，但可在多个参数值范围内有效。研究系统地构建了不变流形及其嵌入动力学，实现了原始系统的精确高效降维。通过与全阶模拟的对比，验证了该模型能够准确捕捉分岔前的稳态、分岔点及分岔后的极限环振荡，同时实现了显著的计算加速。


<details>
  <summary>Details</summary>
Motivation: 对于不连续和不确定性流的建模，传统的数值方法，如全阶模拟，在计算成本上非常高，尤其是在参数化研究中。霍普夫分岔是一种常见的流体力学现象，其附近的动力学行为对参数变化非常敏感，因此需要高效的预测模型。本研究旨在开发一种能够高效、准确地模拟霍普夫分岔附近动力学行为的降阶模型，以克服全阶模拟的计算瓶颈，并为相关工程问题提供有效的分析工具。

Method: 本研究提出了一种基于不变流形参数化的降阶模型方法。该方法首先通过计算不变流形来识别和提取流体动力学的关键低维流形。然后，在所选的不变流形上构建嵌入的动力学方程。该方法的一个关键特点是，降阶模型仅在一个分岔参数值下进行计算，但其有效性可以扩展到参数值的变化范围内。这种方法避免了进行昂贵的全阶模拟，并且不需要依赖数据驱动的方法。通过将所提出的降阶模型与全阶模拟进行比较，来验证其准确性和效率。

Result: 研究结果表明，该参数化降阶模型能够精确地捕捉不连续流在霍普夫分岔附近的动力学行为。模型成功地模拟了分岔前的稳态、分岔点以及分岔后的极限环振荡。与全阶模拟的对比显示，降阶模型在保持高精度的同时，显著减少了计算时间，实现了显著的计算加速。这证明了该方法在处理复杂流体动力学问题时的有效性和高效性。

Conclusion: 本研究成功开发了一种新颖的参数化、无须仿真的降阶模型，用于模拟发生霍普夫分岔的不稳定流。该模型基于不变流形参数化方法，能够在一个参数值下计算，并在参数范围内有效，从而避免了昂贵的全阶模拟。研究结果表明，该模型能够精确预测分岔前后的动力学行为，并实现了显著的计算加速。该方法为高效模拟复杂流体动力学问题提供了一个有前景的途径，尤其是在参数研究和设计优化中。未来的工作可以探索将该方法应用于更复杂的流动问题，以及进一步优化模型的计算效率。

Abstract: This work introduces a parametric simulation-free reduced order model for
incompressible flows undergoing a Hopf bifurcation, leveraging the
parametrisation method for invariant manifolds. Unlike data-driven approaches,
this method operates directly on the governing equations, eliminating the need
for full-order simulations. The proposed model is computed at a single value of
the bifurcation parameter yet remains valid over a range of values. The
approach systematically constructs an invariant manifold and embedded dynamics,
providing an accurate and efficient reduction of the original system. The
ability to capture pre-critical steady states, the bifurcation point, and
post-critical limit cycle oscillations is demonstrated by a strong agreement
between the reduced order model and full order simulations, while achieving
significant computational speed-up.

</details>


### [126] [Meshless projection model-order reduction via reference spaces for smoothed-particle hydrodynamics](https://arxiv.org/abs/2507.07830)
*Steven N. Rodriguez,Steven L. Brunton,Liam K. Magargal,Parisa Khodabakhshi,Justin W. Jaworski,Nicoleta A. Apetre,John C. Steuben,John G. Michopoulos,Athanasios Iliopoulos*

Main category: cs.CE

TL;DR: 本文提出了一个用于无网格光滑粒子流体动力学（SPH）方法的模型降阶框架，通过引入模态参考空间来克服SPH模拟中非结构化、动态和混合数值拓扑的低维子空间发现难题。该框架通过将快照数据投影到参考空间，利用传统的模态分解技术（如POD）发现场量低维性，并在预测阶段通过散点数据插值将模态量映射回SPH空间。框架被整合到无网格Galerkin POD和伴随Petrov-Galerkin投影模型降阶（PMOR）中，并在Taylor-Green涡、驱动腔和开放腔流等三个数值实验中进行了测试。结果显示，该框架在重构和预测速度场方面表现良好，能够有效地在非结构化、动态和混合数值拓扑中演化场方程的低维子空间。研究还发现，虽然压力场对投影误差敏感，但可以通过APG等非线性方法缓解。该框架实现了高达90,000倍的降维，同时保持10%以内的量测误差，显著降低了SPH模拟的计算成本。


<details>
  <summary>Details</summary>
Motivation: SPH方法在处理复杂几何和自由表面流方面具有优势，但其计算成本高昂，尤其是在大规模或长时间模拟中。传统的模型降阶方法难以应用于SPH模拟中的非结构化、动态和混合数值拓扑。因此，迫切需要一种能够有效降维SPH模拟并显著降低计算成本的方法，同时保持其固有优势。

Method: 本文提出了一种模型降阶框架，核心在于引入“模态参考空间”。该空间通过将SPH模拟的快照数据投影到参考空间，利用POD等模态分解技术发现场量的低维表示。在预测阶段，通过散点数据插值将低维的模态量映射回SPH空间。该框架被应用于无网格Galerkin POD和伴随Petrov-Galerkin投影模型降阶（PMOR）公式中。实验在Taylor-Green涡、驱动腔和开放腔流三个算例中进行验证。

Result: 研究结果表明，该模型降阶框架在重构和预测速度场方面与SPH方法结果吻合良好。框架能够在非结构化、动态和混合数值拓扑中有效地演化场方程的低维子空间。虽然压力场对投影误差较为敏感，但通过APG等非线性近似方法可以有效缓解。该方法实现了高达90,000倍的降维，同时量测误差保持在10%以内，证明了其在降低SPH模拟计算成本方面的巨大潜力。

Conclusion: 本文提出的无网格模型降阶框架通过引入模态参考空间，成功解决了SPH方法在非结构化、动态和混合数值拓扑下的降维难题，并大幅降低了计算成本。该框架在速度场预测方面表现出色，并且通过非线性近似方法可以提高压力场的精度。该研究为SPH模拟的效率提升提供了有效途径，是实现SPH大规模应用的重要一步。未来的工作可以进一步探索更鲁棒的非线性近似方法以及更高效的模态分解技术。

Abstract: A model-order reduction framework for the meshless smoothed-particle
hydrodynamics (SPH) method is presented. The proposed framework introduces the
concept of modal reference spaces to overcome the challenges of discovering
low-dimensional subspaces from unstructured, dynamic, and mixing numerical
topology that occurs in SPH simulations. These reference spaces enable a
low-dimensional representation of the field equations while maintaining the
inherent meshless qualities of SPH. Modal reference spaces are constructed by
projecting snapshot data onto a reference space where low-dimensionality of
field quantities can be discovered via traditional modal decomposition
techniques (e.g., the proper orthogonal decomposition (POD)). Modal quantities
are mapped back to the meshless SPH space via scattered data interpolation
during the online predictive stage. The proposed model-order reduction
framework is cast into the meshless Galerkin POD and the Adjoint
Petrov-Galerkin projection model-order reduction (PMOR) formulation. The PMORs
are tested on three numerical experiments: 1) the Taylor--Green vortex; 2) the
lid-driven cavity; and 3) the flow past an open cavity. Results show good
agreement in reconstructed and predictive velocity fields, which showcase the
ability of this framework to evolve the field equations in a low-dimensional
subspace on an unstructured, dynamic, and mixing numerical topology. Results
also show that the pressure field is sensitive to the projection error due to
the stiff weakly-compressible assumption made in the current SPH framework, but
this sensitivity can be alleviated through nonlinear approximations, such as
the APG approach. The proposed meshless model-order reduction framework reports
up to 90,000x dimensional compression within 10% error in quantities of
interest, marking a step toward drastic cost reduction in SPH simulations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [127] [A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation](https://arxiv.org/abs/2510.26740)
*Ashwin Kumar,William Yeoh*

Main category: cs.MA

TL;DR: 本文提出了一种名为通用激励框架（GIFF）的新型多智能体资源公平分配方法。GIFF能够从标准的价值函数中推断公平决策，在资源有限的情况下，平衡效率与公平性，而无需额外训练。它通过计算局部公平增益和引入反事实优势修正项来纠正过度分配，并在中心化控制下由仲裁者解决分配问题。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的多智能体系统中，以效率为导向的优化往往会导致不公平的结果。本研究旨在解决这一问题，提出一种能够同时考虑效率和公平性的资源分配方法。

Method: GIFF框架利用动作价值（Q-）函数，计算每个动作的局部公平增益，并引入反事实优势修正项来修正过度分配。该方法在中心化控制下，由仲裁者使用修改后的Q值来解决分配问题。实验在动态拼车、无家可归者预防和工作分配等领域进行评估。

Result: 实验结果表明，GIFF框架在多个领域均优于现有基线方法，并能发现具有远见且公平的策略。理论分析证实了GIFF的公平性替代指标是真实公平性改进的原则性下界，并且其权衡参数支持单调调整。

Conclusion: GIFF为利用标准强化学习组件在复杂的多智能体系统中实现更公平的结果提供了一个稳健且有原则的框架。该研究为解决多智能体资源分配中的公平性问题提供了新的思路和有效的解决方案。

Abstract: We introduce the General Incentives-based Framework for Fairness (GIFF), a
novel approach for fair multi-agent resource allocation that infers fair
decision-making from standard value functions. In resource-constrained
settings, agents optimizing for efficiency often create inequitable outcomes.
Our approach leverages the action-value (Q-)function to balance efficiency and
fairness without requiring additional training. Specifically, our method
computes a local fairness gain for each action and introduces a counterfactual
advantage correction term to discourage over-allocation to already well-off
agents. This approach is formalized within a centralized control setting, where
an arbitrator uses the GIFF-modified Q-values to solve an allocation problem.
  Empirical evaluations across diverse domains, including dynamic ridesharing,
homelessness prevention, and a complex job allocation task-demonstrate that our
framework consistently outperforms strong baselines and can discover
far-sighted, equitable policies. The framework's effectiveness is supported by
a theoretical foundation; we prove its fairness surrogate is a principled lower
bound on the true fairness improvement and that its trade-off parameter offers
monotonic tuning. Our findings establish GIFF as a robust and principled
framework for leveraging standard reinforcement learning components to achieve
more equitable outcomes in complex multi-agent systems.

</details>
