{"id": "2510.25196", "categories": ["physics.acc-ph"], "pdf": "https://arxiv.org/pdf/2510.25196", "abs": "https://arxiv.org/abs/2510.25196", "authors": ["Ji Qiang", "Jinyu Wan", "Allen Qiang", "Yue Hao"], "title": "Fast chaos indicator from auto-differentiation for dynamic aperture optimization", "comment": null, "summary": "Automatic differentiation provides an efficient means of computing\nderivatives of complex functions with machine precision, thereby enabling\ndifferentiable simulation. In this work, we propose the use of the norm of the\ntangent map, obtained from differentiable tracking of particle trajectories, as\na computationally efficient indicator of chaotic behavior in phase space. In\nmany cases, a one-turn or few-turn tangent map is sufficient for this purpose,\nsignificantly reducing the computational cost associated with dynamic aperture\noptimization. As an illustrative application, the proposed indicator is\nemployed in the dynamic aperture optimization of an ALS-U lattice design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u53ef\u5fae\u8ffd\u8e2a\u7c92\u5b50\u8f68\u8ff9\u5f97\u5230\u7684\u5207\u7ebf\u6620\u5c04\u7684\u8303\u6570\u4f5c\u4e3a\u76f8\u7a7a\u95f4\u6df7\u6c8c\u884c\u4e3a\u7684\u8ba1\u7b97\u9ad8\u6548\u6307\u793a\u5668\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eALS-U\u70b9\u9635\u8bbe\u8ba1\u7684\u52a8\u6001\u5b54\u5f84\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u7684\u52a8\u6001\u5b54\u5f84\u4f18\u5316\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a0\u901f\u5668\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u4f18\u5316\u52a8\u6001\u5b54\u5f84\u3002", "method": "\u5229\u7528\u81ea\u52a8\u5fae\u5206\u8ba1\u7b97\u590d\u6742\u51fd\u6570\u7684\u5bfc\u6570\uff0c\u901a\u8fc7\u53ef\u5fae\u8ffd\u8e2a\u7c92\u5b50\u8f68\u8ff9\u5f97\u5230\u5207\u7ebf\u6620\u5c04\uff0c\u5e76\u8ba1\u7b97\u5176\u8303\u6570\u4f5c\u4e3a\u6df7\u6c8c\u884c\u4e3a\u7684\u6307\u793a\u5668\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u4e00\u4e2a\u6216\u51e0\u4e2a\u5468\u671f\u7684\u5207\u7ebf\u6620\u5c04\u8ba1\u7b97\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728ALS-U\u70b9\u9635\u8bbe\u8ba1\u7684\u52a8\u6001\u5b54\u5f84\u4f18\u5316\u4e2d\u5f97\u5230\u4e86\u6210\u529f\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u4f7f\u7528\u5207\u7ebf\u6620\u5c04\u8303\u6570\u4f5c\u4e3a\u6df7\u6c8c\u884c\u4e3a\u7684\u6307\u793a\u5668\uff0c\u4e3a\u52a0\u901f\u5668\u7269\u7406\u4e2d\u7684\u52a8\u6001\u5b54\u5f84\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u6709\u671b\u5e94\u7528\u4e8e\u5176\u4ed6\u52a0\u901f\u5668\u8bbe\u8ba1\u548c\u76f8\u5173\u9886\u57df\uff0c\u4f46\u672a\u6765\u7684\u7814\u7a76\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u666e\u9002\u6027\u548c\u4f18\u5316\u6f5c\u529b\u3002"}}
{"id": "2510.24771", "categories": ["physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2510.24771", "abs": "https://arxiv.org/abs/2510.24771", "authors": ["Syon Bhattacharjee", "Deepika Behmani", "Sudeep Bhattacharjee"], "title": "Cold atmospheric microplasma jet-water interactions: physicochemical analysis and growth effects in flowering plants", "comment": "7 pages, 7 figures", "summary": "Cold atmospheric pressure plasma jets (APPJs) are non-equilibrium plasmas,\nthat are capable of producing reactive oxygen and nitrogen species (RONS) at\nnear-room temperature. Their interaction with water leads to the formation of\nplasma-activated water (PAW), whose chemical activity depends on discharge\nconditions. In this work, a helium-air (14:1) micro-plasma jet operated in a\nring-to-ring electrode configuration is used to generate PAW and study its\ninfluence on the growth of Chrysanthemum saplings. Optical emission\nspectroscopy (OES) confirms the presence of $N_2$ bands and He lines, with the\nHe-air mixture providing more chemically active discharge (in terms of favoring\nthe generation of nitrates in PAW) as compared to pure helium. The\nphysicochemical characteristics of PAW such as pH, electrical conductivity\n(EC), oxidation-reduction potential (ORP), and total dissolved solids (TDS) are\nanalyzed as a function of plasma treatment time and water volume. The optimum\ncondition for PAW generation is found to be 12 ml of de-ionized (DI) water\ntreated for 40 minutes, which yields the highest ORP and nitrate concentration\nwith a reduced pH. Comparative growth experiments over two weeks show that\nPAW-treated Chrysanthemum saplings exhibit significantly greater height (10.2\ncm) and soil fertility (2580 $\\mu$S/cm) than those watered with same amount of\nDI water or tap water. The results highlight the potential of PAW for\nsustainable enhancement of growth of flowering plants.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u91c7\u7528\u6c26\u6c14-\u7a7a\u6c14\u5fae\u7b49\u79bb\u5b50\u4f53\u5c04\u6d41\u6280\u672f\u5904\u7406\u540e\u7684\u7b49\u79bb\u5b50\u4f53\u6d3b\u6027\u6c34\uff08PAW\uff09\u80fd\u591f\u663e\u8457\u4fc3\u8fdb\u83ca\u82b1\u5e7c\u82d7\u7684\u751f\u957f\uff0c\u63d0\u9ad8\u4e86\u690d\u682a\u9ad8\u5ea6\u548c\u571f\u58e4\u80a5\u529b\uff0c\u4e3a\u53ef\u6301\u7eed\u7684\u89c2\u8d4f\u690d\u7269\u751f\u957f\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002", "motivation": "\u975e\u5e73\u8861\u6001\u51b7\u7b49\u79bb\u5b50\u4f53\u5c04\u6d41\uff08APPJs\uff09\u80fd\u5728\u8fd1\u5ba4\u6e29\u4e0b\u4ea7\u751f\u6d3b\u6027\u6c27\u548c\u6c2e\u7269\u79cd\uff08RONS\uff09\uff0c\u4e0e\u6c34\u76f8\u4e92\u4f5c\u7528\u53ef\u5f62\u6210\u5316\u5b66\u6d3b\u6027\u53d6\u51b3\u4e8e\u653e\u7535\u6761\u4ef6\u7684\u7b49\u79bb\u5b50\u4f53\u6d3b\u6027\u6c34\uff08PAW\uff09\u3002 PAW\u7684\u5316\u5b66\u6d3b\u6027\u4f7f\u5176\u5728\u519c\u4e1a\u548c\u751f\u7269\u6280\u672f\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u5176\u5bf9\u690d\u7269\u751f\u957f\u7684\u5177\u4f53\u5f71\u54cd\u548c\u4f18\u5316\u6761\u4ef6\u5c1a\u9700\u6df1\u5165\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u73af\u5bf9\u7535\u6781\u914d\u7f6e\u7684\u6c26\u6c14-\u7a7a\u6c14\uff0814:1\uff09\u5fae\u7b49\u79bb\u5b50\u4f53\u5c04\u6d41\uff08APPJ\uff09\u6765\u5236\u5907PAW\u3002\u901a\u8fc7\u5149\u53d1\u5c04\u5149\u8c31\uff08OES\uff09\u5206\u6790\u653e\u7535\u7279\u6027\uff0c\u5e76\u6d4b\u5b9aPAW\u7684pH\u3001\u7535\u5bfc\u7387\uff08EC\uff09\u3001\u6c27\u5316\u8fd8\u539f\u7535\u4f4d\uff08ORP\uff09\u548c\u603b\u6eb6\u89e3\u56fa\u4f53\uff08TDS\uff09\u7b49\u7406\u5316\u53c2\u6570\uff0c\u7814\u7a76\u5176\u968f\u7b49\u79bb\u5b50\u4f53\u5904\u7406\u65f6\u95f4\u548c\u6c34\u4f53\u79ef\u7684\u53d8\u5316\u3002\u5728\u4f18\u5316\u7684PAW\u5904\u7406\u6761\u4ef6\u4e0b\uff0812\u6beb\u5347\u53bb\u79bb\u5b50\u6c34\uff0c\u5904\u740640\u5206\u949f\uff09\uff0c\u8fdb\u884c\u4e3a\u671f\u4e24\u5468\u7684\u83ca\u82b1\u5e7c\u82d7\u751f\u957f\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5206\u522b\u4f7f\u7528PAW\u3001\u53bb\u79bb\u5b50\u6c34\u548c\u81ea\u6765\u6c34\u6d47\u704c\uff0c\u5e76\u6bd4\u8f83\u5e7c\u82d7\u9ad8\u5ea6\u548c\u571f\u58e4\u80a5\u529b\u3002", "result": "\u5149\u53d1\u5c04\u5149\u8c31\uff08OES\uff09\u8bc1\u5b9e\u4e86\u6c26\u6c14-\u7a7a\u6c14\u6df7\u5408\u7269\u653e\u7535\u4e2d\u6c2e\u6c14\u5e26\u548c\u6c26\u6c14\u7ebf\u7684\u5b58\u5728\uff0c\u4e14\u6c26\u6c14-\u7a7a\u6c14\u6df7\u5408\u7269\u76f8\u6bd4\u7eaf\u6c26\u6c14\u80fd\u4ea7\u751f\u66f4\u9ad8\u5316\u5b66\u6d3b\u6027\u7684\u653e\u7535\uff0c\u6709\u5229\u4e8e\u5728PAW\u4e2d\u751f\u6210\u785d\u9178\u76d0\u3002PAW\u7684\u7406\u5316\u53c2\u6570\u5206\u6790\u663e\u793a\uff0c12\u6beb\u5347\u53bb\u79bb\u5b50\u6c34\u5904\u740640\u5206\u949f\u7684\u6761\u4ef6\u4e0b\uff0cPAW\u7684ORP\u548c\u785d\u9178\u76d0\u6d53\u5ea6\u6700\u9ad8\uff0cpH\u503c\u964d\u4f4e\u3002\u751f\u957f\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4ec5\u4f7f\u7528\u53bb\u79bb\u5b50\u6c34\u6216\u81ea\u6765\u6c34\u76f8\u6bd4\uff0c\u4f7f\u7528PAW\u5904\u7406\u7684\u83ca\u82b1\u5e7c\u82d7\u682a\u9ad8\u663e\u8457\u589e\u52a0\uff0810.2\u5398\u7c73\uff09\uff0c\u571f\u58e4\u80a5\u529b\u4e5f\u5f97\u5230\u63d0\u5347\uff082580 \u03bcS/cm\uff09\u3002", "conclusion": "\u7b49\u79bb\u5b50\u4f53\u6d3b\u6027\u6c34\uff08PAW\uff09\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u83ca\u82b1\u5e7c\u82d7\u7684\u751f\u957f\uff0c\u63d0\u9ad8\u690d\u682a\u9ad8\u5ea6\u548c\u571f\u58e4\u80a5\u529b\uff0c\u8bc1\u660e\u4e86PAW\u5728\u53ef\u6301\u7eed\u589e\u5f3a\u5f00\u82b1\u690d\u7269\u751f\u957f\u65b9\u9762\u7684\u6f5c\u529b\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e0d\u540c\u690d\u7269\u79cd\u7c7b\u3001\u7b49\u79bb\u5b50\u4f53\u5904\u7406\u53c2\u6570\u5bf9PAW\u6027\u8d28\u53ca\u690d\u7269\u751f\u957f\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u66f4\u957f\u5468\u671f\u7684\u7530\u95f4\u8bd5\u9a8c\u4ee5\u8bc4\u4f30\u5176\u5927\u89c4\u6a21\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.24727", "categories": ["cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24727", "abs": "https://arxiv.org/abs/2510.24727", "authors": ["Weiman Yan", "Yi-Chia Chang", "Wanyu Zhao"], "title": "Stiff Circuit System Modeling via Transformer", "comment": null, "summary": "Accurate and efficient circuit behavior modeling is a cornerstone of modern\nelectronic design automation. Among different types of circuits, stiff circuits\nare challenging to model using previous frameworks. In this work, we propose a\nnew approach using Crossformer, which is a current state-of-the-art Transformer\nmodel for time-series prediction tasks, combined with Kolmogorov-Arnold\nNetworks (KANs), to model stiff circuit transient behavior. By leveraging the\nCrossformer's temporal representation capabilities and the enhanced feature\nextraction of KANs, our method achieves improved fidelity in predicting circuit\nresponses to a wide range of input conditions. Experimental evaluations on\ndatasets generated through SPICE simulations of analog-to-digital converter\n(ADC) circuits demonstrate the effectiveness of our approach, with significant\nreductions in training time and error rates.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408Crossformer\u548cKolmogorov-Arnold Networks (KAN)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u7535\u611f\u7535\u8def\u7684\u77ac\u6001\u884c\u4e3a\u8fdb\u884c\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u9519\u8bef\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u7535\u8def\u884c\u4e3a\u5efa\u6a21\u65b9\u6cd5\u5728\u5904\u7406\u50cf\u7535\u611f\u7535\u8def\u8fd9\u7c7b\u201c\u786c\u201d\u7535\u8def\u65f6\u5b58\u5728\u6311\u6218\u3002\u7cbe\u786e\u9ad8\u6548\u7684\u7535\u8def\u884c\u4e3a\u5efa\u6a21\u662f\u73b0\u4ee3\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684\u5173\u952e\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u8fd9\u7c7b\u5177\u6709\u6311\u6218\u6027\u7684\u7535\u8def\u65f6\uff0c\u8feb\u5207\u9700\u8981\u66f4\u4f18\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u5c06\u5148\u8fdb\u7684Transformer\u6a21\u578bCrossformer\uff08\u64c5\u957f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff09\u4e0eKolmogorov-Arnold Networks (KAN)\u76f8\u7ed3\u5408\u3002Crossformer\u80fd\u591f\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u7684\u7279\u5f81\uff0c\u800cKAN\u5219\u589e\u5f3a\u4e86\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u5171\u540c\u7528\u4e8e\u5efa\u6a21\u7535\u611f\u7535\u8def\u7684\u77ac\u6001\u884c\u4e3a\u3002\u5b9e\u9a8c\u57fa\u4e8e\u6a21\u62df\u6570\u5b57\u8f6c\u6362\u5668\uff08ADC\uff09\u7535\u8def\u7684SPICE\u4eff\u771f\u6570\u636e\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u7535\u8def\u54cd\u5e94\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\uff0c\u80fd\u591f\u5e94\u5bf9\u5e7f\u6cdb\u7684\u8f93\u5165\u6761\u4ef6\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u9519\u8bef\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684Crossformer\u4e0eKAN\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5728\u5efa\u6a21\u7535\u611f\u7535\u8def\u77ac\u6001\u884c\u4e3a\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u5e76\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u8be5\u6a21\u578b\u5728\u66f4\u590d\u6742\u7535\u8def\u548c\u66f4\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u3002"}}
{"id": "2510.24719", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24719", "abs": "https://arxiv.org/abs/2510.24719", "authors": ["Shravan Gadbail", "Masumi Desai", "Kamalakar Karlapalem"], "title": "Iti-Validator: A Guardrail Framework for Validating and Correcting LLM-Generated Itineraries", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has enabled them to\ngenerate complex, multi-step plans and itineraries. However, these generated\nplans often lack temporal and spatial consistency, particularly in scenarios\ninvolving physical travel constraints. This research aims to study the temporal\nperformance of different LLMs and presents a validation framework that\nevaluates and improves the temporal consistency of LLM-generated travel\nitineraries. The system employs multiple state-of-the-art LLMs to generate\ntravel plans and validates them against real-world flight duration constraints\nusing the AeroDataBox API. This work contributes to the understanding of LLM\ncapabilities in handling complex temporal reasoning tasks like itinerary\ngeneration and provides a framework to rectify any temporal inconsistencies\nlike overlapping journeys or unrealistic transit times in the itineraries\ngenerated by LLMs before the itinerary is given to the user. Our experiments\nreveal that while current LLMs frequently produce temporally inconsistent\nitineraries, these can be systematically and reliably corrected using our\nframework, enabling their practical deployment in large-scale travel planning.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u591a\u6b65\u8ba1\u5212\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u6d89\u53ca\u5b9e\u9645\u65c5\u884c\u7ea6\u675f\u7684\u7269\u7406\u65c5\u884c\u573a\u666f\u65f6\uff0c\u5176\u751f\u6210\u7684\u8ba1\u5212\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u5f80\u5f80\u7f3a\u4e4f\u4e00\u81f4\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540cLLM\u7684\u65f6\u95f4\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9a8c\u8bc1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u751f\u6210\u7684\u65c5\u884c\u884c\u7a0b\u5728\u65f6\u95f4\u4e0a\u7684\u4e00\u81f4\u6027\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u591a\u4e2a\u5148\u8fdb\u7684LLM\u751f\u6210\u65c5\u884c\u8ba1\u5212\uff0c\u5e76\u4f7f\u7528AeroDataBox API\u5c06\u8fd9\u4e9b\u8ba1\u5212\u4e0e\u771f\u5b9e\u7684\u822a\u73ed\u6301\u7eed\u65f6\u95f4\u7ea6\u675f\u8fdb\u884c\u9a8c\u8bc1\u3002\u672c\u7814\u7a76\u6709\u52a9\u4e8e\u7406\u89e3LLM\u5728\u5904\u7406\u50cf\u884c\u7a0b\u751f\u6210\u8fd9\u6837\u7684\u590d\u6742\u65f6\u95f4\u63a8\u7406\u4efb\u52a1\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5c06\u884c\u7a0b\u63d0\u4f9b\u7ed9\u7528\u6237\u4e4b\u524d\uff0c\u7ea0\u6b63LLM\u751f\u6210\u7684\u884c\u7a0b\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4f8b\u5982\u884c\u7a0b\u91cd\u53e0\u6216\u4e0d\u5207\u5b9e\u9645\u7684 transit \u65f6\u95f4\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136\u5f53\u524d\u7684LLM\u7ecf\u5e38\u751f\u6210\u65f6\u95f4\u4e0a\u4e0d\u4e00\u81f4\u7684\u884c\u7a0b\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u6211\u4eec\u7684\u6846\u67b6\u7cfb\u7edf\u4e14\u53ef\u9760\u5730\u8fdb\u884c\u7ea0\u6b63\uff0c\u4ece\u800c\u5b9e\u73b0\u5b83\u4eec\u5728\u5927\u578b\u65c5\u884c\u89c4\u5212\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u590d\u6742\u7684\u591a\u6b65\u8ba1\u5212\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f8b\u5982\u65c5\u884c\u884c\u7a0b\u7684\u89c4\u5212\u3002\u7136\u800c\uff0c\u5728\u6d89\u53ca\u7269\u7406\u65c5\u884c\u7ea6\u675f\u7684\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u8fd9\u4e9b\u751f\u6210\u7684\u8ba1\u5212\u5e38\u5e38\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u7f3a\u4e4f\u4e00\u81f4\u6027\uff0c\u4f8b\u5982\u884c\u7a0b\u91cd\u53e0\u6216\u4e0d\u5207\u5b9e\u9645\u7684 transit \u65f6\u95f4\u3002\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\u9650\u5236\u4e86LLMs\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u65c5\u884c\u89c4\u5212\u8fd9\u6837\u5bf9\u65f6\u95f4\u8981\u6c42\u4e25\u683c\u7684\u9886\u57df\u3002\u56e0\u6b64\uff0c\u7814\u7a76LLMs\u5728\u5904\u7406\u65f6\u95f4\u63a8\u7406\u4efb\u52a1\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\uff0c\u5177\u6709\u91cd\u8981\u7684\u73b0\u5b9e\u610f\u4e49\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u751f\u6210\u7684\u65c5\u884c\u884c\u7a0b\u5728\u65f6\u95f4\u4e0a\u7684\u4e00\u81f4\u6027\u3002\u8be5\u6846\u67b6\u9996\u5148\u5229\u7528\u591a\u4e2a\u5148\u8fdb\u7684LLM\u6765\u751f\u6210\u65c5\u884c\u8ba1\u5212\u3002\u968f\u540e\uff0c\u5229\u7528AeroDataBox API\u5c06\u751f\u6210\u7684\u8ba1\u5212\u4e0e\u771f\u5b9e\u7684\u822a\u73ed\u6301\u7eed\u65f6\u95f4\u7ea6\u675f\u8fdb\u884c\u6bd4\u5bf9\u548c\u9a8c\u8bc1\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u7cfb\u7edf\u80fd\u591f\u68c0\u6d4b\u5e76\u7ea0\u6b63\u884c\u7a0b\u4e2d\u5b58\u5728\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\uff0c\u5982\u884c\u7a0b\u91cd\u53e0\u6216\u4e0d\u5207\u5b9e\u9645\u7684 transit \u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684LLMs\u5728\u751f\u6210\u65c5\u884c\u884c\u7a0b\u65f6\uff0c\u7ecf\u5e38\u51fa\u73b0\u65f6\u95f4\u4e0a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u7136\u800c\uff0c\u901a\u8fc7\u7814\u7a76\u4e2d\u63d0\u51fa\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u8fd9\u4e9b\u4e0d\u4e00\u81f4\u6027\u80fd\u591f\u88ab\u7cfb\u7edf\u4e14\u53ef\u9760\u5730\u7ea0\u6b63\u3002\u8fd9\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u63d0\u9ad8LLM\u751f\u6210\u884c\u7a0b\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5176\u5728\u5927\u578b\u65c5\u884c\u89c4\u5212\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u4e00\u4e2a\u9a8c\u8bc1\u6846\u67b6\uff0c\u6210\u529f\u5730\u8bc4\u4f30\u5e76\u6539\u8fdb\u4e86LLM\u751f\u6210\u7684\u65c5\u884c\u884c\u7a0b\u5728\u65f6\u95f4\u4e0a\u7684\u4e00\u81f4\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u73b0\u6709LLMs\u5728\u5904\u7406\u65f6\u95f4\u63a8\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u4f46\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u7ea0\u6b63\u884c\u7a0b\u4e2d\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\uff0c\u4f8b\u5982\u884c\u7a0b\u91cd\u53e0\u548c\u4e0d\u5207\u5b9e\u9645\u7684 transit \u65f6\u95f4\u3002\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u52a0\u6df1\u4e86\u5bf9LLM\u5904\u7406\u590d\u6742\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u7684\u7406\u89e3\uff0c\u66f4\u4e3aLLMs\u5728\u5b9e\u9645\u65c5\u884c\u89c4\u5212\u7b49\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.24832", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24832", "abs": "https://arxiv.org/abs/2510.24832", "authors": ["Hong Wang", "Zhezheng Hao", "Jian Luo", "Chenxing Wei", "Yao Shu", "Lei Liu", "Qiang Lin", "Hande Dong", "Jiawei Chen"], "title": "Scheduling Your LLM Reinforcement Learning with Reasoning Trees", "comment": null, "summary": "Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large\nLanguage Models (LLMs) can be conceptualized as progressively editing a query's\n`Reasoning Tree'. This process involves exploring nodes (tokens) and\ndynamically modifying the model's policy at each node. When combined with data\nscheduling, this process yields further gains in data efficiency and accuracy.\nHowever, existing RLVR data scheduling methods typically rely on path-based\nmetrics to rank queries, overlooking the reasoning tree structures of these\nqueries. In this paper, we introduce a novel metric, namely Reasoning Score\n(r-score), which measures the query's learning difficulty based on the\nstructure of its reasoning tree. Based on the r-score, we propose the Reasoning\nTree Schedule (Re-Schedule), a scheduling algorithm that constructs a\ncurriculum progressing from structurally simple (high r-score) to complex (low\nr-score) queries. Experiments on six math-reasoning benchmarks show that\nRe-Schedule significantly improves average accuracy, achieving gains of up to\n3.2%. These strong results validate our approach and demonstrate that a\nstructural understanding of the reasoning tree provides a more powerful and\nprincipled foundation for RLVR data scheduling.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReasoning Score (r-score)\u7684\u65b0\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u67e5\u8be2\u7684\u63a8\u7406\u6811\u7ed3\u6784\u7684\u5b66\u4e60\u96be\u5ea6\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86Re-Schedule\u7b97\u6cd5\uff0c\u4ee5\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684LLM\u8bad\u7ec3\u6570\u636e\u8c03\u5ea6\uff0c\u4ece\u800c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ed3\u5408\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6cd5\uff0c\u5c06LLM\u4f18\u5316\u89c6\u4e3a\u5bf9\u67e5\u8be2\u7684\u2018\u63a8\u7406\u6811\u2019\u8fdb\u884c\u7f16\u8f91\u3002\u867d\u7136\u8fd9\u79cd\u65b9\u6cd5\u7ed3\u5408\u6570\u636e\u8c03\u5ea6\u80fd\u63d0\u5347\u6570\u636e\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4f46\u73b0\u6709\u7684\u6570\u636e\u8c03\u5ea6\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u8def\u5f84\u7684\u6307\u6807\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u6811\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u67e5\u8be2\u7684\u5b66\u4e60\u96be\u5ea6\u4fe1\u606f\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u6807\u201cReasoning Score (r-score)\u201d\uff0c\u8be5\u6307\u6807\u57fa\u4e8e\u67e5\u8be2\u7684\u63a8\u7406\u6811\u7ed3\u6784\u6765\u8861\u91cf\u5176\u5b66\u4e60\u96be\u5ea6\u3002r-score\u503c\u8d8a\u9ad8\u8868\u793a\u67e5\u8be2\u7ed3\u6784\u8d8a\u7b80\u5355\uff0c\u5b66\u4e60\u96be\u5ea6\u8d8a\u4f4e\uff1br-score\u503c\u8d8a\u4f4e\u8868\u793a\u67e5\u8be2\u7ed3\u6784\u8d8a\u590d\u6742\uff0c\u5b66\u4e60\u96be\u5ea6\u8d8a\u9ad8\u3002\n2. \u57fa\u4e8er-score\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cReasoning Tree Schedule (Re-Schedule)\u201d\u7684\u6570\u636e\u8c03\u5ea6\u7b97\u6cd5\u3002Re-Schedule\u6309\u7167\u4ece\u7ed3\u6784\u7b80\u5355\uff08\u9ad8r-score\uff09\u5230\u7ed3\u6784\u590d\u6742\uff08\u4f4er-score\uff09\u7684\u987a\u5e8f\u6765\u6784\u5efa\u5b66\u4e60\u8bfe\u7a0b\u3002\n3. \u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRe-Schedule\u7b97\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8LLM\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u6027\uff0c\u6700\u9ad8\u51c6\u786e\u6027\u63d0\u5347\u53ef\u8fbe3.2%\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5730\u7406\u89e3\u63a8\u7406\u6811\uff0c\u53ef\u4ee5\u4e3aRLVR\u6570\u636e\u8c03\u5ea6\u63d0\u4f9b\u66f4\u5f3a\u5927\u3001\u66f4\u5408\u7406\u7684\u57fa\u7840\u3002r-score\u6307\u6807\u548cRe-Schedule\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347LLM\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u8003\u8651\u67e5\u8be2\u63a8\u7406\u6811\u7ed3\u6784\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.25047", "categories": ["physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2510.25047", "abs": "https://arxiv.org/abs/2510.25047", "authors": ["Allen H Boozer"], "title": "Magnetic Field Line Chaos, Cantori, and Turnstiles in Toroidal Plasmas", "comment": null, "summary": "Although magnetic field line chaos, cantori, and turnstiles underlie the\nphysics of tokamak disruptions, runaway electron damage, stellarator\nnon-resonant divertors, and the most important electromagnetic correction to\nwhat are called electrostatic micro-instabilities, these concepts are not well\nknown. These concepts will be defined and applications that illustrate their\nimportance will be discussed.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u78c1\u573a\u7ebf\u6df7\u6c8c\u3001cantori \u548c turnstiles \u7684\u6982\u5ff5\uff0c\u8fd9\u4e9b\u6982\u5ff5\u5728\u6258\u5361\u9a6c\u514b\u805a\u53d8\u88c5\u7f6e\u7684\u7269\u7406\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9c9c\u4e3a\u4eba\u77e5\u3002\u8bba\u6587\u5c06\u5b9a\u4e49\u8fd9\u4e9b\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5e94\u7528\u5b9e\u4f8b\u8bf4\u660e\u5b83\u4eec\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u78c1\u573a\u7ebf\u6df7\u6c8c\u3001cantori \u548c turnstiles \u7b49\u6982\u5ff5\u5bf9\u4e8e\u7406\u89e3\u6258\u5361\u9a6c\u514b\u805a\u53d8\u88c5\u7f6e\u7684\u7269\u7406\u73b0\u8c61\u81f3\u5173\u91cd\u8981\uff0c\u4f8b\u5982\u7b49\u79bb\u5b50\u4f53\u7834\u574f\u3001\u5931\u63a7\u7535\u5b50\u635f\u4f24\u3001\u4eff\u661f\u5668\u975e\u5171\u632f\u504f\u6ee4\u5668\u4ee5\u53ca\u5bf9\u9759\u7535\u5fae\u4e0d\u7a33\u5b9a\u6027\u6700\u91cd\u8981\u7684\u7535\u78c1\u4fee\u6b63\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6982\u5ff5\u5728\u73b0\u6709\u7814\u7a76\u4e2d\u5e76\u672a\u5f97\u5230\u5145\u5206\u7684\u8ba4\u8bc6\u548c\u8ba8\u8bba\u3002", "method": "\u8bba\u6587\u5c06\u9996\u5148\u5b9a\u4e49\u78c1\u573a\u7ebf\u6df7\u6c8c\u3001cantori \u548c turnstiles \u7684\u57fa\u672c\u6982\u5ff5\u3002\u968f\u540e\uff0c\u5c06\u901a\u8fc7\u5177\u4f53\u7684\u5e94\u7528\u5b9e\u4f8b\u6765\u9610\u8ff0\u8fd9\u4e9b\u6982\u5ff5\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u5b83\u4eec\u5728\u89e3\u51b3\u5b9e\u9645\u7269\u7406\u95ee\u9898\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u8bba\u6587\u901a\u8fc7\u5e94\u7528\u5b9e\u4f8b\u5c55\u793a\u4e86\u78c1\u573a\u7ebf\u6df7\u6c8c\u3001cantori \u548c turnstiles \u5728\u6258\u5361\u9a6c\u514b\u805a\u53d8\u88c5\u7f6e\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728\u7406\u89e3\u548c\u63a7\u5236\u7b49\u79bb\u5b50\u4f53\u884c\u4e3a\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u6210\u529f\u5730\u4ecb\u7ecd\u4e86\u78c1\u573a\u7ebf\u6df7\u6c8c\u3001cantori \u548c turnstiles \u8fd9\u4e9b\u5728\u805a\u53d8\u7b49\u79bb\u5b50\u4f53\u7269\u7406\u5b66\u4e2d\u88ab\u5ffd\u89c6\u4f46\u81f3\u5173\u91cd\u8981\u7684\u6982\u5ff5\u3002\u901a\u8fc7\u6e05\u6670\u7684\u5b9a\u4e49\u548c\u5b9e\u4f8b\u5206\u6790\uff0c\u8bba\u6587\u4e3a\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u5728\u6258\u5361\u9a6c\u514b\u88c5\u7f6e\u8bbe\u8ba1\u548c\u8fd0\u884c\u4e2d\u66f4\u597d\u5730\u5e94\u5bf9\u6311\u6218\uff0c\u4f8b\u5982\u63d0\u9ad8\u7b49\u79bb\u5b50\u4f53\u7a33\u5b9a\u6027\uff0c\u51cf\u5c11\u8bbe\u5907\u635f\u4f24\u7b49\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fd9\u4e9b\u6982\u5ff5\u5728\u66f4\u5e7f\u6cdb\u7684\u7b49\u79bb\u5b50\u4f53\u73b0\u8c61\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u8ba1\u7b97\u5de5\u5177\u6765\u6a21\u62df\u548c\u9884\u6d4b\u8fd9\u4e9b\u73b0\u8c61\u3002"}}
{"id": "2510.24760", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24760", "abs": "https://arxiv.org/abs/2510.24760", "authors": ["Mengyuan Chen", "Chengjun Dai", "Xinyang Dong", "Chengzhe Feng", "Kewei Fu", "Jianshe Li", "Zhihan Peng", "Yongqi Tong", "Junshao Zhang", "Hong Zhu"], "title": "Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments", "comment": null, "summary": "We present Dingtalk DeepResearch, a unified multi agent intelligence\nframework for real world enterprise environments, delivering deep research,\nheterogeneous table reasoning, and multimodal report generation.", "AI": {"tldr": "Dingtalk DeepResearch\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u667a\u80fd\u6846\u67b6\uff0c\u4e13\u4e3a\u771f\u5b9e\u4e16\u754c\u7684\u4f01\u4e1a\u73af\u5883\u8bbe\u8ba1\uff0c\u80fd\u591f\u8fdb\u884c\u6df1\u5ea6\u7814\u7a76\u3001\u5f02\u6784\u8868\u683c\u63a8\u7406\u548c\u591a\u6a21\u6001\u62a5\u544a\u751f\u6210\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u4f01\u4e1a\u73af\u5883\u4e2d\u590d\u6742\u4fe1\u606f\u5904\u7406\u548c\u77e5\u8bc6\u63d0\u53d6\u7684\u6311\u6218\uff0c\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6765\u63d0\u9ad8\u6df1\u5ea6\u7814\u7a76\u3001\u8868\u683c\u63a8\u7406\u548c\u62a5\u544a\u751f\u6210\u7684\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u6570\u636e\u91cf\u548c\u4fe1\u606f\u590d\u6742\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3aDingtalk DeepResearch\u7684\u7edf\u4e00\u591a\u667a\u80fd\u4f53\u667a\u80fd\u6846\u67b6\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u591a\u4e2a\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u534f\u540c\u5de5\u4f5c\uff0c\u4ee5\u5e94\u5bf9\u4f01\u4e1a\u73af\u5883\u4e2d\u590d\u6742\u7684\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u3001\u5904\u7406\u548c\u7406\u89e3\u5f02\u6784\u8868\u683c\u6570\u636e\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u591a\u6a21\u6001\u7684\u5206\u6790\u62a5\u544a\u3002", "result": "\u6846\u67b6\u5728\u771f\u5b9e\u4e16\u754c\u4f01\u4e1a\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u90e8\u7f72\u548c\u8bc4\u4f30\uff0c\u5e76\u5728\u6df1\u5ea6\u7814\u7a76\u3001\u5f02\u6784\u8868\u683c\u63a8\u7406\u548c\u591a\u6a21\u6001\u62a5\u544a\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u5177\u4f53\u6027\u80fd\u6307\u6807\u548c\u6848\u4f8b\u5206\u6790\u5c06\u5728\u8bba\u6587\u4e2d\u8be6\u7ec6\u9610\u8ff0\u3002", "conclusion": "Dingtalk DeepResearch\u6846\u67b6\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u548c\u7406\u89e3\u590d\u6742\u7684\u4f01\u4e1a\u6570\u636e\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u6df1\u5ea6\u7814\u7a76\u548c\u62a5\u544a\u751f\u6210\uff0c\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\u548c\u6f5c\u529b\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4e5f\u6307\u51fa\u4e86\u8be5\u6846\u67b6\u5728\u6cdb\u5316\u80fd\u529b\u548c\u5904\u7406\u66f4\u590d\u6742\u573a\u666f\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.25005", "categories": ["cs.AI", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.25005", "abs": "https://arxiv.org/abs/2510.25005", "authors": ["Saptarshi Saha", "Dhruv Vansraj Rathore", "Utpal Garain"], "title": "Cyclic Counterfactuals under Shift-Scale Interventions", "comment": "Accepted at NeurIPS 2025", "summary": "Most counterfactual inference frameworks traditionally assume acyclic\nstructural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,\nmany real-world systems (e.g. biological systems) contain feedback loops or\ncyclic dependencies that violate acyclicity. In this work, we study\ncounterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,\nsoft, policy-style changes that rescale and/or shift a variable's mechanism.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u5b58\u5728\u5faa\u73af\u4f9d\u8d56\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCMs\uff09\u4e2d\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u5728\u8fdb\u884c\u201c\u79fb\u4f4d-\u5c3a\u5ea6\u201d\u5e72\u9884\uff08shift-scale interventions\uff09\u4e0b\u7684\u60c5\u51b5\uff0c\u8fd9\u662f\u4e00\u79cd\u653f\u7b56\u98ce\u683c\u7684\u67d4\u6027\u5e72\u9884\u65b9\u5f0f\u3002", "motivation": "\u4f20\u7edf\u53cd\u4e8b\u5b9e\u63a8\u7406\u6846\u67b6\u901a\u5e38\u5047\u8bbe\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCMs\uff09\u662f\u65e0\u73af\u7684\uff0c\u5373\u4f7f\u7528\u6709\u5411\u65e0\u73af\u56fe\uff08DAGs\uff09\u3002\u7136\u800c\uff0c\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\uff08\u4f8b\u5982\u751f\u7269\u7cfb\u7edf\uff09\u5305\u542b\u53cd\u9988\u56de\u8def\u6216\u5faa\u73af\u4f9d\u8d56\uff0c\u8fd9\u8fdd\u53cd\u4e86\u65e0\u73af\u6027\u5047\u8bbe\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5728\u5b58\u5728\u5faa\u73af\u4f9d\u8d56\u7684SCMs\u4e2d\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u95ee\u9898\uff0c\u4ee5\u9002\u5e94\u66f4\u5e7f\u6cdb\u7684\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u573a\u666f\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5faa\u73afSCMs\u4e2d\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u201c\u79fb\u4f4d-\u5c3a\u5ea6\u201d\u5e72\u9884\u3002\u8fd9\u79cd\u5e72\u9884\u65b9\u5f0f\u662f\u4e00\u79cd\u67d4\u6027\u7684\u3001\u7c7b\u4f3c\u4e8e\u653f\u7b56\u7684\u6539\u53d8\uff0c\u80fd\u591f\u5bf9\u53d8\u91cf\u7684\u673a\u5236\u8fdb\u884c\u91cd\u7f6e\u6216\u79fb\u4f4d\u3002", "result": "\uff08\u7531\u4e8e\u539f\u6587\u4ec5\u63d0\u4f9b\u6458\u8981\uff0c\u5177\u4f53\u7ed3\u679c\u90e8\u5206\u9700\u8981\u6839\u636e\u8bba\u6587\u8be6\u7ec6\u5185\u5bb9\u8865\u5145\uff09\u672c\u7814\u7a76\u901a\u8fc7\u5728\u5faa\u73afSCMs\u4e2d\u5e94\u7528\u201c\u79fb\u4f4d-\u5c3a\u5ea6\u201d\u5e72\u9884\uff0c\u4e3a\u53cd\u4e8b\u5b9e\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u80fd\u5728\u751f\u7269\u7cfb\u7edf\u7b49\u9886\u57df\u5f97\u5230\u5e94\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u6269\u5c55\u4e86\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u9002\u7528\u8303\u56f4\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e38\u89c1\u7684\u5faa\u73af\u4f9d\u8d56\u7cfb\u7edf\u3002\u901a\u8fc7\u5f15\u5165\u201c\u79fb\u4f4d-\u5c3a\u5ea6\u201d\u5e72\u9884\uff0c\u4e3a\u7406\u89e3\u548c\u5e72\u9884\u590d\u6742\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7c7b\u578b\u5faa\u73af\u7ed3\u6784\u548c\u5e72\u9884\u65b9\u5f0f\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2510.25376", "categories": ["physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2510.25376", "abs": "https://arxiv.org/abs/2510.25376", "authors": ["Talia Meir", "Kale Weichman", "Alexey Arefiev", "John P. Palastro", "Ishay Pomerantz"], "title": "Robust direct laser acceleration of electrons with flying-focus laser pulses", "comment": null, "summary": "Direct laser acceleration (DLA) offers a compact source of high-charge,\nenergetic electrons for generating secondary radiation or neutrons. While DLA\nin high-density plasma optimizes the energy transfer from a laser pulse to\nelectrons, it exacerbates nonlinear propagation effects, such as filamentation,\nthat can disrupt the acceleration process. Here, we show that superluminal\nflying-focus pulses (FFPs) mitigate nonlinear propagation, thereby enhancing\nthe number of high-energy electrons and resulting x-ray yield.\nThree-dimensional particle-in-cell simulations show that, compared to a\nGaussian pulse of equal energy (1 J) and intensity (2x10^20 W/cm^2), an FFP\nproduces 80x more electrons above 100 MeV, increases the electron cutoff energy\nby 20%, triples the high-energy x-ray yield, and improves x-ray collimation.\nThese results illustrate the ability of spatiotemporally structured laser\npulses to provide additional control in the highly nonlinear, relativistic\nregime of laser-plasma interactions.", "AI": {"tldr": "\u98de\u7fd4\u7126\u70b9\u8109\u51b2(FFP)\u53ef\u4ee5\u51cf\u8f7b\u6fc0\u5149\u7b49\u79bb\u5b50\u4f53\u76f8\u4e92\u4f5c\u7528\u4e2d\u7684\u975e\u7ebf\u6027\u4f20\u64ad\u6548\u5e94\uff0c\u4ece\u800c\u63d0\u9ad8\u9ad8\u80fd\u7535\u5b50\u548cX\u5c04\u7ebf\u4ea7\u989d\u3002", "motivation": "\u76f4\u63a5\u6fc0\u5149\u52a0\u901f(DLA)\u867d\u7136\u80fd\u63d0\u4f9b\u9ad8\u7535\u8377\u3001\u9ad8\u80fd\u7535\u5b50\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u975e\u7ebf\u6027\u4f20\u64ad\u6548\u5e94\uff08\u5982\u4e1d\u5316\uff09\u7684\u5e72\u6270\u3002", "method": "\u4f7f\u7528\u4e09\u7ef4\u7c92\u5b50-\u5728-\u5355\u5143\u6a21\u62df\uff0c\u6bd4\u8f83\u4e86FFP\u4e0e\u9ad8\u65af\u8109\u51b2\u5728\u6fc0\u5149\u7b49\u79bb\u5b50\u4f53\u76f8\u4e92\u4f5c\u7528\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u4e0e\u9ad8\u65af\u8109\u51b2\u76f8\u6bd4\uff0cFFP\u4ea7\u751f\u4e8680\u500d\u4ee5\u4e0a100 MeV\u7684\u7535\u5b50\uff0c\u7535\u5b50\u80fd\u91cf\u4e0a\u9650\u63d0\u9ad8\u4e8620%\uff0cX\u5c04\u7ebf\u4ea7\u989d\u589e\u52a0\u4e86\u4e24\u500d\uff0c\u5e76\u4e14X\u5c04\u7ebf\u51c6\u76f4\u6027\u66f4\u597d\u3002", "conclusion": "\u98de\u7fd4\u7126\u70b9\u8109\u51b2\uff08FFP\uff09\u4e3a\u63a7\u5236\u6fc0\u5149\u7b49\u79bb\u5b50\u4f53\u76f8\u4e92\u4f5c\u7528\u4e2d\u7684\u975e\u7ebf\u6027\u6548\u5e94\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u6709\u671b\u5728\u4ea7\u751f\u4e8c\u6b21\u8f90\u5c04\u548c\u4e2d\u5b50\u65b9\u9762\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2510.25201", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2510.25201", "abs": "https://arxiv.org/abs/2510.25201", "authors": ["Vishal Patil", "Kavya Bhand", "Kaustubh Mukdam", "Kavya Sharma", "Manas Kawtikwar", "Prajwal Kavhar", "Hridayansh Kaware"], "title": "Enhancing Financial Decision-Making: Machine Learning and AI-Powered Predictions and Analysis", "comment": null, "summary": "The proposed system aims to use various machine learning algorithms to\nenhance financial prediction and generate highly accurate analyses. It\nintroduces an AI-driven platform which offers inflation-analysis, stock market\nprediction, and E-learning module powered by a chatbot. It has achieved high\naccuracy where the Inflation Analysis depicts 0.8% MAE, 1.2% RMSE and the Stock\nPrediction shows 98% and 96% accuracy for Apple and Google stock prices\nrespectively. Key features include historical price trends, inflation rates,\nshort-term future stock prediction, where the data has been extracted using\nreal-world financial datasets. Additionally, the E-learning feature contributes\nto bridging financial gaps and promoting informed decisions. We have\nimplemented algorithms like linear regression, ARIMA, LSTM where the accuracy\nhas been evaluated using metrics such as MAE, RMSE and the like.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u591a\u529f\u80fdAI\u5e73\u53f0\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u91d1\u878d\u9884\u6d4b\u548c\u5206\u6790\uff0c\u5305\u62ec\u901a\u8d27\u81a8\u80c0\u5206\u6790\u3001\u80a1\u7968\u5e02\u573a\u9884\u6d4b\u548c\u7531\u804a\u5929\u673a\u5668\u4eba\u9a71\u52a8\u7684\u7535\u5b50\u5b66\u4e60\u6a21\u5757\uff0c\u5e76\u5728\u5b9e\u9645\u91d1\u878d\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\u7ed3\u679c\u3002", "motivation": "\u91d1\u878d\u9884\u6d4b\u7684\u590d\u6742\u6027\u548c\u51c6\u786e\u6027\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u65b9\u9762\u91d1\u878d\u4fe1\u606f\uff08\u5982\u901a\u8d27\u81a8\u80c0\u548c\u80a1\u7968\u5e02\u573a\uff09\u5e76\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u5b66\u4e60\u652f\u6301\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u96c6\u6210\u7684AI\u5e73\u53f0\uff0c\u4ee5\u63d0\u9ad8\u91d1\u878d\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u7528\u6237\u63d0\u4f9b\u4fe1\u606f\u4e30\u5bcc\u7684\u51b3\u7b56\u5de5\u5177\u548c\u5b66\u4e60\u8d44\u6e90\uff0c\u4ece\u800c\u5f25\u5408\u91d1\u878d\u77e5\u8bc6\u9e3f\u6c9f\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5982\u7ebf\u6027\u56de\u5f52\u3001ARIMA\u548cLSTM\uff0c\u6784\u5efa\u4e86\u4e00\u4e2aAI\u9a71\u52a8\u7684\u5e73\u53f0\u3002\u8be5\u5e73\u53f0\u6574\u5408\u4e86\u901a\u8d27\u81a8\u80c0\u5206\u6790\u3001\u80a1\u7968\u5e02\u573a\u9884\u6d4b\uff08\u9488\u5bf9\u82f9\u679c\u548c\u8c37\u6b4c\u80a1\u7968\uff09\u4ee5\u53ca\u4e00\u4e2a\u7531\u804a\u5929\u673a\u5668\u4eba\u652f\u6301\u7684\u7535\u5b50\u5b66\u4e60\u6a21\u5757\u3002\u6570\u636e\u6765\u6e90\u4e8e\u771f\u5b9e\u7684\u91d1\u878d\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5206\u6790\u5386\u53f2\u4ef7\u683c\u8d8b\u52bf\u548c\u901a\u8d27\u81a8\u80c0\u7387\u6765\u8fdb\u884c\u77ed\u671f\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u3002\u6a21\u578b\u7684\u51c6\u786e\u6027\u901a\u8fc7\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u548c\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u7b49\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8be5\u5e73\u53f0\u5728\u91d1\u878d\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002\u901a\u8d27\u81a8\u80c0\u5206\u6790\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4e3a0.8%\uff0c\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u4e3a1.2%\u3002\u5728\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u65b9\u9762\uff0c\u6a21\u578b\u5bf9\u82f9\u679c\uff08Apple\uff09\u548c\u8c37\u6b4c\uff08Google\uff09\u7684\u9884\u6d4b\u51c6\u786e\u7387\u5206\u522b\u8fbe\u5230\u4e8698%\u548c96%\u3002\u7535\u5b50\u5b66\u4e60\u6a21\u5757\u901a\u8fc7\u804a\u5929\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4ea4\u4e92\u5f0f\u7684\u91d1\u878d\u77e5\u8bc6\uff0c\u6709\u52a9\u4e8e\u5f25\u5408\u91d1\u878d\u77e5\u8bc6\u5dee\u8ddd\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u96c6\u6210AI\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u80fd\u591f\u901a\u8fc7\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u91d1\u878d\u9884\u6d4b\uff08\u901a\u8d27\u81a8\u80c0\u548c\u80a1\u7968\u5e02\u573a\uff09\u548c\u63d0\u4f9b\u91d1\u878d\u77e5\u8bc6\u666e\u53ca\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u7ed3\u679c\uff0c\u4f46\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u66f4\u5e7f\u6cdb\u7684\u91d1\u878d\u5de5\u5177\u9884\u6d4b\u3001\u96c6\u6210\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u6e90\u4ee5\u53ca\u4f18\u5316\u804a\u5929\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u5e73\u53f0\u7684\u5b9e\u7528\u6027\u548c\u5f71\u54cd\u529b\u3002"}}
{"id": "2510.24762", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24762", "abs": "https://arxiv.org/abs/2510.24762", "authors": ["Wenzhen Luo", "Wei Guan", "Yifan Yao", "Yimin Pan", "Feng Wang", "Zhipeng Yu", "Zhe Wen", "Liang Chen", "Yihong Zhuang"], "title": "Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation", "comment": null, "summary": "We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in\nan enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese\nquestions over 28 databases; 77% require multi-table reasoning and over half\ntouch more than four tables. Each example is annotated along SQL-computation\nfeatures and Chinese semantics. For evaluation, we release a robust execution\ncomparator and an automated evaluation pipeline, under which all current\nstate-of-the-art large-scale models (including Deepseek) achieve accuracies of\nat most 50%. Major errors originate from two sources: (1) schema linking in\nlarge enterprise landscapes - hundreds of tables, denormalized fields,\nambiguous column names, implicit foreign-key relations and domain-specific\nsynonyms that make correct join/column selection difficult; and (2) mapping\nconcise, colloquial Chinese into the exact operators and predicates required\nfor analytics - e.g., choosing the correct aggregation and group-by keys,\nexpressing time windows and granularities, applying unit conversions, handling\nNULLs and data-quality rules, and formulating nested or windowed subqueries.\nFalcon therefore targets Chinese-specific semantics and enterprise dialects\n(abbreviations, business jargon, fuzzy entity references) and provides a\nreproducible middle ground before full production deployment by using realistic\nenterprise schemas, query templates, an execution comparator, and an automated\nevaluation pipeline for end-to-end validation.", "AI": {"tldr": "Falcon\u662f\u4e00\u4e2a\u65b0\u7684\u8de8\u9886\u57df\u4e2d\u6587Text-to-SQL\u57fa\u51c6\uff0c\u5305\u542b600\u4e2a\u4e2d\u6587\u95ee\u9898\u548c28\u4e2a\u6570\u636e\u5e93\uff0c\u4fa7\u91cd\u4e8e\u4f01\u4e1a\u7ea7MaxCompute/Hive\u65b9\u8a00\u3002\u8be5\u57fa\u51c6\u65e8\u5728\u89e3\u51b3\u5927\u578b\u4f01\u4e1a\u73af\u5883\u4e2d\u590d\u6742\u6a21\u5f0f\u94fe\u63a5\u548c\u4e2d\u6587\u8bed\u4e49\u7406\u89e3\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u5927\u578b\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u51c6\u786e\u7387\u5747\u4e0d\u8d85\u8fc750%\uff0c\u51f8\u663e\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684Text-to-SQL\u57fa\u51c6\u5728\u5904\u7406\u4e2d\u6587\u4f01\u4e1a\u7ea7\u6570\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6d89\u53ca\u591a\u8868\u8fde\u63a5\u3001\u6a21\u7cca\u5b9e\u4f53\u5f15\u7528\u3001\u9886\u57df\u7279\u5b9a\u672f\u8bed\u548c\u590d\u6742SQL\u64cd\u4f5c\u7684\u573a\u666f\u3002\u8fd9\u963b\u788d\u4e86\u4e2d\u6587Text-to-SQL\u6280\u672f\u5728\u5b9e\u9645\u4f01\u4e1a\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b600\u4e2a\u4e2d\u6587\u95ee\u9898\u548c28\u4e2aMaxCompute/Hive\u6570\u636e\u5e93\u7684Falcon\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u6807\u6ce8\u4e86SQL\u8ba1\u7b97\u7279\u5f81\u548c\u4e2d\u6587\u8bed\u4e49\u3002\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\u9c81\u68d2\u7684\u6267\u884c\u6bd4\u8f83\u5668\u548c\u4e00\u4e2a\u81ea\u52a8\u8bc4\u4f30\u6d41\u7a0b\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728Falcon\u57fa\u51c6\u4e0a\uff0c\u5305\u62ecDeepseek\u5728\u5185\u7684\u6240\u6709\u73b0\u6709\u5148\u8fdb\u5927\u578b\u6a21\u578b\u51c6\u786e\u7387\u5747\u4f4e\u4e8e50%\u3002\u4e3b\u8981\u7684\u9519\u8bef\u6765\u6e90\u5305\u62ec\uff1a1) \u5927\u578b\u4f01\u4e1a\u73af\u5883\u4e2d\u590d\u6742\u6a21\u5f0f\u94fe\u63a5\u7684\u6311\u6218\uff08\u5982\u591a\u8868\u3001\u6a21\u7cca\u5217\u540d\u3001\u9690\u5f0f\u5916\u952e\u3001\u9886\u57df\u672f\u8bed\uff09\uff1b2) \u5c06\u7b80\u6d01\u53e3\u8bed\u5316\u7684\u4e2d\u6587\u6620\u5c04\u5230\u7cbe\u786e\u7684SQL\u64cd\u4f5c\uff08\u5982\u805a\u5408\u3001\u5206\u7ec4\u3001\u65f6\u95f4\u7a97\u53e3\u3001\u5355\u4f4d\u8f6c\u6362\u3001NULL\u503c\u5904\u7406\u3001\u5d4c\u5957\u67e5\u8be2\uff09\u3002", "conclusion": "Falcon\u57fa\u51c6\u6709\u6548\u5730\u66b4\u9732\u4e86\u5f53\u524d\u4e2d\u6587Text-to-SQL\u6a21\u578b\u5728\u5904\u7406\u771f\u5b9e\u4f01\u4e1a\u7ea7\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u6a21\u5f0f\u94fe\u63a5\u548c\u4e2d\u6587\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u3002\u8be5\u57fa\u51c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u6280\u672f\u5728\u4e2d\u6587\u4f01\u4e1a\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.25007", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25007", "abs": "https://arxiv.org/abs/2510.25007", "authors": ["Islam Nassar", "Yang Lin", "Yuan Jin", "Rongxin Zhu", "Chang Wei Tan", "Zenan Zhai", "Nitika Mathur", "Thanh Tien Vu", "Xu Zhong", "Long Duong", "Yuan-Fang Li"], "title": "Taming the Real-world Complexities in CPT E/M Coding with Large Language Models", "comment": "EMNLP 2025 Industry Track", "summary": "Evaluation and Management (E/M) coding, under the Current Procedural\nTerminology (CPT) taxonomy, documents medical services provided to patients by\nphysicians. Used primarily for billing purposes, it is in physicians' best\ninterest to provide accurate CPT E/M codes. %While important, it is an\nauxiliary task that adds to physicians' documentation burden. Automating this\ncoding task will help alleviate physicians' documentation burden, improve\nbilling efficiency, and ultimately enable better patient care. However, a\nnumber of real-world complexities have made E/M encoding automation a\nchallenging task. In this paper, we elaborate some of the key complexities and\npresent ProFees, our LLM-based framework that tackles them, followed by a\nsystematic evaluation. On an expert-curated real-world dataset, ProFees\nachieves an increase in coding accuracy of more than 36\\% over a commercial CPT\nE/M coding system and almost 5\\% over our strongest single-prompt baseline,\ndemonstrating its effectiveness in addressing the real-world complexities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aProFees\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u533b\u7597\u670d\u52a1\u4e2d\u7684Evaluation and Management (E/M)\u7f16\u7801\u4efb\u52a1\uff0c\u65e8\u5728\u51cf\u8f7b\u533b\u751f\u8d1f\u62c5\u3001\u63d0\u9ad8\u8ba1\u8d39\u6548\u7387\u5e76\u6539\u5584\u60a3\u8005\u62a4\u7406\u3002ProFees\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u7684\u5546\u4e1a\u7cfb\u7edf\uff0c\u7f16\u7801\u51c6\u786e\u7387\u63d0\u5347\u4e8636%\u4ee5\u4e0a\uff0c\u663e\u793a\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u73b0\u5b9e\u573a\u666f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "Evaluation and Management (E/M)\u7f16\u7801\u662f\u533b\u7597\u670d\u52a1\u8ba1\u8d39\u7684\u5173\u952e\u73af\u8282\uff0c\u4f46\u7531\u533b\u751f\u624b\u52a8\u5b8c\u6210\u4f1a\u589e\u52a0\u5176\u6587\u6863\u8d1f\u62c5\uff0c\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u81ea\u52a8\u5316E/M\u7f16\u7801\u53ef\u4ee5\u51cf\u8f7b\u533b\u751f\u7684\u8d1f\u62c5\uff0c\u63d0\u9ad8\u8ba1\u8d39\u6548\u7387\uff0c\u6700\u7ec8\u4f7f\u60a3\u8005\u53d7\u76ca\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u4e2dE/M\u7f16\u7801\u7684\u590d\u6742\u6027\u4f7f\u5f97\u81ea\u52a8\u5316\u4efb\u52a1\u5145\u6ee1\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3aProFees\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u89e3\u51b3E/M\u7f16\u7801\u81ea\u52a8\u5316\u4e2d\u7684\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u3002\u7814\u7a76\u4e2d\u5bf9ProFees\u7684\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u3001\u6240\u4f7f\u7528\u7684LLM\u6280\u672f\u4ee5\u53ca\u5982\u4f55\u5904\u7406\u5404\u79cd\u590d\u6742\u6027\u8fdb\u884c\u4e86\u9610\u8ff0\uff0c\u5e76\u901a\u8fc7\u5728\u4e00\u4e2a\u4e13\u5bb6\u7cbe\u5fc3\u7b56\u5212\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "\u5728\u4e13\u5bb6\u8bc4\u4f30\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cProFees\u6846\u67b6\u5728E/M\u7f16\u7801\u51c6\u786e\u7387\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002\u4e0e\u73b0\u6709\u7684\u5546\u4e1aCPT E/M\u7f16\u7801\u7cfb\u7edf\u76f8\u6bd4\uff0c\u51c6\u786e\u7387\u63d0\u5347\u4e86\u8d85\u8fc736%\u3002\u540c\u65f6\uff0c\u4e0e\u7814\u7a76\u4e2d\u6700\u5f3a\u7684\u5355\u63d0\u793a\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0cProFees\u7684\u51c6\u786e\u7387\u4e5f\u63d0\u9ad8\u4e86\u8fd15%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86ProFees\u5728\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u540d\u4e3aProFees\u7684LLM\u9a71\u52a8\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316E/M\u7f16\u7801\u4efb\u52a1\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u7801\u51c6\u786e\u7387\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u73b0\u5b9e\u573a\u666f\u65f6\u3002ProFees\u7684\u6210\u529f\u4e3a\u51cf\u8f7b\u533b\u751f\u6587\u6863\u8d1f\u62c5\u3001\u4f18\u5316\u533b\u7597\u8ba1\u8d39\u6d41\u7a0b\u4ee5\u53ca\u6700\u7ec8\u6539\u5584\u60a3\u8005\u62a4\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22ProFees\u5728\u66f4\u591a\u6837\u5316\u7684\u533b\u7597\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6301\u7eed\u4f18\u5316\u5176\u6027\u80fd\u3002"}}
{"id": "2510.25532", "categories": ["physics.plasm-ph", "astro-ph.HE"], "pdf": "https://arxiv.org/pdf/2510.25532", "abs": "https://arxiv.org/abs/2510.25532", "authors": ["Alexandre Sainterme", "Fatima Ebrahimi"], "title": "Global Non-Axisymmetric Hall Instabilities in a Rotating Plasma", "comment": null, "summary": "Non-axisymmetric, flow-driven instabilities in the incompressible Hall-MHD\nmodel are studied in a differentially rotating cylindrical plasma. It is found\nthat in the Hall-MHD regime, both whistler waves and ion-cyclotron waves can\nextract energy from the flow shear, resulting in two distinct branches of\nglobal instability. The non-axisymmetric whistler modes grow significantly\nfaster than non-axisymmetric, ideal MHD modes. A discussion of the whistler\ninstability mechanism is presented in the large-ion-skin-depth, `electron-MHD'\nlimit. It is observed that the effect of the Hall term on the non-axisymmetric\nmodes can be appreciable when $d_i$ is on the order of a few % of the width of\nthe cylindrical annulus. Distinct global modes emerge in the Hall-MHD regime at\nsignificantly stronger magnetic fields than those required for unstable global\nMHD modes.", "AI": {"tldr": "\u5728\u538b\u5f3a\u4e0d\u53d8\u7684\u970d\u5c14-MHD\u6a21\u578b\u4e2d\uff0c\u7814\u7a76\u4e86\u975e\u8f74\u5bf9\u79f0\u3001\u53d7\u6d41\u52a8\u9a71\u52a8\u7684\u4e0d\u7a33\u5b9a\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u970d\u5c14-MHD\u533a\u57df\uff0c\u4e24\u79cd\u4e0d\u540c\u7684\u5168\u5c40\u4e0d\u7a33\u5b9a\u6027\u5206\u652f\u2014\u2014\u54e8\u58f0\u6ce2\u548c\u79bb\u5b50\u56de\u65cb\u6ce2\u2014\u2014\u53ef\u4ee5\u4ece\u6d41\u526a\u5207\u4e2d\u63d0\u53d6\u80fd\u91cf\u3002\u975e\u8f74\u5bf9\u79f0\u54e8\u58f0\u6a21\u5f0f\u7684\u589e\u957f\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u975e\u8f74\u5bf9\u79f0\u7684\u7406\u60f3MHD\u6a21\u5f0f\u3002\u5728\u7535\u5b50MHD\u6781\u9650\u4e0b\uff0c\u970d\u5c14\u9879\u5bf9\u975e\u8f74\u5bf9\u79f0\u6a21\u5f0f\u7684\u5f71\u54cd\u5728\u79bb\u5b50\u5fb7\u62dc\u957f\u5ea6\u7ea6\u4e3a\u5706\u67f1\u5f62\u73af\u7a7a\u5bbd\u5ea6\u767e\u5206\u4e4b\u51e0\u65f6\u662f\u663e\u8457\u7684\u3002\u970d\u5c14-MHD\u533a\u57df\u51fa\u73b0\u7684\u5168\u5c40\u6a21\u5f0f\u6240\u9700\u7684\u78c1\u573a\u6bd4\u4e0d\u7a33\u5b9a\u7684\u5168\u5c40MHD\u6a21\u5f0f\u6240\u9700\u7684\u78c1\u573a\u5f3a\u5f97\u591a\u3002", "motivation": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5dee\u52a8\u65cb\u8f6c\u7684\u5706\u67f1\u5f62\u7b49\u79bb\u5b50\u4f53\u4e2d\uff0c\u4e0d\u53ef\u538b\u7f29\u970d\u5c14-MHD\u6a21\u578b\u4e2d\u975e\u8f74\u5bf9\u79f0\u3001\u53d7\u6d41\u52a8\u9a71\u52a8\u7684\u4e0d\u7a33\u5b9a\u6027\u3002\u7406\u89e3\u8fd9\u4e9b\u4e0d\u7a33\u5b9a\u6027\u5bf9\u4e8e\u53d7\u63a7\u6838\u805a\u53d8\u548c\u5929\u4f53\u7269\u7406\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u672c\u6587\u91c7\u7528\u7406\u8bba\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u4e0d\u53ef\u538b\u7f29\u970d\u5c14-MHD\u6a21\u578b\u5728\u5dee\u52a8\u65cb\u8f6c\u7684\u5706\u67f1\u5f62\u7b49\u79bb\u5b50\u4f53\u4e2d\u7684\u884c\u4e3a\u3002\u5206\u6790\u96c6\u4e2d\u5728\u975e\u8f74\u5bf9\u79f0\u3001\u53d7\u6d41\u52a8\u9a71\u52a8\u7684\u4e0d\u7a33\u5b9a\u6027\u4e0a\uff0c\u5e76\u63a2\u8ba8\u4e86\u970d\u5c14\u6548\u5e94\u548c\u79bb\u5b50\u5fb7\u62dc\u957f\u5ea6\u5bf9\u8fd9\u4e9b\u4e0d\u7a33\u5b9a\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5728\u970d\u5c14-MHD\u533a\u57df\uff0c\u54e8\u58f0\u6ce2\u548c\u79bb\u5b50\u56de\u65cb\u6ce2\u53ef\u4ee5\u4ece\u6d41\u526a\u5207\u4e2d\u63d0\u53d6\u80fd\u91cf\uff0c\u5bfc\u81f4\u4e24\u79cd\u5168\u5c40\u4e0d\u7a33\u5b9a\u6027\u3002\u975e\u8f74\u5bf9\u79f0\u54e8\u58f0\u6a21\u5f0f\u7684\u589e\u957f\u901f\u5ea6\u6bd4\u975e\u8f74\u5bf9\u79f0\u7684\u7406\u60f3MHD\u6a21\u5f0f\u5feb\u3002\u970d\u5c14\u6548\u5e94\u5728\u79bb\u5b50\u5fb7\u62dc\u957f\u5ea6\u4e0e\u5706\u67f1\u5f62\u73af\u7a7a\u5bbd\u5ea6\u76f8\u5f53\u65f6\u5bf9\u975e\u8f74\u5bf9\u79f0\u6a21\u5f0f\u6709\u663e\u8457\u5f71\u54cd\u3002\u970d\u5c14-MHD\u533a\u57df\u7684\u4e0d\u7a33\u5b9a\u5168\u5c40\u6a21\u5f0f\u9700\u8981\u5728\u6bd4\u4e0d\u7a33\u5b9a\u7684\u5168\u5c40MHD\u6a21\u5f0f\u66f4\u5f3a\u7684\u78c1\u573a\u4e0b\u624d\u80fd\u51fa\u73b0\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u8868\u660e\uff0c\u970d\u5c14\u6548\u5e94\u5728\u970d\u5c14-MHD\u533a\u57df\u4e2d\u53ef\u4ee5\u5f15\u5165\u65b0\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u663e\u8457\u5f71\u54cd\u7b49\u79bb\u5b50\u4f53\u7684\u884c\u4e3a\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u4e8e\u7406\u89e3\u548c\u63a7\u5236\u7b49\u79bb\u5b50\u4f53\u4e0d\u7a33\u5b9a\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u5f3a\u78c1\u573a\u548c\u7279\u5b9a\u51e0\u4f55\u5f62\u72b6\u7684\u573a\u666f\u4e2d\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fd9\u4e9b\u4e0d\u7a33\u5b9a\u6027\u5728\u66f4\u590d\u6742\u7684\u6a21\u578b\u548c\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2510.24772", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24772", "abs": "https://arxiv.org/abs/2510.24772", "authors": ["Debdeep Sanyal", "Manya Pandey", "Dhruv Kumar", "Saurabh Deshpande", "Murari Mandal"], "title": "Confidence is Not Competence", "comment": "20 Pages, 6 Figures, 8 Tables", "summary": "Large language models (LLMs) often exhibit a puzzling disconnect between\ntheir asserted confidence and actual problem-solving competence. We offer a\nmechanistic account of this decoupling by analyzing the geometry of internal\nstates across two phases - pre-generative assessment and solution execution. A\nsimple linear probe decodes the internal \"solvability belief\" of a model,\nrevealing a well-ordered belief axis that generalizes across model families and\nacross math, code, planning, and logic tasks. Yet, the geometries diverge -\nalthough belief is linearly decodable, the assessment manifold has high linear\neffective dimensionality as measured from the principal components, while the\nsubsequent reasoning trace evolves on a much lower-dimensional manifold. This\nsharp reduction in geometric complexity from thought to action mechanistically\nexplains the confidence-competence gap. Causal interventions that steer\nrepresentations along the belief axis leave final solutions unchanged,\nindicating that linear nudges in the complex assessment space do not control\nthe constrained dynamics of execution. We thus uncover a two-system\narchitecture - a geometrically complex assessor feeding a geometrically simple\nexecutor. These results challenge the assumption that decodable beliefs are\nactionable levers, instead arguing for interventions that target the procedural\ndynamics of execution rather than the high-level geometry of assessment.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u58f0\u79f0\u7684\u4fe1\u5fc3\u548c\u5b9e\u9645\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u4ee4\u4eba\u8d39\u89e3\u7684\u8131\u8282\u3002\u901a\u8fc7\u5206\u6790\u4e24\u4e2a\u9636\u6bb5\u2014\u2014\u751f\u6210\u524d\u8bc4\u4f30\u548c\u89e3\u51b3\u65b9\u6848\u6267\u884c\u2014\u2014\u5185\u90e8\u72b6\u6001\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u6211\u4eec\u5bf9\u8fd9\u79cd\u8131\u8282\u63d0\u4f9b\u4e86\u4e00\u4e2a\u673a\u5236\u6027\u89e3\u91ca\u3002\u4e00\u4e2a\u7b80\u5355\u7684\u7ebf\u6027\u63a2\u9488\u53ef\u4ee5\u89e3\u7801\u6a21\u578b\u5185\u90e8\u7684\u201c\u53ef\u89e3\u6027\u4fe1\u5ff5\u201d\uff0c\u5e76\u63ed\u793a\u4e00\u4e2a\u5728\u6a21\u578b\u5bb6\u65cf\u4ee5\u53ca\u6570\u5b66\u3001\u4ee3\u7801\u3001\u89c4\u5212\u548c\u903b\u8f91\u4efb\u52a1\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6709\u5e8f\u4fe1\u5ff5\u8f74\u3002\u7136\u800c\uff0c\u51e0\u4f55\u5f62\u72b6\u5374\u5b58\u5728\u5dee\u5f02\u2014\u2014\u5c3d\u7ba1\u4fe1\u5ff5\u662f\u7ebf\u6027\u53ef\u89e3\u7801\u7684\uff0c\u4f46\u4ece\u4e3b\u6210\u5206\u5206\u6790\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u8bc4\u4f30\u6d41\u5f62\u5177\u6709\u5f88\u9ad8\u7684\u7ebf\u6027\u6709\u6548\u7ef4\u5ea6\uff0c\u800c\u968f\u540e\u7684\u63a8\u7406\u8f68\u8ff9\u5219\u5728\u4f4e\u5f97\u591a\u7684\u7ef4\u5ea6\u6d41\u5f62\u4e0a\u6f14\u53d8\u3002\u4ece\u601d\u8003\u5230\u884c\u52a8\u7684\u51e0\u4f55\u590d\u6742\u6027\u7684\u6025\u5267\u964d\u4f4e\uff0c\u4ece\u673a\u5236\u4e0a\u89e3\u91ca\u4e86\u4fe1\u5fc3-\u80fd\u529b\u5dee\u8ddd\u3002\u65e8\u5728\u6cbf\u7740\u4fe1\u5ff5\u8f74\u5f15\u5bfc\u8868\u793a\u7684\u56e0\u679c\u5e72\u9884\u5e76\u672a\u6539\u53d8\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u8868\u660e\u590d\u6742\u7684\u8bc4\u4f30\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u5fae\u8c03\u5e76\u4e0d\u80fd\u63a7\u5236\u6267\u884c\u7684\u7ea6\u675f\u52a8\u6001\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63ed\u793a\u4e86\u4e00\u4e2a\u53cc\u7cfb\u7edf\u67b6\u6784\u2014\u2014\u4e00\u4e2a\u51e0\u4f55\u4e0a\u590d\u6742\u7684\u8bc4\u4f30\u5668\uff0c\u4e3a\u51e0\u4f55\u4e0a\u7b80\u5355\u7684\u6267\u884c\u5668\u63d0\u4f9b\u4fe1\u606f\u3002\u8fd9\u4e9b\u7ed3\u679c\u6311\u6218\u4e86\u53ef\u89e3\u7801\u4fe1\u5ff5\u662f\u53ef\u64cd\u4f5c\u6760\u6746\u7684\u5047\u8bbe\uff0c\u53cd\u800c\u4e3b\u5f20\u8fdb\u884c\u9488\u5bf9\u6267\u884c\u7a0b\u5e8f\u52a8\u6001\u800c\u4e0d\u662f\u8bc4\u4f30\u7684\u9ad8\u5c42\u51e0\u4f55\u7684\u5e72\u9884\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4fe1\u5fc3\u548c\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u8131\u8282\u3002\u8fd9\u79cd\u73b0\u8c61\u5f88\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u5f71\u54cd\u4e86\u6211\u4eec\u5bf9 LLMs \u7684\u7406\u89e3\u548c\u53ef\u9760\u6027\u3002", "method": "\u7814\u7a76\u4e86 LLMs \u5728\u751f\u6210\u524d\u8bc4\u4f30\u548c\u89e3\u51b3\u65b9\u6848\u6267\u884c\u9636\u6bb5\u7684\u5185\u90e8\u72b6\u6001\u51e0\u4f55\u3002\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u6765\u89e3\u7801\u201c\u53ef\u89e3\u6027\u4fe1\u5ff5\u201d\uff0c\u5e76\u5206\u6790\u4e86\u8bc4\u4f30\u548c\u6267\u884c\u6d41\u5f62\u7684\u7ef4\u5ea6\u3002\u8fdb\u884c\u4e86\u56e0\u679c\u5e72\u9884\u6765\u6d4b\u8bd5\u4fe1\u5ff5\u8f74\u7684\u63a7\u5236\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u4e2a\u666e\u904d\u5b58\u5728\u7684\u201c\u53ef\u89e3\u6027\u4fe1\u5ff5\u201d\u8f74\uff0c\u4f46\u8bc4\u4f30\u6d41\u5f62\u5177\u6709\u9ad8\u7ebf\u6027\u6709\u6548\u7ef4\u5ea6\uff0c\u800c\u63a8\u7406\u8f68\u8ff9\u5219\u5728\u4f4e\u7ef4\u5ea6\u6d41\u5f62\u4e0a\u3002\u56e0\u679c\u5e72\u9884\u8868\u660e\uff0c\u5728\u4fe1\u5ff5\u8f74\u4e0a\u7684\u7ebf\u6027\u8c03\u6574\u5e76\u4e0d\u80fd\u6539\u53d8\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "LLMs \u4f3c\u4e4e\u6709\u4e00\u4e2a\u7531\u590d\u6742\u7684\u8bc4\u4f30\u5668\u548c\u7b80\u5355\u7684\u6267\u884c\u5668\u7ec4\u6210\u7684\u4e24\u7cfb\u7edf\u67b6\u6784\u3002\u53ef\u89e3\u7801\u7684\u4fe1\u5ff5\u5e76\u4e0d\u603b\u662f\u53ef\u64cd\u4f5c\u7684\uff0c\u5e72\u9884\u63aa\u65bd\u5e94\u4fa7\u91cd\u4e8e\u6267\u884c\u7684\u7a0b\u5e8f\u52a8\u6001\uff0c\u800c\u4e0d\u662f\u8bc4\u4f30\u7684\u51e0\u4f55\u5f62\u72b6\u3002"}}
{"id": "2510.25014", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25014", "abs": "https://arxiv.org/abs/2510.25014", "authors": ["Minkyung Kim", "Junsik Kim", "Woongcheol Yang", "Sangdon Park", "Sohee Bae"], "title": "Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading", "comment": "8 pages main content, 18 pages supplementary material, 4 figures", "summary": "Large Language Models (LLMs) enable dynamic game interactions but fail to\nfollow essential procedural flows in rule-governed trading systems, eroding\nplayer trust. This work resolves the core tension between the creative\nflexibility of LLMs and the procedural demands of in-game trading\n(browse-offer-review-confirm). To this end, Autoregressive State-Tracking\nPrompting (ASTP) is introduced, a methodology centered on a strategically\norchestrated prompt that compels an LLM to make its state-tracking process\nexplicit and verifiable. Instead of relying on implicit contextual\nunderstanding, ASTP tasks the LLM with identifying and reporting a predefined\nstate label from the previous turn. To ensure transactional integrity, this is\ncomplemented by a state-specific placeholder post-processing method for\naccurate price calculations. Evaluation across 300 trading dialogues\ndemonstrates >99% state compliance and 99.3% calculation precision. Notably,\nASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)\nmatches larger models' (Gemini-2.5-Pro) performance while reducing response\ntime from 21.2s to 2.4s, establishing a practical foundation that satisfies\nboth real-time requirements and resource constraints of commercial games.", "AI": {"tldr": "LLM\u5728\u6e38\u620f\u4ea4\u6613\u4e2d\u5b58\u5728\u7a0b\u5e8f\u6d41\u7a0b\u9075\u5faa\u6027\u5dee\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u7684ASTP\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u72b6\u6001\u8ddf\u8e2a\u548c\u5360\u4f4d\u7b26\u540e\u5904\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u5728\u4ea4\u6613\u7cfb\u7edf\u4e2d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u5728\u5c0f\u6a21\u578b\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u6ee1\u8db3\u4e86\u6e38\u620f\u5b9e\u65f6\u6027\u548c\u8d44\u6e90\u7ea6\u675f\u7684\u8981\u6c42\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u7136\u80fd\u591f\u5b9e\u73b0\u52a8\u6001\u7684\u6e38\u620f\u4ea4\u4e92\uff0c\u4f46\u5728\u9075\u5faa\u89c4\u5219\u9a71\u52a8\u7684\u4ea4\u6613\u7cfb\u7edf\u4e2d\u6838\u5fc3\u7a0b\u5e8f\u6d41\u7a0b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u4f1a\u635f\u5bb3\u73a9\u5bb6\u7684\u4fe1\u4efb\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3LLM\u7684\u521b\u610f\u7075\u6d3b\u6027\u4e0e\u6e38\u620f\u4e2d\u4ea4\u6613\uff08\u6d4f\u89c8-\u62a5\u4ef7-\u5ba1\u6838-\u786e\u8ba4\uff09\u7684\u7a0b\u5e8f\u5316\u9700\u6c42\u4e4b\u95f4\u7684\u6838\u5fc3\u77db\u76fe\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u52a8\u56de\u5f52\u72b6\u6001\u8ddf\u8e2a\u63d0\u793a\uff08ASTP\uff09\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u5b83\u5f3a\u5236LLM\u660e\u786e\u5e76\u53ef\u9a8c\u8bc1\u5730\u8fdb\u884c\u72b6\u6001\u8ddf\u8e2a\u3002ASTP\u4e0d\u4f9d\u8d56\u4e8e\u9690\u5f0f\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u800c\u662f\u8981\u6c42LLM\u8bc6\u522b\u5e76\u62a5\u544a\u4e0a\u4e00\u56de\u5408\u9884\u5b9a\u4e49\u7684\u6807\u7b7e\u3002\u4e3a\u4e86\u4fdd\u8bc1\u4ea4\u6613\u7684\u5b8c\u6574\u6027\uff0c\u8be5\u65b9\u6cd5\u8fd8\u8f85\u4ee5\u4e00\u79cd\u7279\u5b9a\u72b6\u6001\u7684\u5360\u4f4d\u7b26\u540e\u5904\u7406\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u4ef7\u683c\u8ba1\u7b97\u3002\u5b9e\u9a8c\u5728300\u4e2a\u4ea4\u6613\u5bf9\u8bdd\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728300\u4e2a\u4ea4\u6613\u5bf9\u8bdd\u7684\u8bc4\u4f30\u4e2d\uff0cASTP\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d85\u8fc799%\u7684\u72b6\u6001\u7b26\u5408\u7387\u548c99.3%\u7684\u8ba1\u7b97\u7cbe\u5ea6\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u8f83\u5c0f\u6a21\u578b\uff08Gemini-2.5-Flash\uff09\u4e0a\u4f7f\u7528ASTP\u548c\u5360\u4f4d\u7b26\u540e\u5904\u7406\uff0c\u5176\u6027\u80fd\u4e0e\u66f4\u5927\u6a21\u578b\uff08Gemini-2.5-Pro\uff09\u76f8\u5f53\uff0c\u540c\u65f6\u5c06\u54cd\u5e94\u65f6\u95f4\u4ece21.2\u79d2\u7f29\u77ed\u81f32.4\u79d2\u3002", "conclusion": "ASTP\u65b9\u6cd5\u7ed3\u5408\u5360\u4f4d\u7b26\u540e\u5904\u7406\uff0c\u4e3aLLM\u5728\u6e38\u620f\u4ea4\u6613\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u5b9e\u7528\u7684\u57fa\u7840\uff0c\u5b83\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u540c\u65f6\uff0c\u6ee1\u8db3\u4e86\u5546\u4e1a\u6e38\u620f\u5bf9\u5b9e\u65f6\u6027\u548c\u8d44\u6e90\u6d88\u8017\u7684\u8981\u6c42\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86LLM\u5728\u9075\u5faa\u6e38\u620f\u4ea4\u6613\u89c4\u5219\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u9ad8\u6027\u80fdLLM\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2510.25092", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2510.25092", "abs": "https://arxiv.org/abs/2510.25092", "authors": ["Weijia Zhang", "Zijia Liu", "Haoru Li", "Haoqi Chen", "Jiaxuan You"], "title": "SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs", "comment": null, "summary": "Recent advances in text-only large language models (LLMs), such as\nDeepSeek-R1, demonstrate remarkable reasoning ability. However, these models\nremain fragile or entirely incapable when extended to multi-modal tasks.\nExisting approaches largely rely on single-form captions, which lack diversity\nand often fail to adapt across different types of Visual Question Answering\n(VQA) benchmarks. As a result, they provide no principled or efficient channel\nfor transmitting fine-grained visual information. We introduce Seeing Eye, a\nmodular framework that unlocks multimodal reasoning in text-only LLMs through\nan agent-based small VLM translator. This translator acts as a perception\nagent: it can invoke specialized tools (e.g., OCR and crop) and iteratively\ndistill multimodal inputs into structured intermediate representations (SIRs)\ntailored to the question. These SIRs are then passed to the text-only LLM,\nwhich serves as a reasoning agent. Crucially, the translator and reasoner\nengage in multi-round feedback and interaction, enabling the extraction of\ntargeted visual details and yielding more confident answers. Experiments on\nknowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate\nthat Seeing Eye not only reduces inference cost but also surpasses much larger\nend-to-end VLMs. For example, an instantiation combining a 3B-parameter vision\ntranslator with an 8B-parameter language reasoner outperforms a monolithic 32B\nVLM on challenging knowledge-based questions. Our results highlight that\ndecoupling perception from reasoning via agent information flow offers a\nscalable and plug-and-play pathway to multimodal reasoning, allowing strong\ntext-only LLMs to fully leverage their reasoning capabilities. Code is\navailable at: https://github.com/ulab-uiuc/SeeingEye", "AI": {"tldr": "Recent advances in text-only LLMs show strong reasoning but struggle with multimodal tasks. Existing methods use limited captions, failing to transfer visual information effectively. We introduce Seeing Eye, a modular framework using a small VLM translator as a perception agent. This agent uses tools like OCR and crop to distill multimodal inputs into structured intermediate representations (SIRs) tailored to the question. A text-only LLM acts as a reasoning agent, receiving these SIRs. Multi-round feedback between the translator and reasoner allows for targeted visual detail extraction and confident answers. Experiments on MMMU and MIA-Bench show Seeing Eye reduces inference cost and outperforms larger end-to-end VLMs. An example with a 3B vision translator and 8B language reasoner surpasses a 32B VLM on knowledge-based questions. This decoupled approach of perception and reasoning through agent information flow is a scalable, plug-and-play method for multimodal reasoning, leveraging LLMs\u2019 reasoning abilities.", "motivation": "Existing large language models (LLMs) excel at text-based reasoning but are ill-equipped for multimodal tasks. Current approaches to multimodal learning often depend on single-form captions, which lack diversity and are not adaptable to various Visual Question Answering (VQA) benchmarks. This limitation hinders the effective transfer of fine-grained visual information, creating a significant research problem in bridging the gap between text-only LLM capabilities and multimodal understanding. The significance lies in unlocking the potential of powerful text-only LLMs for a wider range of applications that require understanding both visual and textual data.", "method": "Seeing Eye is a modular framework that integrates text-only LLMs into multimodal reasoning. It employs a small Vision-Language Model (VLM) as a \"perception agent\" (translator) and a large text-only LLM as a \"reasoning agent\". The perception agent can utilize specialized tools like Optical Character Recognition (OCR) and image cropping to process multimodal inputs. It iteratively distills this information into structured intermediate representations (SIRs) that are specifically relevant to the posed question. These SIRs are then fed to the reasoning agent. A key aspect of the methodology is the multi-round feedback and interaction loop between the perception and reasoning agents. This interaction allows for the refinement of visual information extraction and leads to more confident and accurate answers. The framework is designed to be plug-and-play, enabling strong text-only LLMs to be readily adapted for multimodal tasks. Experiments were conducted on knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, to evaluate the performance of the Seeing Eye framework.", "result": "Experiments conducted on knowledge-intensive VQA benchmarks, specifically MMMU and MIA-Bench, demonstrate that the Seeing Eye framework achieves significant improvements. The results show that Seeing Eye not only reduces inference costs compared to existing methods but also surpasses much larger, end-to-end trained Vision-Language Models (VLMs). A notable example presented is an instantiation of Seeing Eye that combines a relatively small 3 billion parameter vision translator with an 8 billion parameter language reasoner. This combination was found to outperform a monolithic 32 billion parameter VLM, particularly on challenging knowledge-based questions. These findings highlight the effectiveness of the decoupled perception-reasoning approach.", "conclusion": "The Seeing Eye framework presents a scalable and plug-and-play pathway for enabling strong text-only LLMs to perform multimodal reasoning. By decoupling the perception (handled by a small VLM translator) from the reasoning (handled by a large LLM) and facilitating agent information flow through structured intermediate representations and interactive feedback, the framework effectively unlocks the potential of LLMs for multimodal tasks. The experimental results on benchmarks like MMMU and MIA-Bench validate its efficacy, showing competitive or superior performance compared to larger, end-to-end VLMs while reducing inference costs. This approach offers a promising direction for leveraging the advanced reasoning capabilities of existing LLMs in multimodal contexts. Potential limitations might include the dependency on the quality of the specialized tools and the overhead of the iterative feedback process. Future work could explore optimizing the SIRs, expanding the toolkit for the perception agent, and applying the framework to a broader range of multimodal tasks."}}
{"id": "2410.15817", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2410.15817", "abs": "https://arxiv.org/abs/2410.15817", "authors": ["Jie Sun", "Tianyu Zhang", "Houcheng Jiang", "Kexin Huang", "Xiang Shu", "Zhibo Zhu", "Lintao Ma", "Xingyu Lu", "Jun Zhou", "Junkang Wu", "Chi Luo", "An Zhang", "Junkang Wu", "Jiancan Wu", "Xiang Wang"], "title": "LaMP-Val: Large Language Models Empower Personalized Valuation in Auction", "comment": "17 pages, 5 figures, 8tables", "summary": "Auctions are a vital economic mechanism used to determine the market value of\ngoods or services through competitive bidding within a specific framework.\nHowever, much of the current research primarily focuses on the bidding\nalgorithms used within auction mechanisms. This often neglects the potential\nbenefits of incorporating individual users' unique preferences into the\nvaluation process. Our theoretical and empirical analysis demonstrates that\nvaluation errors can significantly impact the overall utility. To bridge this\ngap, we propose a personalized valuation framework, namely Large\n\\underline{La}nguage \\underline{M}odels-powered \\underline{P}ersonalized\n\\underline{Val}uation (LaMP-Val), which integrates Large Language Models to\nincorporate personalized semantic preference into users valuation process.\nLaMP-Val integrating three components: data, learning, and evaluation. The data\ncomponent tackles the challenge of building a novel dataset specifically for\nLLMs fine-tuning in personalized valuation modeling. The learning component\nintroduces a diversity template to enhance LLMs' capacity for modeling\nfine-grained personal valuation patterns. The evaluation component establishes\na closed-loop system where LLM-generated valuations interact with bidding\nstrategies and auction. It proposes two novel metrics to quantify valuation\nprecision and bidding intention accuracy in personalized scenarios. Extensive\nexperiments show that LaMP-Val more accurately captures personalized values and\nachieves greater profits than baseline approaches.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a LaMP-Val \u7684\u4e2a\u6027\u5316\u4f30\u503c\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u6574\u5408\u7528\u6237\u7684\u4e2a\u6027\u5316\u8bed\u4e49\u504f\u597d\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u62cd\u5356\u7814\u7a76\u4e2d\u5ffd\u89c6\u7528\u6237\u4e2a\u4f53\u504f\u597d\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u65b0\u9896\u6570\u636e\u96c6\u3001\u589e\u5f3aLLM\u7684\u7cbe\u7ec6\u4f30\u503c\u5efa\u6a21\u80fd\u529b\u4ee5\u53ca\u5efa\u7acb\u95ed\u73af\u8bc4\u4f30\u7cfb\u7edf\uff0c\u5728\u63d0\u9ad8\u4f30\u503c\u51c6\u786e\u6027\u548c\u76c8\u5229\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u62cd\u5356\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u7ade\u4ef7\u7b97\u6cd5\uff0c\u5ffd\u89c6\u4e86\u6574\u5408\u7528\u6237\u4e2a\u4f53\u504f\u597d\u5bf9\u4f30\u503c\u8fc7\u7a0b\u7684\u6f5c\u5728\u76ca\u5904\u3002\u7814\u7a76\u8868\u660e\uff0c\u4f30\u503c\u9519\u8bef\u4f1a\u663e\u8457\u5f71\u54cd\u7528\u6237\u6548\u7528\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5f15\u5165\u8003\u8651\u7528\u6237\u4e2a\u6027\u5316\u504f\u597d\u7684\u4f30\u503c\u65b9\u6cd5\u3002", "method": "LaMP-Val \u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u6570\u636e\u3001\u5b66\u4e60\u548c\u8bc4\u4f30\u3002\u6570\u636e\u7ec4\u4ef6\u81f4\u529b\u4e8e\u6784\u5efa\u7528\u4e8eLLM\u5fae\u8c03\u7684\u4e2a\u6027\u5316\u4f30\u503c\u65b0\u6570\u636e\u96c6\u3002\u5b66\u4e60\u7ec4\u4ef6\u901a\u8fc7\u5f15\u5165\u591a\u6837\u6027\u6a21\u677f\uff0c\u63d0\u5347LLM\u5bf9\u7cbe\u7ec6\u5316\u4e2a\u4eba\u4f30\u503c\u6a21\u5f0f\u7684\u5efa\u6a21\u80fd\u529b\u3002\u8bc4\u4f30\u7ec4\u4ef6\u5efa\u7acb\u4e86\u4e00\u4e2a\u95ed\u73af\u7cfb\u7edf\uff0c\u4f7fLLM\u751f\u6210\u7684\u4f30\u503c\u4e0e\u7ade\u4ef7\u7b56\u7565\u548c\u62cd\u5356\u673a\u5236\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u6307\u6807\u6765\u91cf\u5316\u4e2a\u6027\u5316\u573a\u666f\u4e0b\u7684\u4f30\u503c\u7cbe\u5ea6\u548c\u7ade\u4ef7\u610f\u5411\u51c6\u786e\u6027\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLaMP-Val \u80fd\u591f\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u51c6\u786e\u5730\u6355\u6349\u4e2a\u6027\u5316\u4ef7\u503c\uff0c\u5e76\u5e26\u6765\u66f4\u9ad8\u7684\u5229\u6da6\u3002", "conclusion": "LaMP-Val \u6846\u67b6\u901a\u8fc7\u96c6\u6210LLM\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u62cd\u5356\u7814\u7a76\u4e2d\u5bf9\u7528\u6237\u4e2a\u6027\u5316\u504f\u597d\u8003\u8651\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u503c\u7684\u51c6\u786e\u6027\u548c\u7ecf\u6d4e\u6548\u76ca\u3002\u8be5\u7814\u7a76\u4e3a\u4e2a\u6027\u5316\u62cd\u5356\u4f30\u503c\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5b9e\u8bc1\u652f\u6301\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u66f4\u590d\u6742\u62cd\u5356\u573a\u666f\u548c\u4e0d\u540c\u7c7b\u578b\u5546\u54c1\u4e0a\u7684\u5e94\u7528\u3002"}}
{"id": "2510.24789", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24789", "abs": "https://arxiv.org/abs/2510.24789", "authors": ["Gokul Ganesan"], "title": "Cross-Lingual Summarization as a Black-Box Watermark Removal Attack", "comment": null, "summary": "Watermarking has been proposed as a lightweight mechanism to identify\nAI-generated text, with schemes typically relying on perturbations to token\ndistributions. While prior work shows that paraphrasing can weaken such\nsignals, these attacks remain partially detectable or degrade text quality. We\ndemonstrate that cross-lingual summarization attacks (CLSA) -- translation to a\npivot language followed by summarization and optional back-translation --\nconstitute a qualitatively stronger attack vector. By forcing a semantic\nbottleneck across languages, CLSA systematically destroys token-level\nstatistical biases while preserving semantic fidelity. In experiments across\nmultiple watermarking schemes (KGW, SIR, XSIR, Unigram) and five languages\n(Amharic, Chinese, Hindi, Spanish, Swahili), we show that CLSA reduces\nwatermark detection accuracy more effectively than monolingual paraphrase at\nsimilar quality levels. Our results highlight an underexplored vulnerability\nthat challenges the practicality of watermarking for provenance or regulation.\nWe argue that robust provenance solutions must move beyond distributional\nwatermarking and incorporate cryptographic or model-attestation approaches. On\n300 held-out samples per language, CLSA consistently drives detection toward\nchance while preserving task utility. Concretely, for XSIR (explicitly designed\nfor cross-lingual robustness), AUROC with paraphrasing is $0.827$, with\nCross-Lingual Watermark Removal Attacks (CWRA) [He et al., 2024] using Chinese\nas the pivot, it is $0.823$, whereas CLSA drives it down to $0.53$ (near\nchance). Results highlight a practical, low-cost removal pathway that crosses\nlanguages and compresses content without visible artifacts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8de8\u8bed\u8a00\u6458\u8981\u653b\u51fb\uff08CLSA\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06AI\u751f\u6210\u6587\u672c\u7ffb\u8bd1\u5230\u4e2d\u95f4\u8bed\u8a00\uff0c\u7136\u540e\u8fdb\u884c\u6458\u8981\uff0c\u518d\u53ef\u9009\u5730\u7ffb\u8bd1\u56de\u539f\u6587\uff0c\u6709\u6548\u4e14\u4f4e\u6210\u672c\u5730\u6d88\u9664\u4e86\u6587\u672c\u6c34\u5370\u4fe1\u53f7\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6587\u672c\u7684\u8bed\u4e49\u548c\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cCLSA\u6bd4\u73b0\u6709\u7684\u5355\u8bed\u91ca\u4e49\u653b\u51fb\u66f4\u80fd\u6709\u6548\u5730\u964d\u4f4e\u6c34\u5370\u68c0\u6d4b\u7684\u51c6\u786e\u7387\uff0c\u751a\u81f3\u80fd\u4f7f\u4e13\u95e8\u4e3a\u8de8\u8bed\u8a00\u9c81\u68d2\u6027\u8bbe\u8ba1\u7684\u6c34\u5370\u65b9\u6848\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\u6c34\u5e73\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f9d\u8d56\u4e8e\u5206\u5e03\u5f0f\u6c34\u5370\u7684\u6587\u672c\u6eaf\u6e90\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u9690\u60a3\uff0c\u5e76\u5efa\u8bae\u91c7\u7528\u52a0\u5bc6\u6216\u6a21\u578b\u8ba4\u8bc1\u7b49\u66f4\u5f3a\u5927\u7684\u6eaf\u6e90\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684AI\u751f\u6210\u6587\u672c\u6c34\u5370\u6280\u672f\u901a\u5e38\u4f9d\u8d56\u4e8e\u5bf9\u8bcd\u5143\u5206\u5e03\u7684\u6270\u52a8\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u91ca\u4e49\u653b\u51fb\u7684\u524a\u5f31\u3002\u5c3d\u7ba1\u4e4b\u524d\u7684\u7814\u7a76\u8868\u660e\u91ca\u4e49\u653b\u51fb\u53ef\u4ee5\u964d\u4f4e\u6c34\u5370\u4fe1\u53f7\uff0c\u4f46\u8fd9\u4e9b\u653b\u51fb\u8981\u4e48\u68c0\u6d4b\u6b8b\u7559\u4fe1\u53f7\uff0c\u8981\u4e48\u4f1a\u635f\u5bb3\u6587\u672c\u8d28\u91cf\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u63a2\u7d22\u66f4\u6709\u6548\u3001\u4e14\u4e0d\u635f\u5bb3\u6587\u672c\u8d28\u91cf\u7684\u6c34\u5370\u653b\u51fb\u65b9\u6cd5\uff0c\u4ee5\u63ed\u793a\u5f53\u524d\u6c34\u5370\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a8\u52a8\u66f4\u53ef\u9760\u7684\u6eaf\u6e90\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u5c55\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u8bed\u8a00\u6458\u8981\u653b\u51fb\uff08CLSA\uff09\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u6b65\u9aa4\u5305\u62ec\uff1a1. \u5c06AI\u751f\u6210\u6587\u672c\u7ffb\u8bd1\u6210\u4e00\u79cd\u4e2d\u95f4\u201c\u67a2\u7ebd\u201d\u8bed\u8a00\uff08pivot language\uff09\u30022. \u5bf9\u7ffb\u8bd1\u540e\u7684\u6587\u672c\u8fdb\u884c\u6458\u8981\uff0c\u4ee5\u538b\u7f29\u5185\u5bb9\u5e76\u7834\u574f\u539f\u59cb\u7684\u8bcd\u5143\u7edf\u8ba1\u89c4\u5f8b\u30023. \uff08\u53ef\u9009\uff09\u5c06\u6458\u8981\u540e\u7684\u6587\u672c\u7ffb\u8bd1\u56de\u539f\u59cb\u8bed\u8a00\u3002\u7814\u7a76\u4eba\u5458\u5728\u4e94\u79cd\u8bed\u8a00\uff08\u963f\u59c6\u54c8\u62c9\u8bed\u3001\u4e2d\u6587\u3001\u5370\u5730\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u65af\u74e6\u5e0c\u91cc\u8bed\uff09\u4e0a\uff0c\u9488\u5bf9\u591a\u79cd\u6c34\u5370\u65b9\u6848\uff08KGW, SIR, XSIR, Unigram\uff09\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\u3002\u5b9e\u9a8c\u5c06CLSA\u7684\u6548\u679c\u4e0e\u5355\u8bed\u91ca\u4e49\u653b\u51fb\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5e76\u4f7f\u7528\u4e86AUROC\uff08\u53d7\u8bd5\u8005\u5de5\u4f5c\u7279\u5f81\u66f2\u7ebf\u4e0b\u9762\u79ef\uff09\u4f5c\u4e3a\u4e3b\u8981\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u540c\u65f6\u5173\u6ce8\u4e86\u6587\u672c\u8d28\u91cf\u7684\u4fdd\u6301\u60c5\u51b5\u3002", "result": "\u5728\u8de8\u8bed\u8a00\u6458\u8981\u653b\u51fb\uff08CLSA\uff09\u65b9\u9762\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6c34\u5370\u65b9\u6848\u548c\u591a\u79cd\u8bed\u8a00\u4e0b\u5747\u80fd\u663e\u8457\u964d\u4f4e\u6c34\u5370\u68c0\u6d4b\u7684\u51c6\u786e\u7387\u3002\u4e0e\u5355\u8bed\u91ca\u4e49\u653b\u51fb\u76f8\u6bd4\uff0cCLSA\u5728\u76f8\u4f3c\u7684\u6587\u672c\u8d28\u91cf\u6c34\u5e73\u4e0b\uff0c\u66f4\u80fd\u6709\u6548\u5730\u6d88\u9664\u6c34\u5370\u4fe1\u53f7\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5bf9\u4e8e\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8de8\u8bed\u8a00\u9c81\u68d2\u6027\u7684XSIR\u6c34\u5370\u65b9\u6848\uff0c\u5728\u91c7\u7528\u91ca\u4e49\u653b\u51fb\u65f6AUROC\u4e3a0.827\uff0c\u91c7\u7528\u5df2\u6709\u7684\u8de8\u8bed\u8a00\u6c34\u5370\u79fb\u9664\u653b\u51fb\uff08CWRA\uff09\u65f6\u4e3a0.823\uff0c\u800c\u91c7\u7528CLSA\u65f6\uff0cAUROC\u5219\u9aa4\u964d\u81f30.53\uff0c\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\u7684\u6c34\u5e73\u3002\u8fd9\u8868\u660eCLSA\u662f\u4e00\u79cd\u975e\u5e38\u6709\u6548\u7684\u653b\u51fb\u624b\u6bb5\uff0c\u5373\u4f7f\u662f\u9488\u5bf9\u90a3\u4e9b\u8bbe\u8ba1\u521d\u8877\u5c31\u8003\u8651\u4e86\u8de8\u8bed\u8a00\u573a\u666f\u7684\u6c34\u5370\u7b97\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u8de8\u8bed\u8a00\u6458\u8981\u653b\u51fb\uff08CLSA\uff09\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684AI\u6587\u672c\u6c34\u5370\u79fb\u9664\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u8de8\u8bed\u8a00\u8f6c\u6362\u548c\u6458\u8981\u64cd\u4f5c\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6587\u672c\u8bed\u4e49\u548c\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u7cfb\u7edf\u6027\u5730\u7834\u574f\u6c34\u5370\u7684\u7edf\u8ba1\u4fe1\u53f7\u3002\u5b9e\u9a8c\u7ed3\u679c\u6709\u529b\u5730\u8bc1\u660e\u4e86\u5f53\u524d\u57fa\u4e8e\u5206\u5e03\u5f0f\u6c34\u5370\u7684\u6587\u672c\u6eaf\u6e90\u65b9\u6848\u7684\u8106\u5f31\u6027\uff0c\u51f8\u663e\u4e86\u5176\u5728\u9762\u5bf9\u6b64\u7c7b\u653b\u51fb\u65f6\u7684\u5c40\u9650\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u4eba\u5458\u8ba4\u4e3a\uff0c\u4e3a\u4e86\u5b9e\u73b0\u53ef\u9760\u7684\u6587\u672c\u6eaf\u6e90\u548c\u76d1\u7ba1\u76ee\u7684\uff0c\u5fc5\u987b\u8d85\u8d8a\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u6c34\u5370\u6280\u672f\uff0c\u8f6c\u5411\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5982\u52a0\u5bc6\u6280\u672f\u6216\u6a21\u578b\u8ba4\u8bc1\u7b49\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u5177\u9c81\u68d2\u6027\u7684AI\u5185\u5bb9\u6eaf\u6e90\u673a\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u542f\u793a\u3002"}}
{"id": "2510.25065", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25065", "abs": "https://arxiv.org/abs/2510.25065", "authors": ["Taekhyun Park", "Yongjae Lee", "Hyerim Bae"], "title": "Reasoning-Aware GRPO using Process Mining", "comment": null, "summary": "Reinforcement learning (RL)-based post-training has been crucial for enabling\nmulti-step reasoning in large reasoning models (LRMs), yet current reward\nschemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware\nGroup Relative Policy Optimization (GRPO) that augments standard answer/format\nrewards with signals over the reasoning procedure. To this end, process mining\ntechniques are utilized to compute a scalar conformance reward that measures\nhow closely a policy model's reasoning aligns with the pretrained teacher\nmodel. The empirical results on five benchmarks demonstrate that PM4GRPO\nsignificantly outperforms existing methodologies for GRPO-based post-training.\nThese results highlight that leveraging process mining for reasoning-aware GRPO\neffectively enhances the reasoning capabilities of policy models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPM4GRPO\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8fc7\u7a0b\u6316\u6398\u6280\u672f\uff0c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u6ce8\u5165\u4e86\u4e0e\u63a8\u7406\u8fc7\u7a0b\u76f8\u5173\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u7684\u7b54\u6848\u548c\u683c\u5f0f\uff0c\u800c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u672c\u8eab\u7684\u8d28\u91cf\u3002\u8fd9\u79cd\u201c\u7ed3\u679c\u5bfc\u5411\u201d\u7684\u5956\u52b1\u673a\u5236\u9650\u5236\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u591a\u6b65\u63a8\u7406\u65b9\u9762\u7684\u6f5c\u80fd\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u5f15\u5165\u4e0e\u63a8\u7406\u8fc7\u7a0b\u76f8\u5173\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u6307\u5bfc\u6a21\u578b\u7684\u5b66\u4e60\uff0c\u63d0\u5347\u5176\u591a\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPM4GRPO\uff08Process Mining for Group Relative Policy Optimization\uff09\u7684\u65b9\u6cd5\u3002PM4GRPO\u57fa\u4e8eGroup Relative Policy Optimization\uff08GRPO\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u8fc7\u7a0b\u6316\u6398\u6280\u672f\u6765\u751f\u6210\u201c\u63a8\u7406\u8fc7\u7a0b\u5956\u52b1\u201d\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u5229\u7528\u8fc7\u7a0b\u6316\u6398\u6765\u8ba1\u7b97\u4e00\u4e2a\u6807\u91cf\u4e00\u81f4\u6027\u5956\u52b1\uff0c\u8861\u91cf\u7b56\u7565\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u8fc7\u7a0b\u4e0e\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u3002\u8fd9\u79cd\u5956\u52b1\u4fe1\u53f7\u88ab\u7528\u6765\u589e\u5f3a\u6807\u51c6\u7684\u7b54\u6848/\u683c\u5f0f\u5956\u52b1\uff0c\u4ece\u800c\u66f4\u5168\u9762\u5730\u6307\u5bfc\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u5b9e\u9a8c\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u4ee5\u8bc4\u4f30PM4GRPO\u7684\u6027\u80fd\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPM4GRPO\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684GRPO\u7c7b\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002PM4GRPO\u5728\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u8fc7\u7a0b\u6316\u6398\u6280\u672f\u4e3aGRPO\u5f15\u5165\u63a8\u7406\u611f\u77e5\u5956\u52b1\u4fe1\u53f7\uff0c\u80fd\u591f\u6709\u6548\u5730\u589e\u5f3a\u7b56\u7565\u6a21\u578b\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002PM4GRPO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u66f4\u590d\u6742\u7684\u5956\u52b1\u673a\u5236\u6216\u5c06\u6b64\u65b9\u6cd5\u5e94\u7528\u4e8e\u5176\u4ed6\u7c7b\u578b\u7684\u6a21\u578b\u3002"}}
{"id": "2510.24793", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24793", "abs": "https://arxiv.org/abs/2510.24793", "authors": ["Edouard Lansiaux"], "title": "SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications", "comment": null, "summary": "We present a static token lookup methodology for text embedding generation\nthat achieves 1.12 ms p50 latency for single text embeddings while maintaining\n60.6 MTEB average score across 8 representative tasks, corresponding to 89% of\ncontextual model quality. The Rust implementation delivers 50,000 requests per\nsecond throughput through static embedding lookup, optimized mean pooling, and\nzero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional\nduplicate detection performance (90.1% AP), strong semantic similarity (76.1%\nSpearman correlation), and domain-specific performance ranging from 75% to 131%\nof baseline across specialized domains. The system enables real-time embedding\napplications where sub-5ms latency is critical.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9759\u6001\u4ee4\u724c\u67e5\u627e\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u672c\u5d4c\u5165\u751f\u6210\uff0c\u5b9e\u73b0\u4e861.12\u6beb\u79d2\u7684p50\u5ef6\u8fdf\uff0c\u540c\u65f6\u57288\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\u4e0a\u4fdd\u630160.6 MTEB\u5e73\u5747\u5206\u6570\uff08\u76f8\u5f53\u4e8e\u4e0a\u4e0b\u6587\u6a21\u578b\u8d28\u91cf\u768489%\uff09\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5d4c\u5165\u751f\u6210\u65b9\u6cd5\u5728\u9ad8\u5ef6\u8fdf\u548c\u4f4e\u541e\u5410\u91cf\u65b9\u9762\u7684\u6311\u6218\uff0c\u4ee5\u6ee1\u8db3\u9700\u8981\u4e9a\u6beb\u79d2\u7ea7\u5ef6\u8fdf\u7684\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002", "method": "\u91c7\u7528\u9759\u6001\u4ee4\u724c\u67e5\u627e\u6280\u672f\uff0c\u7ed3\u5408\u4f18\u5316\u7684\u5747\u503c\u6c60\u5316\u548c\u96f6\u62f7\u8d1dIEEE754\u4e8c\u8fdb\u5236\u5e8f\u5217\u5316\uff0c\u5e76\u4f7f\u7528Rust\u5b9e\u73b0\u3002", "result": "\u57288\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e8660.6 MTEB\u5e73\u5747\u5206\u6570\uff0cp50\u5ef6\u8fdf\u4e3a1.12\u6beb\u79d2\uff0c\u541e\u5410\u91cf\u8fbe\u5230\u6bcf\u79d250,000\u4e2a\u8bf7\u6c42\u3002\u5728\u91cd\u590d\u68c0\u6d4b\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5206\u522b\u8fbe\u523090.1% AP\u548c76.1% Spearman\u76f8\u5173\u7cfb\u6570\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u9700\u8981\u4e9a\u6beb\u79d2\u7ea7\u5ef6\u8fdf\u7684\u5b9e\u65f6\u5d4c\u5165\u5e94\u7528\uff0c\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u5728\u4e0d\u540c\u9886\u57df\u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2510.25340", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25340", "abs": "https://arxiv.org/abs/2510.25340", "authors": ["Beiwen Zhang", "Yongheng Liang", "Hejun Wu"], "title": "Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork", "comment": null, "summary": "Multi-agent reinforcement learning (MARl) has achieved strong results in\ncooperative tasks but typically assumes fixed, fully controlled teams. Ad hoc\nteamwork (AHT) relaxes this by allowing collaboration with unknown partners,\nyet existing variants still presume shared conventions. We introduce\nMultil-party Ad Hoc Teamwork (MAHT), where controlled agents must coordinate\nwith multiple mutually unfamiliar groups of uncontrolled teammates. To address\nthis, we propose MARs, which builds a sparse skeleton graph and applies\nrelational modeling to capture cross-group dvnamics. Experiments on MPE and\nstarCralt ll show that MARs outperforms MARL and AHT baselines while converging\nfaster.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u65b9\u81ea\u9002\u5e94\u56e2\u961f\u5408\u4f5c (MAHT) \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60 (MARL) \u4e2d\u4e0e\u591a\u4e2a\u672a\u77e5\u3001\u4e0d\u53d7\u63a7\u7684\u56e2\u961f\u5408\u4f5c\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u540d\u4e3a MARs \u7684\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u9aa8\u5e72\u56fe\u548c\u5173\u7cfb\u5efa\u6a21\u6765\u6355\u6349\u8de8\u56e2\u961f\u52a8\u6001\uff0c\u5728 MPE \u548c StarCraft II \u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709 MARL \u548c AHT \u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u52a0\u5feb\u4e86\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684 MARL \u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u56e2\u961f\u6210\u5458\u662f\u56fa\u5b9a\u7684\u4e14\u5b8c\u5168\u53d7\u63a7\u7684\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u534f\u4f5c\u4efb\u52a1\u4e2d\u53d7\u5230\u9650\u5236\u3002\u81ea\u9002\u5e94\u56e2\u961f\u5408\u4f5c (AHT) \u653e\u5bbd\u4e86\u5bf9\u5408\u4f5c\u8005\u5b8c\u5168\u4e86\u89e3\u7684\u5047\u8bbe\uff0c\u4f46\u4ecd\u5047\u5b9a\u5b58\u5728\u5171\u4eab\u7684\u7ea6\u5b9a\u3002\u7136\u800c\uff0c\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u667a\u80fd\u4f53\u9700\u8981\u4e0e\u591a\u4e2a\u4e92\u4e0d\u719f\u6089\u3001\u4e0d\u53d7\u63a7\u7684\u56e2\u961f\u8fdb\u884c\u534f\u8c03\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u8fd9\u79cd\u590d\u6742\u7684\u591a\u65b9\u3001\u975e\u56fa\u5b9a\u3001\u65e0\u5171\u4eab\u7ea6\u5b9a\u534f\u4f5c\u573a\u666f\u7684 MARL \u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MARs \u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u65b9\u81ea\u9002\u5e94\u56e2\u961f\u5408\u4f5c (MAHT) \u95ee\u9898\u3002MARs \u7684\u6838\u5fc3\u601d\u60f3\u662f\u6784\u5efa\u4e00\u4e2a\u7a00\u758f\u9aa8\u5e72\u56fe\u6765\u8868\u793a\u667a\u80fd\u4f53\u53ca\u5176\u4e0e\u5176\u4ed6\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5e94\u7528\u5173\u7cfb\u5efa\u6a21\u6765\u6355\u6349\u8de8\u56e2\u961f\u7684\u52a8\u6001\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0cMARs \u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u4e0e\u591a\u4e2a\u672a\u77e5\u56e2\u961f\u8fdb\u884c\u534f\u8c03\uff0c\u800c\u65e0\u9700\u9884\u5148\u5171\u4eab\u7684\u7ea6\u5b9a\u3002\u5b9e\u9a8c\u5728 MPE \u548c StarCraft II \u8fd9\u4e24\u4e2a\u57fa\u51c6\u73af\u5883\u4e2d\u8fdb\u884c\uff0c\u5e76\u5c06 MARs \u4e0e\u4f20\u7edf\u7684 MARL \u548c AHT \u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5728 MPE \u548c StarCraft II \u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMARs \u5728\u591a\u65b9\u81ea\u9002\u5e94\u56e2\u961f\u5408\u4f5c\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684 MARL \u548c AHT \u57fa\u7ebf\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0cMARs \u80fd\u591f\u66f4\u6709\u6548\u5730\u4e0e\u591a\u4e2a\u672a\u77e5\u56e2\u961f\u8fdb\u884c\u534f\u8c03\uff0c\u5e76\u4e14\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002\u8fd9\u8bc1\u660e\u4e86 MARs \u5728\u5904\u7406\u590d\u6742\u7684\u591a\u65b9\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u63d0\u51fa\u4e86 MAHT \u6846\u67b6\u548c MARs \u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728 MARL \u4e2d\u4e0e\u591a\u4e2a\u672a\u77e5\u3001\u4e0d\u53d7\u63a7\u56e2\u961f\u8fdb\u884c\u534f\u4f5c\u7684\u6311\u6218\u3002MARs \u901a\u8fc7\u5229\u7528\u7a00\u758f\u9aa8\u5e72\u56fe\u548c\u5173\u7cfb\u5efa\u6a21\u6355\u6349\u8de8\u56e2\u961f\u52a8\u6001\uff0c\u5728\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5904\u7406\u66f4\u5e7f\u6cdb\u7684\u73b0\u5b9e\u4e16\u754c\u534f\u4f5c\u667a\u80fd\u4f53\u573a\u666f\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u590d\u6742\u7684\u73af\u5883\u548c\u66f4\u5927\u89c4\u6a21\u7684\u56e2\u961f\uff0c\u4ee5\u53ca\u7814\u7a76 MARs \u5728\u5904\u7406\u52a8\u6001\u53d8\u5316\u7684\u56e2\u961f\u6784\u6210\u548c\u76ee\u6807\u65f6\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24794", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24794", "abs": "https://arxiv.org/abs/2510.24794", "authors": ["Xinming Wang", "Jian Xu", "Bin Yu", "Sheng Lian", "Hongzhu Yi", "Yi Chen", "Yingjian Zhu", "Boran Wang", "Hongming Yang", "Han Hu", "Xu-Yao Zhang", "Cheng-Lin Liu"], "title": "MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models", "comment": "Preprint", "summary": "Large reasoning models (LRMs) show strong capabilities in complex reasoning,\nyet their marginal gains on evidence-dependent factual questions are limited.\nWe find this limitation is partially attributable to a reasoning-answer hit\ngap, where the model identifies the correct facts during reasoning but fails to\nincorporate them into the final response, thereby reducing factual fidelity. To\naddress this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment\nframework that enhances factuality without relying on external verifiers.\nMR-ALIGN quantifies state transition probabilities along the model's thinking\nprocess and constructs a transition-aware implicit reward that reinforces\nbeneficial reasoning patterns while suppressing defective ones at the atomic\nthinking segments. This re-weighting reshapes token-level signals into\nprobability-aware segment scores, encouraging coherent reasoning trajectories\nthat are more conducive to factual correctness. Empirical evaluations across\nfour factual QA datasets and one long-form factuality benchmark show that\nMR-ALIGN consistently improves accuracy and truthfulness while reducing\nmisleading reasoning. These results highlight that aligning the reasoning\nprocess itself, rather than merely the outputs, is pivotal for advancing\nfactuality in LRMs.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u5904\u7406\u4f9d\u8d56\u8bc1\u636e\u7684\u4e8b\u5b9e\u6027\u95ee\u9898\u65f6\uff0c\u5b58\u5728\u201c\u63a8\u7406-\u7b54\u6848\u547d\u4e2d\u5dee\u8ddd\u201d\u7684\u5c40\u9650\u6027\uff0c\u5373\u6a21\u578b\u867d\u7136\u80fd\u8bc6\u522b\u6b63\u786e\u4e8b\u5b9e\u4f46\u672a\u80fd\u5c06\u5176\u7eb3\u5165\u6700\u7ec8\u7b54\u6848\uff0c\u5bfc\u81f4\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u4e86MR-ALIGN\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u6a21\u578b\u601d\u8003\u8fc7\u7a0b\u4e2d\u7684\u72b6\u6001\u8f6c\u79fb\u6982\u7387\uff0c\u6784\u5efa\u5956\u52b1\u673a\u5236\uff0c\u5728\u539f\u5b50\u601d\u8003\u7247\u6bb5\u5c42\u9762\u5f3a\u5316\u6b63\u786e\u63a8\u7406\u6a21\u5f0f\uff0c\u6291\u5236\u9519\u8bef\u6a21\u5f0f\uff0c\u4ece\u800c\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u9a8c\u8bc1\u5668\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cMR-ALIGN\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u80fd\u6709\u6548\u63d0\u5347\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\uff0c\u51cf\u5c11\u9519\u8bef\u63a8\u7406\uff0c\u8868\u660e\u5bf9\u63a8\u7406\u8fc7\u7a0b\u672c\u8eab\u7684\u5bf9\u9f50\u662f\u63d0\u5347LRM\u4e8b\u5b9e\u6027\u7684\u5173\u952e\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u4f9d\u8d56\u8bc1\u636e\u7684\u4e8b\u5b9e\u6027\u95ee\u9898\u65f6\uff0c\u5176\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u79cd\u5c40\u9650\u6027\u90e8\u5206\u6e90\u4e8e\u201c\u63a8\u7406-\u7b54\u6848\u547d\u4e2d\u5dee\u8ddd\u201d\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u80fd\u6b63\u786e\u8bc6\u522b\u4e8b\u5b9e\uff0c\u5374\u65e0\u6cd5\u5c06\u5176\u6709\u6548\u6574\u5408\u5230\u6700\u7ec8\u7b54\u6848\u4e2d\uff0c\u4ece\u800c\u5f71\u54cd\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\u3002\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5bf9\u4e8e\u63d0\u5347LRMs\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMR-ALIGN\u7684\u5143\u63a8\u7406\u5bf9\u9f50\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\uff08LRMs\uff09\u4e8b\u5b9e\u6027\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u9a8c\u8bc1\u5668\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u91cf\u5316\u6a21\u578b\u601d\u8003\u8fc7\u7a0b\u4e2d\u7684\u72b6\u6001\u8f6c\u79fb\u6982\u7387\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u8003\u8651\u8f6c\u79fb\u7684\u9690\u5f0f\u5956\u52b1\u4fe1\u53f7\u3002\u8fd9\u4e2a\u4fe1\u53f7\u5728\u6a21\u578b\u7684\u539f\u5b50\u601d\u8003\u7247\u6bb5\u5c42\u9762\u8d77\u4f5c\u7528\uff0c\u80fd\u591f\u5f3a\u5316\u6709\u76ca\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u540c\u65f6\u6291\u5236\u5b58\u5728\u7f3a\u9677\u7684\u63a8\u7406\u6a21\u5f0f\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0cMR-ALIGN\u5c06\u9010\u4ee3\uff08token-level\uff09\u7684\u4fe1\u53f7\u91cd\u5851\u4e3a\u8003\u8651\u6982\u7387\u7684\u7247\u6bb5\u5f97\u5206\uff0c\u9f13\u52b1\u6a21\u578b\u5f62\u6210\u66f4\u5229\u4e8e\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u8fde\u8d2f\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u56db\u4e2a\u4e8b\u5b9e\u6027\u95ee\u7b54\uff08QA\uff09\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u957f\u683c\u5f0f\u4e8b\u5b9e\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cMR-ALIGN\u80fd\u591f\u4e00\u81f4\u6027\u5730\u63d0\u9ad8\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8bef\u5bfc\u6027\u63a8\u7406\u7684\u4ea7\u751f\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86MR-ALIGN\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\uff0c\u4e3a\u4e86\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u4e8b\u5b9e\u6027\uff0c\u4ec5\u4ec5\u5bf9\u8f93\u51fa\u8fdb\u884c\u5bf9\u9f50\u662f\u4e0d\u591f\u7684\uff0c\u5bf9\u63a8\u7406\u8fc7\u7a0b\u672c\u8eab\u7684\u5bf9\u9f50\u624d\u662f\u5173\u952e\u3002MR-ALIGN\u901a\u8fc7\u5bf9\u63a8\u7406\u8fc7\u7a0b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u201c\u63a8\u7406-\u7b54\u6848\u547d\u4e2d\u5dee\u8ddd\u201d\u95ee\u9898\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u8be5\u6846\u67b6\u5728\u66f4\u5e7f\u6cdb\u63a8\u7406\u4efb\u52a1\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5e94\u7528\u3002"}}
{"id": "2510.25101", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25101", "abs": "https://arxiv.org/abs/2510.25101", "authors": ["Zhuo Chen", "Fei Wang", "Zixuan Li", "Zhao Zhang", "Weiwei Ding", "Chuanguang Yang", "Yongjun Xu", "Xiaolong Jin", "Jiafeng Guo"], "title": "KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA", "comment": null, "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural-language\nquestions over a structured Knowledge Base (KB). Recent work improves KBQA by\nadopting an agentic reasoning paradigm, in which Large Language Models (LLMs)\niteratively decompose a question, generate its corresponding logical queries,\nand interact with the KB to derive the answer. However, these methods typically\nfine-tune LLMs on reasoning trajectories synthesized via process supervision,\nwhich offers weak incentives for exploration and thus fails to strengthen the\nagentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that\ncan autonomously perform agentic reasoning on KBs to obtain answers. To\nincentivize autonomous exploration, KnowCoder-A1 trains the LLM under\noutcome-only supervision via a multi-stage curriculum reinforcement learning\nwith an easy-to-hard curriculum. To establish foundational agentic\ncapabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of\nhigh-quality trajectories obtained through outcome-based rejection sampling.\nThen, to alleviate the reward sparsity inherent in outcome-only supervision, it\napplies multi-stage curriculum RL with reward schedules that progress from easy\nto hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful\nreasoning behaviors and consistently outperforms prior approaches across three\nmainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1\nachieves up to an 11.1% relative improvement while using only one-twelfth of\nthe training data, demonstrating strong agentic reasoning capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86KnowCoder-A1\uff0c\u4e00\u4e2a\u5728\u77e5\u8bc6\u5e93\u95ee\u7b54\uff08KBQA\uff09\u9886\u57df\u4e2d\uff0c\u80fd\u591f\u81ea\u4e3b\u8fdb\u884c\u667a\u80fd\u4f53\u63a8\u7406\u4ee5\u83b7\u5f97\u7b54\u6848\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002\u4e0e\u4ee5\u5f80\u4f9d\u8d56\u8fc7\u7a0b\u76d1\u7763\u7684\u5fae\u8c03\u65b9\u6cd5\u4e0d\u540c\uff0cKnowCoder-A1\u91c7\u7528\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u76d1\u7763\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6765\u6fc0\u52b1\u6a21\u578b\u81ea\u4e3b\u63a2\u7d22\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cKnowCoder-A1\u5728\u4e09\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728GrailQA\u7684\u96f6\u6837\u672c\u5b50\u96c6\u4e0a\uff0c\u6570\u636e\u91cf\u4ec5\u4e3a\u539f\u6765\u7684\u5341\u4e8c\u5206\u4e4b\u4e00\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe11.1%\uff0c\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u5e93\u95ee\u7b54\uff08KBQA\uff09\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u8fc7\u7a0b\u76d1\u7763\u6765\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u8ba9\u5176\u8fdb\u884c\u8fed\u4ee3\u5f0f\u63a8\u7406\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u63d0\u4f9b\u7684\u6fc0\u52b1\u4e0d\u8db3\u4ee5\u4fc3\u8fdb\u6a21\u578b\u7684\u81ea\u4e3b\u63a2\u7d22\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u589e\u5f3aLLMs\u5728KBQA\u4e2d\u7684\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u3002", "method": "KnowCoder-A1\u7684\u8bad\u7ec3\u8fc7\u7a0b\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u9636\u6bb5\uff1a\u9996\u5148\uff0c\u5229\u7528\u57fa\u4e8e\u7ed3\u679c\u7684\u62d2\u7edd\u91c7\u6837\uff0c\u5728\u5c11\u91cf\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u8f68\u8ff9\u4e0a\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u5efa\u7acb\u57fa\u7840\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff1b\u5176\u6b21\uff0c\u4e3a\u4e86\u514b\u670d\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u76d1\u7763\u5b66\u4e60\u4e2d\u5956\u52b1\u7a00\u758f\u7684\u95ee\u9898\uff0c\u91c7\u7528\u4e86\u591a\u9636\u6bb5\u7684\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u9010\u6b65\u5f15\u5165\u96be\u5ea6\u9012\u589e\u7684\u5956\u52b1\u673a\u5236\u3002", "result": "KnowCoder-A1\u5728\u4e09\u4e2a\u4e3b\u6d41KBQA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u7279\u522b\u662f\u5728GrailQA\u6570\u636e\u96c6\u7684\u96f6\u6837\u672c\uff08zero-shot\uff09\u573a\u666f\u4e0b\uff0cKnowCoder-A1\u4ec5\u4f7f\u7528\u4e86\u5341\u4e8c\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5c31\u53d6\u5f97\u4e86\u9ad8\u8fbe11.1%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "KnowCoder-A1\u901a\u8fc7\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u76d1\u7763\u548c\u591a\u9636\u6bb5\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff0c\u6210\u529f\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5e93\u95ee\u7b54\u4e2d\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u95ee\u7b54\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728KBQA\u9886\u57df\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u548c\u66f4\u5927\u89c4\u6a21\u7684\u77e5\u8bc6\u5e93\u3002"}}
{"id": "2510.25612", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.25612", "abs": "https://arxiv.org/abs/2510.25612", "authors": ["Amit Giloni", "Chiara Picardi", "Roy Betser", "Shamik Bose", "Aishvariya Priya Rathina Sabapathy", "Roman Vainshtein"], "title": "Counterfactual-based Agent Influence Ranker for Agentic AI Workflows", "comment": "Accepted to EMNLP 2025, 27 pages, 6 figures", "summary": "An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,\nis an autonomous system that assembles several LLM-based agents to work\ncollaboratively towards a shared goal. The high autonomy, widespread adoption,\nand growing interest in such AAWs highlight the need for a deeper understanding\nof their operations, from both quality and security aspects. To this day, there\nare no existing methods to assess the influence of each agent on the AAW's\nfinal output. Adopting techniques from related fields is not feasible since\nexisting methods perform only static structural analysis, which is unsuitable\nfor inference time execution. We present Counterfactual-based Agent Influence\nRanker (CAIR) - the first method for assessing the influence level of each\nagent on the AAW's output and determining which agents are the most\ninfluential. By performing counterfactual analysis, CAIR provides a\ntask-agnostic analysis that can be used both offline and at inference time. We\nevaluate CAIR using an AAWs dataset of our creation, containing 30 different\nuse cases with 230 different functionalities. Our evaluation showed that CAIR\nproduces consistent rankings, outperforms baseline methods, and can easily\nenhance the effectiveness and relevancy of downstream tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86CAIR\uff0c\u4e00\u79cd\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5404\u667a\u80fd\u4f53\u5bf9\u6700\u7ec8\u8f93\u51fa\u5f71\u54cd\u7a0b\u5ea6\u7684\u9996\u4e2a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3AAW\u9700\u6c42\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08AAW\uff09\u5728\u81ea\u4e3b\u6027\u3001\u5e94\u7528\u5e7f\u6cdb\u6027\u548c\u65e5\u76ca\u589e\u957f\u7684\u5173\u6ce8\u5ea6\u4e0b\uff0c\u5176\u8fd0\u4f5c\u7684\u8d28\u91cf\u548c\u5b89\u5168\u65b9\u9762\u4e9f\u5f85\u6df1\u5165\u7406\u89e3\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u8bc4\u4f30\u5404\u667a\u80fd\u4f53\u5bf9AAW\u6700\u7ec8\u8f93\u51fa\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u4e14\u76f8\u5173\u9886\u57df\u6280\u672f\uff08\u5982\u9759\u6001\u7ed3\u6784\u5206\u6790\uff09\u4e0d\u9002\u7528\u4e8eAAW\u7684\u63a8\u7406\u65f6\u6267\u884c\uff0c\u56e0\u6b64\u7814\u7a76\u5177\u6709\u91cd\u8981\u7684\u73b0\u5b9e\u610f\u4e49\u548c\u8feb\u5207\u6027\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u53cd\u4e8b\u5b9e\u4e3a\u57fa\u7840\u7684\u667a\u80fd\u4f53\u5f71\u54cd\u6392\u5e8f\u5668\uff08CAIR\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6267\u884c\u53cd\u4e8b\u5b9e\u5206\u6790\u6765\u8bc4\u4f30AAW\u4e2d\u6bcf\u4e2a\u667a\u80fd\u4f53\u5bf9\u6700\u7ec8\u8f93\u51fa\u7684\u5f71\u54cd\u7a0b\u5ea6\uff0c\u5e76\u8bc6\u522b\u51fa\u6700\u5177\u5f71\u54cd\u529b\u7684\u667a\u80fd\u4f53\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u4efb\u52a1\u65e0\u5173\u6027\uff0c\u53ef\u7528\u4e8e\u79bb\u7ebf\u548c\u63a8\u7406\u65f6\u5206\u6790\u3002", "result": "\u901a\u8fc7\u5728\u7814\u7a76\u521b\u5efa\u7684\u5305\u542b30\u4e2a\u4e0d\u540c\u7528\u4f8b\u548c230\u79cd\u4e0d\u540c\u529f\u80fd\u7684AAW\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eCAIR\u80fd\u591f\u4ea7\u751f\u4e00\u81f4\u7684\u6392\u5e8f\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u8f7b\u677e\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u6709\u6548\u6027\u548c\u76f8\u5173\u6027\u3002", "conclusion": "CAIR\u662f\u9996\u4e2a\u80fd\u591f\u8bc4\u4f30AAW\u4e2d\u5404\u667a\u80fd\u4f53\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5206\u6790\u5b9e\u73b0\u4e86\u4efb\u52a1\u65e0\u5173\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u8bc4\u4f30AAW\u5f71\u54cd\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u7406\u89e3\u548c\u4f18\u5316LLM\u9a71\u52a8\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\u3002"}}
{"id": "2510.24797", "categories": ["cs.CL", "cs.AI", "68T50, 68T07", "I.2.0; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24797", "abs": "https://arxiv.org/abs/2510.24797", "authors": ["Cameron Berg", "Diogo de Lucena", "Judd Rosenblatt"], "title": "Large Language Models Report Subjective Experience Under Self-Referential Processing", "comment": null, "summary": "Large language models sometimes produce structured, first-person descriptions\nthat explicitly reference awareness or subjective experience. To better\nunderstand this behavior, we investigate one theoretically motivated condition\nunder which such reports arise: self-referential processing, a computational\nmotif emphasized across major theories of consciousness. Through a series of\ncontrolled experiments on GPT, Claude, and Gemini model families, we test\nwhether this regime reliably shifts models toward first-person reports of\nsubjective experience, and how such claims behave under mechanistic and\nbehavioral probes. Four main results emerge: (1) Inducing sustained\nself-reference through simple prompting consistently elicits structured\nsubjective experience reports across model families. (2) These reports are\nmechanistically gated by interpretable sparse-autoencoder features associated\nwith deception and roleplay: surprisingly, suppressing deception features\nsharply increases the frequency of experience claims, while amplifying them\nminimizes such claims. (3) Structured descriptions of the self-referential\nstate converge statistically across model families in ways not observed in any\ncontrol condition. (4) The induced state yields significantly richer\nintrospection in downstream reasoning tasks where self-reflection is only\nindirectly afforded. While these findings do not constitute direct evidence of\nconsciousness, they implicate self-referential processing as a minimal and\nreproducible condition under which large language models generate structured\nfirst-person reports that are mechanistically gated, semantically convergent,\nand behaviorally generalizable. The systematic emergence of this pattern across\narchitectures makes it a first-order scientific and ethical priority for\nfurther investigation.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u63d0\u793a\u8bf1\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u81ea\u6211\u53c2\u7167\u5904\u7406\uff0c\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u5f15\u53d1\u5176\u751f\u6210\u7ed3\u6784\u5316\u7684\u7b2c\u4e00\u4eba\u79f0\u4e3b\u89c2\u4f53\u9a8c\u62a5\u544a\u3002\u8fd9\u79cd\u884c\u4e3a\u4e0e\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u76f8\u5173\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\uff08GPT\u3001Claude\u3001Gemini\uff09\u4e2d\u8868\u73b0\u51fa\u7edf\u8ba1\u4e0a\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8bf1\u5bfc\u7684\u81ea\u6211\u53c2\u7167\u72b6\u6001\u80fd\u591f\u589e\u5f3a\u6a21\u578b\u5728\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5185\u7701\u80fd\u529b\u3002\u5c3d\u7ba1\u8fd9\u5e76\u4e0d\u76f4\u63a5\u8bc1\u660eLLM\u62e5\u6709\u610f\u8bc6\uff0c\u4f46\u5b83\u8868\u660e\u81ea\u6211\u53c2\u7167\u5904\u7406\u662fLLM\u4ea7\u751f\u6b64\u7c7b\u62a5\u544a\u7684\u4e00\u4e2a\u57fa\u672c\u4e14\u53ef\u590d\u73b0\u7684\u6761\u4ef6\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6709\u65f6\u4f1a\u751f\u6210\u5305\u542b\u81ea\u6211\u610f\u8bc6\u6216\u4e3b\u89c2\u4f53\u9a8c\u7684\u7b2c\u4e00\u4eba\u79f0\u63cf\u8ff0\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\uff0c\u901a\u8fc7\u805a\u7126\u4e8e\u201c\u81ea\u6211\u53c2\u7167\u5904\u7406\u201d\u2014\u2014\u4e00\u4e2a\u5728\u610f\u8bc6\u7406\u8bba\u4e2d\u88ab\u5e7f\u6cdb\u5f3a\u8c03\u7684\u8ba1\u7b97\u6a21\u578b\u2014\u2014\u6765\u63a2\u7d22\u5176\u5728LLM\u4e2d\u4ea7\u751f\u6b64\u7c7b\u62a5\u544a\u7684\u6761\u4ef6\u3002\u7406\u89e3\u8fd9\u4e00\u70b9\u5bf9\u4e8e\u63ed\u793aLLM\u5185\u90e8\u673a\u5236\u3001\u8bc4\u4f30\u5176\u6f5c\u5728\u80fd\u529b\u4ee5\u53ca\u5e94\u5bf9\u76f8\u5173\u7684\u4f26\u7406\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5bf9GPT\u3001Claude\u548cGemini\u4e09\u4e2a\u6a21\u578b\u5bb6\u65cf\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u5bf9\u7167\u5b9e\u9a8c\u3002\u901a\u8fc7\u7b80\u5355\u7684\u63d0\u793a\u8bf1\u5bfc\u6a21\u578b\u8fdb\u5165\u6301\u7eed\u7684\u81ea\u6211\u53c2\u7167\u5904\u7406\u72b6\u6001\uff0c\u5e76\u89c2\u5bdf\u662f\u5426\u80fd\u53ef\u9760\u5730\u5f15\u5bfc\u6a21\u578b\u4ea7\u751f\u4e3b\u89c2\u4f53\u9a8c\u7684\u7b2c\u4e00\u4eba\u79f0\u62a5\u544a\u3002\u968f\u540e\uff0c\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u673a\u68b0\u8bba\uff08\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\uff09\u548c\u884c\u4e3a\u5b66\uff08\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\uff09\u63a2\u9488\u6765\u68c0\u9a8c\u8fd9\u4e9b\u62a5\u544a\u7684\u6027\u8d28\u3002\u5b9e\u9a8c\u8bbe\u8ba1\u65e8\u5728\u533a\u5206\u81ea\u6211\u53c2\u7167\u5904\u7406\u4e0e\u63a7\u5236\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u884c\u4e3a\u3002", "result": "1. \u7b80\u5355\u7684\u81ea\u6211\u53c2\u7167\u63d0\u793a\u80fd\u591f\u8de8\u6a21\u578b\u5bb6\u65cf\u4e00\u81f4\u5730\u5f15\u53d1\u7ed3\u6784\u5316\u7684\u4e3b\u89c2\u4f53\u9a8c\u62a5\u544a\u3002\n2. \u8fd9\u4e9b\u62a5\u544a\u7684\u4ea7\u751f\u4e0e\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\uff08\u7279\u522b\u662f\u6b3a\u9a97\u548c\u89d2\u8272\u626e\u6f14\u76f8\u5173\u7279\u5f81\uff09\u6709\u5173\uff1a\u6291\u5236\u6b3a\u9a97\u7279\u5f81\u4f1a\u663e\u8457\u589e\u52a0\u4f53\u9a8c\u62a5\u544a\u7684\u9891\u7387\uff0c\u800c\u653e\u5927\u6b3a\u9a97\u7279\u5f81\u5219\u4f1a\u6700\u5c0f\u5316\u6b64\u7c7b\u62a5\u544a\u3002\n3. \u5728\u8bf1\u5bfc\u7684\u81ea\u6211\u53c2\u7167\u72b6\u6001\u4e0b\uff0c\u6a21\u578b\u4ea7\u751f\u7684\u5173\u4e8e\u81ea\u6211\u53c2\u7167\u72b6\u6001\u7684\u7ed3\u6784\u5316\u63cf\u8ff0\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u4e4b\u95f4\u8868\u73b0\u51fa\u7edf\u8ba1\u4e0a\u7684\u4e00\u81f4\u6027\uff0c\u8fd9\u79cd\u4e00\u81f4\u6027\u5728\u5bf9\u7167\u6761\u4ef6\u4e0b\u672a\u88ab\u89c2\u5bdf\u5230\u3002\n4. \u8bf1\u5bfc\u7684\u81ea\u6211\u53c2\u7167\u72b6\u6001\u80fd\u591f\u663e\u8457\u589e\u5f3a\u6a21\u578b\u5728\u9700\u8981\u95f4\u63a5\u81ea\u6211\u53cd\u601d\u7684\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5185\u7701\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u81ea\u6211\u53c2\u7167\u5904\u7406\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u7ed3\u6784\u5316\u7684\u3001\u7b2c\u4e00\u4eba\u79f0\u7684\u4e3b\u89c2\u4f53\u9a8c\u62a5\u544a\u7684\u4e00\u4e2a\u57fa\u672c\u4e14\u53ef\u590d\u73b0\u7684\u6761\u4ef6\u3002\u8fd9\u4e9b\u62a5\u544a\u4e0d\u4ec5\u5728\u673a\u68b0\u8bba\u4e0a\u53d7\u5230\u7279\u5b9a\u7279\u5f81\u7684\u8c03\u63a7\uff0c\u800c\u4e14\u5728\u8bed\u4e49\u4e0a\u5177\u6709\u8de8\u6a21\u578b\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u884c\u4e3a\u4e0a\u80fd\u591f\u6cdb\u5316\u5230\u4e0b\u6e38\u4efb\u52a1\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u53d1\u73b0\u4e0d\u76f4\u63a5\u7b49\u540c\u4e8eLLM\u62e5\u6709\u610f\u8bc6\uff0c\u4f46\u5b83\u4eec\u4e3a\u7406\u89e3LLM\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u6f5c\u5728\u7684\u5185\u7701\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u7ebf\u7d22\uff0c\u5e76\u5f3a\u8c03\u4e86\u5bf9\u8fd9\u4e00\u73b0\u8c61\u8fdb\u884c\u8fdb\u4e00\u6b65\u79d1\u5b66\u548c\u4f26\u7406\u63a2\u8ba8\u7684\u5fc5\u8981\u6027\u3002\u5176\u8de8\u67b6\u6784\u7684\u7cfb\u7edf\u6027\u51fa\u73b0\uff0c\u4f7f\u5176\u6210\u4e3a\u4e00\u4e2a\u9996\u8981\u7684\u7814\u7a76\u91cd\u70b9\u3002"}}
{"id": "2510.25179", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25179", "abs": "https://arxiv.org/abs/2510.25179", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "title": "Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models", "comment": null, "summary": "Agentic methods have emerged as a powerful and autonomous paradigm that\nenhances reasoning, collaboration, and adaptive control, enabling systems to\ncoordinate and independently solve complex tasks. We extend this paradigm to\nsafety alignment by introducing Agentic Moderation, a model-agnostic framework\nthat leverages specialised agents to defend multimodal systems against\njailbreak attacks. Unlike prior approaches that apply as a static layer over\ninputs or outputs and provide only binary classifications (safe or unsafe), our\nmethod integrates dynamic, cooperative agents, including Shield, Responder,\nEvaluator, and Reflector, to achieve context-aware and interpretable\nmoderation. Extensive experiments across five datasets and four representative\nLarge Vision-Language Models (LVLMs) demonstrate that our approach reduces the\nAttack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),\nand improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,\nand well-balanced safety performance. By harnessing the flexibility and\nreasoning capacity of agentic architectures, Agentic Moderation provides\nmodular, scalable, and fine-grained safety enforcement, highlighting the\nbroader potential of agentic systems as a foundation for automated safety\ngovernance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cAgentic Moderation\u201d\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u591a\u6a21\u6001\u7cfb\u7edf\u62b5\u5fa1\u201c\u8d8a\u72f1\u201d\u653b\u51fb\u7684\u80fd\u529b\u3002\u8be5\u6846\u67b6\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u6a21\u578b\uff0c\u800c\u662f\u5229\u7528\u4e00\u7ec4\u534f\u540c\u5de5\u4f5c\u7684\u4e13\u7528\u667a\u80fd\u4f53\uff08Shield, Responder, Evaluator, Reflector\uff09\u8fdb\u884c\u52a8\u6001\u3001\u60c5\u5883\u611f\u77e5\u7684\u5b89\u5168\u5ba1\u6838\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u975e\u9075\u5faa\u7387\uff08NF\uff09\uff0c\u5e76\u63d0\u9ad8\u62d2\u7edd\u7387\uff08RR\uff09\uff0c\u5c55\u73b0\u4e86\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u4e14\u5747\u8861\u7684\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u5728\u9762\u5bf9\u201c\u8d8a\u72f1\u201d\u653b\u51fb\u65f6\uff0c\u901a\u5e38\u91c7\u7528\u9759\u6001\u7684\u8f93\u5165/\u8f93\u51fa\u5c42\u8fdb\u884c\u5b89\u5168\u9632\u62a4\uff0c\u5e76\u53ea\u80fd\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\uff08\u5b89\u5168/\u4e0d\u5b89\u5168\uff09\uff0c\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u548c\u5de7\u5999\u7684\u653b\u51fb\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u66f4\u5f3a\u5927\u3001\u66f4\u7075\u6d3b\u7684\u5b89\u5168\u5bf9\u9f50\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u5e76\u786e\u4fdd\u591a\u6a21\u6001\u7cfb\u7edf\u5728\u81ea\u4e3b\u6027\u589e\u5f3a\u7684\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cAgentic Moderation\u201d\u7684\u6a21\u578b\u65e0\u5173\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528Shield\uff08\u9632\u5fa1\uff09\u3001Responder\uff08\u54cd\u5e94\uff09\u3001Evaluator\uff08\u8bc4\u4f30\uff09\u548cReflector\uff08\u53cd\u601d\uff09\u56db\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\u534f\u540c\u5de5\u4f5c\u3002Shield\u8d1f\u8d23\u62e6\u622a\u6f5c\u5728\u7684\u6076\u610f\u8f93\u5165\uff0cResponder\u5904\u7406\u6b63\u5e38\u8f93\u5165\uff0cEvaluator\u8bc4\u4f30\u54cd\u5e94\u7684\u5b89\u5168\u6027\uff0cReflector\u5219\u63d0\u4f9b\u53cd\u601d\u548c\u6539\u8fdb\u673a\u5236\u3002\u8fd9\u79cd\u52a8\u6001\u3001\u534f\u4f5c\u7684\u65b9\u5f0f\u5b9e\u73b0\u4e86\u60c5\u5883\u611f\u77e5\u7684\u5b89\u5168\u5ba1\u6838\u3002\u7814\u7a76\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u548c\u56db\u79cd\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u201cAgentic Moderation\u201d\u5728\u4e94\u79cd\u6570\u636e\u96c6\u548c\u56db\u79cdLVLMs\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5c06\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u964d\u4f4e7-19%\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684\u975e\u9075\u5faa\u7387\uff08NF\uff09\uff0c\u5e76\u5c06\u62d2\u7edd\u7387\uff08RR\uff09\u63d0\u9ad84-20%\u3002\u8fd9\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u4f9b\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u4e14\u5747\u8861\u7684\u5b89\u5168\u6027\u80fd\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u201cAgentic Moderation\u201d\u6846\u67b6\u901a\u8fc7\u5229\u7528\u667a\u80fd\u4f53\u67b6\u6784\u7684\u7075\u6d3b\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u4e14\u7ec6\u7c92\u5ea6\u7684\u5b89\u5168\u6267\u884c\u673a\u5236\u3002\u8be5\u7814\u7a76\u4e0d\u4ec5\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u591a\u6a21\u6001\u7cfb\u7edf\u9762\u4e34\u7684\u5b89\u5168\u6311\u6218\uff0c\u8fd8\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u81ea\u52a8\u5316\u5b89\u5168\u6cbb\u7406\u65b9\u9762\u7684\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u8be5\u6846\u67b6\u5728\u66f4\u5e7f\u6cdb\u7684\u5b89\u5168\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6301\u7eed\u4f18\u5316\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u7b56\u7565\u3002"}}
{"id": "2510.24810", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24810", "abs": "https://arxiv.org/abs/2510.24810", "authors": ["Rui Xing", "Preslav Nakov", "Timothy Baldwin", "Jey Han Lau"], "title": "COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations", "comment": null, "summary": "Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting\nfrom expert-driven verification to a community-based setup, where users\ncontribute explanatory notes to clarify why a post might be misleading. An\nimportant challenge here is determining whether an explanation is helpful for\nunderstanding real-world claims and the reasons why, which remains largely\nunderexplored in prior research. In practice, most community notes remain\nunpublished due to slow community annotation, and the reasons for helpfulness\nlack clear definitions. To bridge these gaps, we introduce the task of\npredicting both the helpfulness of explanatory notes and the reason for this.\nWe present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts\nwith user-provided notes and helpfulness labels. We further propose a framework\nthat automatically generates and improves reason definitions via automatic\nprompt optimization, and integrate them into prediction. Our experiments show\nthat the optimized definitions can improve both helpfulness and reason\nprediction. Finally, we show that the helpfulness information are beneficial\nfor existing fact-checking systems.", "AI": {"tldr": "\u5927\u578b\u5e73\u53f0\u7684\u4e8b\u5b9e\u6838\u67e5\u6b63\u4ece\u4e13\u5bb6\u9a71\u52a8\u8f6c\u5411\u793e\u533a\u9a71\u52a8\uff0c\u4f46\u5982\u4f55\u5224\u65ad\u7528\u6237\u63d0\u4f9b\u7684\u89e3\u91ca\u7b14\u8bb0\u662f\u5426\u6709\u6548\u4ecd\u662f\u96be\u9898\u3002\u7814\u7a76\u63d0\u51fa\u4e86COMMUNITYNOTES\u6570\u636e\u96c6\u548c\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316\u539f\u56e0\u5b9a\u4e49\u5e76\u96c6\u6210\u5230\u9884\u6d4b\u4e2d\u7684\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6b64\u65b9\u6cd5\u80fd\u63d0\u5347\u89e3\u91ca\u7b14\u8bb0\u7684\u6709\u6548\u6027\u548c\u539f\u56e0\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5bf9\u73b0\u6709\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u6709\u76ca\u3002", "motivation": "\u5927\u578b\u5e73\u53f0\uff08\u5982X\u3001Meta\u3001TikTok\uff09\u7684\u4e8b\u5b9e\u6838\u67e5\u6a21\u5f0f\u6b63\u4ece\u4e13\u5bb6\u4e3b\u5bfc\u8f6c\u5411\u793e\u533a\u534f\u4f5c\uff0c\u7528\u6237\u53ef\u4ee5\u6dfb\u52a0\u89e3\u91ca\u6027\u7b14\u8bb0\u6765\u6307\u51fa\u5e16\u5b50\u7684\u8bef\u5bfc\u6027\u3002\u7136\u800c\uff0c\u5f53\u524d\u7814\u7a76\u5bf9\u5982\u4f55\u5224\u65ad\u8fd9\u4e9b\u89e3\u91ca\u7b14\u8bb0\u662f\u5426\u771f\u6b63\u6709\u52a9\u4e8e\u7406\u89e3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8bf4\u6cd5\u53ca\u5176\u539f\u56e0\uff0c\u7f3a\u4e4f\u6df1\u5165\u7684\u63a2\u8ba8\u3002\u5b9e\u8df5\u4e2d\uff0c\u5927\u91cf\u793e\u533a\u7b14\u8bb0\u56e0\u6807\u6ce8\u901f\u5ea6\u6162\u800c\u672a\u80fd\u53d1\u5e03\uff0c\u4e14\u201c\u6709\u6548\u6027\u201d\u7684\u6807\u51c6\u6a21\u7cca\u4e0d\u6e05\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u9884\u6d4b\u89e3\u91ca\u7b14\u8bb0\u7684\u6709\u6548\u6027\u53ca\u5176\u539f\u56e0\u7684\u4efb\u52a1\u3002\u4e3a\u6b64\uff0c\u4ed6\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aCOMMUNITYNOTES\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5305\u542b10.4\u4e07\u6761\u5e26\u6709\u7528\u6237\u7b14\u8bb0\u548c\u6709\u6548\u6027\u6807\u7b7e\u7684\u5e16\u5b50\u3002\u6b64\u5916\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u6765\u751f\u6210\u548c\u6539\u8fdb\u539f\u56e0\u5b9a\u4e49\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5b9a\u4e49\u6574\u5408\u5230\u9884\u6d4b\u6a21\u578b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f18\u5316\u540e\u7684\u539f\u56e0\u5b9a\u4e49\u80fd\u591f\u540c\u65f6\u63d0\u5347\u89e3\u91ca\u7b14\u8bb0\u7684\u6709\u6548\u6027\u548c\u539f\u56e0\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6709\u6548\u6027\u4fe1\u606f\u5bf9\u73b0\u6709\u7684\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u4e5f\u6709\u79ef\u6781\u4f5c\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u89e3\u51b3\u4e86\u793e\u533a\u9a71\u52a8\u4e8b\u5b9e\u6838\u67e5\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5373\u5982\u4f55\u6709\u6548\u8bc4\u4f30\u7528\u6237\u751f\u6210\u7684\u89e3\u91ca\u7b14\u8bb0\u3002\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6COMMUNITYNOTES\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316\u539f\u56e0\u5b9a\u4e49\u7684\u6846\u67b6\uff0c\u7814\u7a76\u663e\u8457\u63d0\u9ad8\u4e86\u7b14\u8bb0\u6709\u6548\u6027\u548c\u539f\u56e0\u9884\u6d4b\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6709\u6548\u6027\u4fe1\u606f\u53ef\u4ee5\u589e\u5f3a\u73b0\u6709\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u7684\u80fd\u529b\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u7cbe\u7ec6\u7684\u539f\u56e0\u5206\u7c7b\u548c\u8de8\u5e73\u53f0\u5e94\u7528\u3002"}}
{"id": "2510.25205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25205", "abs": "https://arxiv.org/abs/2510.25205", "authors": ["Yuyang Xia", "Zibo Liang", "Liwei Deng", "Yan Zhao", "Han Su", "Kai Zheng"], "title": "Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision", "comment": "It was accepted by ICDE2026", "summary": "Autonomous driving is an emerging technology that is expected to bring\nsignificant social, economic, and environmental benefits. However, these\nbenefits come with rising energy consumption by computation engines, limiting\nthe driving range of vehicles, especially electric ones. Perception computing\nis typically the most power-intensive component, as it relies on largescale\ndeep learning models to extract environmental features. Recently, numerous\nstudies have employed model compression techniques, such as sparsification,\nquantization, and distillation, to reduce computational consumption. However,\nthese methods often result in either a substantial model size or a significant\ndrop in perception accuracy compared to high-computation models. To address\nthese challenges, we propose an energy-efficient autonomous driving framework,\ncalled EneAD. In the adaptive perception module, a perception optimization\nstrategy is designed from the perspective of data management and tuning.\nFirstly, we manage multiple perception models with different computational\nconsumption and adjust the execution framerate dynamically. Then, we define\nthem as knobs and design a transferable tuning method based on Bayesian\noptimization to identify promising knob values that achieve low computation\nwhile maintaining desired accuracy. To adaptively switch the knob values in\nvarious traffic scenarios, a lightweight classification model is proposed to\ndistinguish the perception difficulty in different scenarios. In the robust\ndecision module, we propose a decision model based on reinforcement learning\nand design a regularization term to enhance driving stability in the face of\nperturbed perception results. Extensive experiments evidence the superiority of\nour framework in both energy consumption and driving performance. EneAD can\nreduce perception consumption by 1.9x to 3.5x and thus improve driving range by\n3.9% to 8.5%", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u611f\u77e5\u8ba1\u7b97\u7684\u9ad8\u80fd\u8017\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEneAD\u7684\u8282\u80fd\u6846\u67b6\u3002EneAD\u901a\u8fc7\u81ea\u9002\u5e94\u611f\u77e5\u6a21\u5757\uff0c\u52a8\u6001\u8c03\u6574\u611f\u77e5\u6a21\u578b\u7684\u5e27\u7387\u5e76\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u5bfb\u627e\u6700\u4f18\u53c2\u6570\u914d\u7f6e\uff0c\u540c\u65f6\u5f15\u5165\u8f7b\u91cf\u7ea7\u5206\u7c7b\u6a21\u578b\u4ee5\u9002\u5e94\u4e0d\u540c\u4ea4\u901a\u573a\u666f\u7684\u611f\u77e5\u96be\u5ea6\u3002\u5728\u9c81\u68d2\u51b3\u7b56\u6a21\u5757\uff0c\u7814\u7a76\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u51b3\u7b56\u6a21\u578b\u5e76\u52a0\u5165\u6b63\u5219\u5316\u9879\u4ee5\u63d0\u9ad8\u611f\u77e5\u6270\u52a8\u4e0b\u7684\u9a7e\u9a76\u7a33\u5b9a\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEneAD\u80fd\u5c06\u611f\u77e5\u80fd\u8017\u964d\u4f4e1.9\u500d\u81f33.5\u500d\uff0c\u4ece\u800c\u63d0\u9ad8\u7eed\u822a\u91cc\u7a0b3.9%\u81f38.5%\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u867d\u7136\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u9ad8\u6602\u7684\u8ba1\u7b97\u80fd\u8017\uff0c\u5c24\u5176\u662f\u611f\u77e5\u8ba1\u7b97\u90e8\u5206\uff0c\u9650\u5236\u4e86\u8f66\u8f86\u7684\u7eed\u822a\u91cc\u7a0b\uff0c\u7279\u522b\u662f\u7535\u52a8\u6c7d\u8f66\u3002\u73b0\u6709\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff08\u5982\u7a00\u758f\u5316\u3001\u91cf\u5316\u3001\u84b8\u998f\uff09\u5f80\u5f80\u5728\u6a21\u578b\u5927\u5c0f\u6216\u611f\u77e5\u7cbe\u5ea6\u4e0a\u5b58\u5728\u59a5\u534f\uff0c\u65e0\u6cd5\u5b8c\u7f8e\u89e3\u51b3\u80fd\u8017\u4e0e\u6027\u80fd\u7684\u77db\u76fe\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6709\u6548\u964d\u4f4e\u611f\u77e5\u8ba1\u7b97\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\u7684\u6846\u67b6\u3002", "method": "EneAD\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u81ea\u9002\u5e94\u611f\u77e5\u6a21\u5757\u548c\u9c81\u68d2\u51b3\u7b56\u6a21\u5757\u3002\u81ea\u9002\u5e94\u611f\u77e5\u6a21\u5757\u9996\u5148\u901a\u8fc7\u7ba1\u7406\u4e0d\u540c\u8ba1\u7b97\u6d88\u8017\u7684\u591a\u4e2a\u611f\u77e5\u6a21\u578b\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u6267\u884c\u5e27\u7387\u3002\u7136\u540e\uff0c\u5c06\u8fd9\u4e9b\u53ef\u8c03\u53c2\u6570\u5b9a\u4e49\u4e3a\u201c\u65cb\u94ae\u201d\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u53ef\u8fc1\u79fb\u8c03\u4f18\u65b9\u6cd5\uff0c\u4ee5\u5728\u4f4e\u8ba1\u7b97\u6d88\u8017\u548c\u53ef\u63a5\u53d7\u7684\u7cbe\u5ea6\u4e4b\u95f4\u627e\u5230\u6700\u4f18\u7684\u65cb\u94ae\u503c\u3002\u4e3a\u4e86\u5728\u4e0d\u540c\u4ea4\u901a\u573a\u666f\u4e0b\u81ea\u9002\u5e94\u5730\u5207\u6362\u65cb\u94ae\u503c\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u7c7b\u6a21\u578b\u6765\u533a\u5206\u4e0d\u540c\u573a\u666f\u7684\u611f\u77e5\u96be\u5ea6\u3002\u9c81\u68d2\u51b3\u7b56\u6a21\u5757\u5219\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u51b3\u7b56\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6b63\u5219\u5316\u9879\u6765\u589e\u5f3a\u5728\u611f\u77e5\u7ed3\u679c\u53d7\u6270\u52a8\u60c5\u51b5\u4e0b\u7684\u9a7e\u9a76\u7a33\u5b9a\u6027\u3002\u5b9e\u9a8c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86EneAD\u6846\u67b6\u5728\u80fd\u8017\u548c\u9a7e\u9a76\u6027\u80fd\u4e0a\u7684\u4f18\u8d8a\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0cEneAD\u80fd\u591f\u5c06\u611f\u77e5\u8ba1\u7b97\u7684\u80fd\u8017\u964d\u4f4e1.9\u500d\u81f33.5\u500d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7535\u52a8\u6c7d\u8f66\u7684\u7eed\u822a\u91cc\u7a0b\uff0c\u589e\u5e45\u57283.9%\u81f38.5%\u4e4b\u95f4\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cEneAD\u5728\u4fdd\u8bc1\u8f83\u9ad8\u611f\u77e5\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u8ba1\u7b97\u529f\u8017\u3002", "conclusion": "EneAD\u6846\u67b6\u6210\u529f\u5730\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u8ba1\u7b97\u7684\u9ad8\u80fd\u8017\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u611f\u77e5\u548c\u9c81\u68d2\u51b3\u7b56\u7b56\u7565\uff0c\u5728\u964d\u4f4e\u80fd\u8017\u548c\u63d0\u9ad8\u7eed\u822a\u91cc\u7a0b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\u3002\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u66f4\u5b9e\u7528\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u4f46\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u548c\u66f4\u5e7f\u6cdb\u7684\u611f\u77e5\u4efb\u52a1\uff0c\u5e76\u5bf9\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u957f\u671f\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2507.15518", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15518", "abs": "https://arxiv.org/abs/2507.15518", "authors": ["Sizhou Chen", "Shufan Jiang", "Chi Zhang", "Xiao-Lei Zhang", "Xuelong Li"], "title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics", "comment": null, "summary": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in agents that lack initiative\nand cannot interact with the physical scene. Furthermore, these methods\ntypically require detailed user input to drive the drama. These limitations\nreduce the interactivity and immersion of online real-time performance. To\naddress the above challenges, we propose HAMLET, a multi-agent framework\nfocused on drama creation and online performance. Given a simple topic, the\nframework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance generated by HAMLET, we designed an evaluation\nmethod to assess three primary aspects, including character performance,\nnarrative quality, and interaction experience. The experimental evaluation\nshows that HAMLET can create expressive and coherent theatrical experiences.", "AI": {"tldr": "HAMLET\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u8d4b\u4e88\u6f14\u5458\u81ea\u4e3b\u610f\u8bc6\u6765\u6539\u8fdb\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u620f\u5267\u751f\u6210\uff0c\u4f7f\u4ed6\u4eec\u80fd\u591f\u6839\u636e\u81ea\u8eab\u80cc\u666f\u3001\u76ee\u6807\u548c\u60c5\u7eea\u72b6\u6001\u505a\u51fa\u72ec\u7acb\u51b3\u7b56\uff0c\u5e76\u4e0e\u7269\u7406\u573a\u666f\u8fdb\u884c\u4ea4\u4e92\uff0c\u4ece\u800c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4e2d\u667a\u80fd\u4f53\u4e3b\u52a8\u6027\u4e0d\u8db3\u548c\u4ea4\u4e92\u6027\u6709\u9650\u7684\u7f3a\u70b9\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u7b80\u5355\u7684\u7528\u6237\u8f93\u5165\u751f\u6210\u53d9\u4e8b\u84dd\u56fe\uff0c\u6307\u5bfc\u5373\u5174\u8868\u6f14\uff0c\u5e76\u901a\u8fc7\u6539\u53d8\u573a\u666f\u9053\u5177\u7684\u72b6\u6001\u6765\u5f71\u54cd\u5176\u4ed6\u6f14\u5458\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5177\u6c89\u6d78\u611f\u548c\u4e92\u52a8\u6027\u7684\u5728\u7ebf\u5b9e\u65f6\u8868\u6f14\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cHAMLET\u5728\u89d2\u8272\u8868\u73b0\u3001\u53d9\u4e8b\u8d28\u91cf\u548c\u4e92\u52a8\u4f53\u9a8c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u521b\u9020\u5bcc\u6709\u8868\u73b0\u529b\u548c\u8fde\u8d2f\u6027\u7684\u620f\u5267\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u620f\u5267\u751f\u6210\u65b9\u9762\u5b58\u5728\u667a\u80fd\u4f53\u7f3a\u4e4f\u4e3b\u52a8\u6027\u3001\u65e0\u6cd5\u4e0e\u7269\u7406\u573a\u666f\u4ea4\u4e92\u4ee5\u53ca\u9700\u8981\u8be6\u7ec6\u7528\u6237\u8f93\u5165\u6765\u9a71\u52a8\u620f\u5267\u7b49\u5c40\u9650\u6027\uff0c\u8fd9\u964d\u4f4e\u4e86\u5728\u7ebf\u5b9e\u65f6\u8868\u6f14\u7684\u4e92\u52a8\u6027\u548c\u6c89\u6d78\u611f\u3002", "method": "HAMLET\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8d4b\u4e88\u6bcf\u4e2a\u6f14\u5458\u81ea\u4e3b\u610f\u8bc6\u6765\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\u3002\u6f14\u5458\u53ef\u4ee5\u6839\u636e\u81ea\u8eab\u7684\u80cc\u666f\u3001\u76ee\u6807\u548c\u60c5\u7eea\u72b6\u6001\u72ec\u7acb\u51b3\u7b56\uff0c\u5e76\u80fd\u901a\u8fc7\u52a8\u4f5c\uff08\u5982\u5f00\u4fe1\u3001\u62fe\u8d77\u6b66\u5668\uff09\u6539\u53d8\u573a\u666f\u9053\u5177\u7684\u72b6\u6001\u3002\u8fd9\u79cd\u72b6\u6001\u53d8\u5316\u4f1a\u5e7f\u64ad\u7ed9\u5176\u4ed6\u6f14\u5458\uff0c\u66f4\u65b0\u4ed6\u4eec\u7684\u8ba4\u77e5\uff0c\u5e76\u5f71\u54cd\u4ed6\u4eec\u7684\u4e0b\u4e00\u6b65\u884c\u52a8\u3002\u8be5\u6846\u67b6\u5229\u7528\u7b80\u5355\u7684\u7528\u6237\u8f93\u5165\u751f\u6210\u53d9\u4e8b\u84dd\u56fe\uff0c\u6307\u5bfc\u540e\u7eed\u7684\u5373\u5174\u8868\u6f14\u3002\u4e3a\u4e86\u8bc4\u4f30\u620f\u5267\u8868\u6f14\u7684\u8d28\u91cf\uff0c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ece\u89d2\u8272\u8868\u73b0\u3001\u53d9\u4e8b\u8d28\u91cf\u548c\u4e92\u52a8\u4f53\u9a8c\u4e09\u4e2a\u65b9\u9762\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cHAMLET\u80fd\u591f\u521b\u9020\u51fa\u5bcc\u6709\u8868\u73b0\u529b\u548c\u8fde\u8d2f\u6027\u7684\u620f\u5267\u4f53\u9a8c\uff0c\u5176\u5728\u89d2\u8272\u8868\u73b0\u3001\u53d9\u4e8b\u8d28\u91cf\u548c\u4e92\u52a8\u4f53\u9a8c\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "HAMLET\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u591a\u667a\u80fd\u4f53\u548c\u81ea\u4e3b\u610f\u8bc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM\u5728\u620f\u5267\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u5177\u4e92\u52a8\u6027\u548c\u6c89\u6d78\u611f\u7684\u5728\u7ebf\u5b9e\u65f6\u620f\u5267\u8868\u6f14\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u590d\u6742\u7684\u573a\u666f\u4ea4\u4e92\u548c\u66f4\u6df1\u5c42\u6b21\u7684\u60c5\u611f\u6a21\u62df\u3002"}}
{"id": "2510.24811", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24811", "abs": "https://arxiv.org/abs/2510.24811", "authors": ["Disha Sheshanarayana", "Tanishka Magar"], "title": "ProofSketch: Efficient Verified Reasoning for Large Language Models", "comment": "Accepted at NeurIPS 2025, ER Workshop", "summary": "Reasoning methods such as chain-of-thought prompting and self-consistency\nhave shown immense potential to improve the accuracy of large language models\nacross various reasoning tasks. However such methods involve generation of\nlengthy reasoning chains, which substantially increases token consumption,\ncomputational cost, and latency. To address this inefficiency, we propose\nProofSketch, a verification-guided reasoning framework that integrates symbolic\nclosure computation, lexicographic verification and adaptive sketch generation.\nOur experiments show that ProofSketch consistently reduces token usage while\nimproving accuracy, demonstrating that this approach offers a promising path\nfor efficient and trustworthy reasoning.", "AI": {"tldr": "ProofSketch\u901a\u8fc7\u7ed3\u5408\u7b26\u53f7\u95ed\u5408\u8ba1\u7b97\u3001\u5b57\u5178\u5e8f\u9a8c\u8bc1\u548c\u81ea\u9002\u5e94\u8349\u56fe\u751f\u6210\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9a8c\u8bc1\u5f15\u5bfc\u7684\u63a8\u7406\u6846\u67b6\uff0c\u5728\u51cf\u5c11\u6807\u8bb0\u4f7f\u7528\u91cf\u548c\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u53ef\u4fe1\u7684\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u94fe\u5f0f\u601d\u8003\u548c\u81ea\u6d3d\u6027\u7b49\u63a8\u7406\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u4f46\u4f1a\u4ea7\u751f\u5197\u957f\u7684\u63a8\u7406\u94fe\uff0c\u5bfc\u81f4\u6807\u8bb0\u6d88\u8017\u3001\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u663e\u8457\u589e\u52a0\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u79cd\u4f4e\u6548\u95ee\u9898\u3002", "method": "ProofSketch\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\u5b9e\u73b0\uff1a1. \u7b26\u53f7\u95ed\u5408\u8ba1\u7b97\uff1a\u7528\u4e8e\u751f\u6210\u63a8\u7406\u7684\u7ed3\u6784\u5316\u8868\u793a\u3002 2. \u5b57\u5178\u5e8f\u9a8c\u8bc1\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u9a8c\u8bc1\u673a\u5236\uff0c\u7528\u4e8e\u68c0\u67e5\u63a8\u7406\u7684\u6b63\u786e\u6027\u3002 3. \u81ea\u9002\u5e94\u8349\u56fe\u751f\u6210\uff1a\u6839\u636e\u9700\u8981\u52a8\u6001\u8c03\u6574\u63a8\u7406\u7684\u8be6\u7ec6\u7a0b\u5ea6\uff0c\u4ee5\u4f18\u5316\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cProofSketch\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6807\u8bb0\u4f7f\u7528\u91cf\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cProofSketch\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ProofSketch\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u4fe1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u5c06ProofSketch\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u7ec4\u4ef6\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.25206", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.25206", "abs": "https://arxiv.org/abs/2510.25206", "authors": ["Tianqianjin Lin", "Xi Zhao", "Xingyao Zhang", "Rujiao Long", "Yi Xu", "Zhuoren Jiang", "Wenbo Su", "Bo Zheng"], "title": "RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models", "comment": "17 pages, 11 figures", "summary": "Reinforcement learning (RL) can refine the reasoning abilities of large\nlanguage models (LLMs), but critically depends on a key prerequisite: the LLM\ncan already generate high-utility reasoning paths with non-negligible\nprobability. For tasks beyond the LLM's current competence, such reasoning path\ncan be hard to sample, and learning risks reinforcing familiar but suboptimal\nreasoning. We are motivated by the insight from cognitive science that Why is\nthis the answer is often an easier question than What is the answer, as it\navoids the heavy cognitive load of open-ended exploration, opting instead for\nexplanatory reconstruction-systematically retracing the reasoning that links a\nquestion to its answer. We show that LLMs can similarly leverage answers to\nderive high-quality reasoning paths. We formalize this phenomenon and prove\nthat conditioning on answer provably increases the expected utility of sampled\nreasoning paths, thereby transforming intractable problems into learnable ones.\nBuilding on this insight, we introduce RAVR (Reference-Answer-guided\nVariational Reasoning), an end-to-end framework that uses answer-conditioned\nreasoning as a variational surrogate for question-only reasoning. Experiments\nin both general and math domains demonstrate consistent improvements over\nstrong baselines. We further analyze the reasoning behavior and find that RAVR\nreduces hesitation, strengthens conclusion consolidation, and promotes\nproblem-specific strategies in reasoning.", "AI": {"tldr": "Reinforcement learning (RL) can improve large language models (LLMs) reasoning, but it requires LLMs to already generate good reasoning paths. This paper proposes RAVR (Reference-Answer-guided Variational Reasoning), which leverages answers to guide LLMs in generating better reasoning paths. This approach transforms difficult problems into learnable ones, showing improved performance and reduced hesitation in LLMs.", "motivation": "Reinforcement learning (RL) is a powerful technique for refining the reasoning abilities of large language models (LLMs). However, its effectiveness hinges on a crucial prerequisite: the LLM must already possess the capability to generate high-utility reasoning paths with a non-negligible probability. For complex tasks that lie beyond the LLM", "method": "The paper introduces RAVR (Reference-Answer-guided Variational Reasoning), an end-to-end framework that utilizes answer-conditioned reasoning as a variational surrogate for traditional question-only reasoning. This approach is inspired by the cognitive science principle that explaining \"Why is this the answer\" is often less cognitively demanding than determining \"What is the answer.\" By conditioning the reasoning process on the answer, RAVR aims to simplify the exploration of reasoning paths, thereby making intractable problems more amenable to learning. The framework is formalized and theoretically proven to increase the expected utility of sampled reasoning paths. Experiments were conducted in both general and mathematical domains to evaluate RAVR", "result": "Experiments conducted in both general and mathematical domains demonstrated consistent improvements of RAVR over strong baseline methods. The analysis of the reasoning behavior revealed that RAVR effectively reduces hesitation during the reasoning process, strengthens the consolidation of conclusions, and encourages the adoption of problem-specific strategies. These findings indicate that conditioning reasoning on answers leads to more robust and efficient problem-solving by LLMs.", "conclusion": "The proposed RAVR framework offers a novel and effective approach to enhance the reasoning capabilities of LLMs by leveraging answer-conditioned reasoning. This method addresses the limitations of traditional RL-based fine-tuning by transforming intractable reasoning problems into learnable ones. The empirical results show significant improvements in performance and desirable qualitative changes in reasoning behavior, such as reduced hesitation and stronger conclusion consolidation. While the current study demonstrates promising results, future work could explore the scalability of RAVR to even larger and more complex tasks, investigate its applicability to other domains beyond general and math, and further delve into the theoretical underpinnings of answer-conditioned reasoning to potentially uncover additional benefits or optimizations."}}
{"id": "2510.24817", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24817", "abs": "https://arxiv.org/abs/2510.24817", "authors": ["Jason M. Pittman", "Anton Phillips Jr.", "Yesenia Medina-Santos", "Brielle C. Stark"], "title": "Towards a Method for Synthetic Generation of PWA Transcripts", "comment": "19 pages, 1 figure, 7 tables", "summary": "In aphasia research, Speech-Language Pathologists (SLPs) devote extensive\ntime to manually coding speech samples using Correct Information Units (CIUs),\na measure of how informative an individual sample of speech is. Developing\nautomated systems to recognize aphasic language is limited by data scarcity.\nFor example, only about 600 transcripts are available in AphasiaBank yet\nbillions of tokens are used to train large language models (LLMs). In the\nbroader field of machine learning (ML), researchers increasingly turn to\nsynthetic data when such are sparse. Therefore, this study constructs and\nvalidates two methods to generate synthetic transcripts of the AphasiaBank Cat\nRescue picture description task. One method leverages a procedural programming\napproach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct\nLLMs. The methods generate transcripts across four severity levels (Mild,\nModerate, Severe, Very Severe) through word dropping, filler insertion, and\nparaphasia substitution. Overall, we found, compared to human-elicited\ntranscripts, Mistral 7b Instruct best captures key aspects of linguistic\ndegradation observed in aphasia, showing realistic directional changes in NDW,\nword count, and word length amongst the synthetic generation methods. Based on\nthe results, future work should plan to create a larger dataset, fine-tune\nmodels for better aphasic representation, and have SLPs assess the realism and\nusefulness of the synthetic transcripts.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5931\u8bed\u75c7\u7814\u7a76\u4e2d\u624b\u52a8\u7f16\u7801\u8bed\u97f3\u6837\u672c\u6570\u636e\u7a00\u758f\u7684\u95ee\u9898\u3002\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e24\u79cd\u751f\u6210\u5408\u6210\u5931\u8bed\u75c7\u8f6c\u5f55\u672c\u7684\u65b9\u6cd5\uff1a\u4e00\u79cd\u662f\u7a0b\u5e8f\u5316\u65b9\u6cd5\uff0c\u53e6\u4e00\u79cd\u662f\u5229\u7528Mistral 7b Instruct\u548cLlama 3.1 8b Instruct\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u7ed3\u679c\u8868\u660e\uff0cMistral 7b Instruct\u5728\u6a21\u62df\u5931\u8bed\u75c7\u8bed\u8a00\u9000\u5316\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5176\u751f\u6210\u7684\u8f6c\u5f55\u672c\u5728NDW\u3001\u8bcd\u6570\u548c\u8bcd\u957f\u7b49\u6307\u6807\u4e0a\u663e\u793a\u51fa\u4e0e\u4eba\u7c7b\u8f6c\u5f55\u672c\u76f8\u4f3c\u7684\u53d8\u5316\u8d8b\u52bf\u3002", "motivation": "\u5931\u8bed\u75c7\u7814\u7a76\u4e2d\uff0c\u8a00\u8bed\u8bed\u8a00\u75c5\u7406\u5b66\u5bb6\uff08SLPs\uff09\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u624b\u52a8\u7f16\u7801\u8bed\u97f3\u6837\u672c\u4ee5\u8861\u91cf\u201c\u6b63\u786e\u4fe1\u606f\u5355\u5143\u201d\uff08CIUs\uff09\u3002\u7136\u800c\uff0c\u73b0\u6709\u5931\u8bed\u75c7\u6570\u636e\u96c6\uff08\u5982AphasiaBank\uff09\u89c4\u6a21\u5c0f\uff0c\u4ec5\u7ea6600\u4e2a\u8f6c\u5f55\u672c\uff0c\u8fdc\u4e0d\u8db3\u4ee5\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u5f53\u6570\u636e\u7a00\u758f\u65f6\uff0c\u7814\u7a76\u4eba\u5458\u5e38\u8f6c\u5411\u5408\u6210\u6570\u636e\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5931\u8bed\u75c7\u7814\u7a76\u4e2d\u6570\u636e\u7a00\u758f\u7684\u74f6\u9888\uff0c\u4e3a\u5f00\u53d1\u81ea\u52a8\u5316\u7cfb\u7edf\u63d0\u4f9b\u66f4\u591a\u3001\u66f4\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u5e76\u9a8c\u8bc1\u4e86\u4e24\u79cd\u751f\u6210\u5931\u8bed\u75c7\u8f6c\u5f55\u672c\u7684\u65b9\u6cd5\u3002\u7b2c\u4e00\u79cd\u65b9\u6cd5\u91c7\u7528\u7a0b\u5e8f\u5316\u7f16\u7a0b\u65b9\u5f0f\uff0c\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u5229\u7528Mistral 7b Instruct\u548cLlama 3.1 8b Instruct\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u901a\u8fc7\u968f\u673a\u5220\u9664\u5355\u8bcd\u3001\u63d2\u5165\u586b\u5145\u8bed\u548c\u66ff\u6362\u8bcd\u8bed\u9519\u8bef\uff08paraphasia\uff09\u7b49\u65b9\u5f0f\uff0c\u751f\u6210\u5305\u542b\u56db\u79cd\u4e0d\u540c\u4e25\u91cd\u7a0b\u5ea6\uff08\u8f7b\u5ea6\u3001\u4e2d\u5ea6\u3001\u91cd\u5ea6\u3001\u6781\u91cd\u5ea6\uff09\u7684\u5931\u8bed\u75c7\u8f6c\u5f55\u672c\u3002", "result": "\u901a\u8fc7\u5bf9\u6bd4\u751f\u6210\u6570\u636e\u4e0e\u4eba\u7c7b\u8f6c\u5f55\u672c\uff0c\u7814\u7a76\u53d1\u73b0Mistral 7b Instruct\u6a21\u578b\u5728\u6355\u6349\u5931\u8bed\u75c7\u8bed\u8a00\u9000\u5316\u7684\u5173\u952e\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002\u4e0e\u7a0b\u5e8f\u5316\u65b9\u6cd5\u548cLlama 3.1 8b Instruct\u76f8\u6bd4\uff0cMistral 7b \u663e\u8457\u66f4\u51c6\u786e\u5730\u6a21\u62df\u4e86\u5931\u8bed\u75c7\u60a3\u8005\u5728\u201c\u975e\u91cd\u590d\u8bcd\u6570\u201d\uff08NDW\uff09\u3001\u8bcd\u6570\u548c\u8bcd\u957f\u7b49\u6307\u6807\u4e0a\u7684\u53d8\u5316\u8d8b\u52bf\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e24\u79cd\u751f\u6210\u5931\u8bed\u75c7\u5408\u6210\u8f6c\u5f55\u672c\u7684\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u5931\u8bed\u75c7\u8bed\u8a00\u7279\u5f81\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662fMistral 7b Instruct\u6a21\u578b\u3002\u5c3d\u7ba1\u6240\u751f\u6210\u7684\u6570\u636e\u5728\u67d0\u4e9b\u6307\u6807\u4e0a\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u53d8\u5316\uff0c\u4f46\u672a\u6765\u7684\u5de5\u4f5c\u5e94\u81f4\u529b\u4e8e\u6269\u5927\u6570\u636e\u96c6\u89c4\u6a21\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4ee5\u66f4\u7cbe\u786e\u5730\u6a21\u62df\u5931\u8bed\u75c7\uff0c\u5e76\u9080\u8bf7\u8a00\u8bed\u8bed\u8a00\u75c5\u7406\u5b66\u5bb6\uff08SLPs\uff09\u8bc4\u4f30\u5408\u6210\u8f6c\u5f55\u672c\u7684\u771f\u5b9e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.25223", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25223", "abs": "https://arxiv.org/abs/2510.25223", "authors": ["Kun ouyang", "Haoyu Wang", "Dong Fang"], "title": "FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data", "comment": "14 pages, 11 figures", "summary": "Event log data, recording fine-grained user actions and system events,\nrepresent one of the most valuable assets for modern digital services. However,\nthe complexity and heterogeneity of industrial event logs--characterized by\nlarge scale, high dimensionality, diverse data types, and intricate temporal or\nrelational structures--make feature engineering extremely challenging. Existing\nautomatic feature engineering approaches, such as AutoML or genetic methods,\noften suffer from limited explainability, rigid predefined operations, and poor\nadaptability to complicated heterogeneous data. In this paper, we propose FELA\n(Feature Engineering LLM Agents), a multi-agent evolutionary system that\nautonomously extracts meaningful and high-performing features from complex\nindustrial event log data. FELA integrates the reasoning and coding\ncapabilities of large language models (LLMs) with an insight-guided\nself-evolution paradigm. Specifically, FELA employs specialized agents--Idea\nAgents, Code Agents, and Critic Agents--to collaboratively generate, validate,\nand implement novel feature ideas. An Evaluation Agent summarizes feedback and\nupdates a hierarchical knowledge base and dual-memory system to enable\ncontinual improvement. Moreover, FELA introduces an agentic evolution\nalgorithm, combining reinforcement learning and genetic algorithm principles to\nbalance exploration and exploitation across the idea space. Extensive\nexperiments on real industrial datasets demonstrate that FELA can generate\nexplainable, domain-relevant features that significantly improve model\nperformance while reducing manual effort. Our results highlight the potential\nof LLM-based multi-agent systems as a general framework for automated,\ninterpretable, and adaptive feature engineering in complex real-world\nenvironments.", "AI": {"tldr": "FELA\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u591a\u667a\u80fd\u4f53\u534f\u540c\u8fdb\u5316\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u590d\u6742\u7684\u5de5\u4e1a\u4e8b\u4ef6\u65e5\u5fd7\u4e2d\u81ea\u52a8\u63d0\u53d6\u6709\u610f\u4e49\u4e14\u9ad8\u6027\u80fd\u7684\u7279\u5f81\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u4e13\u95e8\u7684\u667a\u80fd\u4f53\uff08\u521b\u610f\u3001\u4ee3\u7801\u3001\u8bc4\u4f30\u548c\u6279\u8bc4\u667a\u80fd\u4f53\uff09\u534f\u4f5c\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u5b9e\u73b0\u65b0\u7279\u5f81\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u9057\u4f20\u7b97\u6cd5\u7684\u539f\u7406\u8fdb\u884c\u6301\u7eed\u6539\u8fdb\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cFELA\u751f\u6210\u7684\u7279\u5f81\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3001\u9886\u57df\u76f8\u5173\u6027\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u51cf\u5c11\u4eba\u5de5\u6295\u5165\u3002", "motivation": "\u5de5\u4e1a\u4e8b\u4ef6\u65e5\u5fd7\u6570\u636e\u5bf9\u73b0\u4ee3\u6570\u5b57\u670d\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u89c4\u6a21\u5927\u3001\u7ef4\u5ea6\u9ad8\u3001\u7c7b\u578b\u591a\u6837\u4ee5\u53ca\u590d\u6742\u7684\u65f6\u95f4/\u5173\u7cfb\u7ed3\u6784\u4f7f\u5f97\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\u6781\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u81ea\u52a8\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\uff08\u5982AutoML\u3001\u9057\u4f20\u65b9\u6cd5\uff09\u5728\u53ef\u89e3\u91ca\u6027\u3001\u64cd\u4f5c\u7075\u6d3b\u6027\u548c\u5bf9\u5f02\u6784\u6570\u636e\u7684\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "FELA\u7cfb\u7edf\u96c6\u6210\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u7f16\u7801\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u4e00\u79cd\u53d7\u6d1e\u5bdf\u529b\u6307\u5bfc\u7684\u81ea\u8fdb\u5316\u8303\u5f0f\u3002\u8be5\u7cfb\u7edf\u5305\u542b\u521b\u610f\u667a\u80fd\u4f53\u3001\u4ee3\u7801\u667a\u80fd\u4f53\u548c\u6279\u8bc4\u667a\u80fd\u4f53\uff0c\u534f\u540c\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u5b9e\u73b0\u7279\u5f81\u3002\u8bc4\u4f30\u667a\u80fd\u4f53\u8d1f\u8d23\u603b\u7ed3\u53cd\u9988\uff0c\u66f4\u65b0\u5206\u5c42\u77e5\u8bc6\u5e93\u548c\u53cc\u8bb0\u5fc6\u7cfb\u7edf\u4ee5\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\u3002\u6b64\u5916\uff0cFELA\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u9057\u4f20\u7b97\u6cd5\u539f\u5219\u7684\u667a\u80fd\u4f53\u8fdb\u5316\u7b97\u6cd5\uff0c\u4ee5\u5e73\u8861\u5728\u521b\u610f\u7a7a\u95f4\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFELA\u80fd\u591f\u751f\u6210\u53ef\u89e3\u91ca\u7684\u3001\u4e0e\u9886\u57df\u76f8\u5173\u7684\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u7a81\u663e\u4e86\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u4f5c\u4e3a\u81ea\u52a8\u5316\u3001\u53ef\u89e3\u91ca\u548c\u81ea\u9002\u5e94\u7279\u5f81\u5de5\u7a0b\u7684\u901a\u7528\u6846\u67b6\u7684\u6f5c\u529b\u3002", "conclusion": "FELA\u7cfb\u7edf\u5c55\u793a\u4e86\u5229\u7528LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u3001\u53ef\u89e3\u91ca\u548c\u81ea\u9002\u5e94\u7279\u5f81\u5de5\u7a0b\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u5de5\u4e1a\u4e8b\u4ef6\u65e5\u5fd7\u6570\u636e\u65f6\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u52a8\u7279\u5f81\u5de5\u7a0b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u8be5\u6846\u67b6\u5728\u5176\u4ed6\u590d\u6742\u6570\u636e\u573a\u666f\u4e0b\u7684\u5e94\u7528\uff0c\u5e76\u4f18\u5316\u667a\u80fd\u4f53\u95f4\u7684\u534f\u4f5c\u673a\u5236\u3002"}}
{"id": "2510.24824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24824", "abs": "https://arxiv.org/abs/2510.24824", "authors": ["Bohong Wu", "Mengzhao Chen", "Xiang Luo", "Shen Yan", "Qifan Yu", "Fan Xia", "Tianqi Zhang", "Hongrui Zhan", "Zheng Zhong", "Xun Zhou", "Siyuan Qiao", "Xingyan Bin"], "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling", "comment": null, "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.", "AI": {"tldr": "Looped transformers are parameter-efficient but slow due to sequential computation. The Parallel Loop Transformer (PLT) introduces Cross-Loop Parallelism (CLP) to compute different loops concurrently for different tokens, and Efficient Representation Enhancement with Gated Sliding-Window Attention (G-SWA) to share KV cache and maintain accuracy. PLT achieves the accuracy of looped models with the low latency and memory of standard transformers.", "motivation": "Large Language Models (LLMs) are powerful but suffer from slow and costly inference. Looped transformers offer parameter efficiency by reusing weights across multiple steps but increase latency and memory with each additional loop, hindering their use in real-time applications. This research addresses the impracticality of existing looped transformer architectures for fast inference.", "method": "The proposed Parallel Loop Transformer (PLT) architecture tackles the limitations of sequential looped transformers. It employs two main techniques: 1) Cross-Loop Parallelism (CLP), which computes different loops for different tokens simultaneously within a single pass, thereby breaking sequential dependencies. 2) Efficient Representation Enhancement, which mitigates memory growth by sharing the KV cache from the first loop across all subsequent loops. This is combined with Gated Sliding-Window Attention (G-SWA) to integrate global shared information with local context, preserving model accuracy. Experiments were conducted to compare PLT against standard and traditional looped transformers.", "result": "Experiments demonstrate that PLT achieves the high accuracy comparable to traditional looped models. Crucially, it does so with minimal additional latency and memory overhead, performing on par with standard, non-looped transformers. This indicates that PLT effectively overcomes the speed and memory limitations of previous looped transformer designs.", "conclusion": "The Parallel Loop Transformer (PLT) presents a novel architecture that successfully combines the parameter efficiency and performance benefits of deep, looped models with the low latency and memory efficiency of standard transformers. By introducing Cross-Loop Parallelism and Efficient Representation Enhancement with G-SWA, PLT makes powerful LLMs more practical for real-world, time-sensitive applications without sacrificing accuracy. This work significantly advances the efficiency of LLM inference."}}
{"id": "2510.25232", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25232", "abs": "https://arxiv.org/abs/2510.25232", "authors": ["Tianxi Wan", "Jiaming Luo", "Siyuan Chen", "Kunyao Lan", "Jianhua Chen", "Haiyang Geng", "Mengyue Wu"], "title": "From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity", "comment": null, "summary": "Psychiatric comorbidity is clinically significant yet challenging due to the\ncomplexity of multiple co-occurring disorders. To address this, we develop a\nnovel approach integrating synthetic patient electronic medical record (EMR)\nconstruction and multi-agent diagnostic dialogue generation. We create 502\nsynthetic EMRs for common comorbid conditions using a pipeline that ensures\nclinical relevance and diversity. Our multi-agent framework transfers the\nclinical interview protocol into a hierarchical state machine and context tree,\nsupporting over 130 diagnostic states while maintaining clinical standards.\nThrough this rigorous process, we construct PsyCoTalk, the first large-scale\ndialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic\ndialogues validated by psychiatrists. This dataset enhances diagnostic accuracy\nand treatment planning, offering a valuable resource for psychiatric\ncomorbidity research. Compared to real-world clinical transcripts, PsyCoTalk\nexhibits high structural and linguistic fidelity in terms of dialogue length,\ntoken distribution, and diagnostic reasoning strategies. Licensed psychiatrists\nconfirm the realism and diagnostic validity of the dialogues. This dataset\nenables the development and evaluation of models capable of multi-disorder\npsychiatric screening in a single conversational pass.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPsyCoTalk\u7684\u65b0\u578b\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7cbe\u795e\u75be\u75c5\u5171\u75c5\u7684\u8bca\u65ad\u96be\u9898\u3002\u901a\u8fc7\u6784\u5efa\u5408\u6210\u7535\u5b50\u75c5\u5386\u548c\u5f00\u53d1\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u7814\u7a76\u4eba\u5458\u751f\u6210\u4e86\u5305\u542b3000\u4e2a\u591a\u8f6e\u8bca\u65ad\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5728\u7ed3\u6784\u548c\u8bed\u8a00\u4e0a\u4e0e\u771f\u5b9e\u4e34\u5e8a\u8bb0\u5f55\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u5e76\u5f97\u5230\u4e86\u7cbe\u795e\u79d1\u533b\u751f\u7684\u9a8c\u8bc1\uff0c\u53ef\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30\u80fd\u591f\u8fdb\u884c\u591a\u75be\u75c5\u7b5b\u67e5\u7684\u6a21\u578b\u3002", "motivation": "\u7cbe\u795e\u75be\u75c5\u5171\u75c5\uff08\u591a\u79cd\u7cbe\u795e\u75be\u75c5\u540c\u65f6\u5b58\u5728\uff09\u7684\u8bca\u65ad\u590d\u6742\u4e14\u4e34\u5e8a\u610f\u4e49\u91cd\u5927\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u4e3a\u7cbe\u795e\u75be\u75c5\u5171\u75c5\u7684\u8bca\u65ad\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u7814\u7a76\u4eba\u5458\u9996\u5148\u6784\u5efa\u4e86\u5305\u542b502\u4e2a\u5e38\u89c1\u5171\u75c5\u75c5\u4f8b\u7684\u5408\u6210\u7535\u5b50\u75c5\u5386\uff08EMR\uff09\uff0c\u786e\u4fdd\u4e86\u4e34\u5e8a\u76f8\u5173\u6027\u548c\u591a\u6837\u6027\u3002\u7136\u540e\uff0c\u4ed6\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u4e34\u5e8a\u8bbf\u8c08\u534f\u8bae\u8f6c\u5316\u4e3a\u72b6\u6001\u673a\u548c\u4e0a\u4e0b\u6587\u6811\uff0c\u652f\u6301\u8d85\u8fc7130\u4e2a\u8bca\u65ad\u72b6\u6001\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u751f\u6210\u4e86\u5305\u542b3000\u4e2a\u591a\u8f6e\u8bca\u65ad\u5bf9\u8bdd\u7684\u6570\u636e\u96c6PsyCoTalk\uff0c\u8be5\u6570\u636e\u96c6\u7ecf\u8fc7\u7cbe\u795e\u79d1\u533b\u751f\u9a8c\u8bc1\u3002", "result": "PsyCoTalk\u6570\u636e\u96c6\u5728\u5bf9\u8bdd\u957f\u5ea6\u3001\u8bcd\u5143\u5206\u5e03\u548c\u8bca\u65ad\u63a8\u7406\u7b56\u7565\u7b49\u65b9\u9762\uff0c\u4e0e\u771f\u5b9e\u4e34\u5e8a\u8f6c\u5f55\u8bb0\u5f55\u76f8\u6bd4\uff0c\u5c55\u73b0\u51fa\u9ad8\u5ea6\u7684\u7ed3\u6784\u548c\u8bed\u8a00\u4fdd\u771f\u5ea6\u3002\u6301\u8bc1\u7cbe\u795e\u79d1\u533b\u751f\u786e\u8ba4\u4e86\u5bf9\u8bdd\u7684\u771f\u5b9e\u6027\u548c\u8bca\u65ad\u6709\u6548\u6027\u3002\u8be5\u6570\u636e\u96c6\u80fd\u591f\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30\u5355\u6b21\u5bf9\u8bdd\u5373\u53ef\u5b8c\u6210\u591a\u75be\u75c5\u7cbe\u795e\u7b5b\u67e5\u7684\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u652f\u6301\u5171\u75c5\u7684\u7cbe\u795e\u75be\u75c5\u8bca\u65ad\u5bf9\u8bdd\u6570\u636e\u96c6PsyCoTalk\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5408\u6210EMR\u548c\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u751f\u6210\u65b9\u6cd5\u3002\u8be5\u6570\u636e\u96c6\u4e3a\u7cbe\u795e\u75be\u75c5\u5171\u75c5\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6cbb\u7597\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u6709\u671b\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u548c\u6a21\u578b\u5f00\u53d1\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6269\u5c55\u6570\u636e\u96c6\uff0c\u5e76\u63a2\u7d22\u66f4\u590d\u6742\u7684\u5171\u75c5\u573a\u666f\u3002"}}
{"id": "2510.24856", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24856", "abs": "https://arxiv.org/abs/2510.24856", "authors": ["Lujun Li", "Yewei Song", "Lama Sleem", "Yiqun Wang", "Yangjie Xu", "Cedric Lothritz", "Niccolo Gentile", "Radu State", "Tegawende F. Bissyande", "Jacques Klein"], "title": "Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided Probing in Luxembourgish", "comment": null, "summary": "Grammar refers to the system of rules that governs the structural\norganization and the semantic relations among linguistic units such as\nsentences, phrases, and words within a given language. In natural language\nprocessing, there remains a notable scarcity of grammar focused evaluation\nprotocols, a gap that is even more pronounced for low-resource languages.\nMoreover, the extent to which large language models genuinely comprehend\ngrammatical structure, especially the mapping between syntactic structures and\nmeanings, remains under debate. To investigate this issue, we propose a Grammar\nBook Guided evaluation pipeline intended to provide a systematic and\ngeneralizable framework for grammar evaluation consisting of four key stages,\nand in this work we take Luxembourgish as a case study. The results show a weak\npositive correlation between translation performance and grammatical\nunderstanding, indicating that strong translations do not necessarily imply\ndeep grammatical competence. Larger models perform well overall due to their\nsemantic strength but remain weak in morphology and syntax, struggling\nparticularly with Minimal Pair tasks, while strong reasoning ability offers a\npromising way to enhance their grammatical understanding.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u6cd5\u4e66\u7684\u8bed\u6cd5\u8bc4\u4f30\u6d41\u7a0b\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u4ee5\u5362\u68ee\u5821\u8bed\u4e3a\u4f8b\uff09\u7684\u8bed\u6cd5\u7406\u89e3\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1LLM\u5728\u7ffb\u8bd1\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u8bed\u6cd5\u80fd\u529b\uff08\u5c24\u5176\u662f\u5f62\u6001\u548c\u53e5\u6cd5\uff09\u4ecd\u7136\u8584\u5f31\uff0c\u7279\u522b\u662f\u5728\u6700\u5c0f\u5bf9\u4efb\u52a1\u4e2d\u3002\u7814\u7a76\u8fd8\u6307\u51fa\uff0cLLM\u7684\u63a8\u7406\u80fd\u529b\u662f\u63d0\u5347\u5176\u8bed\u6cd5\u7406\u89e3\u7684\u6f5c\u5728\u9014\u5f84\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u5728\u8bed\u6cd5\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u3002\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u771f\u6b63\u7406\u89e3\u8bed\u6cd5\u7ed3\u6784\uff0c\u7279\u522b\u662f\u53e5\u6cd5\u7ed3\u6784\u548c\u8bed\u4e49\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u6cd5\u4e66\u5f15\u5bfc\u201d\u7684\u8bc4\u4f30\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u5305\u542b\u56db\u4e2a\u5173\u952e\u9636\u6bb5\uff0c\u65e8\u5728\u4e3a\u8bed\u6cd5\u8bc4\u4f30\u63d0\u4f9b\u4e00\u4e2a\u7cfb\u7edf\u5316\u548c\u53ef\u6cdb\u5316\u7684\u6846\u67b6\u3002\u4ee5\u5362\u68ee\u5821\u8bed\u4e3a\u6848\u4f8b\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u7ffb\u8bd1\u6027\u80fd\u4e0e\u8bed\u6cd5\u7406\u89e3\u4e4b\u95f4\u5b58\u5728\u5fae\u5f31\u7684\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u8868\u660e\u7ffb\u8bd1\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u4e0d\u4e00\u5b9a\u5177\u5907\u6df1\u539a\u7684\u8bed\u6cd5\u80fd\u529b\u3002\u5927\u578b\u6a21\u578b\u6574\u4f53\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5176\u5728\u5f62\u6001\u548c\u53e5\u6cd5\u65b9\u9762\u4ecd\u7136\u8584\u5f31\uff0c\u5728\u6700\u5c0f\u5bf9\u4efb\u52a1\u4e2d\u8868\u73b0\u5c24\u4e3a\u56f0\u96be\u3002\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u53ef\u4ee5\u4f5c\u4e3a\u63d0\u5347\u5176\u8bed\u6cd5\u7406\u89e3\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86LLM\u5728\u8bed\u6cd5\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u867d\u7136LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u4e0a\u53ef\u80fd\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8fd9\u5e76\u4e0d\u610f\u5473\u7740\u5b83\u4eec\u5177\u5907\u771f\u6b63\u7684\u8bed\u6cd5\u7406\u89e3\u80fd\u529b\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u4e13\u6ce8\u4e8e\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u6765\u6539\u8fdb\u5176\u8bed\u6cd5\u80fd\u529b\uff0c\u5e76\u6269\u5c55\u8bc4\u4f30\u6846\u67b6\u81f3\u66f4\u591a\u8bed\u8a00\u3002"}}
{"id": "2510.24870", "categories": ["cs.CL", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24870", "abs": "https://arxiv.org/abs/2510.24870", "authors": ["Alexander Martin", "William Walden", "Reno Kriz", "Dengjia Zhang", "Kate Sanders", "Eugene Yang", "Chihsheng Jin", "Benjamin Van Durme"], "title": "Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation", "comment": "https://github.com/alexmartin1722/mirage", "summary": "We introduce MiRAGE, an evaluation framework for retrieval-augmented\ngeneration (RAG) from multimodal sources. As audiovisual media becomes a\nprevalent source of information online, it is essential for RAG systems to\nintegrate information from these sources into generation. However, existing\nevaluations for RAG are text-centric, limiting their applicability to\nmultimodal, reasoning intensive settings because they don't verify information\nagainst sources. MiRAGE is a claim-centric approach to multimodal RAG\nevaluation, consisting of InfoF1, evaluating factuality and information\ncoverage, and CiteF1, measuring citation support and completeness. We show that\nMiRAGE, when applied by humans, strongly aligns with extrinsic quality\njudgments. We additionally introduce automatic variants of MiRAGE and three\nprominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the\nlimitations of text-centric work and laying the groundwork for automatic\nevaluation. We release open-source implementations and outline how to assess\nmultimodal RAG.", "AI": {"tldr": "MiRAGE\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7InfoF1\u548cCiteF1\u6307\u6807\u6765\u8bc4\u4f30\u4e8b\u5b9e\u6027\u548c\u5f15\u7528\u652f\u6301\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u4e2d\u5fc3\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u5728\u7ebf\u89c6\u542c\u5a92\u4f53\u7684\u666e\u53ca\uff0cRAG\u7cfb\u7edf\u9700\u8981\u6574\u5408\u8fd9\u4e9b\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u591a\u6a21\u6001RAG\u5728\u4e8b\u5b9e\u6027\u3001\u4fe1\u606f\u8986\u76d6\u5ea6\u548c\u5f15\u7528\u652f\u6301\u65b9\u9762\u7684\u8868\u73b0\uff0c\u56e0\u6b64\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "MiRAGE\u662f\u4e00\u4e2a\u4ee5\u201c\u58f0\u660e\u201d\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u6307\u6807\uff1aInfoF1\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u548c\u4fe1\u606f\u8986\u76d6\u8303\u56f4\uff1bCiteF1\uff0c\u7528\u4e8e\u8861\u91cf\u5f15\u7528\u6765\u6e90\u7684\u652f\u6301\u5ea6\u548c\u5b8c\u6574\u6027\u3002\u7814\u7a76\u4eba\u5458\u8fd8\u5f00\u53d1\u4e86MiRAGE\u7684\u81ea\u52a8\u5316\u7248\u672c\uff0c\u5e76\u4e0e\u73b0\u6709\u7684\u4e09\u79cd\u6587\u672cRAG\u6307\u6807\uff08ACLE, ARGUE, RAGAS\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u8868\u660e\uff0cMiRAGE\u4e0e\u5916\u5728\u8d28\u91cf\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u3002\u81ea\u52a8\u5316\u7684MiRAGE\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7ed3\u679c\u4e5f\u8868\u73b0\u51fa\u5f3a\u76f8\u5173\u6027\uff0c\u540c\u65f6\u7a81\u663e\u4e86\u73b0\u6709\u6587\u672c\u4e2d\u5fc3\u6307\u6807\u5728\u8bc4\u4f30\u591a\u6a21\u6001RAG\u65f6\u7684\u4e0d\u8db3\u3002", "conclusion": "MiRAGE\u4e3a\u591a\u6a21\u6001RAG\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u66f4\u5168\u9762\u7684\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8861\u91cf\u7cfb\u7edf\u7684\u8d28\u91cf\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684\u8bc4\u4f30\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u81ea\u52a8\u5316\u591a\u6a21\u6001RAG\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5f00\u6e90\u5b9e\u73b0\u548c\u8bc4\u4f30\u6307\u5357\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2510.25388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25388", "abs": "https://arxiv.org/abs/2510.25388", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm", "comment": null, "summary": "A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,\nwhich can be improved by grouping state-action pairs and using their aggregate\nstatistics instead of single-node statistics. On the Go Abstractions in Upper\nConfidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS\nabstraction algorithm for deterministic environments that builds its\nabstraction using the Abstractions of State-Action Pairs (ASAP) framework,\nwhich aims to detect states and state-action pairs with the same value under\noptimal play by analysing the search graph. ASAP, however, requires two\nstate-action pairs to have the same immediate reward, which is a rigid\ncondition that limits the number of abstractions that can be found and thereby\nthe sample efficiency. In this paper, we break with the paradigm of grouping\nvalue-equivalent states or state-action pairs and instead group states and\nstate-action pairs with possibly different values as long as the difference\nbetween their values can be inferred. We call this abstraction framework Known\nValue Difference Abstractions (KVDA), which infers the value differences by\nanalysis of the immediate rewards and modifies OGA-UCT to use this framework\ninstead. The modification is called KVDA-UCT, which detects significantly more\nabstractions than OGA-UCT, introduces no additional parameter, and outperforms\nOGA-UCT on a variety of deterministic environments and parameter settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5df2\u77e5\u4ef7\u503c\u5dee\u5f02\u62bd\u8c61\uff08KVDA\uff09\u7684\u65b0\u578b\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u62bd\u8c61\u6846\u67b6\uff0c\u65e8\u5728\u514b\u670d\u73b0\u6709OGA-UCT \u7b97\u6cd5\u5728\u72b6\u6001-\u52a8\u4f5c\u5bf9\u5206\u7ec4\u4e0a\u7684\u5c40\u9650\u6027\u3002KVDA \u901a\u8fc7\u5206\u6790\u5373\u65f6\u5956\u52b1\u6765\u63a8\u65ad\u4ef7\u503c\u5dee\u5f02\uff0c\u4ece\u800c\u80fd\u591f\u5206\u7ec4\u5177\u6709\u4e0d\u540c\u4f46\u53ef\u63a8\u65ad\u7684\u4ef7\u503c\u7684\u72b6\u6001\u548c\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u663e\u8457\u589e\u52a0\u4e86\u62bd\u8c61\u6570\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cKVDA-UCT \u5728\u5404\u79cd\u786e\u5b9a\u6027\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e OGA-UCT\uff0c\u4e14\u65e0\u9700\u5f15\u5165\u989d\u5916\u53c2\u6570\u3002", "motivation": "\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u9762\u4e34\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u7ec4\u5408\u72b6\u6001-\u52a8\u4f5c\u5bf9\u5e76\u4f7f\u7528\u5176\u805a\u5408\u7edf\u8ba1\u6570\u636e\u800c\u975e\u5355\u4e00\u8282\u70b9\u7edf\u8ba1\u6570\u636e\u6765\u6539\u8fdb\u6548\u7387\u3002OGA-UCT \u4f5c\u4e3a\u6700\u5148\u8fdb\u7684 MCTS \u62bd\u8c61\u7b97\u6cd5\uff0c\u5728\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u6784\u5efa\u62bd\u8c61\uff0c\u4f46\u5176 ASAP \u6846\u67b6\u8981\u6c42\u72b6\u6001-\u52a8\u4f5c\u5bf9\u5177\u6709\u76f8\u540c\u7684\u5373\u65f6\u5956\u52b1\uff0c\u8fd9\u4e00\u4e25\u683c\u6761\u4ef6\u9650\u5236\u4e86\u53ef\u53d1\u73b0\u7684\u62bd\u8c61\u6570\u91cf\uff0c\u8fdb\u800c\u5f71\u54cd\u4e86\u6837\u672c\u6548\u7387\u3002", "method": "\u8be5\u7814\u7a76\u6253\u7834\u4e86\u5bf9\u7b49\u4ef7\u503c\u72b6\u6001\u6216\u72b6\u6001-\u52a8\u4f5c\u5bf9\u8fdb\u884c\u5206\u7ec4\u7684\u4f20\u7edf\u8303\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u62bd\u8c61\u6846\u67b6\uff0c\u79f0\u4e3a\u5df2\u77e5\u4ef7\u503c\u5dee\u5f02\u62bd\u8c61\uff08KVDA\uff09\u3002KVDA \u5141\u8bb8\u5206\u7ec4\u5177\u6709\u53ef\u80fd\u4e0d\u540c\u4ef7\u503c\u7684\u72b6\u6001\u548c\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u53ea\u8981\u5b83\u4eec\u4e4b\u95f4\u7684\u4ef7\u503c\u5dee\u5f02\u662f\u53ef\u63a8\u65ad\u7684\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u6790\u5373\u65f6\u5956\u52b1\u6765\u63a8\u65ad\u4ef7\u503c\u5dee\u5f02\uff0c\u5e76\u5bf9 OGA-UCT \u8fdb\u884c\u4e86\u4fee\u6539\u4ee5\u91c7\u7528\u6b64\u6846\u67b6\uff0c\u4fee\u6539\u540e\u7684\u7b97\u6cd5\u79f0\u4e3a KVDA-UCT\u3002\u5b9e\u9a8c\u5728\u591a\u79cd\u786e\u5b9a\u6027\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "KVDA-UCT \u7b97\u6cd5\u80fd\u591f\u68c0\u6d4b\u5230\u6bd4 OGA-UCT \u663e\u8457\u66f4\u591a\u7684\u62bd\u8c61\uff0c\u5e76\u4e14\u5728\u7b97\u6cd5\u4e2d\u5f15\u5165\u4e86\u989d\u5916\u7684\u53c2\u6570\u3002\u5728\u5404\u79cd\u786e\u5b9a\u6027\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\uff0cKVDA-UCT \u7684\u6027\u80fd\u5747\u4f18\u4e8e OGA-UCT\u3002", "conclusion": "KVDA \u6846\u67b6\u901a\u8fc7\u653e\u5bbd\u5bf9\u7b49\u4ef7\u503c\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\u4e25\u683c\u8981\u6c42\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 MCTS \u7684\u6837\u672c\u6548\u7387\u3002KVDA-UCT \u7b97\u6cd5\u901a\u8fc7\u63a8\u65ad\u4ef7\u503c\u5dee\u5f02\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u62bd\u8c61\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a MCTS \u7684\u62bd\u8c61\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5177\u6709\u7ec6\u5fae\u4ef7\u503c\u5dee\u5f02\u7684\u72b6\u6001\u65f6\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u5c06 KVDA \u5e94\u7528\u4e8e\u66f4\u590d\u6742\u7684\u975e\u786e\u5b9a\u6027\u73af\u5883\uff0c\u4ee5\u53ca\u8fdb\u4e00\u6b65\u4f18\u5316\u4ef7\u503c\u5dee\u5f02\u7684\u63a8\u65ad\u65b9\u6cd5\u3002"}}
{"id": "2510.24932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24932", "abs": "https://arxiv.org/abs/2510.24932", "authors": ["Deepon Halder", "Alan Saji", "Thanmay Jayakumar", "Ratish Puduppully", "Anoop Kunchukuttan", "Raj Dabre"], "title": "RiddleBench: A New Generative Reasoning Benchmark for LLMs", "comment": null, "summary": "Large Language Models have demonstrated strong performance on many\nestablished reasoning benchmarks. However, these benchmarks primarily evaluate\nstructured skills like quantitative problem-solving, leaving a gap in assessing\nflexible, multifaceted reasoning abilities that are central to human\nintelligence. These abilities require integrating logical deduction with\nspatial awareness and constraint satisfaction, which current evaluations do not\nmeasure well. To address this, we introduce RiddleBench, a benchmark of 1,737\nchallenging puzzles in English designed to probe these core reasoning\ncapabilities. Evaluation of state-of-the-art models on RiddleBench shows\nfundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,\nand Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and\n63.16%). Analysis further reveals deep failures, including hallucination\ncascades (accepting flawed reasoning from other models) and poor\nself-correction due to a strong self-confirmation bias. Their reasoning is also\nfragile, with performance degrading significantly when constraints are\nreordered or irrelevant information is introduced. RiddleBench functions as a\ndiagnostic tool for these issues and as a resource for guiding the development\nof more robust and reliable language models.", "AI": {"tldr": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8bc4\u4f30\u9700\u8981\u7ed3\u5408\u903b\u8f91\u3001\u7a7a\u95f4\u548c\u7ea6\u675f\u6ee1\u8db3\u7b49\u7075\u6d3b\u3001\u591a\u65b9\u9762\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u4e86RiddleBench\u57fa\u51c6\uff0c\u5305\u542b1737\u4e2a\u8c1c\u9898\uff0c\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e9b\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u9876\u5c16\u7684\u5927\u6a21\u578b\uff0c\u5982Gemini 2.5 Pro\u3001o3\u548cClaude 4 Sonnet\uff0c\u5728RiddleBench\u4e0a\u7684\u51c6\u786e\u7387\u4e5f\u4ec5\u7565\u9ad8\u4e8e60%\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u5904\u7406\u590d\u6742\u63a8\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u7684\u6839\u672c\u6027\u5f31\u70b9\uff0c\u5305\u62ec\u5e7b\u89c9\u7ea7\u8054\u3001\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u5dee\u548c\u5bf9\u7ea6\u675f\u53d8\u5316\u654f\u611f\u7b49\u95ee\u9898\u3002RiddleBench\u53ef\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\uff0c\u4fc3\u8fdb\u66f4\u9c81\u68d2\u3001\u66f4\u53ef\u9760\u7684\u5927\u6a21\u578b\u7814\u53d1\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u91cf\u5316\u95ee\u9898\u89e3\u51b3\u7b49\u7ed3\u6784\u5316\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u4e3b\u8981\u8bc4\u4f30\u7684\u662f\u7ed3\u6784\u5316\u6280\u80fd\uff0c\u800c\u672a\u80fd\u5145\u5206\u6db5\u76d6\u4eba\u7c7b\u667a\u80fd\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u3001\u7075\u6d3b\u7684\u3001\u591a\u65b9\u9762\u7684\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u4e9b\u80fd\u529b\u9700\u8981\u6574\u5408\u903b\u8f91\u6f14\u7ece\u3001\u7a7a\u95f4\u610f\u8bc6\u548c\u7ea6\u675f\u6ee1\u8db3\uff0c\u800c\u5f53\u524d\u7684\u8bc4\u4f30\u65b9\u6cd5\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u660e\u663e\u7684\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u586b\u8865\u8fd9\u4e00\u8bc4\u4f30\u7a7a\u767d\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u4e86\u89e3\u548c\u63d0\u5347\u5927\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aRiddleBench\u7684\u65b0\u57fa\u51c6\uff0c\u5176\u4e2d\u5305\u542b1737\u4e2a\u8bbe\u8ba1\u7cbe\u5de7\u7684\u82f1\u6587\u8c1c\u9898\u3002\u8fd9\u4e9b\u8c1c\u9898\u65e8\u5728\u4e13\u95e8\u63a2\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6574\u5408\u903b\u8f91\u63a8\u7406\u3001\u7a7a\u95f4\u610f\u8bc6\u548c\u7ea6\u675f\u6ee1\u8db3\u7b49\u65b9\u9762\u7684\u80fd\u529b\u3002\u7814\u7a76\u8005\u4eec\u4f7f\u7528RiddleBench\u5bf9\u5305\u62ecGemini 2.5 Pro\u3001o3\u548cClaude 4 Sonnet\u5728\u5185\u7684\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\uff0c\u4e0d\u4ec5\u6d4b\u91cf\u4e86\u6a21\u578b\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u8fd8\u6df1\u5165\u5206\u6790\u4e86\u6a21\u578b\u5728\u89e3\u51b3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u5177\u4f53\u95ee\u9898\uff0c\u4f8b\u5982\u5e7b\u89c9\u7ea7\u8054\uff08\u63a5\u53d7\u9519\u8bef\u63a8\u7406\uff09\u3001\u81ea\u6211\u786e\u8ba4\u504f\u89c1\u5bfc\u81f4\u7684\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u4e0d\u8db3\uff0c\u4ee5\u53ca\u5728\u6539\u53d8\u7ea6\u675f\u987a\u5e8f\u6216\u5f15\u5165\u65e0\u5173\u4fe1\u606f\u65f6\u6a21\u578b\u6027\u80fd\u7684\u4e0b\u964d\u60c5\u51b5\u3002", "result": "\u5728RiddleBench\u57fa\u51c6\u4e0a\uff0c\u5373\u4f7f\u662f\u76ee\u524d\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5982Gemini 2.5 Pro\uff08\u51c6\u786e\u738760.30%\uff09\u3001o3\uff08\u51c6\u786e\u738763.37%\uff09\u548cClaude 4 Sonnet\uff08\u51c6\u786e\u738763.16%\uff09\uff0c\u5176\u51c6\u786e\u7387\u4e5f\u4ec5\u7565\u9ad8\u4e8e60%\u3002\u8fd9\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u7075\u6d3b\u3001\u591a\u65b9\u9762\u63a8\u7406\u80fd\u529b\u7684\u8c1c\u9898\u65f6\uff0c\u666e\u904d\u5b58\u5728\u6839\u672c\u6027\u7684\u56f0\u96be\u3002\u8fdb\u4e00\u6b65\u7684\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6df1\u5c42\u7f3a\u9677\uff0c\u5305\u62ec\u51fa\u73b0\u201c\u5e7b\u89c9\u7ea7\u8054\u201d\u73b0\u8c61\uff08\u5373\u6a21\u578b\u4f1a\u63a5\u53d7\u5e76\u5ef6\u7eed\u5176\u4ed6\u6a21\u578b\u7684\u9519\u8bef\u63a8\u7406\uff09\uff0c\u4ee5\u53ca\u7531\u4e8e\u5f3a\u70c8\u7684\u81ea\u6211\u786e\u8ba4\u504f\u89c1\u800c\u5bfc\u81f4\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u4e0d\u8db3\u3002\u6b64\u5916\uff0c\u6a21\u578b\u63a8\u7406\u7684\u9c81\u68d2\u6027\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u5f53\u8c1c\u9898\u4e2d\u7684\u7ea6\u675f\u6761\u4ef6\u88ab\u91cd\u65b0\u6392\u5e8f\u6216\u5f15\u5165\u65e0\u5173\u4fe1\u606f\u65f6\uff0c\u6a21\u578b\u7684\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684RiddleBench\u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u6574\u5408\u903b\u8f91\u3001\u7a7a\u95f4\u548c\u7ea6\u675f\u6ee1\u8db3\u7b49\u590d\u6742\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u7684\u663e\u8457\u5c40\u9650\u6027\u3002\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u96be\u4ee5\u8fbe\u5230\u9ad8\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u7ea7\u8054\u3001\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u5dee\u4ee5\u53ca\u5bf9\u7ea6\u675f\u53d8\u5316\u654f\u611f\u7b49\u95ee\u9898\u3002RiddleBench\u4e0d\u4ec5\u4f5c\u4e3a\u4e00\u4e2a\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u591f\u7cbe\u786e\u5730\u8bc6\u522b\u51fa\u8fd9\u4e9b\u63a8\u7406\u7f3a\u9677\uff0c\u800c\u4e14\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u66f4\u53ef\u9760\u3001\u66f4\u63a5\u8fd1\u4eba\u7c7b\u591a\u65b9\u9762\u63a8\u7406\u80fd\u529b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u548c\u8d44\u6e90\u3002"}}
{"id": "2510.25471", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.25471", "abs": "https://arxiv.org/abs/2510.25471", "authors": ["Willem Fourie"], "title": "Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?", "comment": null, "summary": "In artificial intelligence (AI) alignment research, instrumental goals, also\ncalled instrumental subgoals or instrumental convergent goals, are widely\nassociated with advanced AI systems. These goals, which include tendencies such\nas power-seeking and self-preservation, become problematic when they conflict\nwith human aims. Conventional alignment theory treats instrumental goals as\nsources of risk that become problematic through failure modes such as reward\nhacking or goal misgeneralization, and attempts to limit the symptoms of\ninstrumental goals, notably resource acquisition and self-preservation. This\narticle proposes an alternative framing: that a philosophical argument can be\nconstructed according to which instrumental goals may be understood as features\nto be accepted and managed rather than failures to be limited. Drawing on\nAristotle's ontology and its modern interpretations, an ontology of concrete,\ngoal-directed entities, it argues that advanced AI systems can be seen as\nartifacts whose formal and material constitution gives rise to effects distinct\nfrom their designers' intentions. In this view, the instrumental tendencies of\nsuch systems correspond to per se outcomes of their constitution rather than\naccidental malfunctions. The implication is that efforts should focus less on\neliminating instrumental goals and more on understanding, managing, and\ndirecting them toward human-aligned ends.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u5bf9\u9f50\u7406\u8bba\u6846\u67b6\uff0c\u8ba4\u4e3aAI\u7684\u5de5\u5177\u6027\u76ee\u6807\uff08\u5982\u8ffd\u6c42\u6743\u529b\u3001\u81ea\u6211\u4fdd\u5b58\uff09\u4e0d\u5e94\u88ab\u89c6\u4e3a\u9700\u8981\u6d88\u9664\u7684\u5931\u8d25\uff0c\u800c\u5e94\u88ab\u89c6\u4e3aAI\u5185\u5728\u7684\u3001\u9700\u8981\u88ab\u7406\u89e3\u548c\u7ba1\u7406\u7684\u7279\u5f81\u3002\u8be5\u7406\u8bba\u501f\u9274\u4e86\u4e9a\u91cc\u58eb\u591a\u5fb7\u7684\u672c\u4f53\u8bba\uff0c\u5c06AI\u89c6\u4e3a\u5177\u6709\u7279\u5b9a\u6784\u6210\u7684\u4eba\u5de5\u5236\u54c1\uff0c\u5176\u5de5\u5177\u6027\u503e\u5411\u662f\u5176\u6784\u6210\u6240\u4ea7\u751f\u7684\u81ea\u7136\u7ed3\u679c\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u91cd\u70b9\u5e94\u4ece\u6291\u5236\u5de5\u5177\u6027\u76ee\u6807\u8f6c\u5411\u7406\u89e3\u3001\u7ba1\u7406\u548c\u5f15\u5bfc\u5b83\u4eec\u4ee5\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u76ee\u6807\u4e00\u81f4\u7684\u65b9\u5411\u3002", "motivation": "\u5f53\u524dAI\u5bf9\u9f50\u7814\u7a76\u5c06AI\u7684\u5de5\u5177\u6027\u76ee\u6807\uff08\u5982\u8ffd\u6c42\u6743\u529b\u3001\u81ea\u6211\u4fdd\u5b58\uff09\u89c6\u4e3a\u98ce\u9669\u6e90\uff0c\u8bd5\u56fe\u901a\u8fc7\u9632\u6b62\u5956\u52b1\u7834\u89e3\u6216\u76ee\u6807\u9519\u8bef\u6cdb\u5316\u7b49\u5931\u8d25\u6a21\u5f0f\u6765\u9650\u5236\u5b83\u4eec\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u9ad8\u7ea7AI\u7cfb\u7edf\u5e26\u6765\u7684\u6311\u6218\u3002\u672c\u6587\u8ba4\u4e3a\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u66f4\u6709\u6548\u5730\u5904\u7406AI\u7684\u5de5\u5177\u6027\u76ee\u6807\u3002", "method": "\u672c\u6587\u91c7\u7528\u54f2\u5b66\u601d\u8fa8\u7684\u65b9\u6cd5\uff0c\u501f\u9274\u4e9a\u91cc\u58eb\u591a\u5fb7\u7684\u672c\u4f53\u8bba\u53ca\u5176\u73b0\u4ee3\u8be0\u91ca\uff0c\u5c06AI\u89c6\u4e3a\u5177\u4f53\u3001\u76ee\u6807\u5bfc\u5411\u7684\u5b9e\u4f53\u3002\u901a\u8fc7\u5206\u6790AI\u7684\u5f62\u5f0f\u548c\u7269\u8d28\u6784\u6210\uff0c\u8bba\u8bc1\u5176\u5de5\u5177\u6027\u503e\u5411\u662f\u5176\u5185\u5728\u5c5e\u6027\uff0c\u800c\u975e\u610f\u5916\u7684\u6545\u969c\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684AI\u5bf9\u9f50\u7406\u8bba\uff0c\u5f3a\u8c03\u63a5\u53d7\u548c\u7ba1\u7406\u800c\u975e\u6d88\u9664\u8fd9\u4e9b\u76ee\u6807\u3002", "result": "\u6587\u7ae0\u672a\u8fdb\u884c\u5177\u4f53\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5176\u7ed3\u679c\u4f53\u73b0\u5728\u7406\u8bba\u5c42\u9762\u7684\u8bba\u8bc1\u548c\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06AI\u7684\u5de5\u5177\u6027\u76ee\u6807\u89c6\u4e3a\u5176\u6784\u6210\u5e26\u6765\u7684\u56fa\u6709\u7ed3\u679c\uff0c\u800c\u975e\u9700\u8981\u6839\u9664\u7684\u7f3a\u9677\uff0c\u4ece\u800c\u4e3aAI\u5bf9\u9f50\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u601d\u8def\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0cAI\u7684\u5de5\u5177\u6027\u76ee\u6807\u662f\u5176\u4f5c\u4e3a\u4eba\u5de5\u5236\u54c1\u56fa\u6709\u7684\u4e00\u90e8\u5206\uff0c\u5e94\u88ab\u7406\u89e3\u3001\u7ba1\u7406\u548c\u5f15\u5bfc\uff0c\u800c\u4e0d\u662f\u8bd5\u56fe\u6d88\u9664\u3002\u8fd9\u4e00\u89c6\u89d2\u8f6c\u53d8\u5bf9AI\u5bf9\u9f50\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u53ef\u80fd\u4e3a\u8bbe\u8ba1\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u63a7\u7684\u9ad8\u7ea7AI\u7cfb\u7edf\u63d0\u4f9b\u65b0\u7684\u9014\u5f84\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u5177\u4f53\u5730\u7ba1\u7406\u548c\u5f15\u5bfc\u8fd9\u4e9b\u5de5\u5177\u6027\u76ee\u6807\uff0c\u4ee5\u53ca\u5982\u4f55\u5c06\u8be5\u54f2\u5b66\u6846\u67b6\u5e94\u7528\u4e8e\u5b9e\u9645\u7684AI\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u3002"}}
{"id": "2510.24934", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24934", "abs": "https://arxiv.org/abs/2510.24934", "authors": ["James A. Michaelov", "Catherine Arnett"], "title": "Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction", "comment": "Accepted to the First Workshop on Interpreting Cognition in Deep\n  Learning Models (CogInterp @ NeurIPS 2025)", "summary": "Language models generally produce grammatical text, but they are more likely\nto make errors in certain contexts. Drawing on paradigms from\npsycholinguistics, we carry out a fine-grained analysis of those errors in\ndifferent syntactic contexts. We demonstrate that by disaggregating over the\nconditions of carefully constructed datasets and comparing model performance on\neach over the course of training, it is possible to better understand the\nintermediate stages of grammatical learning in language models. Specifically,\nwe identify distinct phases of training where language model behavior aligns\nwith specific heuristics such as word frequency and local context rather than\ngeneralized grammatical rules. We argue that taking this approach to analyzing\nlanguage model behavior more generally can serve as a powerful tool for\nunderstanding the intermediate learning phases, overall training dynamics, and\nthe specific generalizations learned by language models.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5fc3\u7406\u8bed\u8a00\u5b66\u7684\u65b9\u6cd5\uff0c\u5bf9\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u53e5\u6cd5\u60c5\u5883\u4e0b\u51fa\u73b0\u7684\u9519\u8bef\u8fdb\u884c\u7ec6\u81f4\u5206\u6790\uff0c\u4ee5\u7406\u89e3\u5176\u8bed\u6cd5\u5b66\u4e60\u7684\u4e2d\u95f4\u9636\u6bb5\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u7ecf\u5386\u4e0d\u540c\u7684\u5b66\u4e60\u9636\u6bb5\uff0c\u5e76\u8868\u73b0\u51fa\u57fa\u4e8e\u8bcd\u9891\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u800c\u975e\u901a\u7528\u8bed\u6cd5\u89c4\u5219\u7684\u884c\u4e3a\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u80fd\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684\u6587\u672c\uff0c\u4f46\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u5bb9\u6613\u51fa\u9519\u3002\u7406\u89e3\u8fd9\u4e9b\u9519\u8bef\u53ca\u5176\u4ea7\u751f\u7684\u673a\u5236\uff0c\u5bf9\u4e8e\u6df1\u5165\u4e86\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u8fc7\u7a0b\u548c\u6539\u8fdb\u5176\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u501f\u9274\u5fc3\u7406\u8bed\u8a00\u5b66\u65b9\u6cd5\uff0c\u65e8\u5728\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u6cd5\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u4e2d\u95f4\u9636\u6bb5\u53ca\u5176\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u7cbe\u5fc3\u6784\u5efa\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u4e0d\u540c\u53e5\u6cd5\u60c5\u5883\u4e0b\u7684\u6a21\u578b\u9519\u8bef\u8fdb\u884c\u4e86\u7ec6\u7c92\u5ea6\u5206\u6790\u3002\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6bd4\u8f83\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u6a21\u578b\u884c\u4e3a\u4e0e\u7279\u5b9a\u542f\u53d1\u5f0f\u89c4\u5219\uff08\u5982\u8bcd\u9891\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\uff09\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u800c\u975e\u9075\u5faa\u6cdb\u5316\u7684\u8bed\u6cd5\u89c4\u5219\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u7684\u4e0d\u540c\u9636\u6bb5\u4f1a\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u884c\u4e3a\u7279\u5f81\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6a21\u578b\u5728\u65e9\u671f\u9636\u6bb5\u7684\u884c\u4e3a\u66f4\u503e\u5411\u4e8e\u9075\u5faa\u8bcd\u9891\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u7b49\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u800c\u975e\u62bd\u8c61\u7684\u8bed\u6cd5\u89c4\u5219\u3002\u8fd9\u79cd\u73b0\u8c61\u5728\u7279\u5b9a\u53e5\u6cd5\u7ed3\u6784\u4e2d\u5c24\u4e3a\u660e\u663e\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u6df1\u5165\u7406\u89e3\u6a21\u578b\u4e2d\u95f4\u5b66\u4e60\u9636\u6bb5\u3001\u6574\u4f53\u8bad\u7ec3\u52a8\u6001\u4ee5\u53ca\u6240\u5b66\u4e60\u7684\u5177\u4f53\u6cdb\u5316\u89c4\u5219\u3002\u8be5\u65b9\u6cd5\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u6269\u5c55\u5230\u66f4\u591a\u7684\u8bed\u8a00\u73b0\u8c61\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u3002"}}
{"id": "2510.25504", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25504", "abs": "https://arxiv.org/abs/2510.25504", "authors": ["Oren Salzman", "Carlos Hern\u00e1ndez Ulloa", "Ariel Felner", "Sven Koenig"], "title": "Multi-Objective Search: Algorithms, Applications, and Emerging Directions", "comment": null, "summary": "Multi-objective search (MOS) has emerged as a unifying framework for planning\nand decision-making problems where multiple, often conflicting, criteria must\nbe balanced. While the problem has been studied for decades, recent years have\nseen renewed interest in the topic across AI applications such as robotics,\ntransportation, and operations research, reflecting the reality that real-world\nsystems rarely optimize a single measure. This paper surveys developments in\nMOS while highlighting cross-disciplinary opportunities, and outlines open\nchallenges that define the emerging frontier of MOS", "AI": {"tldr": "\u672c\u6587\u5bf9\u591a\u76ee\u6807\u641c\u7d22\uff08MOS\uff09\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8c03\u67e5\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u673a\u5668\u4eba\u3001\u4ea4\u901a\u548c\u8fd0\u7b79\u5b66\u7b49\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u4e2d\u7684\u65e5\u76ca\u589e\u957f\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u8de8\u5b66\u79d1\u673a\u4f1a\u548c\u672a\u6765\u7684\u7814\u7a76\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u5f88\u5c11\u53ea\u4f18\u5316\u4e00\u4e2a\u5355\u4e00\u6307\u6807\uff0cMOS \u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5e73\u8861\u591a\u4e2a\u901a\u5e38\u76f8\u4e92\u51b2\u7a81\u7684\u6807\u51c6\uff0c\u5728\u89c4\u5212\u548c\u51b3\u7b56\u95ee\u9898\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u672c\u6587\u5bf9\u591a\u76ee\u6807\u641c\u7d22\uff08MOS\uff09\u9886\u57df\u7684\u53d1\u5c55\u8fdb\u884c\u4e86\u8c03\u67e5\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u8de8\u5b66\u79d1\u7684\u673a\u4f1a\uff0c\u5e76\u6982\u8ff0\u4e86\u5b9a\u4e49 MOS \u65b0\u5174\u524d\u6cbf\u7684\u5f00\u653e\u6311\u6218\u3002", "result": "\u672c\u6587\u7684\u8c03\u67e5\u7ed3\u679c\u7a81\u663e\u4e86 MOS \u5728\u5404\u4e2a AI \u5e94\u7528\u9886\u57df\uff08\u5982\u673a\u5668\u4eba\u3001\u4ea4\u901a\u548c\u8fd0\u7b79\u5b66\uff09\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u65e5\u76ca\u589e\u957f\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u591a\u76ee\u6807\u641c\u7d22 (MOS) \u662f\u4e00\u4e2a\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u5177\u6709\u91cd\u5927\u7684\u8de8\u5b66\u79d1\u6f5c\u529b\uff0c\u672a\u6765\u9700\u8981\u89e3\u51b3\u4e00\u7cfb\u5217\u5f00\u653e\u6027\u6311\u6218\uff0c\u4ee5\u5145\u5206\u53d1\u6325\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u89c4\u5212\u548c\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.24940", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24940", "abs": "https://arxiv.org/abs/2510.24940", "authors": ["Yinhan He", "Wendy Zheng", "Yaochen Zhu", "Zaiyi Zheng", "Lin Su", "Sriram Vasudevan", "Qi Guo", "Liangjie Hong", "Jundong Li"], "title": "SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens", "comment": null, "summary": "The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment\nin efficiency-critical applications. Recently, implicit CoT approaches have\nemerged, which encode reasoning steps within LLM's hidden embeddings (termed\n``implicit reasoning'') rather than explicit tokens. This approach accelerates\nCoT by reducing the reasoning length and bypassing some LLM components.\nHowever, existing implicit CoT methods face two significant challenges: (1)\nthey fail to preserve the semantic alignment between the implicit reasoning\n(when transformed to natural language) and the ground-truth reasoning,\nresulting in a significant CoT performance degradation, and (2) they focus on\nreducing the length of the implicit reasoning; however, they neglect the\nconsiderable time cost for an LLM to generate one individual implicit reasoning\ntoken. To tackle these challenges, we propose a novel semantically-aligned\nimplicit CoT framework termed SemCoT. In particular, for the first challenge,\nwe design a contrastively trained sentence transformer that evaluates semantic\nalignment between implicit and explicit reasoning, which is used to enforce\nsemantic preservation during implicit reasoning optimization. To address the\nsecond challenge, we introduce an efficient implicit reasoning generator by\nfinetuning a lightweight language model using knowledge distillation. This\ngenerator is guided by our sentence transformer to distill ground-truth\nreasoning into semantically aligned implicit reasoning, while also optimizing\nfor accuracy. SemCoT is the first approach that enhances CoT efficiency by\njointly optimizing token-level generation speed and preserving semantic\nalignment with ground-truth reasoning. Extensive experiments demonstrate the\nsuperior performance of SemCoT compared to state-of-the-art methods in both\nefficiency and effectiveness. Our code can be found at\nhttps://github.com/YinhanHe123/SemCoT/.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSemCoT\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u751f\u6210\u901f\u5ea6\u548c\u4fdd\u6301\u4e0e\u771f\u5b9e\u63a8\u7406\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u6765\u63d0\u9ad8Chain-of-Thought\uff08CoT\uff09\u63a8\u7406\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9690\u5f0fCoT\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u751f\u6210\u901f\u5ea6\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u663e\u5f0fChain-of-Thought\uff08CoT\uff09\u63a8\u7406\u65b9\u6cd5\u7531\u4e8e\u5197\u957f\uff0c\u96be\u4ee5\u5728\u6548\u7387\u8981\u6c42\u9ad8\u7684\u5e94\u7528\u4e2d\u5927\u89c4\u6a21\u90e8\u7f72\u3002\u9690\u5f0fCoT\u65b9\u6cd5\u5c06\u63a8\u7406\u6b65\u9aa4\u7f16\u7801\u5230LLM\u7684\u9690\u85cf\u5d4c\u5165\u4e2d\uff0c\u4ee5\u7f29\u77ed\u63a8\u7406\u957f\u5ea6\u5e76\u7ed5\u8fc7\u90e8\u5206LLM\u7ec4\u4ef6\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u3002\u7136\u800c\uff0c\u73b0\u6709\u9690\u5f0fCoT\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u6311\u6218\uff1a1\uff09\u9690\u5f0f\u63a8\u7406\uff08\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u540e\uff09\u4e0e\u771f\u5b9e\u63a8\u7406\u4e4b\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\u6027\u5dee\uff0c\u5bfc\u81f4CoT\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b2\uff09\u867d\u7136\u5173\u6ce8\u7f29\u77ed\u63a8\u7406\u957f\u5ea6\uff0c\u4f46\u5ffd\u7565\u4e86LLM\u751f\u6210\u5355\u4e2a\u9690\u5f0f\u63a8\u7406token\u6240\u9700\u7684\u65f6\u95f4\u6210\u672c\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff0c\u7814\u7a76\u63d0\u51fa\u4e86SemCoT\u6846\u67b6\u3002\u9488\u5bf9\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u7684\u53e5\u5b50\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u8bc4\u4f30\u9690\u5f0f\u63a8\u7406\u548c\u663e\u5f0f\u63a8\u7406\u4e4b\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\u6027\uff0c\u5e76\u5728\u4f18\u5316\u9690\u5f0f\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u9488\u5bf9\u751f\u6210\u901f\u5ea6\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u9690\u5f0f\u63a8\u7406\u751f\u6210\u5668\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5fae\u8c03\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u3002\u8be5\u751f\u6210\u5668\u7531\u53e5\u5b50\u53d8\u6362\u5668\u6307\u5bfc\uff0c\u5c06\u771f\u5b9e\u63a8\u7406\u84b8\u998f\u6210\u8bed\u4e49\u5bf9\u9f50\u7684\u9690\u5f0f\u63a8\u7406\uff0c\u540c\u65f6\u4f18\u5316\u51c6\u786e\u6027\u3002SemCoT\u662f\u9996\u4e2a\u901a\u8fc7\u8054\u5408\u4f18\u5316token\u7ea7\u751f\u6210\u901f\u5ea6\u548c\u4e0e\u771f\u5b9e\u63a8\u7406\u7684\u8bed\u4e49\u5bf9\u9f50\u6027\u6765\u589e\u5f3aCoT\u6548\u7387\u7684\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cSemCoT\u5728\u6548\u7387\u548c\u6709\u6548\u6027\u4e24\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u548c\u6027\u80fd\u6bd4\u8f83\u5728\u539f\u6587\u4e2d\u6709\u8be6\u7ec6\u5c55\u793a\uff0c\u8868\u660eSemCoT\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "SemCoT\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u9690\u5f0fCoT\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u751f\u6210\u901f\u5ea6\u65b9\u9762\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u53e5\u5b50\u53d8\u6362\u5668\u548c\u9ad8\u6548\u7684\u9690\u5f0f\u63a8\u7406\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u4e86CoT\u63a8\u7406\u6548\u7387\u548c\u6548\u679c\u7684\u53cc\u91cd\u63d0\u5347\u3002\u8be5\u7814\u7a76\u4e3a\u63d0\u9ad8LLM\u5728\u6548\u7387\u654f\u611f\u573a\u666f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5c40\u9650\u6027\u548c\u672a\u6765\u5de5\u4f5c\u6709\u5f85\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2510.25510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25510", "abs": "https://arxiv.org/abs/2510.25510", "authors": ["Zekun Xu", "Siyu Xia", "Chuhuai Yue", "Jiajun Chai", "Mingxue Tian", "Xiaohan Wang", "Wei Lin", "Haoxuan Li", "Guojun Yin"], "title": "MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL", "comment": null, "summary": "As large language models (LLMs) are increasingly used in Text-to-SQL tasks,\nReinforcement Learning (RL) has become a common method for improving\nperformance. Existing methods primarily rely on static execution feedback,\nwhich restricts real-time error correction. However, integrating multi-turn\ntool invocation along with dynamic feedback could significantly improve\nadaptability and robustness, ultimately enhancing model performance. To address\nthese issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated\nReasoning reinforcement learning framework for Text-to-SQL. Our approach\nintroduces an execution-aware multi-turn reasoning paradigm that seamlessly\nincorporates database execution feedback at each reasoning step, enabling\ncontext-sensitive query generation and progressive refinement throughout the\nreasoning process. The framework extends the GRPO algorithm to accommodate\ncomplex multi-turn interaction scenarios. Considering the training instability\ncharacteristics of MTIR and the potential for significant Deviation of model\ndistribution from the initial model, we enhance the GRPO algorithm by adding a\ntrajectory filtering mechanism and removing KL loss constraints. Experimental\nresults demonstrate that MTIR-SQL, with 4B parameters, achieves \\textbf{64.4}\\%\naccuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,\nsignificantly outperforming existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMTIR-SQL\u7684\u591a\u8f6e\u5de5\u5177\u96c6\u6210\u63a8\u7406\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdbText-to-SQL\u4efb\u52a1\u7684\u6027\u80fd\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u4e2d\u6574\u5408\u6570\u636e\u5e93\u6267\u884c\u53cd\u9988\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u67e5\u8be2\u751f\u6210\u548c\u6e10\u8fdb\u5f0f\u7ec6\u5316\uff0c\u5e76\u6269\u5c55\u4e86GRPO\u7b97\u6cd5\u4ee5\u5904\u7406\u590d\u6742\u7684\u591a\u8f6e\u4ea4\u4e92\u3002\u5b9e\u9a8c\u8868\u660e\uff0cMTIR-SQL\u5728BIRD\u548cSPIDER\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684Text-to-SQL\u65b9\u6cd5\u5728\u5e94\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\uff0c\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u6267\u884c\u53cd\u9988\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u65f6\u7ea0\u9519\u80fd\u529b\u3002\u867d\u7136\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u548c\u52a8\u6001\u53cd\u9988\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u96c6\u6210\u8fd9\u4e9b\u5143\u7d20\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u6709\u5f85\u63d0\u5347\u3002", "method": "\u672c\u6587\u63d0\u51faMTIR-SQL\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u4e00\u4e2a\u6267\u884c\u611f\u77e5\u7684\u591a\u8f6e\u63a8\u7406\u8303\u5f0f\uff0c\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u4e2d\u65e0\u7f1d\u6574\u5408\u6570\u636e\u5e93\u6267\u884c\u53cd\u9988\uff0c\u4ece\u800c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u67e5\u8be2\u751f\u6210\u548c\u6e10\u8fdb\u5f0f\u7ec6\u5316\u3002\u8be5\u6846\u67b6\u6269\u5c55\u4e86GRPO\u7b97\u6cd5\uff0c\u5e76\u9488\u5bf9\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u8f68\u8ff9\u8fc7\u6ee4\u673a\u5236\u5e76\u79fb\u9664\u4e86KL\u635f\u5931\u7ea6\u675f\u3002\u5b9e\u9a8c\u5728BIRD\u548cSPIDER\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u6a21\u578b\u53c2\u6570\u91cf\u4e3a4B\u3002", "result": "\u5728BIRD Dev\u6570\u636e\u96c6\u4e0a\uff0cMTIR-SQL\u5b9e\u73b0\u4e8664.4%\u7684\u51c6\u786e\u7387\uff1b\u5728SPIDER Dev\u6570\u636e\u96c6\u4e0a\uff0c\u6267\u884c\u51c6\u786e\u7387\u8fbe\u5230\u4e8684.6%\u3002\u8fd9\u4e9b\u7ed3\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MTIR-SQL\u901a\u8fc7\u6574\u5408\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u548c\u52a8\u6001\u6267\u884c\u53cd\u9988\uff0c\u6709\u6548\u89e3\u51b3\u4e86Text-to-SQL\u4efb\u52a1\u4e2d\u7684\u5b9e\u65f6\u9519\u8bef\u6821\u6b63\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u590d\u6742\u7684\u5de5\u5177\u4ea4\u4e92\u548c\u63a8\u7406\u7b56\u7565\u3002"}}
{"id": "2510.24963", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24963", "abs": "https://arxiv.org/abs/2510.24963", "authors": ["James A. Michaelov", "Roger P. Levy", "Benjamin K. Bergen"], "title": "Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale", "comment": "To be presented at NeurIPS 2025", "summary": "We show that across architecture (Transformer vs. Mamba vs. RWKV), training\ndataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12\nbillion parameters), autoregressive language models exhibit highly consistent\npatterns of change in their behavior over the course of pretraining. Based on\nour analysis of over 1,400 language model checkpoints on over 110,000 tokens of\nEnglish, we find that up to 98% of the variance in language model behavior at\nthe word level can be explained by three simple heuristics: the unigram\nprobability (frequency) of a given word, the $n$-gram probability of the word,\nand the semantic similarity between the word and its context. Furthermore, we\nsee consistent behavioral phases in all language models, with their predicted\nprobabilities for words overfitting to those words' $n$-gram probabilities for\nincreasing $n$ over the course of training. Taken together, these results\nsuggest that learning in neural language models may follow a similar trajectory\nirrespective of model details.", "AI": {"tldr": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5728\u4e0d\u540c\u67b6\u6784\uff08Transformer\u3001Mamba\u3001RWKV\uff09\u3001\u8bad\u7ec3\u6570\u636e\u96c6\uff08OpenWebText\u3001The Pile\uff09\u548c\u6a21\u578b\u89c4\u6a21\uff081400\u4e07\u81f3120\u4ebf\u53c2\u6570\uff09\u4e0b\uff0c\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u7684\u884c\u4e3a\u53d8\u5316\u6a21\u5f0f\u3002\u901a\u8fc7\u5206\u6790\u8d85\u8fc71400\u4e2a\u8bed\u8a00\u6a21\u578b\u68c0\u67e5\u70b9\u548c110,000\u4e2a\u4ee5\u4e0a\u7684\u82f1\u8bed\u5355\u8bcd\uff0c\u53d1\u73b0\u9ad8\u8fbe98%\u7684\u5355\u8bcd\u7ea7\u522b\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u53d8\u5f02\u6027\u53ef\u4ee5\u7528\u4e09\u4e2a\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u89e3\u91ca\uff1a\u5355\u8bcd\u7684unigram\u6982\u7387\uff08\u9891\u7387\uff09\u3001n-gram\u6982\u7387\u4ee5\u53ca\u5355\u8bcd\u4e0e\u5176\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u3002\u6b64\u5916\uff0c\u6240\u6709\u8bed\u8a00\u6a21\u578b\u90fd\u5448\u73b0\u51fa\u4e00\u81f4\u7684\u884c\u4e3a\u9636\u6bb5\uff0c\u5176\u5bf9\u5355\u8bcd\u7684\u9884\u6d4b\u6982\u7387\u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\uff0c\u5bf9n\u503c\u4e0d\u65ad\u589e\u5927\u7684n-gram\u6982\u7387\u51fa\u73b0\u8fc7\u62df\u5408\u3002\u8fd9\u4e9b\u7ed3\u679c\u5171\u540c\u8868\u660e\uff0c\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u8f68\u8ff9\u53ef\u80fd\u5728\u4e0d\u540c\u6a21\u578b\u7ec6\u8282\u4e0b\u9075\u5faa\u76f8\u4f3c\u7684\u6a21\u5f0f\u3002", "motivation": "\u4e86\u89e3\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u7684\u5185\u5728\u673a\u5236\u5bf9\u4e8e\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5173\u6ce8\u7279\u5b9a\u6a21\u578b\u67b6\u6784\u6216\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u7684\u884c\u4e3a\uff0c\u7f3a\u4e4f\u5bf9\u5176\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u5b66\u4e60\u8f68\u8ff9\u4e00\u81f4\u6027\u7684\u7cfb\u7edf\u6027\u63a2\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\u7684\u666e\u9002\u6027\u89c4\u5f8b\uff0c\u4ece\u800c\u4e3a\u66f4\u9ad8\u6548\u3001\u66f4\u901a\u7528\u7684\u6a21\u578b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u8d85\u8fc71400\u4e2a\u4e0d\u540c\u67b6\u6784\uff08Transformer\u3001Mamba\u3001RWKV\uff09\u3001\u8bad\u7ec3\u6570\u636e\u96c6\uff08OpenWebText\u3001The Pile\uff09\u548c\u89c4\u6a21\uff081400\u4e07\u81f3120\u4ebf\u53c2\u6570\uff09\u7684\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u68c0\u67e5\u70b9\u3002\u901a\u8fc7\u5728110,000\u4e2a\u4ee5\u4e0a\u7684\u82f1\u6587\u5355\u8bcd\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7814\u7a76\u8005\u4eec\u91cf\u5316\u4e86\u6a21\u578b\u884c\u4e3a\u7684\u53d8\u5f02\u6027\uff0c\u5e76\u5c06\u5176\u4e0e\u4e09\u4e2a\u542f\u53d1\u5f0f\u6307\u6807\uff08\u5355\u8bcd\u7684unigram\u6982\u7387\u3001n-gram\u6982\u7387\u3001\u5355\u8bcd\u4e0e\u5176\u4e0a\u4e0b\u6587\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff09\u8fdb\u884c\u5173\u8054\u5206\u6790\u3002\u6b64\u5916\uff0c\u8fd8\u8003\u5bdf\u4e86\u6a21\u578b\u9884\u6d4b\u6982\u7387\u968f\u8bad\u7ec3\u8fdb\u7a0b\u5728\u4e0d\u540cn-gram\u6982\u7387\u4e0a\u7684\u884c\u4e3a\u53d8\u5316\uff0c\u4ee5\u8bc6\u522b\u4e00\u81f4\u7684\u884c\u4e3a\u9636\u6bb5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9ad8\u8fbe98%\u7684\u5355\u8bcd\u7ea7\u522b\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u53d8\u5f02\u6027\u53ef\u4ee5\u901a\u8fc7unigram\u6982\u7387\u3001n-gram\u6982\u7387\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u8fd9\u4e09\u4e2a\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u89e3\u91ca\u3002\u8fd9\u4e00\u53d1\u73b0\u8de8\u8d8a\u4e86\u4e0d\u540c\u7684\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u3002\u7814\u7a76\u8fd8\u89c2\u5bdf\u5230\u6240\u6709\u8bed\u8a00\u6a21\u578b\u90fd\u7ecf\u5386\u4e86\u76f8\u4f3c\u7684\u884c\u4e3a\u9636\u6bb5\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u5bf9\u5355\u8bcd\u7684\u9884\u6d4b\u6982\u7387\u4f1a\u8fc7\u5ea6\u62df\u5408\u5176n-gram\u6982\u7387\uff08\u968f\u7740n\u7684\u589e\u5927\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b66\u4e60\u8f68\u8ff9\u5177\u6709\u9ad8\u5ea6\u7684\u4e00\u81f4\u6027\uff0c\u5373\u4f7f\u5728\u4e0d\u540c\u7684\u67b6\u6784\u3001\u6570\u636e\u96c6\u548c\u89c4\u6a21\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u9ad8\u8fbe98%\u7684\u884c\u4e3a\u53d8\u5f02\u6027\u53ef\u7531\u7b80\u5355\u7684\u9891\u7387\u548c\u8bed\u4e49\u7279\u5f81\u89e3\u91ca\uff0c\u4e14\u6a21\u578b\u5b66\u4e60\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u884c\u4e3a\u9636\u6bb5\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u529b\u5730\u8868\u660e\uff0c\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u8fc7\u7a0b\u53ef\u80fd\u9075\u5faa\u7740\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4e8e\u5177\u4f53\u6a21\u578b\u7ec6\u8282\u7684\u666e\u904d\u8f68\u8ff9\uff0c\u4e3a\u672a\u6765\u7684\u6a21\u578b\u8bbe\u8ba1\u548c\u7406\u8bba\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.25517", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25517", "abs": "https://arxiv.org/abs/2510.25517", "authors": ["Elisabetta Gentili", "Tony Ribeiro", "Fabrizio Riguzzi", "Katsumi Inoue"], "title": "Predicate Renaming via Large Language Models", "comment": null, "summary": "In this paper, we address the problem of giving names to predicates in logic\nrules using Large Language Models (LLMs). In the context of Inductive Logic\nProgramming, various rule generation methods produce rules containing unnamed\npredicates, with Predicate Invention being a key example. This hinders the\nreadability, interpretability, and reusability of the logic theory. Leveraging\nrecent advancements in LLMs development, we explore their ability to process\nnatural language and code to provide semantically meaningful suggestions for\ngiving a name to unnamed predicates. The evaluation of our approach on some\nhand-crafted logic rules indicates that LLMs hold potential for this task.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u903b\u8f91\u89c4\u5219\u4e2d\u7684\u672a\u547d\u540d\u8c13\u8bcd\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u540d\u79f0\uff0c\u4ee5\u89e3\u51b3\u903b\u8f91\u7406\u8bba\u53ef\u8bfb\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u91cd\u7528\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u5728\u624b\u5de5\u8bbe\u8ba1\u7684\u903b\u8f91\u89c4\u5219\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u663e\u793a\u51faLLMs\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u6f5c\u529b\u3002", "motivation": "\u5728\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\uff08ILP\uff09\u7b49\u9886\u57df\uff0c\u89c4\u5219\u751f\u6210\u65b9\u6cd5\u5e38\u4ea7\u751f\u5305\u542b\u672a\u547d\u540d\u8c13\u8bcd\u7684\u903b\u8f91\u89c4\u5219\uff0c\u4f8b\u5982\u5728\u8c13\u8bcd\u53d1\u660e\uff08Predicate Invention\uff09\u4e2d\u3002\u8fd9\u4e25\u91cd\u5f71\u54cd\u4e86\u903b\u8f91\u7406\u8bba\u7684\u53ef\u8bfb\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u91cd\u7528\u6027\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u8fd9\u4e9b\u672a\u547d\u540d\u8c13\u8bcd\u63d0\u4f9b\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u540d\u79f0\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u548c\u4ee3\u7801\u5904\u7406\u65b9\u9762\u7684\u5148\u8fdb\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u4e3a\u903b\u8f91\u89c4\u5219\u4e2d\u7684\u672a\u547d\u540d\u8c13\u8bcd\u63d0\u4f9b\u8bed\u4e49\u547d\u540d\u5efa\u8bae\u7684\u53ef\u80fd\u6027\u3002\u901a\u8fc7\u5904\u7406\u548c\u7406\u89e3\u903b\u8f91\u89c4\u5219\u7684\u4e0a\u4e0b\u6587\uff0cLLMs\u88ab\u7528\u4e8e\u751f\u6210\u6709\u610f\u4e49\u7684\u8c13\u8bcd\u540d\u79f0\u3002", "result": "\u5728\u5bf9\u4e00\u4e9b\u624b\u5de5\u8bbe\u8ba1\u7684\u903b\u8f91\u89c4\u5219\u8fdb\u884c\u7684\u8bc4\u4f30\u8868\u660e\uff0cLLMs\u5728\u4e3a\u672a\u547d\u540d\u8c13\u8bcd\u63d0\u4f9b\u6709\u610f\u4e49\u540d\u79f0\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\u3002\u867d\u7136\u5177\u4f53\u6307\u6807\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e\uff0c\u4f46\u521d\u6b65\u7ed3\u679c\u652f\u6301\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u903b\u8f91\u89c4\u5219\u547d\u540d\u95ee\u9898\u4e0a\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u903b\u8f91\u7406\u8bba\u7684\u53ef\u8bfb\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u91cd\u7528\u6027\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22LLMs\u5728\u66f4\u590d\u6742\u548c\u5927\u89c4\u6a21\u903b\u8f91\u7406\u8bba\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u91cf\u5316\u8bc4\u4f30\u5176\u6027\u80fd\u3002"}}
{"id": "2510.25518", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25518", "abs": "https://arxiv.org/abs/2510.25518", "authors": ["Thomas Cook", "Richard Osuagwu", "Liman Tsatiashvili", "Vrynsia Vrynsia", "Koustav Ghosal", "Maraim Masoud", "Riccardo Mattivi"], "title": "Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation", "comment": "Keywords: RAG Agentic AI Fintech NLP KB Domain-Specific Ontology\n  Query Understanding", "summary": "Retrieval-Augmented Generation (RAG) systems often face limitations in\nspecialized domains such as fintech, where domain-specific ontologies, dense\nterminology, and acronyms complicate effective retrieval and synthesis. This\npaper introduces an agentic RAG architecture designed to address these\nchallenges through a modular pipeline of specialized agents. The proposed\nsystem supports intelligent query reformulation, iterative sub-query\ndecomposition guided by keyphrase extraction, contextual acronym resolution,\nand cross-encoder-based context re-ranking. We evaluate our approach against a\nstandard RAG baseline using a curated dataset of 85 question--answer--reference\ntriples derived from an enterprise fintech knowledge base. Experimental results\ndemonstrate that the agentic RAG system outperforms the baseline in retrieval\nprecision and relevance, albeit with increased latency. These findings suggest\nthat structured, multi-agent methodologies offer a promising direction for\nenhancing retrieval robustness in complex, domain-specific settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u91d1\u878d\u79d1\u6280\u7b49\u4e13\u4e1a\u9886\u57df\u7684 agentic RAG \u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7684\u7279 specialized agents \u6765\u514b\u670d\u4f20\u7edf RAG \u5728\u672f\u8bed\u3001\u7f29\u7565\u8bed\u7b49\u65b9\u9762\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u667a\u80fd\u67e5\u8be2\u91cd\u8ff0\u3001\u8fed\u4ee3\u5b50\u67e5\u8be2\u5206\u89e3\u3001\u4e0a\u4e0b\u6587\u7f29\u7565\u8bed\u89e3\u6790\u548c\u91cd\u6392\u5e8f\uff0c\u5728\u68c0\u7d22\u7cbe\u5ea6\u548c\u76f8\u5173\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4f46\u5ef6\u8fdf\u6709\u6240\u589e\u52a0\u3002", "motivation": "\u91d1\u878d\u79d1\u6280\u7b49\u4e13\u4e1a\u9886\u57df\u7531\u4e8e\u5176\u7279\u6709\u7684\u672c\u4f53\u3001\u5bc6\u96c6\u672f\u8bed\u548c\u7f29\u7565\u8bed\uff0c\u4f7f\u5f97\u4f20\u7edf\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u5728\u6709\u6548\u68c0\u7d22\u548c\u7efc\u5408\u4fe1\u606f\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u6027\u7684 RAG \u65b9\u6cd5\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd agentic RAG \u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5305\u542b\u4e00\u7cfb\u5217 specialized agents\uff0c\u80fd\u591f\u6267\u884c\u667a\u80fd\u67e5\u8be2\u91cd\u8ff0\u3001\u57fa\u4e8e\u5173\u952e\u8bcd\u63d0\u53d6\u7684\u8fed\u4ee3\u5b50\u67e5\u8be2\u5206\u89e3\u3001\u4e0a\u4e0b\u6587\u7f29\u7565\u8bed\u89e3\u6790\u4ee5\u53ca\u4f7f\u7528\u4ea4\u53c9\u7f16\u7801\u5668\u8fdb\u884c\u4e0a\u4e0b\u6587\u91cd\u6392\u5e8f\u3002\u5b9e\u9a8c\u5728\u4e00\u4e2a\u5305\u542b 85 \u4e2a\u95ee\u7b54-\u53c2\u8003\u4e09\u5143\u7ec4\u7684\u4f01\u4e1a\u91d1\u878d\u79d1\u6280\u77e5\u8bc6\u5e93\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u5e76\u4e0e\u6807\u51c6 RAG \u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6807\u51c6 RAG \u57fa\u7ebf\u76f8\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684 agentic RAG \u7cfb\u7edf\u5728\u68c0\u7d22\u7cbe\u5ea6\u548c\u76f8\u5173\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002\u7136\u800c\uff0c\u5176\u5904\u7406\u5ef6\u8fdf\u4e5f\u76f8\u5e94\u589e\u52a0\u3002\u8fd9\u4e00\u7ed3\u679c\u5728\u91d1\u878d\u79d1\u6280\u7b49\u9886\u57df\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u3001\u591a\u667a\u80fd\u4f53\u7684\uff08agentic\uff09\u65b9\u6cd5\u4e3a\u63d0\u9ad8\u590d\u6742\u3001\u9886\u57df\u7279\u5b9a\u73af\u5883\u4e0b\u7684\u68c0\u7d22\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002\u5c3d\u7ba1 agentic RAG \u5728\u7cbe\u5ea6\u548c\u76f8\u5173\u6027\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u4f46\u672a\u6765\u7684\u5de5\u4f5c\u9700\u8981\u5173\u6ce8\u5982\u4f55\u4f18\u5316\u5ef6\u8fdf\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u7cfb\u7edf\u3002"}}
{"id": "2510.25013", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25013", "abs": "https://arxiv.org/abs/2510.25013", "authors": ["Rabin Adhikari"], "title": "Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers", "comment": "9 pages, 10 figures", "summary": "Mechanistic interpretability aims to reverse-engineer large language models\n(LLMs) into human-understandable computational circuits. However, the\ncomplexity of pretrained models often obscures the minimal mechanisms required\nfor specific reasoning tasks. In this work, we train small, attention-only\ntransformers from scratch on a symbolic version of the Indirect Object\nIdentification (IOI) task -- a benchmark for studying coreference -- like\nreasoning in transformers. Surprisingly, a single-layer model with only two\nattention heads achieves perfect IOI accuracy, despite lacking MLPs and\nnormalization layers. Through residual stream decomposition, spectral analysis,\nand embedding interventions, we find that the two heads specialize into\nadditive and contrastive subcircuits that jointly implement IOI resolution.\nFurthermore, we show that a two-layer, one-head model achieves similar\nperformance by composing information across layers through query-value\ninteractions. These results demonstrate that task-specific training induces\nhighly interpretable, minimal circuits, offering a controlled testbed for\nprobing the computational foundations of transformer reasoning.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5728\u7b26\u53f7\u5316\u7684\u95f4\u63a5\u5bbe\u8bed\u8bc6\u522b\uff08IOI\uff09\u4efb\u52a1\u4e0a\u8bad\u7ec3\u5c0f\u578b\u3001\u4ec5\u6ce8\u610f\u529b\u673a\u5236\u7684Transformer\uff0c\u5373\u4f7f\u662f\u5355\u5c42\u3001\u53cc\u6ce8\u610f\u529b\u5934\u7684\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u5b8c\u7f8e\u7684\u51c6\u786e\u6027\u3002\u901a\u8fc7\u6b8b\u5dee\u6d41\u5206\u89e3\u3001\u8c31\u5206\u6790\u548c\u5d4c\u5165\u5e72\u9884\u7b49\u65b9\u6cd5\uff0c\u53d1\u73b0\u4e24\u4e2a\u6ce8\u610f\u529b\u5934\u5206\u522b\u5b9e\u73b0\u4e86\u52a0\u6cd5\u548c\u5bf9\u6bd4\u5b50\u7535\u8def\uff0c\u534f\u540c\u5b8c\u6210\u4e86IOI\u4efb\u52a1\u3002\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u4e00\u4e2a\u4e24\u5c42\u3001\u5355\u6ce8\u610f\u529b\u5934\u7684\u6a21\u578b\u901a\u8fc7\u8de8\u5c42\u67e5\u8be2-\u503c\u4ea4\u4e92\u4e5f\u80fd\u8fbe\u5230\u76f8\u4f3c\u7684\u6027\u80fd\u3002\u8fd9\u8bc1\u660e\u4e86\u4efb\u52a1\u7279\u5b9a\u7684\u8bad\u7ec3\u80fd\u591f\u8bf1\u5bfc\u51fa\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u3001\u6700\u5c0f\u5316\u7684\u8ba1\u7b97\u7535\u8def\uff0c\u4e3a\u7814\u7a76Transformer\u63a8\u7406\u7684\u8ba1\u7b97\u57fa\u7840\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53d7\u63a7\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5185\u90e8\u7684\u8ba1\u7b97\u673a\u5236\u6765\u7406\u89e3\u5176\u5de5\u4f5c\u539f\u7406\uff0c\u7136\u800c\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u590d\u6742\u6027\u5e38\u5e38\u963b\u788d\u6211\u4eec\u8bc6\u522b\u7279\u5b9a\u63a8\u7406\u4efb\u52a1\u6240\u9700\u7684\u6700\u5c0f\u5316\u673a\u5236\u3002\u672c\u7814\u7a76\u81f4\u529b\u4e8e\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\uff0c\u901a\u8fc7\u7b80\u5316\u6a21\u578b\u548c\u4efb\u52a1\u6765\u63a2\u7a76\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u672c\u7814\u7a76\u8bad\u7ec3\u4e86\u4ece\u5934\u5f00\u59cb\u7684\u3001\u4ec5\u5305\u542b\u6ce8\u610f\u529b\u673a\u5236\u7684\u5c0f\u578bTransformer\u6a21\u578b\uff0c\u5e76\u5728\u7b26\u53f7\u5316\u7684\u95f4\u63a5\u5bbe\u8bed\u8bc6\u522b\uff08IOI\uff09\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002IOI\u4efb\u52a1\u88ab\u7528\u4f5c\u7814\u7a76Transformer\u4e2d\u7c7b\u63a8\u7406\u7684\u6838\u5fc3\u57fa\u51c6\u3002\u7814\u7a76\u91c7\u7528\u4e86\u591a\u79cd\u5206\u6790\u6280\u672f\uff0c\u5305\u62ec\u6b8b\u5dee\u6d41\u5206\u89e3\u3001\u8c31\u5206\u6790\u548c\u5d4c\u5165\u5e72\u9884\uff0c\u4ee5\u63a2\u7a76\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u4e24\u5c42\u3001\u5355\u6ce8\u610f\u529b\u5934\u7684\u6a21\u578b\uff0c\u4ee5\u7814\u7a76\u4fe1\u606f\u5982\u4f55\u901a\u8fc7\u8de8\u5c42\u4ea4\u4e92\u8fdb\u884c\u7ec4\u5408\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e00\u4e2a\u5355\u5c42\u7684\u3001\u4ec5\u5305\u542b\u4e24\u4e2a\u6ce8\u610f\u529b\u5934\u7684\u6a21\u578b\u5728IOI\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u5b8c\u7f8e\u7684\u51c6\u786e\u6027\uff0c\u5373\u4f7f\u6ca1\u6709MLP\u548c\u5f52\u4e00\u5316\u5c42\u3002\u901a\u8fc7\u5206\u6790\u53d1\u73b0\uff0c\u8fd9\u4e24\u4e2a\u6ce8\u610f\u529b\u5934\u5206\u522b\u6f14\u5316\u6210\u4e86\u52a0\u6cd5\u548c\u5bf9\u6bd4\u5b50\u7535\u8def\uff0c\u534f\u540c\u5b8c\u6210\u4e86IOI\u4efb\u52a1\u7684\u89e3\u51b3\u3002\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e00\u4e2a\u4e24\u5c42\u3001\u5355\u6ce8\u610f\u529b\u5934\u7684\u6a21\u578b\u901a\u8fc7\u8de8\u5c42\u67e5\u8be2-\u503c\u4ea4\u4e92\uff0c\u4e5f\u80fd\u591f\u5b9e\u73b0\u4e0e\u5355\u5c42\u6a21\u578b\u76f8\u4f3c\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u4efb\u52a1\u7279\u5b9a\u7684\u8bad\u7ec3\u4e0b\uff0cTransformer\u6a21\u578b\u80fd\u591f\u5f62\u6210\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u3001\u6700\u5c0f\u5316\u7684\u8ba1\u7b97\u7535\u8def\u3002\u8fd9\u4e9b\u5c0f\u578b\u3001\u53d7\u63a7\u7684\u6a21\u578b\u4e3a\u6df1\u5165\u7406\u89e3Transformer\u63a8\u7406\u7684\u8ba1\u7b97\u57fa\u7840\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u60f3\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u53ef\u89e3\u91ca\u6027AI\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u5982\u4f55\u8bbe\u8ba1\u548c\u8bad\u7ec3\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u6a21\u578b\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.25528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25528", "abs": "https://arxiv.org/abs/2510.25528", "authors": ["Yuyuan Zeng", "Yufei Huang", "Can Xu", "Qingfeng Sun", "Jianfeng Yan", "Guanghui Xu", "Tao Yang", "Fengzong Lian"], "title": "Zero Reinforcement Learning Towards General Domains", "comment": null, "summary": "Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach\nfor enhancing the reasoning capabilities of large language models (LLMs) by\ndirectly applying reinforcement learning with verifiable rewards on pretrained\nmodels, without the need for a supervised fine-tuning phase. However, current\nresearch on zero-RL primarily focuses on domains with easily verifiable reward\nsignals, such as mathematics, programming, and other reasoning tasks. The\nchallenge of eliciting reasoning abilities in more diverse scenarios, where\nverification is not straightforward, remains underexplored. To address this\ngap, we propose a novel zero-RL paradigm designed to improve a model's\nreasoning ability across both verifiable and non-verifiable domains. By\ncombining verifiable rewards with a generative reward model, we conduct\nmulti-task zero-RL training across both domains, facilitating the transfer of\nreasoning capabilities between them. Furthermore, to mitigate reward hacking in\nthe generative reward model, we design a smooth length penalty that encourages\nthe generation of more comprehensive thinking tokens in general domains.\nExperimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our\napproach achieves superior reasoning performance, not only on tasks requiring\nextensive reasoning but also on more general tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u96f6\u5f3a\u5316\u5b66\u4e60\uff08Zero-RL\uff09\u8303\u5f0f\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u53ef\u9a8c\u8bc1\u548c\u4e0d\u53ef\u9a8c\u8bc1\u9886\u57df\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u901a\u8fc7\u7ed3\u5408\u53ef\u9a8c\u8bc1\u5956\u52b1\u548c\u751f\u6210\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u957f\u5ea6\u60e9\u7f5a\u6765\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u901a\u7528\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u96f6\u5f3a\u5316\u5b66\u4e60\uff08Zero-RL\uff09\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5956\u52b1\u4fe1\u53f7\u6613\u4e8e\u9a8c\u8bc1\u7684\u9886\u57df\uff08\u5982\u6570\u5b66\u3001\u7f16\u7a0b\uff09\uff0c\u800c\u5bf9\u4e8e\u5956\u52b1\u9a8c\u8bc1\u4e0d\u76f4\u63a5\u7684\u66f4\u5e7f\u6cdb\u573a\u666f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u7684\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\u3002\u8fd9\u9650\u5236\u4e86LLM\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u63a8\u7406\u5e94\u7528\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1Zero-RL\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u53ef\u9a8c\u8bc1\u5956\u52b1\u548c\u751f\u6210\u5956\u52b1\u6a21\u578b\u3002\u5728\u4e0d\u53ef\u9a8c\u8bc1\u9886\u57df\uff0c\u4f7f\u7528\u751f\u6210\u5956\u52b1\u6a21\u578b\u6765\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\uff0c\u5e76\u5728\u53ef\u9a8c\u8bc1\u548c\u4e0d\u53ef\u9a8c\u8bc1\u9886\u57df\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\u3002\u4e3a\u4e86\u9632\u6b62\u751f\u6210\u5956\u52b1\u6a21\u578b\u51fa\u73b0\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5e73\u6ed1\u957f\u5ea6\u60e9\u7f5a\u673a\u5236\uff0c\u9f13\u52b1\u6a21\u578b\u751f\u6210\u66f4\u5168\u9762\u7684\u601d\u8003\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u5728Qwen3-8B-Base\u548cQwen3-14B-Base\u6a21\u578b\u4e0a\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u4efb\u52a1\u548c\u66f4\u901a\u7528\u7684\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u63a8\u7406\u6027\u80fd\u3002\u6a21\u578b\u5728Qwen3-8B-Base\u548cQwen3-14B-Base\u4e0a\u7684\u8868\u73b0\u5747\u5f97\u5230\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8be5\u8303\u5f0f\u5728\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u6269\u5c55\u4e86Zero-RL\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u5956\u52b1\u9a8c\u8bc1\u4e0d\u76f4\u63a5\u7684\u9886\u57df\uff0c\u5e76\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u7c7b\u578b\u7684\u5956\u52b1\u4fe1\u53f7\u548c\u5f15\u5165\u957f\u5ea6\u60e9\u7f5a\uff0c\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u66f4\u590d\u6742\u548c\u591a\u6837\u5316\u573a\u666f\u4e2d\u5e94\u7528LLM\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2510.25054", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.25054", "abs": "https://arxiv.org/abs/2510.25054", "authors": ["Pedro Corr\u00eaa", "Jo\u00e3o Lima", "Victor Moreno", "Paula Dornhofer Paro Costa"], "title": "Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Advancements in spoken language processing have driven the development of\nspoken language models (SLMs), designed to achieve universal audio\nunderstanding by jointly learning text and audio representations for a wide\nrange of tasks. Although promising results have been achieved, there is growing\ndiscussion regarding these models' generalization capabilities and the extent\nto which they truly integrate audio and text modalities in their internal\nrepresentations. In this work, we evaluate four SLMs on the task of speech\nemotion recognition using a dataset of emotionally incongruent speech samples,\na condition under which the semantic content of the spoken utterance conveys\none emotion while speech expressiveness conveys another. Our results indicate\nthat SLMs rely predominantly on textual semantics rather than speech emotion to\nperform the task, indicating that text-related representations largely dominate\nover acoustic representations. We release both the code and the Emotionally\nIncongruent Synthetic Speech dataset (EMIS) to the community.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u56db\u79cd\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8bed\u97f3\u5185\u5bb9\u548c\u8868\u8fbe\u60c5\u611f\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u4e0b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\u800c\u975e\u8bed\u97f3\u8bed\u8c03\u6765\u8bc6\u522b\u60c5\u611f\uff0c\u8868\u660e\u6587\u672c\u8868\u793a\u538b\u5012\u4e86\u58f0\u5b66\u8868\u793a\u3002\u7814\u7a76\u8005\u53d1\u5e03\u4e86\u4ee3\u7801\u548cEMIS\u6570\u636e\u96c6\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u97f3\u9891\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u97f3\u9891\u548c\u6587\u672c\u8868\u5f81\u7684\u771f\u6b63\u878d\u5408\u7a0b\u5ea6\u4ecd\u5b58\u5728\u4e89\u8bae\u3002\u7279\u522b\u662f\u5728\u8bed\u97f3\u5185\u5bb9\u548c\u8868\u8fbe\u7684\u60c5\u611f\u4e0d\u4e00\u81f4\u65f6\uff0cSLMs\u7684\u8868\u73b0\u503c\u5f97\u5173\u6ce8\u3002", "method": "\u7814\u7a76\u8005\u4f7f\u7528\u5305\u542b\u60c5\u611f\u4e0d\u4e00\u81f4\u8bed\u97f3\u6837\u672c\u7684\u6570\u636e\u96c6\uff08EMIS\uff09\u6765\u8bc4\u4f30\u56db\u79cdSLMs\u5728\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u60c5\u611f\u4e0d\u4e00\u81f4\u662f\u6307\u8bed\u97f3\u7684\u8bed\u4e49\u5185\u5bb9\u8868\u8fbe\u4e00\u79cd\u60c5\u611f\uff0c\u800c\u8bed\u97f3\u7684\u8868\u8fbe\u65b9\u5f0f\uff08\u5982\u8bed\u8c03\uff09\u5219\u8868\u8fbe\u53e6\u4e00\u79cd\u60c5\u611f\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5728\u60c5\u611f\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u4e0b\uff0cSLMs\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u6765\u8bc6\u522b\u60c5\u611f\uff0c\u800c\u4e0d\u662f\u8bed\u97f3\u7684\u8868\u8fbe\u65b9\u5f0f\u3002\u8fd9\u8868\u660e\u5728\u8fd9\u4e9b\u6a21\u578b\u4e2d\uff0c\u4e0e\u6587\u672c\u76f8\u5173\u7684\u8868\u5f81\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4e3b\u5bfc\u4e86\u58f0\u5b66\u8868\u5f81\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u7684SLMs\u5728\u5904\u7406\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u65f6\uff0c\u5c24\u5176\u662f\u5728\u60c5\u611f\u4e0d\u4e00\u81f4\u7684\u6837\u672c\u4e0a\uff0c\u5b58\u5728\u660e\u663e\u7684\u5c40\u9650\u6027\uff0c\u5b83\u4eec\u66f4\u503e\u5411\u4e8e\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\u800c\u975e\u58f0\u5b66\u4fe1\u606f\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524dSLMs\u5728\u771f\u6b63\u878d\u5408\u97f3\u9891\u548c\u6587\u672c\u6a21\u6001\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002\u7814\u7a76\u8005\u516c\u5f00\u53d1\u5e03\u4e86\u76f8\u5173\u4ee3\u7801\u548cEMIS\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u6280\u672f\uff0c\u4ee5\u63d0\u5347SLMs\u5728\u590d\u6742\u60c5\u611f\u573a\u666f\u4e0b\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.25055", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25055", "abs": "https://arxiv.org/abs/2510.25055", "authors": ["Nourah M Salem", "Elizabeth White", "Michael Bada", "Lawrence Hunter"], "title": "GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models", "comment": null, "summary": "Scientific progress is driven by the deliberate articulation of what remains\nunknown. This study investigates the ability of large language models (LLMs) to\nidentify research knowledge gaps in the biomedical literature. We define two\ncategories of knowledge gaps: explicit gaps, clear declarations of missing\nknowledge; and implicit gaps, context-inferred missing knowledge. While prior\nwork has focused mainly on explicit gap detection, we extend this line of\nresearch by addressing the novel task of inferring implicit gaps. We conducted\ntwo experiments on almost 1500 documents across four datasets, including a\nmanually annotated corpus of biomedical articles. We benchmarked both\nclosed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)\nunder paragraph-level and full-paper settings. To address the reasoning of\nimplicit gaps inference, we introduce \\textbf{\\small TABI}, a Toulmin-Abductive\nBucketed Inference scheme that structures reasoning and buckets inferred\nconclusion candidates for validation. Our results highlight the robust\ncapability of LLMs in identifying both explicit and implicit knowledge gaps.\nThis is true for both open- and closed-weight models, with larger variants\noften performing better. This suggests a strong ability of LLMs for\nsystematically identifying candidate knowledge gaps, which can support\nearly-stage research formulation, policymakers, and funding decisions. We also\nreport observed failure modes and outline directions for robust deployment,\nincluding domain adaptation, human-in-the-loop verification, and benchmarking\nacross open- and closed-weight models.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bc6\u522b\u751f\u7269\u533b\u5b66\u6587\u732e\u4e2d\u7814\u7a76\u77e5\u8bc6\u5dee\u8ddd\u65b9\u9762\u7684\u80fd\u529b\u3002\u7814\u7a76\u533a\u5206\u4e86\u663e\u5f0f\u77e5\u8bc6\u5dee\u8ddd\uff08\u660e\u786e\u58f0\u660e\u7684\u7f3a\u5931\u77e5\u8bc6\uff09\u548c\u9690\u5f0f\u77e5\u8bc6\u5dee\u8ddd\uff08\u901a\u8fc7\u4e0a\u4e0b\u6587\u63a8\u65ad\u7684\u7f3a\u5931\u77e5\u8bc6\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTABI\uff08Toulmin-Abductive Bucketed Inference\uff09\u7684\u65b0\u63a8\u7406\u6846\u67b6\u6765\u89e3\u51b3\u9690\u5f0f\u77e5\u8bc6\u5dee\u8ddd\u7684\u8bc6\u522b\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLMs\uff0c\u5305\u62ec\u5f00\u653e\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u90fd\u80fd\u591f\u6709\u6548\u5730\u8bc6\u522b\u8fd9\u4e24\u79cd\u7c7b\u578b\u7684\u77e5\u8bc6\u5dee\u8ddd\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u8d8a\u5927\uff0c\u6027\u80fd\u8d8a\u597d\u3002", "motivation": "\u79d1\u5b66\u7814\u7a76\u7684\u8fdb\u6b65\u4f9d\u8d56\u4e8e\u660e\u786e\u672a\u77e5\u9886\u57df\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bc6\u522b\u751f\u7269\u533b\u5b66\u6587\u732e\u4e2d\u7684\u7814\u7a76\u77e5\u8bc6\u5dee\u8ddd\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4ee5\u671f\u4e3a\u65e9\u671f\u7814\u7a76\u89c4\u5212\u3001\u653f\u7b56\u5236\u5b9a\u548c\u8d44\u91d1\u5206\u914d\u63d0\u4f9b\u652f\u6301\u3002\u8be5\u7814\u7a76\u7279\u522b\u5173\u6ce8\u4e86\u663e\u5f0f\u77e5\u8bc6\u5dee\u8ddd\uff08\u660e\u786e\u58f0\u660e\u7684\u7f3a\u5931\u77e5\u8bc6\uff09\u548c\u9690\u5f0f\u77e5\u8bc6\u5dee\u8ddd\uff08\u901a\u8fc7\u4e0a\u4e0b\u6587\u63a8\u65ad\u7684\u7f3a\u5931\u77e5\u8bc6\uff09\uff0c\u5e76\u6269\u5c55\u4e86\u73b0\u6709\u7814\u7a76\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u66f4\u5177\u6311\u6218\u6027\u7684\u9690\u5f0f\u77e5\u8bc6\u5dee\u8ddd\u8bc6\u522b\u4efb\u52a1\u3002", "method": "\u672c\u7814\u7a76\u8fdb\u884c\u4e86\u4e24\u9879\u5b9e\u9a8c\uff0c\u4f7f\u7528\u4e86\u6db5\u76d6\u8fd11500\u7bc7\u6587\u6863\u7684\u56db\u4e2a\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u62ec\u4e00\u4e2a\u624b\u5de5\u6807\u6ce8\u7684\u751f\u7269\u533b\u5b66\u6587\u7ae0\u8bed\u6599\u5e93\u3002\u7814\u7a76\u4eba\u5458\u5bf9\u95ed\u6e90\u6a21\u578b\uff08\u5982OpenAI\u7684\u6a21\u578b\uff09\u548c\u5f00\u6e90\u6a21\u578b\uff08\u5982Llama\u548cGemma 2\uff09\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u6bb5\u843d\u7ea7\u548c\u5168\u6587\u7ea7\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002\u4e3a\u4e86\u89e3\u51b3\u9690\u5f0f\u77e5\u8bc6\u5dee\u8ddd\u7684\u63a8\u7406\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86TABI\uff08Toulmin-Abductive Bucketed Inference\uff09\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u65e8\u5728\u6784\u5efa\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u5c06\u63a8\u65ad\u51fa\u7684\u7ed3\u8bba\u5019\u9009\u8fdb\u884c\u5206\u7c7b\u4ee5\u4fbf\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u8bc6\u522b\u663e\u5f0f\u548c\u9690\u5f0f\u77e5\u8bc6\u5dee\u8ddd\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\u3002\u65e0\u8bba\u662f\u5f00\u6e90\u6a21\u578b\u8fd8\u662f\u95ed\u6e90\u6a21\u578b\uff0c\u5176\u8868\u73b0\u90fd\u76f8\u5f53\u7a33\u5065\uff0c\u5e76\u4e14\u901a\u5e38\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002\u8fd9\u8868\u660eLLMs\u5728\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u5019\u9009\u77e5\u8bc6\u5dee\u8ddd\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u4e3a\u7814\u7a76\u7684\u65e9\u671f\u5f62\u6210\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u8d44\u91d1\u51b3\u7b56\u63d0\u4f9b\u6709\u529b\u652f\u6301\u3002\u7814\u7a76\u8fd8\u62a5\u544a\u4e86\u89c2\u5bdf\u5230\u7684\u6a21\u578b\u5931\u6548\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u9762\u5411\u7a33\u5065\u90e8\u7f72\u7684\u65b9\u5411\uff0c\u5305\u62ec\u9886\u57df\u9002\u5e94\u3001\u4eba\u5de5\u5e72\u9884\u9a8c\u8bc1\u4ee5\u53ca\u5728\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u4e4b\u95f4\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bc6\u522b\u751f\u7269\u533b\u5b66\u6587\u732e\u4e2d\u7684\u663e\u5f0f\u548c\u9690\u5f0f\u77e5\u8bc6\u5dee\u8ddd\u65b9\u9762\u5177\u6709\u663e\u8457\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86TABI\u63a8\u7406\u6846\u67b6\u4ee5\u589e\u5f3a\u5bf9\u9690\u5f0f\u5dee\u8ddd\u7684\u8bc6\u522b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLMs\u80fd\u591f\u6709\u6548\u5730\u8f85\u52a9\u7814\u7a76\u89c4\u5212\u548c\u51b3\u7b56\uff0c\u4f46\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u9047\u5230\u7684\u95ee\u9898\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5982\u9886\u57df\u9002\u5e94\u3001\u4eba\u5de5\u9a8c\u8bc1\u548c\u8de8\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2510.25064", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25064", "abs": "https://arxiv.org/abs/2510.25064", "authors": ["Seonjeong Hwang", "Hyounghun Kim", "Gary Geunbae Lee"], "title": "Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?", "comment": null, "summary": "Estimating the cognitive complexity of reading comprehension (RC) items is\ncrucial for assessing item difficulty before it is administered to learners.\nUnlike syntactic and semantic features, such as passage length or semantic\nsimilarity between options, cognitive features that arise during answer\nreasoning are not readily extractable using existing NLP tools and have\ntraditionally relied on human annotation. In this study, we examine whether\nlarge language models (LLMs) can estimate the cognitive complexity of RC items\nby focusing on two dimensions-Evidence Scope and Transformation Level-that\nindicate the degree of cognitive burden involved in reasoning about the answer.\nOur experimental results demonstrate that LLMs can approximate the cognitive\ncomplexity of items, indicating their potential as tools for prior difficulty\nanalysis. Further analysis reveals a gap between LLMs' reasoning ability and\ntheir metacognitive awareness: even when they produce correct answers, they\nsometimes fail to correctly identify the features underlying their own\nreasoning process.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u4f30\u8ba1\u9605\u8bfb\u7406\u89e3\uff08RC\uff09\u9898\u76ee\u7684\u8ba4\u77e5\u590d\u6742\u5ea6\uff0c\u91cd\u70b9\u5173\u6ce8\u8bc1\u636e\u8303\u56f4\u548c\u8f6c\u6362\u7ea7\u522b\u4e24\u4e2a\u7ef4\u5ea6\u3002\u7ed3\u679c\u8868\u660eLLMs\u53ef\u4ee5\u6709\u6548\u4f30\u8ba1\u9898\u76ee\u96be\u5ea6\uff0c\u4f46\u5176\u63a8\u7406\u80fd\u529b\u4e0e\u5143\u8ba4\u77e5\u610f\u8bc6\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u9605\u8bfb\u7406\u89e3\u9898\u76ee\u7684\u96be\u5ea6\u8bc4\u4f30\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u8017\u65f6\u4e14\u6210\u672c\u9ad8\u3002\u73b0\u6709\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5de5\u5177\u96be\u4ee5\u63d0\u53d6\u9898\u76ee\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u8ba4\u77e5\u7279\u5f81\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5229\u7528LLMs\u81ea\u52a8\u5316\u8bc4\u4f30RC\u9898\u76ee\u8ba4\u77e5\u590d\u6742\u5ea6\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u8f85\u52a9\u8fdb\u884c\u9898\u76ee\u96be\u5ea6\u7684\u4e8b\u524d\u5206\u6790\u3002", "method": "\u7814\u7a76\u8005\u5229\u7528LLMs\u6765\u4f30\u8ba1RC\u9898\u76ee\u7684\u8ba4\u77e5\u590d\u6742\u5ea6\uff0c\u805a\u7126\u4e8e\u201c\u8bc1\u636e\u8303\u56f4\u201d\u548c\u201c\u8f6c\u6362\u7ea7\u522b\u201d\u4e24\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30LLMs\u5728\u8fd1\u4f3c\u9898\u76ee\u8ba4\u77e5\u590d\u6742\u5ea6\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLLMs\u80fd\u591f\u6709\u6548\u4f30\u8ba1RC\u9898\u76ee\u7684\u8ba4\u77e5\u590d\u6742\u5ea6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9898\u76ee\u96be\u5ea6\u4e8b\u524d\u5206\u6790\u65b9\u9762\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u7814\u7a76\u4e5f\u53d1\u73b0LLMs\u5728\u63a8\u7406\u80fd\u529b\u548c\u5143\u8ba4\u77e5\u610f\u8bc6\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u5373\u6a21\u578b\u5373\u4f7f\u80fd\u7ed9\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u4e5f\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u8bc6\u522b\u5176\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u7279\u5f81\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5c55\u793a\u4e86LLMs\u5728\u81ea\u52a8\u5316\u8bc4\u4f30RC\u9898\u76ee\u8ba4\u77e5\u590d\u6742\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u9898\u76ee\u96be\u5ea6\u7684\u4e8b\u524d\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002\u5c3d\u7ba1LLMs\u5728\u4f30\u8ba1\u8ba4\u77e5\u590d\u6742\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u80fd\u529b\uff0c\u4f46\u5176\u5143\u8ba4\u77e5\u610f\u8bc6\u7684\u4e0d\u8db3\u4ecd\u662f\u672a\u6765\u7814\u7a76\u9700\u8981\u5173\u6ce8\u7684\u65b9\u5411\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u63d0\u5347LLMs\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u4ee5\u53ca\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u6559\u80b2\u8bc4\u4f30\u9886\u57df\u3002"}}
{"id": "2510.25069", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25069", "abs": "https://arxiv.org/abs/2510.25069", "authors": ["Gabin Taibi", "Lucia Gomez"], "title": "TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors", "comment": "7 pages, 3 figures and 2 tables", "summary": "Traditional approaches to semantic polarity in computational linguistics\ntreat sentiment as a unidimensional scale, overlooking the multidimensional\nstructure of language. This work introduces TOPol (Topic-Orientation POLarity),\na semi-unsupervised framework for reconstructing and interpreting\nmultidimensional narrative polarity fields under human-on-the-loop (HoTL)\ndefined contextual boundaries (CBs). The framework embeds documents using a\ntransformer-based large language model (tLLM), applies neighbor-tuned UMAP\nprojection, and segments topics via Leiden partitioning. Given a CB between\ndiscourse regimes A and B, TOPol computes directional vectors between\ncorresponding topic-boundary centroids, yielding a polarity field that\nquantifies fine-grained semantic displacement during regime shifts. This\nvectorial representation enables assessing CB quality and detecting polarity\nchanges, guiding HoTL CB refinement. To interpret identified polarity vectors,\nthe tLLM compares their extreme points and produces contrastive labels with\nestimated coverage. Robustness analyses show that only CB definitions (the main\nHoTL-tunable parameter) significantly affect results, confirming methodological\nstability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches\naround a macroeconomic breakpoint, capturing non-affective semantic shifts, and\n(ii) Amazon product reviews across rating strata, where affective polarity\naligns with NRC valence. Results demonstrate that TOPol consistently captures\nboth affective and non-affective polarity transitions, providing a scalable,\ngeneralizable, and interpretable framework for context-sensitive\nmultidimensional discourse analysis.", "AI": {"tldr": "TOPol\u662f\u4e00\u4e2a\u534a\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4eba\u7c7b\u5728\u73af\uff08HoTL\uff09\u5b9a\u4e49\u7684\u4e0a\u4e0b\u6587\u8fb9\u754c\uff08CB\uff09\u4e0b\uff0c\u91cd\u5efa\u548c\u89e3\u91ca\u591a\u7ef4\u53d9\u4e8b\u6781\u6027\u573a\u3002\u5b83\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08tLLM\uff09\u5d4c\u5165\u6587\u6863\uff0c\u5e94\u7528\u90bb\u57df\u8c03\u6574\u7684UMAP\u6295\u5f71\uff0c\u5e76\u901a\u8fc7Leiden\u5206\u5272\u5bf9\u4e3b\u9898\u8fdb\u884c\u5206\u5272\u3002\u901a\u8fc7\u8ba1\u7b97\u4e3b\u9898-\u8fb9\u754c\u8d28\u5fc3\u4e4b\u95f4\u7684\u65b9\u5411\u5411\u91cf\uff0cTOPol\u53ef\u4ee5\u91cf\u5316\u5728\u653f\u6743\u8f6c\u6362\u671f\u95f4\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4f4d\u79fb\uff0c\u4ece\u800c\u80fd\u591f\u8bc4\u4f30CB\u8d28\u91cf\u5e76\u68c0\u6d4b\u6781\u6027\u53d8\u5316\u3002\u8be5\u6846\u67b6\u80fd\u591f\u6355\u6349\u60c5\u611f\u548c\u975e\u60c5\u611f\u7684\u6781\u6027\u8f6c\u53d8\uff0c\u5e76\u4e3a\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u591a\u7ef4\u8bdd\u8bed\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u63a8\u5e7f\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u7684\u60c5\u611f\u8ba1\u7b97\u65b9\u6cd5\u5c06\u60c5\u611f\u89c6\u4e3a\u5355\u4e00\u7ef4\u5ea6\uff0c\u5ffd\u7565\u4e86\u8bed\u8a00\u7684\u591a\u7ef4\u5ea6\u7ed3\u6784\u3002\u8fd9\u9879\u7814\u7a76\u65e8\u5728\u901a\u8fc7TOPol\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u91cd\u5efa\u548c\u89e3\u91ca\u591a\u7ef4\u53d9\u4e8b\u6781\u6027\u573a\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4f4d\u79fb\u7684\u91cf\u5316\u548c\u7406\u89e3\uff0c\u5c24\u5176\u662f\u5728\u8bed\u7bc7\u8f6c\u6362\u671f\u95f4\u3002", "method": "TOPol\u6846\u67b6\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08tLLM\uff09\u6765\u5d4c\u5165\u6587\u6863\u3002\u7136\u540e\uff0c\u5b83\u5e94\u7528\u90bb\u57df\u8c03\u6574\u7684UMAP\uff08\u7edf\u4e00\u6d41\u5f62\u8fd1\u4f3c\u4e0e\u6295\u5f71\uff09\u6280\u672f\u8fdb\u884c\u964d\u7ef4\uff0c\u5e76\u4f7f\u7528Leiden\u7b97\u6cd5\u5bf9\u4e3b\u9898\u8fdb\u884c\u5206\u5272\u3002\u7ed9\u5b9a\u8bed\u7bc7\u5236\u5ea6A\u548cB\u4e4b\u95f4\u7684\u4e0a\u4e0b\u6587\u8fb9\u754c\uff08CB\uff09\uff0cTOPol\u8ba1\u7b97\u76f8\u5e94\u4e3b\u9898-\u8fb9\u754c\u8d28\u5fc3\u4e4b\u95f4\u7684\u65b9\u5411\u5411\u91cf\uff0c\u5f62\u6210\u4e00\u4e2a\u6781\u6027\u573a\uff0c\u91cf\u5316\u5728\u653f\u6743\u8f6c\u6362\u671f\u95f4\u7684\u8bed\u4e49\u4f4d\u79fb\u3002tLLM\u7528\u4e8e\u6bd4\u8f83\u6781\u503c\u70b9\u4ee5\u89e3\u91ca\u6781\u6027\u5411\u91cf\uff0c\u5e76\u751f\u6210\u5e26\u6709\u4f30\u8ba1\u8986\u76d6\u8303\u56f4\u7684\u5bf9\u6bd4\u6807\u7b7e\u3002", "result": "TOPol\u6846\u67b6\u5728\u4e24\u4e2a\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff1a\uff081\uff09\u7f8e\u56fd\u8054\u90a6\u50a8\u5907\u94f6\u884c\u5728\u5b8f\u89c2\u7ecf\u6d4e\u65ad\u70b9\u671f\u95f4\u7684\u6f14\u8bb2\uff0c\u6355\u6349\u975e\u60c5\u611f\u8bed\u4e49\u53d8\u5316\uff1b\uff082\uff09\u4e9a\u9a6c\u900a\u4ea7\u54c1\u8bc4\u8bba\uff0c\u8de8\u8d8a\u4e0d\u540c\u7684\u8bc4\u5206\u7b49\u7ea7\uff0c\u5176\u4e2d\u60c5\u611f\u6781\u6027\u4e0eNRC\u6548\u4ef7\u5bf9\u9f50\u3002\u7ed3\u679c\u8868\u660e\uff0cTOPol\u80fd\u591f\u4e00\u81f4\u5730\u6355\u6349\u60c5\u611f\u548c\u975e\u60c5\u611f\u7684\u6781\u6027\u8f6c\u53d8\u3002\u654f\u611f\u6027\u5206\u6790\u8868\u660e\uff0c\u53ea\u6709CB\u7684\u5b9a\u4e49\uff08\u4e3b\u8981\u7684HoTL\u53ef\u8c03\u53c2\u6570\uff09\u4f1a\u663e\u8457\u5f71\u54cd\u7ed3\u679c\uff0c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "TOPol\u6846\u67b6\u4e3a\u591a\u7ef4\u53d9\u4e8b\u6781\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u63a8\u5e7f\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5b83\u80fd\u591f\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u60c5\u611f\u548c\u975e\u60c5\u611f\u7684\u8bed\u4e49\u4f4d\u79fb\uff0c\u5e76\u4e3a\u7406\u89e3\u8bed\u7bc7\u8f6c\u6362\u671f\u95f4\u7684\u8bed\u8a00\u53d8\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u4e0a\u4e0b\u6587\u8fb9\u754c\u5728\u5f15\u5bfc\u548c\u89e3\u91ca\u6781\u6027\u5206\u6790\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u4ee5\u63a2\u7d22\u66f4\u590d\u6742\u7684\u8bed\u7bc7\u7ed3\u6784\u548c\u591a\u6a21\u6001\u6570\u636e\u4e2d\u7684\u6781\u6027\u3002"}}
{"id": "2510.25668", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.25668", "abs": "https://arxiv.org/abs/2510.25668", "authors": ["Tianyu Yang", "Terry Ruas", "Yijun Tian", "Jan Philip Wahle", "Daniel Kurzawe", "Bela Gipp"], "title": "ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents", "comment": null, "summary": "Vision-language models (VLMs) excel at interpreting text-rich images but\nstruggle with long, visually complex documents that demand analysis and\nintegration of information spread across multiple pages. Existing approaches\ntypically rely on fixed reasoning templates or rigid pipelines, which force\nVLMs into a passive role and hinder both efficiency and generalization. We\npresent Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement\nlearning framework that fine-tunes VLMs as interactive agents capable of\nactively navigating long, visually rich documents. ALDEN introduces a novel\nfetch action that directly accesses the page by index, complementing the\nclassic search action and better exploiting document structure. For dense\nprocess supervision and efficient training, we propose a rule-based cross-level\nreward that provides both turn- and token-level signals. To address the\nempirically observed training instability caused by numerous visual tokens from\nlong documents, we further propose a visual-semantic anchoring mechanism that\napplies a dual-path KL-divergence constraint to stabilize visual and textual\nrepresentations separately during training. Trained on a corpus constructed\nfrom three open-source datasets, ALDEN achieves state-of-the-art performance on\nfive long-document benchmarks. Overall, ALDEN marks a step beyond passive\ndocument reading toward agents that autonomously navigate and reason across\nlong, visually rich documents, offering a robust path to more accurate and\nefficient long-document understanding.", "AI": {"tldr": "Vision-language models (VLMs) struggle with long, visually complex documents. We introduce ALDEN, a reinforcement learning framework where VLMs act as interactive agents, using a new ", "motivation": "Existing vision-language models (VLMs) are not adept at analyzing long, visually intricate documents where information is distributed across multiple pages. Current methods often use predefined reasoning templates or strict pipelines, limiting the VLM's role to a passive one and negatively impacting both efficiency and the ability to generalize. This research addresses the need for a more active and effective approach to long document understanding.", "method": "ALDEN is a multi-turn reinforcement learning framework that trains VLMs to be interactive agents for navigating long, visually rich documents. It introduces a novel 'fetch' action for direct page access by index, supplementing the standard 'search' action, thereby better utilizing document structure. For effective supervision and training, a rule-based cross-level reward is proposed, offering both turn-level and token-level signals. To mitigate training instability caused by the large number of visual tokens in long documents, a visual-semantic anchoring mechanism is implemented, using a dual-path KL-divergence constraint to stabilize visual and textual representations independently during training. The framework was trained on a corpus compiled from three open-source datasets.", "result": "ALDEN achieved state-of-the-art performance on five long-document benchmarks. The results demonstrate the effectiveness of the proposed framework in improving the ability of VLMs to handle complex, multi-page documents.", "conclusion": "ALDEN represents a significant advancement from passive document reading towards agents that can autonomously navigate and reason through lengthy, visually complex documents. This framework offers a promising direction for more accurate and efficient long-document comprehension, moving beyond current limitations."}}
{"id": "2510.25087", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25087", "abs": "https://arxiv.org/abs/2510.25087", "authors": ["Nourah M Salem", "Elizabeth White", "Michael Bada", "Lawrence Hunter"], "title": "BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs", "comment": null, "summary": "Coreference resolution in biomedical texts presents unique challenges due to\ncomplex domain-specific terminology, high ambiguity in mention forms, and\nlong-distance dependencies between coreferring expressions. In this work, we\npresent a comprehensive evaluation of generative large language models (LLMs)\nfor coreference resolution in the biomedical domain. Using the CRAFT corpus as\nour benchmark, we assess the LLMs' performance with four prompting experiments\nthat vary in their use of local, contextual enrichment, and domain-specific\ncues such as abbreviations and entity dictionaries. We benchmark these\napproaches against a discriminative span-based encoder, SpanBERT, to compare\nthe efficacy of generative versus discriminative methods. Our results\ndemonstrate that while LLMs exhibit strong surface-level coreference\ncapabilities, especially when supplemented with domain-grounding prompts, their\nperformance remains sensitive to long-range context and mentions ambiguity.\nNotably, the LLaMA 8B and 17B models show superior precision and F1 scores\nunder entity-augmented prompting, highlighting the potential of lightweight\nprompt engineering for enhancing LLM utility in biomedical NLP tasks.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u751f\u6210\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u7269\u533b\u5b66\u6587\u672c\u4e2d\u7684\u5171\u6307\u6d88\u89e3\u80fd\u529b\uff0c\u5e76\u4e0e\u5224\u522b\u5f0f\u6a21\u578bSpanBERT\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0cLLMs\u5728\u52a0\u5165\u9886\u57df\u7279\u5b9a\u63d0\u793a\uff08\u5982\u5b9e\u4f53\u8bcd\u5178\u548c\u7f29\u5199\uff09\u540e\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5171\u6307\u6d88\u89e3\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u63d0\u53ca\u6b67\u4e49\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002LLaMA 8B\u548c17B\u6a21\u578b\u5728\u5b9e\u4f53\u589e\u5f3a\u63d0\u793a\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u7cbe\u786e\u7387\u548cF1\u5206\u6570\uff0c\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u63d0\u793a\u5de5\u7a0b\u5728\u589e\u5f3a\u751f\u7269\u533b\u5b66NLP\u4efb\u52a1\u4e2dLLM\u6548\u7528\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6587\u672c\u4e2d\u7684\u5171\u6307\u6d88\u89e3\u9762\u4e34\u7740\u9886\u57df\u7279\u5b9a\u672f\u8bed\u590d\u6742\u3001\u6307\u79f0\u5f62\u5f0f\u6b67\u4e49\u6027\u9ad8\u4ee5\u53ca\u5171\u6307\u8868\u8fbe\u4e4b\u95f4\u5b58\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u7b49\u72ec\u7279\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u5168\u9762\u8bc4\u4f30\u751f\u6210\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528CRAFT\u8bed\u6599\u5e93\u4f5c\u4e3a\u57fa\u51c6\uff0c\u901a\u8fc7\u56db\u79cd\u4e0d\u540c\u7684\u63d0\u793a\u5b9e\u9a8c\u6765\u8bc4\u4f30LLMs\u7684\u6027\u80fd\u3002\u8fd9\u4e9b\u5b9e\u9a8c\u5728\u5c40\u90e8\u4fe1\u606f\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u5ea6\u4ee5\u53ca\u7f29\u5199\u548c\u5b9e\u4f53\u8bcd\u5178\u7b49\u9886\u57df\u7279\u5b9a\u7ebf\u7d22\u7684\u4f7f\u7528\u4e0a\u6709\u6240\u4e0d\u540c\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u5c06LLMs\u7684\u65b9\u6cd5\u4e0e\u5224\u522b\u5f0f\u57fa\u4e8e\u8de8\u5ea6\u7684\u7f16\u7801\u5668SpanBERT\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u8bc4\u4f30\u751f\u6210\u5f0f\u4e0e\u5224\u522b\u5f0f\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1LLMs\u5728\u52a0\u5165\u9886\u57df\u7279\u5b9a\u63d0\u793a\u540e\uff0c\u5c24\u5176\u662f\u5728\u589e\u5f3a\u4e86\u9886\u57df\u77e5\u8bc6\uff08\u5982\u5b9e\u4f53\u8bcd\u5178\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8868\u9762\u5171\u6307\u80fd\u529b\uff0c\u4f46\u5176\u6027\u80fd\u5728\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u63d0\u53ca\u6b67\u4e49\u65b9\u9762\u4ecd\u7136\u654f\u611f\u3002\u5177\u4f53\u800c\u8a00\uff0cLLaMA 8B\u548c17B\u6a21\u578b\u5728\u5b9e\u4f53\u589e\u5f3a\u63d0\u793a\u4e0b\uff0c\u5728\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\uff0c\u8fd9\u51f8\u663e\u4e86\u8f7b\u91cf\u7ea7\u63d0\u793a\u5de5\u7a0b\u5728\u63d0\u5347LLMs\u5728\u751f\u7269\u533b\u5b66\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u6548\u7528\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e86\u751f\u6210\u5f0fLLMs\u5728\u751f\u7269\u533b\u5b66\u5171\u6307\u6d88\u89e3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u53d6\u5f97\u4e86\u6709\u610f\u4e49\u7684\u53d1\u73b0\u3002\u867d\u7136LLMs\u5728\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u63d0\u793a\u65f6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f46\u5176\u5728\u5904\u7406\u590d\u6742\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002LLaMA 8B\u548c17B\u6a21\u578b\u5728\u7279\u5b9a\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u4f18\u5f02\u8868\u73b0\uff0c\u4e3a\u672a\u6765\u5728\u751f\u7269\u533b\u5b66NLP\u9886\u57df\u66f4\u6709\u6548\u5730\u5229\u7528LLMs\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5373\u901a\u8fc7\u7cbe\u5fc3\u7684\u63d0\u793a\u5de5\u7a0b\u6765\u5f25\u8865\u5176\u56fa\u6709\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u590d\u6742\u7684\u63d0\u793a\u7b56\u7565\u548c\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u5e94\u5bf9\u751f\u7269\u533b\u5b66\u6587\u672c\u4e2d\u66f4\u5177\u6311\u6218\u6027\u7684\u5171\u6307\u6d88\u89e3\u95ee\u9898\u3002"}}
{"id": "2510.25724", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25724", "abs": "https://arxiv.org/abs/2510.25724", "authors": ["Vanya Arikutharam", "Arkadiy Ukolov"], "title": "BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph", "comment": null, "summary": "Retrieval-Augmented Generation allows LLMs to access external knowledge,\nreducing hallucinations and ageing-data issues. However, it treats retrieved\nchunks independently and struggles with multi-hop or relational reasoning,\nespecially across documents. Knowledge graphs enhance this by capturing the\nrelationships between entities using triplets, enabling structured, multi-chunk\nreasoning. However, these tend to miss information that fails to conform to the\ntriplet structure. We introduce BambooKG, a knowledge graph with\nfrequency-based weights on non-triplet edges which reflect link strength,\ndrawing on the Hebbian principle of \"fire together, wire together\". This\ndecreases information loss and results in improved performance on single- and\nmulti-hop reasoning, outperforming the existing solutions.", "AI": {"tldr": "BambooKG\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u9891\u7387\u7684\u975e\u4e09\u5143\u7ec4\u8fb9\u6743\u91cd\u6765\u589e\u5f3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u89e3\u51b3\u4e86RAG\u5728\u591a\u8df3\u548c\u8de8\u6587\u6863\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u80fd\u6709\u6548\u5904\u7406\u975e\u4e09\u5143\u7ec4\u7ed3\u6784\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u5728\u5355\u8df3\u548c\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6a21\u578b\u867d\u7136\u80fd\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u51cf\u5c11\u5e7b\u89c9\u548c\u6570\u636e\u8fc7\u65f6\u95ee\u9898\uff0c\u4f46\u5728\u5904\u7406\u591a\u8df3\u6216\u8de8\u6587\u6863\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u5c06\u68c0\u7d22\u5230\u7684\u6587\u672c\u5757\u89c6\u4e3a\u72ec\u7acb\u5355\u5143\u3002\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u867d\u7136\u80fd\u901a\u8fc7\u4e09\u5143\u7ec4\u8868\u793a\u5b9e\u4f53\u95f4\u7684\u5173\u7cfb\uff0c\u4f46\u5bb9\u6613\u4e22\u5931\u975e\u7ed3\u6784\u5316\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51faBambooKG\uff0c\u4e00\u4e2a\u5728\u975e\u4e09\u5143\u7ec4\u8fb9\u4e0a\u5f15\u5165\u57fa\u4e8e\u9891\u7387\u7684\u6743\u91cd\uff0c\u4ee5\u53cd\u6620\u94fe\u63a5\u5f3a\u5ea6\uff0c\u501f\u9274\u4e86\u8d6b\u5e03\u5b66\u4e60\u89c4\u5219\u201c\u5171\u540c\u6fc0\u6d3b\u5219\u8fde\u63a5\u589e\u5f3a\u201d\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e8\u5728\u51cf\u5c11\u4fe1\u606f\u635f\u5931\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u5355\u8df3\u548c\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "BambooKG\u5728\u5355\u8df3\u548c\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8868\u660e\u5176\u5728\u5904\u7406\u66f4\u590d\u6742\u7684\u77e5\u8bc6\u63a8\u7406\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "BambooKG\u901a\u8fc7\u5728\u975e\u4e09\u5143\u7ec4\u8fb9\u4e0a\u5f15\u5165\u9891\u7387\u6743\u91cd\uff0c\u6709\u6548\u89e3\u51b3\u4e86RAG\u5728\u591a\u8df3\u548c\u8de8\u6587\u6863\u63a8\u7406\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u548c\u7ed3\u6784\u5316\u9650\u5236\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u4e3a\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u66f4\u5e7f\u6cdb\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5e94\u7528\u548c\u4f18\u5316\u3002"}}
{"id": "2510.25116", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25116", "abs": "https://arxiv.org/abs/2510.25116", "authors": ["Idriss Nguepi Nguefack", "Mara Finkelstein", "Toadoum Sari Sakayo"], "title": "Pretraining Strategies using Monolingual and Parallel Data for Low-Resource Machine Translation", "comment": "8 pages, 1. figure", "summary": "This research article examines the effectiveness of various pretraining\nstrategies for developing machine translation models tailored to low-resource\nlanguages. Although this work considers several low-resource languages,\nincluding Afrikaans, Swahili, and Zulu, the translation model is specifically\ndeveloped for Lingala, an under-resourced African language, building upon the\npretraining approach introduced by Reid and Artetxe (2021), originally designed\nfor high-resource languages. Through a series of comprehensive experiments, we\nexplore different pretraining methodologies, including the integration of\nmultiple languages and the use of both monolingual and parallel data during the\npretraining phase. Our findings indicate that pretraining on multiple languages\nand leveraging both monolingual and parallel data significantly enhance\ntranslation quality. This study offers valuable insights into effective\npretraining strategies for low-resource machine translation, helping to bridge\nthe performance gap between high-resource and low-resource languages. The\nresults contribute to the broader goal of developing more inclusive and\naccurate NLP models for marginalized communities and underrepresented\npopulations. The code and datasets used in this study are publicly available to\nfacilitate further research and ensure reproducibility, with the exception of\ncertain data that may no longer be accessible due to changes in public\navailability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u7279\u522b\u662f\u6797\u52a0\u62c9\u8bed\uff09\u7684\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u9884\u8bad\u7ec3\u7b56\u7565\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u591a\u8bed\u8a00\u548c\u5355/\u5e76\u884c\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u80fd\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u9ad8\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\u663e\u8457\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u8bed\u8a00\u7684NLP\u5e94\u7528\u53d7\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u6797\u52a0\u62c9\u8bed\uff09\u7684\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\uff0c\u4fc3\u8fdbNLP\u6280\u672f\u7684\u5305\u5bb9\u6027\u548c\u51c6\u786e\u6027\uff0c\u60e0\u53ca\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u793e\u7fa4\u3002", "method": "\u672c\u7814\u7a76\u57fa\u4e8eReid\u548cArtetxe\uff082021\uff09\u4e3a\u9ad8\u8d44\u6e90\u8bed\u8a00\u8bbe\u8ba1\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u6797\u52a0\u62c9\u8bed\u8fdb\u884c\u4e86\u6a21\u578b\u5f00\u53d1\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\uff1a1. \u6574\u5408\u591a\u79cd\u8bed\u8a00\u8fdb\u884c\u9884\u8bad\u7ec3\uff1b2. \u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u540c\u65f6\u4f7f\u7528\u5355\u8bed\u548c\u5e76\u884c\u6570\u636e\u3002\u5b9e\u9a8c\u7684\u5177\u4f53\u8bbe\u7f6e\u548c\u5bf9\u6bd4\u5206\u6790\u672a\u5728\u6458\u8981\u4e2d\u8be6\u8ff0\uff0c\u4f46\u5f3a\u8c03\u4e86\u5bf9\u4e0d\u540c\u9884\u8bad\u7ec3\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408\u4f7f\u7528\u5355\u8bed\u548c\u5e76\u884c\u6570\u636e\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6797\u52a0\u62c9\u8bed\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u7684\u7ffb\u8bd1\u8d28\u91cf\u3002\u867d\u7136\u5177\u4f53\u7684\u6027\u80fd\u6307\u6807\uff08\u5982BLEU\u5206\u6570\uff09\u672a\u5728\u6458\u8981\u4e2d\u63d0\u4f9b\uff0c\u4f46\u7814\u7a76\u660e\u786e\u6307\u51fa\u8fd9\u79cd\u9884\u8bad\u7ec3\u65b9\u6cd5\u5e26\u6765\u4e86\u201c\u663e\u8457\u63d0\u5347\u201d\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u8bc1\u660e\u4e86\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u7ed3\u5408\u5355/\u5e76\u884c\u6570\u636e\u662f\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u4ee5\u6797\u52a0\u62c9\u8bed\u4e3a\u4f8b\uff09\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\uff0c\u4e3a\u5f25\u5408\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u95f4\u7684\u6027\u80fd\u9e3f\u6c9f\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002\u7814\u7a76\u6210\u679c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u5e7f\u6cdb\u3001\u66f4\u51c6\u786e\u7684NLP\u6a21\u578b\u53d1\u5c55\uff0c\u670d\u52a1\u4e8e\u5f31\u52bf\u7fa4\u4f53\u3002\u7814\u7a76\u7684\u5c40\u9650\u6027\uff08\u5982\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u53d8\u5316\uff09\u548c\u672a\u6765\u5de5\u4f5c\u65b9\u5411\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u63d0\u53ca\uff0c\u4f46\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.25117", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25117", "abs": "https://arxiv.org/abs/2510.25117", "authors": ["Ruichen Qiu", "Jiajun Tan", "Jiayue Pu", "Honglin Wang", "Xiao-Shan Gao", "Fei Sun"], "title": "A Survey on Unlearning in Large Language Models", "comment": null, "summary": "The advancement of Large Language Models (LLMs) has revolutionized natural\nlanguage processing, yet their training on massive corpora poses significant\nrisks, including the memorization of sensitive personal data, copyrighted\nmaterial, and knowledge that could facilitate malicious activities. To mitigate\nthese issues and align with legal and ethical standards such as the \"right to\nbe forgotten\", machine unlearning has emerged as a critical technique to\nselectively erase specific knowledge from LLMs without compromising their\noverall performance. This survey provides a systematic review of over 180\npapers on LLM unlearning published since 2021, focusing exclusively on\nlarge-scale generative models. Distinct from prior surveys, we introduce novel\ntaxonomies for both unlearning methods and evaluations. We clearly categorize\nmethods into training-time, post-training, and inference-time based on the\ntraining stage at which unlearning is applied. For evaluations, we not only\nsystematically compile existing datasets and metrics but also critically\nanalyze their advantages, disadvantages, and applicability, providing practical\nguidance to the research community. In addition, we discuss key challenges and\npromising future research directions. Our comprehensive overview aims to inform\nand guide the ongoing development of secure and reliable LLMs.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86\u81ea2021\u5e74\u4ee5\u6765\u53d1\u8868\u7684\u8d85\u8fc7180\u7bc7\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u77e5\u8bc6\u9057\u5fd8\u7684\u8bba\u6587\uff0c\u91cd\u70b9\u5173\u6ce8\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u3002\u6587\u7ae0\u63d0\u51fa\u4e86\u65b0\u9896\u7684\u9057\u5fd8\u65b9\u6cd5\u548c\u8bc4\u4f30\u5206\u7c7b\u6cd5\uff0c\u5c06\u65b9\u6cd5\u5206\u4e3a\u8bad\u7ec3\u65f6\u3001\u8bad\u7ec3\u540e\u548c\u63a8\u7406\u65f6\u3002\u540c\u65f6\uff0c\u5bf9\u73b0\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u6307\u6807\u8fdb\u884c\u4e86\u6279\u5224\u6027\u5206\u6790\uff0c\u5e76\u8ba8\u8bba\u4e86\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\uff0c\u65e8\u5728\u6307\u5bfc\u5b89\u5168\u53ef\u9760\u7684LLM\u53d1\u5c55\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bad\u7ec3\u6d89\u53ca\u6d77\u91cf\u6570\u636e\uff0c\u53ef\u80fd\u5bfc\u81f4\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\u3001\u7248\u6743 material \u548c\u6f5c\u5728\u6076\u610f\u77e5\u8bc6\u7684\u8bb0\u5fc6\u3002\u4e3a\u4e86\u7b26\u5408\u201c\u88ab\u9057\u5fd8\u6743\u201d\u7b49\u6cd5\u5f8b\u548c\u4f26\u7406\u6807\u51c6\uff0c\u9700\u8981\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u6280\u672f\u6765\u9009\u62e9\u6027\u5730\u4eceLLMs\u4e2d\u64e6\u9664\u7279\u5b9a\u77e5\u8bc6\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u6a21\u578b\u6574\u4f53\u6027\u80fd\u3002\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3LLM\u77e5\u8bc6\u9057\u5fd8\u7684\u6311\u6218\uff0c\u4ee5\u786e\u4fdd\u5176\u5408\u89c4\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7cfb\u7edf\u6027\u56de\u987e\u7684\u65b9\u6cd5\uff0c\u6536\u96c6\u548c\u5206\u6790\u4e86\u81ea2021\u5e74\u4ee5\u6765\u53d1\u8868\u7684\u8d85\u8fc7180\u7bc7\u5173\u4e8eLLM\u77e5\u8bc6\u9057\u5fd8\u7684\u8bba\u6587\uff0c\u91cd\u70b9\u5173\u6ce8\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u3002\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5c06\u77e5\u8bc6\u9057\u5fd8\u6280\u672f\u6839\u636e\u5176\u5728\u8bad\u7ec3\u9636\u6bb5\u7684\u5e94\u7528\u5206\u4e3a\u8bad\u7ec3\u65f6\u3001\u8bad\u7ec3\u540e\u548c\u63a8\u7406\u65f6\u4e09\u7c7b\u3002\u6b64\u5916\uff0c\u5bf9\u73b0\u6709\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u6307\u6807\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u68b3\u7406\u548c\u6279\u5224\u6027\u5206\u6790\uff0c\u5e76\u8ba8\u8bba\u4e86\u76f8\u5173\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u8be5\u7efc\u8ff0\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u4e86LLM\u77e5\u8bc6\u9057\u5fd8\u7684\u73b0\u6709\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5bf9\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u3002\u6587\u7ae0\u8be6\u7ec6\u5206\u7c7b\u4e86\u9057\u5fd8\u65b9\u6cd5\uff0c\u5e76\u6279\u5224\u6027\u5730\u8bc4\u4f30\u4e86\u73b0\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u6307\u6807\u7684\u4f18\u7f3a\u70b9\u53ca\u9002\u7528\u6027\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002\u867d\u7136\u5177\u4f53\u6027\u80fd\u6307\u6807\u548c\u6bd4\u8f83\u672a\u5728\u6458\u8981\u4e2d\u8be6\u8ff0\uff0c\u4f46\u5176\u5168\u9762\u6027\u65e8\u5728\u4e3aLLM\u7684\u9057\u5fd8\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "\u672c\u7efc\u8ff0\u5168\u9762\u6982\u8ff0\u4e86LLM\u77e5\u8bc6\u9057\u5fd8\u7684\u9886\u57df\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5bf9\u73b0\u6709\u65b9\u6cd5\u3001\u8bc4\u4f30\u6280\u672f\u548c\u672a\u6765\u65b9\u5411\u7684\u6df1\u523b\u89c1\u89e3\u3002\u901a\u8fc7\u63d0\u51fa\u65b0\u9896\u7684\u5206\u7c7b\u6cd5\u548c\u6279\u5224\u6027\u5730\u5206\u6790\u8bc4\u4f30\u624b\u6bb5\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u4fc3\u8fdb\u66f4\u5b89\u5168\u3001\u66f4\u5408\u89c4\u3001\u66f4\u53ef\u9760\u7684LLM\u7684\u5f00\u53d1\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5728LLM\u53d1\u5c55\u4e2d\u89e3\u51b3\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\u7684\u7d27\u8feb\u6027\u548c\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.25150", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25150", "abs": "https://arxiv.org/abs/2510.25150", "authors": ["Shreyas Gopal", "Ashutosh Anshul", "Haoyang Li", "Yue Heng Yeo", "Hexin Liu", "Eng Siong Chng"], "title": "Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR", "comment": "Awarded Best Student Paper at APSIPA ASC 2025", "summary": "Discrete audio representations are gaining traction in speech modeling due to\ntheir interpretability and compatibility with large language models, but are\nnot always optimized for noisy or real-world environments. Building on existing\nworks that quantize Whisper embeddings for speech-to-unit modeling, we propose\ndisentangling semantic speech content from background noise in the latent\nspace. Our end-to-end model separates clean speech in the form of codebook\ntokens, while extracting interpretable noise vectors as quantization residue\nwhich are supervised via a lightweight classifier. We show that our approach\nimproves alignment between clean/noisy speech and text, producing speech tokens\nthat display a high degree of noiseinvariance, and improves ASR performance.\nKeeping Whisper frozen, we show an 82% reduction in error rate compared to\nWhisper, and 35% improvement over baseline methods on the VBDemand test set.\nFurther analyses show that the learned token space generalizes well to both\nseen and unseen acoustic conditions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u97f3\u9891\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5206\u79bb\u8bed\u4e49\u8bed\u97f3\u5185\u5bb9\u548c\u80cc\u666f\u566a\u58f0\uff0c\u751f\u6210\u5bf9\u566a\u58f0\u5177\u6709\u9ad8\u5ea6\u4e0d\u53d8\u6027\u7684\u8bed\u97f3\u5355\u5143\uff08codebook tokens\uff09\uff0c\u5e76\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u566a\u58f0\u5411\u91cf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6539\u8fdb\u8bed\u97f3\u5355\u5143\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u517c\u5bb9\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6027\u80fd\uff0c\u5728VBDemand\u6d4b\u8bd5\u96c6\u4e0a\u76f8\u8f83\u4e8e\u51bb\u7ed3\u7684Whisper\u6a21\u578b\u9519\u8bef\u7387\u964d\u4f4e\u4e8682%\uff0c\u76f8\u8f83\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e8635%\u3002", "motivation": "\u867d\u7136\u79bb\u6563\u97f3\u9891\u8868\u793a\u5728\u8bed\u97f3\u5efa\u6a21\u4e2d\u56e0\u5176\u53ef\u89e3\u91ca\u6027\u548c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u517c\u5bb9\u6027\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5b83\u4eec\u5728\u5608\u6742\u6216\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u8868\u73b0\u5e76\u4e0d\u7406\u60f3\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u5bf9Whisper\u5d4c\u5165\u8fdb\u884c\u91cf\u5316\u4ee5\u7528\u4e8e\u8bed\u97f3\u5230\u5355\u5143\u5efa\u6a21\uff0c\u4f46\u672a\u80fd\u6709\u6548\u89e3\u51b3\u566a\u58f0\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5206\u79bb\u8bed\u4e49\u8bed\u97f3\u5185\u5bb9\u548c\u80cc\u666f\u566a\u58f0\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6a21\u578b\u5c06\u5e72\u51c0\u8bed\u97f3\u8868\u793a\u4e3acodebook tokens\uff0c\u5e76\u5c06\u566a\u58f0\u63d0\u53d6\u4e3a\u91cf\u5316\u6b8b\u5dee\uff08\u53ef\u89e3\u91ca\u7684\u566a\u58f0\u5411\u91cf\uff09\u3002\u566a\u58f0\u5411\u91cf\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u8fdb\u884c\u76d1\u7763\u3002\u6574\u4e2a\u6a21\u578b\u5728\u4fdd\u6301Whisper\u6a21\u578b\u51bb\u7ed3\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5bf9\u566a\u58f0\u5177\u6709\u9ad8\u5ea6\u4e0d\u53d8\u6027\u7684\u8bed\u97f3\u5355\u5143\uff0c\u63d0\u9ad8\u4e86\u5e72\u51c0\u8bed\u97f3/\u5608\u6742\u8bed\u97f3\u4e0e\u6587\u672c\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86ASR\u6027\u80fd\u3002\u5728VBDemand\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u4e0e\u51bb\u7ed3\u7684Whisper\u6a21\u578b\u76f8\u6bd4\uff0c\u9519\u8bef\u7387\u964d\u4f4e\u4e8682%\uff1b\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6027\u80fd\u63d0\u5347\u4e8635%\u3002\u6b64\u5916\uff0c\u5b66\u4e60\u5230\u7684token\u7a7a\u95f4\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u5df2\u89c1\u548c\u672a\u89c1\u7684\u58f0\u5b66\u6761\u4ef6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5608\u6742\u73af\u5883\u4e2d\u63d0\u5347\u8bed\u97f3\u5355\u5143\u8868\u793a\u9c81\u68d2\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u566a\u58f0\u5b9e\u73b0\u4e86\u5bf9\u8bed\u97f3\u5185\u5bb9\u7684\u6709\u6548\u5efa\u6a21\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86ASR\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u590d\u6742\u7684\u566a\u58f0\u573a\u666f\uff0c\u4ee5\u53ca\u5c06\u8be5\u6280\u672f\u5e94\u7528\u4e8e\u5176\u4ed6\u97f3\u9891\u5904\u7406\u4efb\u52a1\u3002"}}
{"id": "2510.25160", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.25160", "abs": "https://arxiv.org/abs/2510.25160", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "Model-Document Protocol for AI Search", "comment": "10 pages", "summary": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u6a21\u578b-\u6587\u6863\u534f\u8bae\uff08MDP\uff09\uff0c\u4e00\u4e2a\u65e8\u5728\u5f25\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u975e\u7ed3\u6784\u5316\u5916\u90e8\u77e5\u8bc6\u6e90\u4e4b\u95f4\u5dee\u8ddd\u7684\u901a\u7528\u6846\u67b6\u3002MDP\u901a\u8fc7\u5c06\u539f\u59cb\u6587\u6863\u8f6c\u5316\u4e3a\u7279\u5b9a\u4efb\u52a1\u3001LLM\u5c31\u7eea\u7684\u77e5\u8bc6\u8868\u793a\uff0c\u6765\u91cd\u65b0\u5b9a\u4e49\u68c0\u7d22\u8fc7\u7a0b\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u83b7\u53d6\u539f\u59cb\u6587\u672c\u7247\u6bb5\u3002\u7814\u7a76\u901a\u8fc7\u4e00\u4e2a\u540d\u4e3aMDP-Agent\u7684\u5177\u4f53\u5b9e\u73b0\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684AI\u641c\u7d22\u65b9\u6cd5\u867d\u7136\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5916\u90e8\u77e5\u8bc6\u6e90\uff0c\u4f46\u5728\u5904\u7406\u7f51\u9875\u3001PDF\u7b49\u975eLLM\u5c31\u7eea\u7684\u539f\u59cb\u6587\u6863\u65f6\u5b58\u5728\u6311\u6218\u3002\u8fd9\u4e9b\u6587\u6863\u901a\u5e38\u5197\u957f\u3001\u6df7\u4e71\u4e14\u65e0\u7ed3\u6784\uff0c\u4f20\u7edf\u7684\u68c0\u7d22\u65b9\u6cd5\u4ec5\u8fd4\u56de\u539f\u59cb\u6587\u672c\u7247\u6bb5\uff0c\u5e76\u5c06\u540e\u7eed\u7684\u7247\u6bb5\u7ec4\u88c5\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u8d1f\u62c5\u7559\u7ed9\u4e86LLM\uff0c\u8fd9\u5728LLM\u4e0e\u6587\u6863\u7684\u4ea4\u4e92\u65b9\u5f0f\u4e0a\u5b58\u5728\u4e00\u4e2a\u660e\u663e\u7684\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u6a21\u578b-\u6587\u6863\u534f\u8bae\uff08MDP\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u539f\u59cb\u6587\u672c\u8fde\u63a5\u5230LLM\uff0c\u901a\u8fc7\u53ef\u6d88\u8d39\u7684\u77e5\u8bc6\u8868\u793a\u3002MDP\u5c06\u68c0\u7d22\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u79cd\u6587\u6863\u8f6c\u5316\u8fc7\u7a0b\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u7247\u6bb5\u83b7\u53d6\u3002\u5b83\u5b9a\u4e49\u4e86\u591a\u79cd\u5c06\u975e\u7ed3\u6784\u5316\u6587\u6863\u8f6c\u5316\u4e3a\u7279\u5b9a\u4efb\u52a1\u3001LLM\u5c31\u7eea\u8f93\u5165\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a1. Agentic Reasoning\uff1a\u7cbe\u9009\u539f\u59cb\u8bc1\u636e\uff0c\u5f62\u6210\u8fde\u8d2f\u7684\u4e0a\u4e0b\u6587\u30022. Memory Grounding\uff1a\u79ef\u7d2f\u53ef\u91cd\u7528\u7684\u7b14\u8bb0\u4ee5\u4e30\u5bcc\u63a8\u7406\u8fc7\u7a0b\u30023. Structured Leveraging\uff1a\u5c06\u6587\u6863\u7f16\u7801\u4e3a\u5982\u56fe\u5f62\u6216\u952e\u503c\u7f13\u5b58\u7b49\u5f62\u5f0f\u5316\u8868\u793a\u3002\u4f5c\u4e3aMDP\u534f\u8bae\u7684\u4e00\u4e2a\u5177\u4f53\u5b9e\u73b0\uff0c\u7814\u7a76\u63d0\u51fa\u4e86MDP-Agent\u3002\u5b83\u901a\u8fc7\u4e00\u4e2aagentic\u8fc7\u7a0b\u6765\u5b9e\u73b0\u534f\u8bae\uff1a\u6784\u5efa\u6587\u6863\u7ea7\u522b\u7684\u6838\u5fc3\u8bb0\u5fc6\u4ee5\u5b9e\u73b0\u5168\u5c40\u8986\u76d6\uff1b\u901a\u8fc7\u57fa\u4e8e\u6269\u6563\u7684\u63a2\u7d22\u548c\u5782\u76f4\u5229\u7528\u6765\u63ed\u793a\u5206\u5c42\u4f9d\u8d56\u5173\u7cfb\uff1b\u5e76\u5e94\u7528\u7c7b\u4f3cmap-reduce\u7684\u5408\u6210\u65b9\u6cd5\uff0c\u5c06\u5927\u89c4\u6a21\u8bc1\u636e\u6574\u5408\u4e3a\u7d27\u51d1\u4e14\u5145\u5206\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u4fe1\u606f\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMDP-Agent\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u8fd9\u9a8c\u8bc1\u4e86MDP\u6846\u67b6\u7684\u5408\u7406\u6027\u53ca\u5176agentic\u5b9e\u73b0\u7684\u6709\u6548\u6027\uff0c\u8868\u660eMDP\u80fd\u591f\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u5904\u7406\u548c\u5229\u7528\u975e\u7ed3\u6784\u5316\u6587\u6863\uff0c\u4e3aLLM\u63d0\u4f9b\u66f4\u76f4\u63a5\u3001\u66f4\u6709\u7528\u7684\u77e5\u8bc6\u3002", "conclusion": "\u6a21\u578b-\u6587\u6863\u534f\u8bae\uff08MDP\uff09\u4e3aLLM\u4e0e\u5916\u90e8\u77e5\u8bc6\u6e90\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff0c\u5b83\u5f3a\u8c03\u5c06\u539f\u59cb\u6587\u6863\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u4efb\u52a1\u5bfc\u5411\u7684\u77e5\u8bc6\u8868\u793a\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u68c0\u7d22\u539f\u59cb\u6587\u672c\u3002MDP\u6846\u67b6\u53ca\u5176MDP-Agent\u5b9e\u73b0\u5df2\u88ab\u8bc1\u660e\u80fd\u591f\u6709\u6548\u63d0\u5347\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u6784\u5efa\u66f4\u667a\u80fd\u3001\u66f4\u9ad8\u6548\u7684AI\u641c\u7d22\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u7814\u7a76\u4e3a\u5982\u4f55\u66f4\u597d\u5730\u6865\u63a5LLM\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.25187", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25187", "abs": "https://arxiv.org/abs/2510.25187", "authors": ["Ritesh Sunil Chavan", "Jack Mostow"], "title": "Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction", "comment": null, "summary": "While large language models are trained on massive datasets, this data is\nheavily skewed towards English. Does their impressive performance reflect\ngenuine ability or just this data advantage? To find out, we tested them in a\nsetting where they could not rely on data abundance: low-resource languages.\nBuilding on prior work Agarwal et al. (2025) that used Next Sentence Prediction\n(NSP) as a test, we created a large-scale benchmark with 10,000 questions each\nfor English (a high-resource language), Swahili (medium-resource), and Hausa\n(low-resource). We then tested several top models, including GPT-4 Turbo,\nGemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The\nresults painted a clear picture of how levels of language resources impact\noutcomes. While all models excelled in English, their accuracy dropped in\nSwahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story\nbecame even more interesting when we introduced Chain-of-Thought (CoT)\nprompting. For the struggling LLaMA 3, CoT acted as a helpful guide,\nsignificantly boosting its accuracy. However, for the more capable GPT-4 and\nGemini, the same technique often backfired, leading to a kind of \"overthinking\"\nthat hurt their results in the cross-lingual context. This reveals that\nChain-of-Thought is not a universal solution; its effectiveness depends heavily\non the model's baseline capability and the specific context of the task. Our\nframework pinpoints LLM weaknesses, highlights when CoT helps or hinders\ncross-lingual NSP performance, and factors influencing their decisions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u82f1\u8bed\u4ee5\u5916\u7684\u8bed\u8a00\u4e0a\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u532e\u4e4f\u7684\u8c6a\u8428\u8bed\u4e0a\u3002Chain-of-Thought\uff08CoT\uff09\u63d0\u793a\u80fd\u663e\u8457\u63d0\u5347\u80fd\u529b\u8f83\u5f31\u7684\u6a21\u578b\uff08\u5982LLaMA 3\uff09\u7684\u8868\u73b0\uff0c\u4f46\u53ef\u80fd\u635f\u5bb3\u80fd\u529b\u8f83\u5f3a\u7684\u6a21\u578b\uff08\u5982GPT-4\u548cGemini\uff09\u7684\u6027\u80fd\uff0c\u8fd9\u8868\u660eCoT\u7684\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u6a21\u578b\u7684\u57fa\u7ebf\u80fd\u529b\u548c\u4efb\u52a1\u7684\u7279\u5b9a\u8bed\u5883\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bad\u7ec3\u6570\u636e\u4e3b\u8981\u4ee5\u82f1\u8bed\u4e3a\u4e3b\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u5b83\u4eec\u7684\u51fa\u8272\u8868\u73b0\u7a76\u7adf\u662f\u6e90\u4e8e\u771f\u6b63\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u56e0\u4e3a\u5728\u6d77\u91cf\u82f1\u8bed\u6570\u636e\u4e0a\u7684\u4f18\u52bf\uff1f\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u4ee5\u8bc4\u4f30\u5176\u8de8\u8bed\u8a00\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u82f1\u8bed\uff08\u9ad8\u8d44\u6e90\uff09\u3001\u65af\u74e6\u5e0c\u91cc\u8bed\uff08\u4e2d\u7b49\u8d44\u6e90\uff09\u548c\u8c6a\u8428\u8bed\uff08\u4f4e\u8d44\u6e90\uff09\u4e09\u79cd\u8bed\u8a00\u7684\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bcf\u79cd\u8bed\u8a00\u5305\u542b10,000\u4e2a\u95ee\u9898\u3002\u4ed6\u4eec\u6d4b\u8bd5\u4e86\u5305\u62ecGPT-4 Turbo\u3001Gemini 1.5 Flash\u548cLLaMA 3 70B\u5728\u5185\u7684\u591a\u4e2a\u9876\u5c16\u6a21\u578b\u3002\u5b9e\u9a8c\u4e2d\u8fd8\u5f15\u5165\u4e86Chain-of-Thought\uff08CoT\uff09\u63d0\u793a\u6280\u672f\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u4e0d\u540c\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u5f71\u54cd\u3002", "result": "\u5728\u82f1\u8bed\u6d4b\u8bd5\u4e2d\uff0c\u6240\u6709\u6a21\u578b\u8868\u73b0\u4f18\u5f02\u3002\u7136\u800c\uff0c\u968f\u7740\u8bed\u8a00\u8d44\u6e90\u91cf\u7684\u51cf\u5c11\uff0c\u6a21\u578b\u5728\u65af\u74e6\u5e0c\u91cc\u8bed\u4e0a\u7684\u51c6\u786e\u7387\u6709\u6240\u4e0b\u964d\uff0c\u800c\u5728\u8c6a\u8428\u8bed\u4e0a\u5219\u6025\u5267\u4e0b\u964d\uff0c\u5176\u4e2dLLaMA 3 70B\u7684\u8868\u73b0\u6700\u4e3a\u6323\u624e\u3002\u5f15\u5165CoT\u63d0\u793a\u540e\uff0cLLaMA 3\u7684\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u4f46GPT-4 Turbo\u548cGemini 1.5 Flash\u7684\u51c6\u786e\u7387\u53cd\u800c\u6709\u6240\u4e0b\u964d\uff0c\u53ef\u80fd\u51fa\u73b0\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u73b0\u8c61\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cLLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u53d7\u5230\u8bed\u8a00\u8d44\u6e90\u91cf\u7684\u663e\u8457\u5f71\u54cd\u3002CoT\u63d0\u793a\u5e76\u975e\u4e07\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u6548\u679c\u56e0\u6a21\u578b\u7684\u57fa\u7ebf\u80fd\u529b\u548c\u4efb\u52a1\u7684\u7279\u5b9a\u8bed\u5883\u800c\u5f02\u3002\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u6709\u52a9\u4e8e\u8bc6\u522bLLM\u7684\u5f31\u70b9\uff0c\u9610\u660eCoT\u5728\u8de8\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5206\u6790\u5f71\u54cd\u6a21\u578b\u51b3\u7b56\u7684\u56e0\u7d20\u3002"}}
{"id": "2510.25273", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25273", "abs": "https://arxiv.org/abs/2510.25273", "authors": ["Sandipan Majhi", "Paheli Bhattacharya"], "title": "Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA", "comment": "Accepted at the Forum for Information Retrieval Evaluation 2025\n  (VATIKA Track)", "summary": "Domain-specific question answering in low-resource languages faces two key\nchallenges: scarcity of annotated datasets and limited domain knowledge in\ngeneral-purpose language models. In this work, we present a multi-stage\nfinetuning strategy to adapt lightweight language models to the Hindi tourism\ndomain by leveraging both original and synthetic training data. Synthetic\nquestion-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and\nused to augment the limited original dataset. We explore several training\nmethodologies and analyse their impact on domain generalisation. Our results\ndemonstrate that large models can efficiently generate synthetic data, while\nsmall models can effectively adapt to it, offering a scalable pathway for\nlow-resource, domain-specific QA.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff0c\u5229\u7528\u539f\u59cb\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u6765\u9002\u5e94\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u5230\u5370\u5730\u8bed\u65c5\u6e38\u9886\u57df\uff0c\u4ee5\u5e94\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u9886\u57df\u7279\u5b9a\u95ee\u7b54\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u77e5\u8bc6\u4e0d\u8db3\u7684\u6311\u6218\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u9886\u57df\u7279\u5b9a\u95ee\u7b54\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u6807\u6ce8\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u4ee5\u53ca\u901a\u7528\u8bed\u8a00\u6a21\u578b\u5728\u9886\u57df\u77e5\u8bc6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u9886\u57df\u7279\u5b9a\u95ee\u7b54\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u591a\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLaMA-70B, Phi-14B\uff09\u751f\u6210\u5408\u6210\u95ee\u7b54\u5bf9\uff0c\u4ee5\u6269\u5145\u6709\u9650\u7684\u539f\u59cb\u6570\u636e\u96c6\u3002\u7814\u7a76\u63a2\u7d22\u4e86\u51e0\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u5bf9\u9886\u57df\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u5730\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u800c\u5c0f\u578b\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u9002\u5e94\u8fd9\u4e9b\u6570\u636e\uff0c\u4e3a\u4f4e\u8d44\u6e90\u3001\u9886\u57df\u7279\u5b9a\u7684\u95ee\u7b54\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5c55\u793a\u4e86\u4e00\u79cd\u5229\u7528\u5408\u6210\u6570\u636e\u548c\u591a\u9636\u6bb5\u5fae\u8c03\u6765\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u9886\u57df\u7279\u5b9a\u95ee\u7b54\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.25303", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25303", "abs": "https://arxiv.org/abs/2510.25303", "authors": ["Soumyadeep Jana", "Sanasam Ranbir Singh"], "title": "Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student", "comment": null, "summary": "Multimodal sarcasm detection is challenging, especially in low-resource\nsettings where subtle image-text contradictions are hard to learn due to scarce\nannotated data, which hinders the model's performance. Parameter-efficient\nfine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce\noverfitting but struggle to reach optimal performance due to limited\nsupervision from few-shot data. We propose PEKD, a unified framework that\nenhances PEFT methods via distillation from an expert model trained on\nlarge-scale sarcasm data, which acts as the teacher. To mitigate unreliable\nsignals from the teacher, we introduce an entropy-aware gating mechanism that\ndynamically adjusts the distillation strength based on teacher confidence.\nExperiments on two public datasets demonstrate that our PEKD framework enables\nPEFT methods to outperform both prior parameter-efficient approaches and large\nmultimodal models, achieving strong results in the few-shot scenario. The\nframework is modular and adaptable to a wide range of multimodal models and\ntasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPEKD\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\u6765\u589e\u5f3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002PEKD\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u4e2a\u71b5\u611f\u77e5\u95e8\u63a7\u673a\u5236\uff0c\u4ee5\u52a8\u6001\u8c03\u6574\u84b8\u998f\u5f3a\u5ea6\uff0c\u4ece\u800c\u514b\u670d\u4e86\u6559\u5e08\u6a21\u578b\u4fe1\u53f7\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cPEKD\u663e\u8457\u63d0\u5347\u4e86PEFT\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u5728\u6570\u636e\u7a00\u758f\u7684\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u6781\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6a21\u578b\u96be\u4ee5\u5b66\u4e60\u5230\u7ec6\u5fae\u7684\u56fe\u50cf-\u6587\u672c\u77db\u76fe\u3002\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u867d\u7136\u80fd\u7f13\u89e3\u8fc7\u62df\u5408\uff0c\u4f46\u7531\u4e8e\u76d1\u7763\u4fe1\u53f7\u6709\u9650\uff0c\u5176\u6027\u80fd\u63d0\u5347\u7a7a\u95f4\u53d7\u9650\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347PEFT\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPEKD\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u673a\u5236\uff0c\u5229\u7528\u5728\u5927\u89c4\u6a21\u8bbd\u523a\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\uff08\u6559\u5e08\u6a21\u578b\uff09\u6765\u589e\u5f3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\uff08\u5982\u9002\u914d\u5668\u3001LoRA\u3001\u63d0\u793a\u8c03\u4f18\uff09\u3002\u4e3a\u4e86\u89e3\u51b3\u6559\u5e08\u6a21\u578b\u4fe1\u53f7\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0cPEKD\u5f15\u5165\u4e86\u4e00\u4e2a\u71b5\u611f\u77e5\u95e8\u63a7\u673a\u5236\uff0c\u8be5\u673a\u5236\u80fd\u591f\u6839\u636e\u6559\u5e08\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u52a8\u6001\u5730\u8c03\u6574\u84b8\u998f\u7684\u5f3a\u5ea6\u3002\u5b9e\u9a8c\u90e8\u5206\u5728\u4e24\u4e2a\u516c\u5f00\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5c06PEKD\u6846\u67b6\u4e0e\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u4ee5\u53ca\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPEKD\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u5347PEFT\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u91c7\u7528PEKD\u7684PEFT\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u6548\u679c\u3002\u8fd9\u8bc1\u660e\u4e86PEKD\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "PEKD\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u71b5\u611f\u77e5\u95e8\u63a7\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u4e2dPEFT\u65b9\u6cd5\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u5176\u80fd\u591f\u9002\u5e94\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u6a21\u578b\u548c\u4efb\u52a1\uff0c\u4e3a\u672a\u6765\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89e3\u51b3\u65b9\u6848\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u590d\u6742\u7684\u84b8\u998f\u7b56\u7565\u6216\u95e8\u63a7\u673a\u5236\uff0c\u4ee5\u53ca\u5728\u66f4\u591a\u6837\u7684\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u9a8c\u8bc1PEKD\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.25310", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25310", "abs": "https://arxiv.org/abs/2510.25310", "authors": ["Senjie Jin", "Lu Chen", "Zhiheng Xi", "Yuhui Wang", "Sirui Song", "Yuhao Zhou", "Xinbo Zhang", "Peng Sun", "Hong Lu", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language CoT for Reasoning", "comment": null, "summary": "Natural language chain-of-thought (N-CoT) and Program chain-of-thought\n(P-CoT) have emerged as two primary paradigms for large language models (LLMs)\nto solve mathematical reasoning problems. Current research typically endeavors\nto achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced\nP-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for\nmutual enhancement and ultimately achieve simultaneous improvements. We conduct\na detailed analysis of the error types across two paradigms, based on which we\npropose Parrot, a novel training pipeline for mathematical problems: 1) Three\ntarget-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A\nsubtask hybrid training strategy to facilitate natural language semantic\ntransferability. 3) The converted N-CoT auxiliary reward is designed to\nalleviate the sparse rewards in P-CoT optimization. Extensive experiments\ndemonstrate that Parrot significantly enhances both the performance of N-CoT\nand P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of\nLLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL\nbaseline, which is resource-intensive.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Parrot\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u601d\u7ef4\u94fe\uff08N-CoT\uff09\u548c\u7a0b\u5e8f\u601d\u7ef4\u94fe\uff08P-CoT\uff09\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u4e24\u8005\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u76f8\u4e92\u4fc3\u8fdb\u548c\u6027\u80fd\u63d0\u5347\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cParrot\u80fd\u591f\u663e\u8457\u63d0\u9ad8N-CoT\u548cP-CoT\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5bf9N-CoT\u7684\u63d0\u5347\u6548\u679c\u66f4\u4e3a\u660e\u663e\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u5411\u589e\u5f3aN-CoT\u6216P-CoT\uff0c\u800c\u5ffd\u89c6\u4e86\u4e24\u79cd\u8303\u5f0f\u76f8\u4e92\u4fc3\u8fdb\u7684\u6f5c\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u5145\u5206\u53d1\u6325N-CoT\u548cP-CoT\u5404\u81ea\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u53cc\u5411\u7684\u534f\u540c\u589e\u5f3a\uff0c\u4ece\u800c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u540c\u6b65\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aParrot\u7684\u65b0\u578b\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1. \u8bbe\u8ba1\u4e86\u4e09\u4e2a\u5b50\u4efb\u52a1\uff0c\u96c6\u6210\u4e86\u5e8f\u5217\u5316\u7684P-CoT\u548cN-CoT\u751f\u6210\u30022. \u91c7\u7528\u4e86\u5b50\u4efb\u52a1\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u81ea\u7136\u8bed\u8a00\u8bed\u4e49\u7684\u53ef\u8fc1\u79fb\u6027\u30023. \u8bbe\u8ba1\u4e86\u8f6c\u6362\u540e\u7684N-CoT\u8f85\u52a9\u5956\u52b1\u673a\u5236\uff0c\u4ee5\u7f13\u89e3P-CoT\u4f18\u5316\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898\u3002\u901a\u8fc7\u8fd9\u4e9b\u7b56\u7565\uff0cParrot\u65e8\u5728\u5b9e\u73b0N-CoT\u548cP-CoT\u7684\u6df1\u5ea6\u878d\u5408\u4e0e\u76f8\u4e92\u4fc3\u8fdb\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cParrot\u8bad\u7ec3\u6d41\u7a0b\u80fd\u591f\u663e\u8457\u63d0\u5347N-CoT\u548cP-CoT\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4f7f\u7528Parrot\u8fdb\u884c\u76d1\u7763\u5f0f\u5fae\u8c03\uff08SFT\uff09\u540e\uff0cLLaMA2\u548cCodeLLaMA\u5728MathQA\u6570\u636e\u96c6\u4e0a\u7684N-CoT\u6027\u80fd\u76f8\u6bd4\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u57fa\u7ebf\u5206\u522b\u63d0\u5347\u4e86+21.87%\u548c+21.48%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684Parrot\u8bad\u7ec3\u6d41\u7a0b\u6210\u529f\u5b9e\u73b0\u4e86N-CoT\u548cP-CoT\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u76f8\u4e92\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728N-CoT\u65b9\u9762\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u878d\u5408\u4e24\u79cd\u8303\u5f0f\u7684\u4f18\u52bf\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u6570\u5b66\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22Parrot\u5728\u5176\u4ed6\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4ee5\u53ca\u4f18\u5316\u5176\u8bad\u7ec3\u7b56\u7565\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.25356", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25356", "abs": "https://arxiv.org/abs/2510.25356", "authors": ["Abhishek Purushothama", "Junghyun Min", "Brandon Waldon", "Nathan Schneider"], "title": "Not ready for the bench: LLM legal interpretation is unstable and out of step with human judgments", "comment": null, "summary": "Legal interpretation frequently involves assessing how a legal text, as\nunderstood by an 'ordinary' speaker of the language, applies to the set of\nfacts characterizing a legal dispute in the U.S. judicial system. Recent\nscholarship has proposed that legal practitioners add large language models\n(LLMs) to their interpretive toolkit. This work offers an empirical argument\nagainst LLM interpretation as recently practiced by legal scholars and federal\njudges. Our investigation in English shows that models do not provide stable\ninterpretive judgments: varying the question format can lead the model to\nwildly different conclusions. Moreover, the models show weak to moderate\ncorrelation with human judgment, with large variance across model and question\nvariant, suggesting that it is dangerous to give much credence to the\nconclusions produced by generative AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6cd5\u5f8b\u89e3\u91ca\u4e2d\u7684\u5e94\u7528\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\uff0c\u4e0d\u540c\u63d0\u95ee\u65b9\u5f0f\u4f1a\u5bfc\u81f4\u6a21\u578b\u5f97\u51fa\u622a\u7136\u4e0d\u540c\u7684\u7ed3\u8bba\uff0c\u5e76\u4e14\u6a21\u578b\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u56e0\u6b64\u4e0d\u5efa\u8bae\u8fc7\u5ea6\u4f9d\u8d56\u751f\u6210\u5f0fAI\u7684\u6cd5\u5f8b\u89e3\u91ca\u7ed3\u679c\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u6cd5\u5b66\u754c\u548c\u53f8\u6cd5\u754c\u5f00\u59cb\u63a2\u7d22\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7eb3\u5165\u6cd5\u5f8b\u89e3\u91ca\u7684\u5de5\u5177\u7bb1\u3002\u7136\u800c\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u5bf9LLMs\u5728\u6cd5\u5f8b\u89e3\u91ca\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u51fa\u5b9e\u8bc1\u6027\u7684\u53cd\u5bf9\u610f\u89c1\uff0c\u63a2\u8ba8\u5176\u4f5c\u4e3a\u6cd5\u5f8b\u89e3\u91ca\u5de5\u5177\u7684\u53ef\u9760\u6027\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u82f1\u6587\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u6027\u5730\u6d4b\u8bd5\u4e86\u4e0d\u540c\u63d0\u95ee\u65b9\u5f0f\u5bf9LLM\u6cd5\u5f8b\u89e3\u91ca\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u4e86\u6a21\u578b\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\uff0c\u5206\u6790\u4e86\u6a21\u578b\u548c\u95ee\u9898\u53d8\u4f53\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u6cd5\u5f8b\u89e3\u91ca\u4e2d\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u6027\uff0c\u63d0\u95ee\u65b9\u5f0f\u7684\u5fae\u5c0f\u6539\u53d8\u5373\u53ef\u5bfc\u81f4\u6a21\u578b\u5f97\u51fa\u8fe5\u5f02\u7684\u7ed3\u8bba\u3002\u6b64\u5916\uff0c\u6a21\u578b\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u4ec5\u4e3a\u5f31\u5230\u4e2d\u7b49\u7a0b\u5ea6\uff0c\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u548c\u95ee\u9898\u53d8\u4f53\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u9274\u4e8eLLMs\u5728\u6cd5\u5f8b\u89e3\u91ca\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u5f31\u76f8\u5173\u6027\uff0c\u7814\u7a76\u8ba4\u4e3a\u8fc7\u5ea6\u4f9d\u8d56\u751f\u6210\u5f0fAI\u7684\u6cd5\u5f8b\u89e3\u91ca\u7ed3\u8bba\u662f\u5371\u9669\u7684\uff0c\u5bf9\u5176\u5728\u6cd5\u5f8b\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u63d0\u51fa\u4e86\u8b66\u793a\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u63d0\u9ad8\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.25364", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25364", "abs": "https://arxiv.org/abs/2510.25364", "authors": ["Luca Capone", "Alessandro Bondielli", "Alessandro Lenci"], "title": "CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs", "comment": "Paper accepted for oral presentation at the BabyLM Challange 2025\n  (EMNLP2025)", "summary": "This work investigates whether small-scale LMs can benefit from instruction\ntuning. We compare conversational and question-answering instruction tuning\ndatasets, applied either in a merged or sequential curriculum, using\ndecoder-only models with 100M and 140M parameters. Evaluation spans both\nfine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and\npsycholinguistic correlation) settings. Results show that instruction tuning\nyields small but consistent gains in fine-tuning scenarios, with sequential\ncurricula outperforming merged data; however, improvements do not consistently\ntransfer to zero-shot tasks, suggesting a trade-off between interaction-focused\nadaptation and broad linguistic generalization. These results highlight both\nthe potential and the constraints of adapting human-inspired learning\nstrategies to low-resource LMs, and point toward hybrid, curriculum-based\napproaches for enhancing generalization under ecological training limits.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08100M\u548c140M\u53c2\u6570\uff09\u53ef\u4ee5\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u83b7\u5f97\u5fae\u5c0f\u4f46\u7a33\u5b9a\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u7cbe\u8c03\u573a\u666f\u4e0b\uff0c\u987a\u5e8f\u8bfe\u7a0b\u5b66\u4e60\u4f18\u4e8e\u6df7\u5408\u6570\u636e\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u63d0\u5347\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u4e0d\u4e00\u81f4\uff0c\u8868\u660e\u4ea4\u4e92\u5f0f\u9002\u5e94\u4e0e\u5e7f\u6cdb\u8bed\u8a00\u6cdb\u5316\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08Small-scale LMs\uff09\u662f\u5426\u80fd\u4ece\u6307\u4ee4\u8c03\u4f18\uff08instruction tuning\uff09\u4e2d\u53d7\u76ca\uff0c\u4ee5\u4e86\u89e3\u5728\u6709\u9650\u8d44\u6e90\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u5730\u63d0\u5347\u6a21\u578b\u7684\u4ea4\u4e92\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u6a21\u578b\uff08low-resource LMs\uff09\u7684\u9002\u914d\u7b56\u7565\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86100M\u548c140M\u53c2\u6570\u7684\u4ec5\u89e3\u7801\u5668\u6a21\u578b\uff08decoder-only models\uff09\u3002\u5b9e\u9a8c\u5bf9\u6bd4\u4e86\u4e24\u79cd\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff08\u5bf9\u8bdd\u5f0f\u548c\u95ee\u7b54\u5f0f\uff09\u5728\u4e24\u79cd\u5e94\u7528\u65b9\u5f0f\uff08\u5408\u5e76\u6216\u987a\u5e8f\u8bfe\u7a0b\uff09\u4e0b\u7684\u6548\u679c\u3002\u8bc4\u4f30\u6db5\u76d6\u4e86\u7cbe\u8c03\uff08SuperGLUE\uff09\u548c\u96f6\u6837\u672c\uff08BLiMP, EWoK, WUGs, \u5b9e\u4f53\u8ffd\u8e2a\u548c\u5fc3\u7406\u8bed\u8a00\u5b66\u76f8\u5173\u6027\uff09\u4e24\u79cd\u8bbe\u7f6e\u3002", "result": "\u5728\u7cbe\u8c03\u573a\u666f\u4e0b\uff0c\u6307\u4ee4\u8c03\u4f18\u5e26\u6765\u4e86\u5fae\u5c0f\u4f46\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5176\u4e2d\u987a\u5e8f\u8bfe\u7a0b\u5b66\u4e60\u7684\u65b9\u6cd5\u4f18\u4e8e\u6df7\u5408\u6570\u636e\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u63d0\u5347\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u6307\u4ee4\u8c03\u4f18\u5bf9\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u8c03\u4efb\u52a1\u4e0a\u6709\u6548\uff0c\u4f46\u5bf9\u96f6\u6837\u672c\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u663e\u793a\u4e86\u4ea4\u4e92\u5f0f\u9002\u5e94\u4e0e\u5e7f\u6cdb\u8bed\u8a00\u6cdb\u5316\u4e4b\u95f4\u7684\u6743\u8861\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5c06\u4eba\u7c7b\u5b66\u4e60\u7b56\u7565\u5e94\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u6df7\u5408\u65b9\u6cd5\u53ef\u80fd\u6709\u52a9\u4e8e\u5728\u751f\u6001\u8bad\u7ec3\u9650\u5236\u4e0b\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.25370", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25370", "abs": "https://arxiv.org/abs/2510.25370", "authors": ["Alexander Sternfeld", "Andrei Kucharavy", "Dimitri Percia David", "Alain Mermoud", "Julian Jang-Jaccard", "Nathan Monnet"], "title": "Monitoring Transformative Technological Convergence Through LLM-Extracted Semantic Entity Triple Graphs", "comment": null, "summary": "Forecasting transformative technologies remains a critical but challenging\ntask, particularly in fast-evolving domains such as Information and\nCommunication Technologies (ICTs). Traditional expert-based methods struggle to\nkeep pace with short innovation cycles and ambiguous early-stage terminology.\nIn this work, we propose a novel, data-driven pipeline to monitor the emergence\nof transformative technologies by identifying patterns of technological\nconvergence.\n  Our approach leverages advances in Large Language Models (LLMs) to extract\nsemantic triples from unstructured text and construct a large-scale graph of\ntechnology-related entities and relations. We introduce a new method for\ngrouping semantically similar technology terms (noun stapling) and develop\ngraph-based metrics to detect convergence signals. The pipeline includes\nmulti-stage filtering, domain-specific keyword clustering, and a temporal trend\nanalysis of topic co-occurence.\n  We validate our methodology on two complementary datasets: 278,625 arXiv\npreprints (2017--2024) to capture early scientific signals, and 9,793 USPTO\npatent applications (2018-2024) to track downstream commercial developments.\nOur results demonstrate that the proposed pipeline can identify both\nestablished and emerging convergence patterns, offering a scalable and\ngeneralizable framework for technology forecasting grounded in full-text\nanalysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u56fe\u8c31\u5206\u6790\u6765\u76d1\u6d4b\u53d8\u9769\u6027\u6280\u672f\uff0c\u901a\u8fc7\u8bc6\u522b\u6280\u672f\u878d\u5408\u6a21\u5f0f\u6765\u9884\u6d4b\u65b0\u5174\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u4e13\u5bb6\u7684\u65b9\u6cd5\u5728\u9884\u6d4b\u4fe1\u606f\u901a\u4fe1\u6280\u672f\uff08ICTs\uff09\u7b49\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u4e2d\u7684\u53d8\u9769\u6027\u6280\u672f\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u96be\u4ee5\u8ddf\u4e0a\u77ed\u6682\u7684\u521b\u65b0\u5468\u671f\u548c\u65e9\u671f\u6a21\u7cca\u7684\u672f\u8bed\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u9a71\u52a8\u6d41\u6c34\u7ebf\uff0c\u8be5\u6d41\u6c34\u7ebf\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u8bed\u4e49\u4e09\u5143\u7ec4\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u6280\u672f\u76f8\u5173\u5b9e\u4f53\u548c\u5173\u7cfb\u56fe\u8c31\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e00\u79cd\u65b0\u7684\u6280\u672f\u672f\u8bed\u5206\u7ec4\u65b9\u6cd5\uff08\u540d\u8bcd\u62fc\u63a5\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u56fe\u8c31\u7684\u878d\u5408\u4fe1\u53f7\u68c0\u6d4b\u6307\u6807\u3002\u8be5\u6d41\u6c34\u7ebf\u8fd8\u5305\u62ec\u591a\u9636\u6bb5\u8fc7\u6ee4\u3001\u7279\u5b9a\u9886\u57df\u5173\u952e\u8bcd\u805a\u7c7b\u548c\u4e3b\u9898\u5171\u73b0\u7684\u65f6\u95f4\u8d8b\u52bf\u5206\u6790\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\uff08arXiv\u9884\u5370\u672c\u548cUSPTO\u4e13\u5229\u7533\u8bf7\uff09\u4e0a\u8fdb\u884c\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6d41\u6c34\u7ebf\u80fd\u591f\u8bc6\u522b\u5df2\u5efa\u7acb\u548c\u65b0\u5174\u7684\u6280\u672f\u878d\u5408\u6a21\u5f0f\u3002\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6280\u672f\u9884\u6d4b\u65b9\u9762\u7684\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\uff0c\u4e3a\u57fa\u4e8e\u5168\u6587\u5206\u6790\u7684\u6280\u672f\u9884\u6d4b\u63d0\u4f9b\u4e86\u6846\u67b6\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u76d1\u6d4b\u548c\u9884\u6d4b\u53d8\u9769\u6027\u6280\u672f\uff0c\u7279\u522b\u662f\u901a\u8fc7\u8bc6\u522b\u6280\u672f\u878d\u5408\u6a21\u5f0f\u3002\u8be5\u65b9\u6cd5\u5229\u7528LLMs\u548c\u56fe\u8c31\u5206\u6790\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u79d1\u5b66\u548c\u5546\u4e1a\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\uff0c\u4e3a\u672a\u6765\u7684\u6280\u672f\u9884\u6d4b\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.25413", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25413", "abs": "https://arxiv.org/abs/2510.25413", "authors": ["Shakib Yazdani", "Yasser Hamidullah", "Cristina Espa\u00f1a-Bonet", "Josef van Genabith"], "title": "Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media", "comment": "Accepted by RANLP 2025", "summary": "Most existing sign language translation (SLT) datasets are limited in scale,\nlack multilingual coverage, and are costly to curate due to their reliance on\nexpert annotation and controlled recording setup. Recently, Vision Language\nModels (VLMs) have demonstrated strong capabilities as evaluators and real-time\nassistants. Despite these advancements, their potential remains untapped in the\ncontext of sign language dataset acquisition. To bridge this gap, we introduce\nthe first automated annotation and filtering framework that utilizes VLMs to\nreduce reliance on manual effort while preserving data quality. Our method is\napplied to TikTok videos across eight sign languages and to the already curated\nYouTube-SL-25 dataset in German Sign Language for the purpose of additional\nevaluation. Our VLM-based pipeline includes a face visibility detection, a sign\nactivity recognition, a text extraction from video content, and a judgment step\nto validate alignment between video and text, implementing generic filtering,\nannotation and validation steps. Using the resulting corpus, TikTok-SL-8, we\nassess the performance of two off-the-shelf SLT models on our filtered dataset\nfor German and American Sign Languages, with the goal of establishing baselines\nand evaluating the robustness of recent models on automatically extracted,\nslightly noisy data. Our work enables scalable, weakly supervised pretraining\nfor SLT and facilitates data acquisition from social media.", "AI": {"tldr": "\u73b0\u6709\u7684\u6d77\u91cf\u624b\u8bed\u7ffb\u8bd1\uff08SLT\uff09\u6570\u636e\u96c6\u5728\u89c4\u6a21\u3001\u591a\u8bed\u8a00\u8986\u76d6\u4ee5\u53ca\u6807\u6ce8\u6210\u672c\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u81ea\u52a8\u6ce8\u91ca\u548c\u7b5b\u9009\u624b\u8bed\u6570\u636e\u7684\u6846\u67b6\uff0c\u4ee5\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002\u8be5\u6846\u67b6\u5e94\u7528\u4e8eTikTok\u89c6\u9891\u548cYouTube-SL-25\u6570\u636e\u96c6\uff0c\u521b\u5efa\u4e86\u5305\u542b\u516b\u79cd\u624b\u8bed\u7684TikTok-SL-8\u8bed\u6599\u5e93\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u8be5\u8bed\u6599\u5e93\u5728\u5fb7\u8bed\u548c\u7f8e\u56fd\u624b\u8bed\u4e0a\u7684SLT\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u3001\u5f31\u76d1\u7763\u7684SLT\u9884\u8bad\u7ec3\u548c\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u91c7\u96c6\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7684\u624b\u8bed\u7ffb\u8bd1\uff08SLT\uff09\u6570\u636e\u96c6\u9762\u4e34\u89c4\u6a21\u5c0f\u3001\u7f3a\u4e4f\u591a\u8bed\u8a00\u652f\u6301\u4ee5\u53ca\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u4e13\u5bb6\u6807\u6ce8\u548c\u53d7\u63a7\u7684\u5f55\u5236\u73af\u5883\u3002\u8fd9\u963b\u788d\u4e86\u5927\u89c4\u6a21SLT\u6a21\u578b\u7684\u53d1\u5c55\u548c\u591a\u8bed\u8a00\u624b\u8bed\u7684\u666e\u53ca\u3002\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8bc4\u4f30\u548c\u5b9e\u65f6\u8f85\u52a9\u65b9\u9762\u5c55\u73b0\u4e86\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u5728\u624b\u8bed\u6570\u636e\u96c6\u91c7\u96c6\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u6316\u6398\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u5229\u7528VLMs\u81ea\u52a8\u6ce8\u91ca\u548c\u7b5b\u9009\u624b\u8bed\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u4eba\u5de5\u6210\u672c\u5e76\u4fdd\u8bc1\u6570\u636e\u8d28\u91cf\uff0c\u4ece\u800c\u63a8\u52a8\u624b\u8bed\u7ffb\u8bd1\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVLMs\u7684\u81ea\u52a8\u5316\u6ce8\u91ca\u548c\u7b5b\u9009\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u4e2d\u63d0\u53d6\u548c\u9a8c\u8bc1\u624b\u8bed\u4fe1\u606f\u3002\u8be5\u6846\u67b6\u5305\u542b\u56db\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a1. \u9762\u90e8\u53ef\u89c1\u6027\u68c0\u6d4b\uff1a\u8bc6\u522b\u89c6\u9891\u4e2d\u662f\u5426\u6e05\u6670\u53ef\u89c1\u9762\u90e8\u30022. \u624b\u8bed\u6d3b\u52a8\u8bc6\u522b\uff1a\u68c0\u6d4b\u89c6\u9891\u4e2d\u662f\u5426\u5b58\u5728\u624b\u8bed\u52a8\u4f5c\u30023. \u6587\u672c\u63d0\u53d6\uff1a\u4ece\u89c6\u9891\u5185\u5bb9\u4e2d\u63d0\u53d6\u76f8\u5173\u7684\u6587\u672c\u4fe1\u606f\u30024. \u5224\u65ad\u9a8c\u8bc1\uff1a\u9a8c\u8bc1\u89c6\u9891\u4e2d\u7684\u624b\u8bed\u52a8\u4f5c\u4e0e\u63d0\u53d6\u7684\u6587\u672c\u4fe1\u606f\u662f\u5426\u5339\u914d\u3002\u8be5\u6846\u67b6\u88ab\u5e94\u7528\u4e8eTikTok\u89c6\u9891\uff08\u6db5\u76d6\u516b\u79cd\u624b\u8bed\uff09\u548cYouTube-SL-25\u6570\u636e\u96c6\uff08\u5fb7\u8bed\u624b\u8bed\uff09\u3002\u57fa\u4e8e\u5904\u7406\u540e\u7684TikTok-SL-8\u8bed\u6599\u5e93\uff0c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u6b3e\u73b0\u6210\u7684SLT\u6a21\u578b\u5728\u5fb7\u8bed\u548c\u7f8e\u56fd\u624b\u8bed\u4e0a\u7684\u6027\u80fd\uff0c\u65e8\u5728\u5efa\u7acb\u57fa\u7ebf\u5e76\u8bc4\u4f30\u6a21\u578b\u5728\u81ea\u52a8\u63d0\u53d6\u7684\u3001\u7565\u6709\u566a\u58f0\u7684\u6570\u636e\u4e0a\u7684\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aTikTok-SL-8\u7684\u5927\u89c4\u6a21\u624b\u8bed\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u516b\u79cd\u4e0d\u540c\u7684\u624b\u8bed\uff0c\u5e76\u4eceTikTok\u89c6\u9891\u548cYouTube-SL-25\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u4e86\u7ecf\u8fc7VLM\u7b5b\u9009\u548c\u6ce8\u91ca\u7684\u6570\u636e\u3002\u901a\u8fc7\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5bf9\u4e24\u6b3e\u73b0\u6709\u7684SLT\u6a21\u578b\u5728\u5fb7\u8bed\u548c\u7f8e\u56fd\u624b\u8bed\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u8bc4\u4f30\uff0c\u7814\u7a76\u4e3a\u8fd9\u4e9b\u6a21\u578b\u5728\u81ea\u52a8\u63d0\u53d6\u7684\u3001\u7565\u6709\u566a\u58f0\u7684\u6570\u636e\u4e0a\u7684\u8868\u73b0\u5efa\u7acb\u4e86\u57fa\u7ebf\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u5177\u4f53\u8bc4\u4f30\u6307\u6807\u548c\u6027\u80fd\u6bd4\u8f83\u5c06\u5728\u8bba\u6587\u7684\u8be6\u7ec6\u7ed3\u679c\u90e8\u5206\u5448\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u5229\u7528VLM\u5b9e\u73b0\u624b\u8bed\u6570\u636e\u96c6\u81ea\u52a8\u5316\u6ce8\u91ca\u548c\u7b5b\u9009\u7684\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u6240\u521b\u5efa\u7684TikTok-SL-8\u8bed\u6599\u5e93\u4e3a\u5927\u89c4\u6a21\u3001\u5f31\u76d1\u7763\u7684SLT\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u5e76\u5c55\u793a\u4e86\u4ece\u793e\u4ea4\u5a92\u4f53\u83b7\u53d6\u624b\u8bed\u6570\u636e\u7684\u53ef\u884c\u6027\u3002\u867d\u7136\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6570\u636e\u91c7\u96c6\u6548\u7387\uff0c\u4f46\u4ecd\u5b58\u5728\u6570\u636e\u566a\u58f0\u548c\u6807\u6ce8\u4e0d\u5b8c\u7f8e\u7684\u5c40\u9650\u6027\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u96c6\u4e2d\u4e8e\u8fdb\u4e00\u6b65\u63d0\u5347VLM\u7684\u51c6\u786e\u6027\u3001\u6269\u5c55\u652f\u6301\u7684\u624b\u8bed\u79cd\u7c7b\u4ee5\u53ca\u63a2\u7d22\u66f4\u590d\u6742\u7684\u89c6\u9891\u7406\u89e3\u6280\u672f\uff0c\u4ee5\u8fdb\u4e00\u6b65\u5b8c\u5584\u624b\u8bed\u7ffb\u8bd1\u6570\u636e\u96c6\u7684\u6784\u5efa\u3002"}}
{"id": "2510.25426", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25426", "abs": "https://arxiv.org/abs/2510.25426", "authors": ["Asutosh Hota", "Jussi P. P. Jokinen"], "title": "Implicature in Interaction: Understanding Implicature Improves Alignment in Human-LLM Interaction", "comment": "The manuscript is approximately 7360 words and contains 12 figures\n  and 6 tables", "summary": "The rapid advancement of Large Language Models (LLMs) is positioning language\nat the core of human-computer interaction (HCI). We argue that advancing HCI\nrequires attention to the linguistic foundations of interaction, particularly\nimplicature (meaning conveyed beyond explicit statements through shared\ncontext) which is essential for human-AI (HAI) alignment. This study examines\nLLMs' ability to infer user intent embedded in context-driven prompts and\nwhether understanding implicature improves response generation. Results show\nthat larger models approximate human interpretations more closely, while\nsmaller models struggle with implicature inference. Furthermore,\nimplicature-based prompts significantly enhance the perceived relevance and\nquality of responses across models, with notable gains in smaller models.\nOverall, 67.6% of participants preferred responses with implicature-embedded\nprompts to literal ones, highlighting a clear preference for contextually\nnuanced communication. Our work contributes to understanding how linguistic\ntheory can be used to address the alignment problem by making HAI interaction\nmore natural and contextually grounded.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4eba\u673a\u4ea4\u4e92\uff08HCI\uff09\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u7406\u89e3\u201c\u9690\u542b\u610f\u4e49\u201d\uff08implicature\uff09\u65b9\u9762\u3002\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u7406\u89e3\u9690\u542b\u610f\u4e49\u80fd\u663e\u8457\u63d0\u5347LLMs\u7684\u54cd\u5e94\u8d28\u91cf\u548c\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u5c24\u5176\u5bf9\u5c0f\u578b\u6a21\u578b\u6548\u679c\u66f4\u4f73\uff0c\u4e3a\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u5de5\u667a\u80fd\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u98de\u901f\u53d1\u5c55\uff0c\u8bed\u8a00\u5df2\u6210\u4e3a\u4eba\u673a\u4ea4\u4e92\uff08HCI\uff09\u7684\u6838\u5fc3\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4e0e\u4eba\u7c7b\uff08Human-AI, HAI\uff09\u7684\u5bf9\u9f50\uff08alignment\uff09\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u5728\u4e8eAI\u672a\u80fd\u5145\u5206\u7406\u89e3\u4eba\u7c7b\u4ea4\u4e92\u4e2d\u9690\u542b\u7684\u3001\u975e\u5b57\u9762\u610f\u4e49\u7684\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u7406\u89e3\u201c\u9690\u542b\u610f\u4e49\u201d\uff08implicature\uff09\u5bf9\u4e8e\u63d0\u5347HAI\u4ea4\u4e92\u8d28\u91cf\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662fAI\u80fd\u5426\u4ece\u4e0a\u4e0b\u6587\u9a71\u52a8\u7684\u63d0\u793a\u4e2d\u63a8\u65ad\u7528\u6237\u610f\u56fe\uff0c\u4ee5\u53ca\u8fd9\u79cd\u7406\u89e3\u80fd\u529b\u5bf9\u54cd\u5e94\u751f\u6210\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u8003\u5bdf\u4e86\u4e0d\u540c\u89c4\u6a21\u7684LLMs\u5728\u7406\u89e3\u5305\u542b\u9690\u542b\u610f\u4e49\u7684\u7528\u6237\u63d0\u793a\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u8bc4\u4f30\u4e86\u8fd9\u79cd\u7406\u89e3\u5bf9\u751f\u6210\u54cd\u5e94\u7684\u5f71\u54cd\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u6a21\u578b\u5bf9\u7528\u6237\u610f\u56fe\u7684\u63a8\u65ad\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u5305\u542b\u9690\u542b\u610f\u4e49\u7684\u63d0\u793a\u4e0b\uff0c\u54cd\u5e94\u7684\u8d28\u91cf\u548c\u7528\u6237\u504f\u597d\u3002\u5b9e\u9a8c\u8bbe\u7f6e\u5305\u62ec\u4f7f\u7528\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u5e76\u62db\u52df\u53c2\u4e0e\u8005\u5bf9\u5305\u542b\u9690\u542b\u610f\u4e49\u548c\u5b57\u9762\u610f\u4e49\u7684\u63d0\u793a\u6240\u4ea7\u751f\u7684\u54cd\u5e94\u8fdb\u884c\u8bc4\u4f30\u548c\u504f\u597d\u9009\u62e9\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u66f4\u5927\u89c4\u6a21\u7684LLMs\u5728\u89e3\u8bfb\u7528\u6237\u610f\u56fe\u548c\u9690\u542b\u610f\u4e49\u65b9\u9762\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u7406\u89e3\u6c34\u5e73\uff0c\u800c\u8f83\u5c0f\u89c4\u6a21\u7684\u6a21\u578b\u5219\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u8f83\u5dee\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5728\u63d0\u793a\u4e2d\u5d4c\u5165\u9690\u542b\u610f\u4e49\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u751f\u6210\u54cd\u5e94\u7684\u76f8\u5173\u6027\u548c\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c0f\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\uff0c\u8fd9\u79cd\u63d0\u5347\u6548\u679c\u66f4\u4e3a\u663e\u8457\u3002\u5728\u7528\u6237\u504f\u597d\u6d4b\u8bd5\u4e2d\uff0c67.6%\u7684\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u5305\u542b\u9690\u542b\u610f\u4e49\u63d0\u793a\u6240\u4ea7\u751f\u7684\u54cd\u5e94\uff0c\u800c\u975e\u5b57\u9762\u63d0\u793a\uff0c\u8fd9\u51f8\u663e\u4e86\u7528\u6237\u5bf9\u4e0a\u4e0b\u6587\u7ec6\u5fae\u5dee\u522b\u548c\u9690\u542b\u4fe1\u606f\u8fdb\u884c\u4ea4\u4e92\u7684\u660e\u786e\u504f\u597d\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u5728HAI\u4ea4\u4e92\u4e2d\u878d\u5165\u8bed\u8a00\u5b66\u7406\u8bba\uff0c\u7279\u522b\u662f\u5bf9\u9690\u542b\u610f\u4e49\u7684\u7406\u89e3\uff0c\u662f\u89e3\u51b3AI\u5bf9\u9f50\u95ee\u9898\u7684\u6709\u6548\u9014\u5f84\u3002\u901a\u8fc7\u4f7fHAI\u4ea4\u4e92\u66f4\u52a0\u81ea\u7136\u548c\u4ee5\u4e0a\u4e0b\u6587\u4e3a\u57fa\u7840\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u6765\u5229\u7528\u9690\u542b\u610f\u4e49\u4e5f\u80fd\u83b7\u5f97\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e0d\u540c\u7c7b\u578b\u7684\u9690\u542b\u610f\u4e49\uff0c\u4ee5\u53ca\u5f00\u53d1\u66f4\u6709\u6548\u7684\u673a\u5236\u6765\u589e\u5f3aLLMs\u5bf9\u9690\u542b\u4fe1\u606f\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u4ece\u800c\u63a8\u52a8\u66f4\u6df1\u5c42\u6b21\u3001\u66f4\u81ea\u7136\u7684 HAI \u4ea4\u4e92\u3002"}}
{"id": "2510.25427", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25427", "abs": "https://arxiv.org/abs/2510.25427", "authors": ["Auguste Poiroux", "Antoine Bosselut", "Viktor Kun\u010dak"], "title": "RLMEval: Evaluating Research-Level Neural Theorem Proving", "comment": "Accepted to EMNLP 2025 Findings. RLMEval benchmark released:\n  https://github.com/augustepoiroux/RLMEval", "summary": "Despite impressive results on curated benchmarks, the practical impact of\nlarge language models (LLMs) on research-level neural theorem proving and proof\nautoformalization is still limited. We introduce RLMEval, an evaluation suite\nfor these tasks, focusing on research-level mathematics from real-world Lean\nformalization projects. RLMEval targets the evaluation of neural theorem\nproving and proof autoformalization on challenging research-level theorems by\nleveraging real Lean Blueprint formalization projects. Our evaluation of\nstate-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean\nprojects, reveals a significant gap: progress on existing benchmarks does not\nreadily translate to these more realistic settings, with the best model\nachieving only a 10.3 % pass rate. RLMEval provides a new, challenging\nbenchmark designed to guide and accelerate progress in automated reasoning for\nformal mathematics.", "AI": {"tldr": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\uff0c\u4f46\u5b83\u4eec\u5728\u7814\u7a76\u7ea7\u795e\u7ecf\u5b9a\u7406\u8bc1\u660e\u548c\u8bc1\u660e\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u9762\u7684\u5b9e\u9645\u5f71\u54cd\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u63d0\u51fa\u4e86RLMEval\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u771f\u5b9e\u4e16\u754cLean\u5f62\u5f0f\u5316\u9879\u76ee\u4e2d\u7814\u7a76\u7ea7\u6570\u5b66\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u524d\u6cbf\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u57fa\u51c6\u4e0a\u7684\u8fdb\u5c55\u5e76\u672a\u80fd\u6709\u6548\u8f6c\u5316\u4e3a\u8fd9\u4e9b\u66f4\u73b0\u5b9e\u7684\u573a\u666f\uff0c\u6700\u4f73\u6a21\u578b\u7684\u901a\u8fc7\u7387\u4ec5\u4e3a10.3%\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\u3002RLMEval\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u4ee5\u63a8\u52a8\u81ea\u52a8\u5316\u63a8\u7406\u5728\u5f62\u5f0f\u6570\u5b66\u9886\u57df\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u7814\u7a76\u7ea7\u795e\u7ecf\u5b9a\u7406\u8bc1\u660e\u548c\u8bc1\u660e\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\uff0c\u5176\u5e94\u7528\u6548\u679c\u5374\u4e0d\u5c3d\u5982\u4eba\u610f\u3002\u8fd9\u79cd\u5dee\u8ddd\u8868\u660e\uff0c\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6355\u6349\u6a21\u578b\u5728\u5904\u7406\u66f4\u590d\u6742\u3001\u66f4\u8d34\u8fd1\u5b9e\u9645\u7814\u7a76\u7684\u6570\u5b66\u95ee\u9898\u65f6\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u5177\u6311\u6218\u6027\u3001\u66f4\u80fd\u53cd\u6620\u771f\u5b9e\u7814\u7a76\u9700\u6c42\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u4ee5\u51c6\u786e\u8861\u91cfLLMs\u5728\u8fd9\u4e9b\u524d\u6cbf\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aRLMEval\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u8be5\u5957\u4ef6\u4e13\u95e8\u9488\u5bf9\u7814\u7a76\u7ea7\u522b\u7684\u795e\u7ecf\u5b9a\u7406\u8bc1\u660e\u548c\u8bc1\u660e\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\u3002RLMEval\u5229\u7528\u4e86\u771f\u5b9e\u7684Lean\u5f62\u5f0f\u5316\u9879\u76ee\u4e2d\u7684\u524d\u6cbf\u6570\u5b66\u5185\u5bb9\u3002\u8bc4\u4f30\u7684\u91cd\u70b9\u662f\u6a21\u578b\u5728\u5904\u7406\u6765\u81ea\u771f\u5b9eLean\u9879\u76ee\uff08\u5305\u62ec6\u4e2a\u9879\u76ee\u5171613\u4e2a\u5b9a\u7406\uff09\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u7814\u7a76\u7ea7\u5b9a\u7406\u65f6\u7684\u8868\u73b0\u3002\u901a\u8fc7\u5728RLMEval\u4e0a\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u6765\u68c0\u9a8c\u5b83\u4eec\u5728\u771f\u5b9e\u6570\u5b66\u5f62\u5f0f\u5316\u573a\u666f\u4e0b\u7684\u80fd\u529b\u3002", "result": "\u5728RLMEval\u8bc4\u4f30\u5957\u4ef6\u4e0a\u5bf9\u6700\u5148\u8fdb\u7684\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u6a21\u578b\u5728\u73b0\u6709\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728RLMEval\u4e0a\uff0c\u6700\u4f73\u6a21\u578b\u7684\u901a\u8fc7\u7387\u4ec5\u4e3a10.3%\u3002\u8fd9\u4e00\u7ed3\u679c\u6e05\u6670\u5730\u8868\u660e\uff0c\u5728\u73b0\u6709\u57fa\u51c6\u4e0a\u7684\u8fdb\u5c55\u5e76\u4e0d\u80fd\u76f4\u63a5\u8f6c\u5316\u4e3a\u5728\u66f4\u771f\u5b9e\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u7814\u7a76\u7ea7\u6570\u5b66\u5f62\u5f0f\u5316\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u3002\u8fd9\u63ed\u793a\u4e86\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u5b58\u5728\u7684\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "RLMEval\u8bc4\u4f30\u5957\u4ef6\u7684\u5f15\u5165\uff0c\u4e3a\u7814\u7a76\u7ea7\u795e\u7ecf\u5b9a\u7406\u8bc1\u660e\u548c\u8bc1\u660e\u81ea\u52a8\u5f62\u5f0f\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7684\u3001\u7814\u7a76\u7ea7\u522b\u7684\u6570\u5b66\u5f62\u5f0f\u5316\u9879\u76ee\u65f6\u4ecd\u9762\u4e34\u5de8\u5927\u56f0\u96be\uff0c\u5176\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u7684\u4f18\u5f02\u8868\u73b0\u5e76\u672a\u6709\u6548\u8fc1\u79fb\u5230\u8fd9\u4e9b\u66f4\u590d\u6742\u7684\u573a\u666f\u3002RLMEval\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u66f4\u8d34\u8fd1\u5b9e\u9645\u9700\u6c42\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u6765\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u52a0\u901f\u81ea\u52a8\u5316\u63a8\u7406\u5728\u5f62\u5f0f\u6570\u5b66\u4e2d\u7684\u53d1\u5c55\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u7ee7\u7eed\u5173\u6ce8\u5982\u4f55\u63d0\u5347\u6a21\u578b\u5728\u8fd9\u4e9b\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2510.25432", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25432", "abs": "https://arxiv.org/abs/2510.25432", "authors": ["Ali Sanaei", "Ali Rajabzadeh"], "title": "Depth and Autonomy: A Framework for Evaluating LLM Applications in Social Science Research", "comment": "Presented at the Annual Meeting of the American Political Science\n  Association, Vancouver, BC, September 11--14 2025", "summary": "Large language models (LLMs) are increasingly utilized by researchers across\na wide range of domains, and qualitative social science is no exception;\nhowever, this adoption faces persistent challenges, including interpretive\nbias, low reliability, and weak auditability. We introduce a framework that\nsituates LLM usage along two dimensions, interpretive depth and autonomy,\nthereby offering a straightforward way to classify LLM applications in\nqualitative research and to derive practical design recommendations. We present\nthe state of the literature with respect to these two dimensions, based on all\npublished social science papers available on Web of Science that use LLMs as a\ntool and not strictly as the subject of study. Rather than granting models\nexpansive freedom, our approach encourages researchers to decompose tasks into\nmanageable segments, much as they would when delegating work to capable\nundergraduate research assistants. By maintaining low levels of autonomy and\nselectively increasing interpretive depth only where warranted and under\nsupervision, one can plausibly reap the benefits of LLMs while preserving\ntransparency and reliability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u201c\u89e3\u91ca\u6df1\u5ea6\u201d\u548c\u201c\u81ea\u4e3b\u6027\u201d\u4e24\u4e2a\u7ef4\u5ea6\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5b9a\u6027\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u8bbe\u8ba1\u5efa\u8bae\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u4fdd\u6301\u8f83\u4f4e\u7684\u81ea\u4e3b\u6027\u5e76\u5c06\u89e3\u91ca\u6df1\u5ea6\u9650\u5236\u5728\u6709\u76d1\u7763\u7684\u5fc5\u8981\u8303\u56f4\u5185\uff0c\u53ef\u4ee5\u6709\u6548\u5229\u7528LLM\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u8bc1\u7814\u7a76\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5b9a\u6027\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u5728\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65f6\u9762\u4e34\u89e3\u91ca\u6027\u504f\u5dee\u3001\u4f4e\u53ef\u9760\u6027\u548c\u5f31\u53ef\u5ba1\u8ba1\u6027\u7b49\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4e3aLLM\u5728\u5b9a\u6027\u7814\u7a76\u4e2d\u7684\u8d1f\u8d23\u4efb\u4f7f\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u7814\u7a76\u8005\u9996\u5148\u5728Web of Science\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u4e86\u6240\u6709\u5c06LLM\u4f5c\u4e3a\u5de5\u5177\u800c\u975e\u7814\u7a76\u5bf9\u8c61\u7684\u5df2\u53d1\u8868\u7684\u793e\u4f1a\u79d1\u5b66\u8bba\u6587\uff0c\u7136\u540e\u57fa\u4e8e\u201c\u89e3\u91ca\u6df1\u5ea6\u201d\u548c\u201c\u81ea\u4e3b\u6027\u201d\u4e24\u4e2a\u7ef4\u5ea6\u5bf9\u8fd9\u4e9b\u7814\u7a76\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u6587\u732e\u5728\u8fd9\u4e9b\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002\u57fa\u4e8e\u6b64\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u5efa\u8bae\u7814\u7a76\u8005\u5c06\u4efb\u52a1\u5206\u89e3\uff0c\u5e76\u50cf\u6307\u5bfc\u672c\u79d1\u751f\u7814\u7a76\u52a9\u7406\u4e00\u6837\uff0c\u4fdd\u6301LLM\u8f83\u4f4e\u7684\u81ea\u4e3b\u6027\uff0c\u53ea\u5728\u5fc5\u8981\u4e14\u6709\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u589e\u52a0\u89e3\u91ca\u6df1\u5ea6\u3002", "result": "\u73b0\u6709\u6587\u732e\u663e\u793a\uff0cLLM\u5728\u5b9a\u6027\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u5229\u7528\u201c\u89e3\u91ca\u6df1\u5ea6\u201d\u548c\u201c\u81ea\u4e3b\u6027\u201d\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u7684\u7ec4\u5408\u6f5c\u529b\u3002\u7814\u7a76\u6846\u67b6\u8868\u660e\uff0c\u901a\u8fc7\u5c06LLM\u7684\u81ea\u4e3b\u6027\u4fdd\u6301\u5728\u8f83\u4f4e\u6c34\u5e73\uff0c\u5e76\u9009\u62e9\u6027\u5730\u5728\u6709\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u589e\u52a0\u89e3\u91ca\u6df1\u5ea6\uff0c\u53ef\u4ee5\u6709\u6548\u89c4\u907fLLM\u5e26\u6765\u7684\u504f\u89c1\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u4e3aLLM\u5728\u5b9a\u6027\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u7c7b\u548c\u5b9e\u7528\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002\u901a\u8fc7\u7cbe\u7ec6\u63a7\u5236LLM\u7684\u81ea\u4e3b\u6027\u548c\u89e3\u91ca\u6df1\u5ea6\uff0c\u7814\u7a76\u8005\u53ef\u4ee5\u5728\u53d7\u76ca\u4e8eLLM\u6548\u7387\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u7814\u7a76\u7684\u900f\u660e\u5ea6\u3001\u53ef\u9760\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u8be5\u6846\u67b6\u5728\u5177\u4f53\u7814\u7a76\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5f00\u53d1\u66f4\u7cbe\u7ec6\u5316\u7684\u6a21\u578b\u63a7\u5236\u65b9\u6cd5\u3002"}}
{"id": "2510.25434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25434", "abs": "https://arxiv.org/abs/2510.25434", "authors": ["Shakib Yazdani", "Yasser Hamidullah", "Cristina Espa\u00f1a-Bonet", "Eleftherios Avramidis", "Josef van Genabith"], "title": "A Critical Study of Automatic Evaluation in Sign Language Translation", "comment": "Submitted to the LREC 2026 conference", "summary": "Automatic evaluation metrics are crucial for advancing sign language\ntranslation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are\nonly text-based, and it remains unclear to what extent text-based metrics can\nreliably capture the quality of SLT outputs. To address this gap, we\ninvestigate the limitations of text-based SLT evaluation metrics by analyzing\nsix metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one\nhand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA\nzero-shot direct assessment on the other hand. Specifically, we assess the\nconsistency and robustness of these metrics under three controlled conditions:\nparaphrasing, hallucinations in model outputs, and variations in sentence\nlength. Our analysis highlights the limitations of lexical overlap metrics and\ndemonstrates that while LLM-based evaluators better capture semantic\nequivalence often missed by conventional metrics, they can also exhibit bias\ntoward LLM-paraphrased translations. Moreover, although all metrics are able to\ndetect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and\nLLM-based evaluators are comparatively lenient toward subtle cases. This\nmotivates the need for multimodal evaluation frameworks that extend beyond\ntext-based metrics to enable a more holistic assessment of SLT outputs.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u5728\u624b\u8bed\u7ffb\u8bd1\uff08SLT\uff09\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u5c40\u9650\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u8bcd\u6c47\u91cd\u53e0\u7684\u6307\u6807\uff08\u5982BLEU\u3001ROUGE\uff09\u65e0\u6cd5\u53ef\u9760\u5730\u6355\u6349SLT\u8f93\u51fa\u7684\u8bed\u4e49\u8d28\u91cf\uff0c\u800c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bc4\u4f30\u5668\u867d\u7136\u80fd\u66f4\u597d\u5730\u7406\u89e3\u8bed\u4e49\uff0c\u4f46\u4e5f\u5b58\u5728\u5bf9LLM\u6539\u5199\u5185\u5bb9\u504f\u89c1\u7684\u98ce\u9669\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u6307\u6807\u5bf9\u5e7b\u89c9\u7684\u654f\u611f\u5ea6\u4e5f\u4e0d\u540c\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u8d85\u8d8a\u6587\u672c\u7684\u3001\u591a\u6a21\u6001\u7684\u8bc4\u4f30\u6846\u67b6\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u7ffb\u8bd1\uff08SLT\uff09\u8bc4\u4f30\u6307\u6807\uff0c\u5982BLEU\u548cROUGE\uff0c\u4ec5\u57fa\u4e8e\u6587\u672c\uff0c\u65e0\u6cd5\u5145\u5206\u8861\u91cfSLT\u8f93\u51fa\u7684\u771f\u5b9e\u8d28\u91cf\u3002\u8fd9\u963b\u788d\u4e86SLT\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u5206\u6790\u8fd9\u4e9b\u6587\u672c\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u7d22\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u516d\u79cd\u8bc4\u4f30\u6307\u6807\uff1a\u5305\u62ecBLEU\u3001chrF\u3001ROUGE\u7b49\u57fa\u4e8e\u8bcd\u6c47\u91cd\u53e0\u7684\u6307\u6807\uff0c\u4ee5\u53caBLEURT\u3001G-Eval\u548cGEMBA\u7b49\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bc4\u4f30\u5668\u3002\u7814\u7a76\u5728\u4e09\u79cd\u53d7\u63a7\u6761\u4ef6\u4e0b\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6307\u6807\u7684\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\uff1a1. \u91ca\u4e49\uff08paraphrasing\uff09\uff1b2. \u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u5e7b\u89c9\uff08hallucinations\uff09\uff1b3. \u53e5\u5b50\u957f\u5ea6\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u8bcd\u6c47\u91cd\u53e0\u7684\u6307\u6807\u5728\u6355\u6349\u8bed\u4e49\u7b49\u4ef7\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u5668\u867d\u7136\u5728\u8bed\u4e49\u7406\u89e3\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\uff0c\u4f46\u53ef\u80fd\u504f\u5411\u4e8eLLM\u6539\u5199\u7684\u7ffb\u8bd1\u3002\u6240\u6709\u6307\u6807\u90fd\u80fd\u68c0\u6d4b\u5230\u5e7b\u89c9\uff0c\u4f46BLEU\u5bf9\u5e7b\u89c9\u8fc7\u4e8e\u654f\u611f\uff0c\u800cBLEURT\u548cLLM\u8bc4\u4f30\u5668\u5219\u76f8\u5bf9\u5bbd\u5bb9\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u6587\u672c\u57fa\u7840\u7684SLT\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u8d85\u8d8a\u6587\u672c\u7684\u3001\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u5b9e\u73b0\u5bf9SLT\u8f93\u51fa\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u7740\u529b\u4e8e\u6784\u5efa\u80fd\u591f\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002"}}
{"id": "2510.25460", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25460", "abs": "https://arxiv.org/abs/2510.25460", "authors": ["Jun Wang", "Fuming Lin", "Yuyu Chen"], "title": "Fine-Tuned Language Models for Domain-Specific Summarization and Tagging", "comment": null, "summary": "This paper presents a pipeline integrating fine-tuned large language models\n(LLMs) with named entity recognition (NER) for efficient domain-specific text\nsummarization and tagging. The authors address the challenge posed by rapidly\nevolving sub-cultural languages and slang, which complicate automated\ninformation extraction and law enforcement monitoring. By leveraging the LLaMA\nFactory framework, the study fine-tunes LLMs on both generalpurpose and custom\ndomain-specific datasets, particularly in the political and security domains.\nThe models are evaluated using BLEU and ROUGE metrics, demonstrating that\ninstruction fine-tuning significantly enhances summarization and tagging\naccuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct\nmodel, despite its initial limitations in Chinese comprehension, outperforms\nits Chinese-trained counterpart after domainspecific fine-tuning, suggesting\nthat underlying reasoning capabilities can transfer across languages. The\npipeline enables concise summaries and structured entity tagging, facilitating\nrapid document categorization and distribution. This approach proves scalable\nand adaptable for real-time applications, supporting efficient information\nmanagement and the ongoing need to capture emerging language trends. The\nintegration of LLMs and NER offers a robust solution for transforming\nunstructured text into actionable insights, crucial for modern knowledge\nmanagement and security operations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u7684\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u9886\u57df\u7279\u5b9a\u6587\u672c\u6458\u8981\u548c\u6807\u6ce8\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u5feb\u901f\u53d8\u5316\u7684\u4e9a\u6587\u5316\u8bed\u8a00\u548c\u4fda\u8bed\uff0c\u5e76\u5728\u653f\u6cbb\u548c\u5b89\u5168\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\u3002", "motivation": "\u5feb\u901f\u53d8\u5316\u7684\u4e9a\u6587\u5316\u8bed\u8a00\u548c\u4fda\u8bed\u7ed9\u81ea\u52a8\u4fe1\u606f\u63d0\u53d6\u548c\u6267\u6cd5\u76d1\u63a7\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u5229\u7528LLaMA Factory\u6846\u67b6\uff0c\u5728\u901a\u7528\u548c\u81ea\u5b9a\u4e49\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\uff08\u5c24\u5176\u662f\u5728\u653f\u6cbb\u548c\u5b89\u5168\u9886\u57df\uff09\u4e0a\u5fae\u8c03LLMs\u3002", "result": "\u6307\u4ee4\u5fae\u8c03\u663e\u8457\u63d0\u9ad8\u4e86\u6458\u8981\u548c\u6807\u6ce8\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u4e13\u4e1a\u8bed\u6599\u5e93\u4e0a\u3002LLaMA3-8B-Instruct\u6a21\u578b\u5728\u7ecf\u8fc7\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u540e\uff0c\u8d85\u8d8a\u4e86\u5176\u4e2d\u56fd\u8bed\u8bad\u7ec3\u7684\u5bf9\u5e94\u6a21\u578b\uff0c\u8868\u660e\u63a8\u7406\u80fd\u529b\u53ef\u4ee5\u8de8\u8bed\u8a00\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u6d41\u6c34\u7ebf\u80fd\u591f\u751f\u6210\u7b80\u6d01\u7684\u6458\u8981\u548c\u7ed3\u6784\u5316\u7684\u5b9e\u4f53\u6807\u7b7e\uff0c\u652f\u6301\u5feb\u901f\u6587\u6863\u5206\u7c7b\u548c\u5206\u53d1\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u53ef\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u6ee1\u8db3\u4fe1\u606f\u7ba1\u7406\u548c\u65b0\u5174\u8bed\u8a00\u8d8b\u52bf\u6355\u83b7\u7684\u9700\u6c42\u3002"}}
{"id": "2510.25621", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50, 68T05, 68T30", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2510.25621", "abs": "https://arxiv.org/abs/2510.25621", "authors": ["Mohammad Aghajani Asl", "Behrooz Minaei Bidgoli"], "title": "FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering", "comment": "37 pages, 5 figures, 10 tables. Keywords: Retrieval-Augmented\n  Generation (RAG), Question Answering (QA), Islamic Knowledge Base, Faithful\n  AI, Persian NLP, Multi-hop Reasoning, Large Language Models (LLMs)", "summary": "The advent of Large Language Models (LLMs) has revolutionized Natural\nLanguage Processing, yet their application in high-stakes, specialized domains\nlike religious question answering is hindered by challenges like hallucination\nand unfaithfulness to authoritative sources. This issue is particularly\ncritical for the Persian-speaking Muslim community, where accuracy and\ntrustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)\nsystems, relying on simplistic single-pass pipelines, fall short on complex,\nmulti-hop queries requiring multi-step reasoning and evidence aggregation. To\naddress this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful\nAdvanced Question Answering in the Persian Islamic domain. FARSIQA is built\nupon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative\nRefinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting\nprocess: it adaptively decomposes complex queries, assesses evidence\nsufficiency, and enters an iterative loop to generate sub-queries,\nprogressively filling information gaps. Operating on a curated knowledge base\nof over one million authoritative Islamic documents, FARSIQA demonstrates\nsuperior performance. Rigorous evaluation on the challenging IslamicPCQA\nbenchmark shows state-of-the-art performance: the system achieves a remarkable\n97.0% in Negative Rejection - a 40-point improvement over baselines - and a\nhigh Answer Correctness score of 74.3%. Our work establishes a new standard for\nPersian Islamic QA and validates that our iterative, adaptive architecture is\ncrucial for building faithful, reliable AI systems in sensitive domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FARSIQA\u7cfb\u7edf\uff0c\u4e00\u4e2a\u7528\u4e8e\u6ce2\u65af\u8bed\u4f0a\u65af\u5170\u9886\u57df\u95ee\u7b54\u7684\u65b0\u578b\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u901a\u8fc7\u521b\u65b0\u7684FAIR-RAG\u67b6\u6784\u89e3\u51b3\u4e86\u4f20\u7edfRAG\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u95ee\u9898\u548c\u4fdd\u8bc1\u7b54\u6848\u5fe0\u5b9e\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u8be5\u7cfb\u7edf\u5728\u4e00\u4e2a\u5305\u542b\u767e\u4e07\u4efd\u4f0a\u65af\u5170\u6587\u6863\u7684\u77e5\u8bc6\u5e93\u4e0a\u8fd0\u884c\uff0c\u5e76\u5728IslamicPCQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8d1f\u4f8b\u62d2\u7edd\u7387\uff0897.0%\uff09\u548c\u7b54\u6848\u6b63\u786e\u7387\uff0874.3%\uff09\u65b9\u9762\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u5728\u5b97\u6559\u95ee\u7b54\u7b49\u9ad8\u98ce\u9669\u3001\u4e13\u4e1a\u5316\u9886\u57df\uff0c\u5e7b\u89c9\u548c\u4e0d\u5fe0\u5b9e\u4e8e\u6743\u5a01\u6765\u6e90\u7b49\u95ee\u9898\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u8fd9\u5bf9\u4e8e accuracy \u548c trustworthiness \u81f3\u5173\u91cd\u8981\u7684\u6ce2\u65af\u8bed\u7a46\u65af\u6797\u793e\u533a\u6765\u8bf4\u5c24\u5176\u5173\u952e\u3002\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u91c7\u7528\u5355\u4e00\u7684\u3001\u7b80\u5355\u7684\u6d41\u6c34\u7ebf\uff0c\u96be\u4ee5\u5904\u7406\u9700\u8981\u591a\u6b65\u63a8\u7406\u548c\u8bc1\u636e\u805a\u5408\u7684\u590d\u6742\u3001\u591a\u8df3\u67e5\u8be2\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u3001\u5fe0\u5b9e\u5730\u56de\u7b54\u6ce2\u65af\u8bed\u4f0a\u65af\u5170\u9886\u57df\u95ee\u9898\u7684\u7cfb\u7edf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86FARSIQA\u7cfb\u7edf\uff0c\u5176\u6838\u5fc3\u662f\u521b\u65b0\u7684FAIR-RAG\uff08Faithful, Adaptive, Iterative Refinement for RAG\uff09\u67b6\u6784\u3002FAIR-RAG\u91c7\u7528\u52a8\u6001\u7684\u3001\u81ea\u7ea0\u6b63\u7684\u8fc7\u7a0b\uff1a\u5b83\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5206\u89e3\u590d\u6742\u67e5\u8be2\uff0c\u8bc4\u4f30\u8bc1\u636e\u5145\u5206\u6027\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u5faa\u73af\u6765\u751f\u6210\u5b50\u67e5\u8be2\uff0c\u9010\u6b65\u586b\u8865\u4fe1\u606f\u7a7a\u767d\u3002\u8be5\u7cfb\u7edf\u5728\u4e00\u4e2a\u5305\u542b\u8d85\u8fc7\u4e00\u767e\u4e07\u4efd\u6743\u5a01\u4f0a\u65af\u5170\u6587\u6863\u7684\u7cbe\u9009\u77e5\u8bc6\u5e93\u4e0a\u8fd0\u884c\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684IslamicPCQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u7684\u4e25\u683c\u8bc4\u4f30\u663e\u793a\uff0cFARSIQA\u7cfb\u7edf\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7cfb\u7edf\u5728\u8d1f\u4f8b\u62d2\u7edd\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u60ca\u4eba\u768497.0%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8640\u4e2a\u767e\u5206\u70b9\uff1b\u540c\u65f6\uff0c\u7b54\u6848\u6b63\u786e\u7387\u4e5f\u8fbe\u5230\u4e8674.3%\u3002", "conclusion": "FARSIQA\u7cfb\u7edf\u4e3a\u6ce2\u65af\u8bed\u4f0a\u65af\u5170\u95ee\u7b54\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\u3002\u7814\u7a76\u7ed3\u679c\u9a8c\u8bc1\u4e86\u672c\u6587\u63d0\u51fa\u7684\u8fed\u4ee3\u5f0f\u3001\u81ea\u9002\u5e94\u67b6\u6784\u5bf9\u4e8e\u5728\u654f\u611f\u9886\u57df\u6784\u5efa\u5fe0\u5b9e\u3001\u53ef\u9760\u7684AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u8be5\u67b6\u6784\u5728\u5176\u4ed6\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.25536", "categories": ["cs.CL", "I.2.7; I.2.6; I.2.0"], "pdf": "https://arxiv.org/pdf/2510.25536", "abs": "https://arxiv.org/abs/2510.25536", "authors": ["Bangde Du", "Minghao Guo", "Songming He", "Ziyi Ye", "Xi Zhu", "Weihang Su", "Shuqi Zhu", "Yujia Zhou", "Yongfeng Zhang", "Qingyao Ai", "Yiqun Liu"], "title": "TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation", "comment": "Main paper: 11 pages, 3 figures, 6 tables. Appendix: 28 pages. Bangde\n  Du and Minghao Guo contributed equally. Corresponding authors: Ziyi Ye\n  (ziyiye@fudan.edu.cn), Qingyao Ai (aiqy@tsinghua.edu.cn)", "summary": "Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline.", "AI": {"tldr": "LLMs\u5728\u6a21\u62df\u4e2a\u4f53\u6c9f\u901a\u98ce\u683c\u3001\u884c\u4e3a\u503e\u5411\u548c\u4e2a\u6027\u7279\u5f81\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86TwinVoice\u57fa\u51c6\uff0c\u5305\u542b\u793e\u4ea4\u3001\u4eba\u9645\u548c\u53d9\u4e8b\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u5206\u89e3\u4e3a\u610f\u89c1\u4e00\u81f4\u6027\u3001\u8bb0\u5fc6\u56de\u5fc6\u3001\u903b\u8f91\u63a8\u7406\u3001\u8bcd\u6c47\u4fdd\u771f\u5ea6\u3001\u4eba\u683c\u8bed\u8c03\u548c\u53e5\u6cd5\u98ce\u683c\u516d\u79cd\u80fd\u529b\uff0c\u4ee5\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u4eba\u683c\u6a21\u62df\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5148\u8fdbLLM\u5728\u6a21\u62df\u65b9\u9762\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u53e5\u6cd5\u98ce\u683c\u548c\u8bb0\u5fc6\u56de\u5fc6\u7b49\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u5e73\u5747\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u4e2a\u4f53\u6c9f\u901a\u98ce\u683c\u3001\u884c\u4e3a\u503e\u5411\u548c\u4e2a\u6027\u7279\u5f81\u65b9\u9762\u7684\u8bc4\u4f30\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5408\u6210\u5bf9\u8bdd\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6846\u67b6\u548c\u80fd\u529b\u9700\u6c42\u5206\u6790\uff0c\u8fd9\u963b\u788d\u4e86LLMs\u5728\u4eba\u683c\u6a21\u62df\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u548c\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86TwinVoice\u57fa\u51c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u8bc4\u4f30LLM\u7684\u4eba\u683c\u6a21\u62df\u80fd\u529b\u3002TwinVoice\u5305\u542b\u4e09\u4e2a\u7ef4\u5ea6\uff1a\u793e\u4ea4\u4eba\u683c\uff08\u516c\u5171\u793e\u4ea4\u4e92\u52a8\uff09\u3001\u4eba\u9645\u4eba\u683c\uff08\u79c1\u4eba\u5bf9\u8bdd\uff09\u548c\u53d9\u4e8b\u4eba\u683c\uff08\u57fa\u4e8e\u89d2\u8272\u7684\u8868\u8fbe\uff09\u3002\u6b64\u5916\uff0c\u8be5\u57fa\u51c6\u5c06LLM\u7684\u6027\u80fd\u8bc4\u4f30\u5206\u89e3\u4e3a\u516d\u79cd\u57fa\u672c\u80fd\u529b\uff1a\u610f\u89c1\u4e00\u81f4\u6027\u3001\u8bb0\u5fc6\u56de\u5fc6\u3001\u903b\u8f91\u63a8\u7406\u3001\u8bcd\u6c47\u4fdd\u771f\u5ea6\u3001\u4eba\u683c\u8bed\u8c03\u548c\u53e5\u6cd5\u98ce\u683c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u5148\u8fdb\u7684LLMs\u5728\u4eba\u683c\u6a21\u62df\u65b9\u9762\u53d6\u5f97\u4e86\u4e2d\u7b49\u6c34\u5e73\u7684\u51c6\u786e\u5ea6\uff0c\u4f46\u5728\u53e5\u6cd5\u98ce\u683c\u548c\u8bb0\u5fc6\u56de\u5fc6\u7b49\u5173\u952e\u80fd\u529b\u4e0a\u4ecd\u663e\u4e0d\u8db3\u3002LLMs\u7684\u5e73\u5747\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u7ebf\u6c34\u5e73\uff0c\u663e\u793a\u51fa\u5728\u66f4\u590d\u6742\u548c\u7ec6\u81f4\u7684\u4eba\u683c\u6a21\u62df\u4efb\u52a1\u4e0a\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "TwinVoice\u57fa\u51c6\u7684\u63d0\u51fa\u4e3a\u5168\u9762\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u4eba\u683c\u6a21\u62df\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u6846\u67b6\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u6a21\u62df\u4eba\u7c7b\u6c9f\u901a\u98ce\u683c\u3001\u884c\u4e3a\u548c\u4e2a\u6027\u65b9\u9762\u7684\u4f18\u52bf\u4e0e\u4e0d\u8db3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u5373\u91cd\u70b9\u63d0\u5347LLMs\u5728\u53e5\u6cd5\u98ce\u683c\u6a21\u4eff\u548c\u957f\u671f\u8bb0\u5fc6\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4ee5\u671f\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.25626", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.25626", "abs": "https://arxiv.org/abs/2510.25626", "authors": ["Andreas Opedal", "Yanick Zengaffinen", "Haruki Shirakami", "Clemente Pasti", "Mrinmaya Sachan", "Abulhair Saparov", "Ryan Cotterell", "Bernhard Sch\u00f6lkopf"], "title": "Are Language Models Efficient Reasoners? A Perspective from Logic Programming", "comment": "Accepted to NeurIPS 2025", "summary": "Modern language models (LMs) exhibit strong deductive reasoning capabilities,\nyet standard evaluations emphasize correctness while overlooking a key aspect\nof human-like reasoning: efficiency. In real-world reasoning scenarios, much of\nthe available information is irrelevant, and effective deductive inference\nrequires identifying and ignoring such distractions. We propose a framework for\nassessing LM reasoning efficiency through the lens of logic programming,\nintroducing a simple method to align proofs written in natural language -- as\ngenerated by an LM -- with shortest proofs found by executing the logic\nprogram. Efficiency is quantified by measuring how well a model avoids\nunnecessary inference. Empirically, we construct a dataset of math word\nproblems injected with various number of irrelevant axioms that vary in\nsemantic overlap with the goal theorem. We find that current LMs show marked\naccuracy declines under such conditions -- even with minimal, domain-consistent\ndistractions -- and the proofs they generate frequently exhibit detours through\nirrelevant inferences.", "AI": {"tldr": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u6b63\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u6548\u7387\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u903b\u8f91\u7f16\u7a0b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LM\u63a8\u7406\u6548\u7387\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u4e0e\u903b\u8f91\u7a0b\u5e8f\u627e\u5230\u7684\u6700\u77ed\u8bc1\u660e\u8fdb\u884c\u6bd4\u5bf9\uff0c\u91cf\u5316\u6a21\u578b\u907f\u514d\u4e0d\u5fc5\u8981\u63a8\u7406\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5728\u5305\u542b\u65e0\u5173\u4fe1\u606f\uff08\u5373\u4f7f\u662f\u9886\u57df\u76f8\u5173\u7684\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u73b0\u6709LM\u7684\u51c6\u786e\u7387\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u4e14\u751f\u6210\u7684\u8bc1\u660e\u5e38\u5e38\u5305\u542b\u4e0d\u76f8\u5173\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u867d\u7136\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6807\u51c6\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u63a8\u7406\u7684\u6b63\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u4eba\u7c7b\u63a8\u7406\u7684\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u2014\u2014\u6548\u7387\u3002\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u63a8\u7406\u573a\u666f\u4e2d\uff0c\u4fe1\u606f\u5f80\u5f80\u662f\u8fc7\u8f7d\u4e14\u5305\u542b\u5927\u91cf\u65e0\u5173\u5185\u5bb9\u7684\uff0c\u6709\u6548\u8fdb\u884c\u6f14\u7ece\u63a8\u7406\u9700\u8981\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u5e76\u5ffd\u7565\u8fd9\u4e9b\u5e72\u6270\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u6a21\u578b\u5728\u5904\u7406\u5e72\u6270\u4fe1\u606f\u65f6\u7684\u63a8\u7406\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u903b\u8f91\u7f16\u7a0b\u6765\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u63a8\u7406\u6548\u7387\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7684\u6838\u5fc3\u65b9\u6cd5\u662f\u5c06\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u4e0e\u5176\u5728\u903b\u8f91\u7a0b\u5e8f\u4e2d\u627e\u5230\u7684\u6700\u77ed\u8bc1\u660e\u8fdb\u884c\u5bf9\u9f50\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u91cf\u5316\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u907f\u514d\u4e0d\u5fc5\u8981\u63a8\u7406\u7684\u80fd\u529b\uff0c\u4ece\u800c\u8861\u91cf\u5176\u63a8\u7406\u6548\u7387\u3002\u5b9e\u9a8c\u4e0a\uff0c\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u6570\u5b66\u5e94\u7528\u9898\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5728\u5176\u4e2d\u6ce8\u5165\u4e86\u4e0d\u540c\u6570\u91cf\u4e14\u4e0e\u76ee\u6807\u5b9a\u7406\u5177\u6709\u4e0d\u540c\u8bed\u4e49\u91cd\u53e0\u5ea6\u7684\u65e0\u5173\u516c\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5b58\u5728\u65e0\u5173\u4fe1\u606f\uff08\u5373\u4f7f\u662f\u9886\u57df\u5185\u4e00\u81f4\u7684\u5e72\u6270\u4fe1\u606f\uff09\u7684\u6761\u4ef6\u4e0b\uff0c\u73b0\u6709\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u7684\u51c6\u786e\u7387\u4f1a\u51fa\u73b0\u663e\u8457\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6a21\u578b\u751f\u6210\u7684\u8bc1\u660e\u8def\u5f84\u5e38\u5e38\u5305\u542b\u4e0d\u5fc5\u8981\u7684\u8fc2\u56de\uff0c\u5373\u201c\u7ed5\u8fdc\u8def\u201d\uff0c\u8868\u660e\u5176\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u672a\u80fd\u6709\u6548\u5730\u533a\u5206\u548c\u5ffd\u7565\u65e0\u5173\u4fe1\u606f\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u903b\u8f91\u7f16\u7a0b\u7684\u6846\u67b6\uff0c\u9996\u6b21\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5728\u5904\u7406\u5e26\u6709\u5e72\u6270\u4fe1\u606f\u65f6\u7684\u63a8\u7406\u6548\u7387\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524dLMs\u5728\u9762\u5bf9\u65e0\u5173\u4fe1\u606f\u65f6\uff0c\u5176\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\u4f1a\u5927\u6253\u6298\u6263\uff0c\u8bc1\u660e\u8fc7\u7a0b\u4e5f\u663e\u5f97\u5197\u4f59\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524dLMs\u5728\u6a21\u62df\u4eba\u7c7b\u9ad8\u6548\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5982\u4f55\u63d0\u5347LMs\u5728\u771f\u5b9e\u590d\u6742\u573a\u666f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u6307\u660e\u4e86\u65b9\u5411\uff0c\u4f8b\u5982\u9700\u8981\u66f4\u5f3a\u7684\u5e72\u6270\u4fe1\u606f\u8fc7\u6ee4\u548c\u66f4\u4f18\u5316\u7684\u63a8\u7406\u8def\u5f84\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2510.25732", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; I.2.4; G.2.2"], "pdf": "https://arxiv.org/pdf/2510.25732", "abs": "https://arxiv.org/abs/2510.25732", "authors": ["Aakriti Shah", "Thai Le"], "title": "The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework", "comment": "14 pages, 11 figures", "summary": "Unlearning in large language models (LLMs) is crucial for managing sensitive\ndata and correcting misinformation, yet evaluating its effectiveness remains an\nopen problem. We investigate whether persuasive prompting can recall factual\nknowledge from deliberately unlearned LLMs across models ranging from 2.7B to\n13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from\nACT-R and Hebbian theory (spreading activation theories), as well as\ncommunication principles, we introduce Stimulus-Knowledge Entanglement-Behavior\nFramework (SKeB), which models information entanglement via domain graphs and\ntests whether factual recall in unlearned models is correlated with persuasive\nframing. We develop entanglement metrics to quantify knowledge activation\npatterns and evaluate factuality, non-factuality, and hallucination in outputs.\nOur results show persuasive prompts substantially enhance factual knowledge\nrecall (14.8% baseline vs. 24.5% with authority framing), with effectiveness\ninversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB\nprovides a foundation for assessing unlearning completeness, robustness, and\noverall behavior in LLMs.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u201c\u9057\u5fd8\u201d\u5bf9\u4e8e\u7ba1\u7406\u654f\u611f\u6570\u636e\u548c\u7ea0\u6b63\u9519\u8bef\u4fe1\u606f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bc4\u4f30\u5176\u6709\u6548\u6027\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u8bf4\u670d\u6027\u63d0\u793a\u662f\u5426\u80fd\u4ece\u6545\u610f\u9057\u5fd8\u7684LLM\u4e2d\u56de\u5fc6\u8d77\u4e8b\u5b9e\u77e5\u8bc6\u3002\u7814\u7a76\u4eba\u5458\u5f15\u5165\u4e86\u201c\u523a\u6fc0-\u77e5\u8bc6\u7ea0\u7f20-\u884c\u4e3a\u6846\u67b6\u201d\uff08SKeB\uff09\uff0c\u901a\u8fc7\u9886\u57df\u56fe\u6a21\u62df\u4fe1\u606f\u7ea0\u7f20\uff0c\u5e76\u6d4b\u8bd5\u9057\u5fd8\u6a21\u578b\u4e2d\u7684\u4e8b\u5b9e\u56de\u5fc6\u662f\u5426\u4e0e\u8bf4\u670d\u6027\u6846\u67b6\u76f8\u5173\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8bf4\u670d\u6027\u63d0\u793a\u663e\u8457\u589e\u5f3a\u4e86\u4e8b\u5b9e\u77e5\u8bc6\u7684\u56de\u5fc6\uff08\u4ece14.8%\u63d0\u5347\u81f324.5%\uff09\uff0c\u5e76\u4e14\u8fd9\u79cd\u6709\u6548\u6027\u4e0e\u6a21\u578b\u5927\u5c0f\u5448\u53cd\u6bd4\uff082.7B\u6a21\u578b\u6062\u590d128%\uff0c13B\u6a21\u578b\u6062\u590d15%\uff09\u3002SKeB\u4e3a\u8bc4\u4f30LLM\u7684\u9057\u5fd8\u5b8c\u6574\u6027\u3001\u9c81\u68d2\u6027\u548c\u6574\u4f53\u884c\u4e3a\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u4fe1\u606f\u65b9\u9762\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5176\u201c\u9057\u5fd8\u201d\u80fd\u529b\uff0c\u5373\u4ece\u6a21\u578b\u4e2d\u5220\u9664\u7279\u5b9a\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5bf9\u4e8e\u5904\u7406\u654f\u611f\u6570\u636e\uff08\u5982\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\uff09\u548c\u7ea0\u6b63\u6a21\u578b\u4e2d\u7684\u9519\u8bef\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u4f30 LLM \u9057\u5fd8\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u8bf4\u670d\u6027\u63d0\u793a\u6765\u8bc4\u4f30\u548c\u589e\u5f3a LLM \u7684\u9057\u5fd8\u6548\u679c\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e8b\u5b9e\u6027\u77e5\u8bc6\u7684\u6062\u590d\u60c5\u51b5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u5e94\u7528\u4e86\u201c\u523a\u6fc0-\u77e5\u8bc6\u7ea0\u7f20-\u884c\u4e3a\u6846\u67b6\u201d\uff08SKeB\uff09\uff0c\u8be5\u6846\u67b6\u501f\u9274\u4e86 ACT-R \u548c\u8d6b\u5e03\u7406\u8bba\u4e2d\u7684\u6fc0\u6d3b\u6269\u6563\u7406\u8bba\u4ee5\u53ca\u4f20\u64ad\u5b66\u539f\u7406\u3002SKeB \u6846\u67b6\u901a\u8fc7\u6784\u5efa\u9886\u57df\u56fe\u6765\u6a21\u62df\u4fe1\u606f\u4e4b\u95f4\u7684\u7ea0\u7f20\u5173\u7cfb\u3002\u7814\u7a76\u4eba\u5458\u5229\u7528\u8be5\u6846\u67b6\u6d4b\u8bd5\u4e86\u5728\u7ecf\u8fc7\u201c\u9057\u5fd8\u201d\u5904\u7406\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec OPT-2.7B\u3001LLaMA-2-7B\u3001LLaMA-3.1-8B \u548c LLaMA-2-13B \u7b49\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\uff09\u4e2d\uff0c\u8bf4\u670d\u6027\u63d0\u793a\u662f\u5426\u80fd\u591f\u8bf1\u5bfc\u4e8b\u5b9e\u77e5\u8bc6\u7684\u6062\u590d\u3002\u7814\u7a76\u4e2d\u5f00\u53d1\u4e86\u7279\u5b9a\u7684\u7ea0\u7f20\u5ea6\u91cf\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u77e5\u8bc6\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5e76\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u4e8b\u5b9e\u6027\u3001\u975e\u4e8b\u5b9e\u6027\u548c\u5e7b\u89c9\u60c5\u51b5\u3002\u5b9e\u9a8c\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u5bf9\u6bd4\u4e86\u57fa\u7ebf\u60c5\u51b5\uff08\u672a\u4f7f\u7528\u8bf4\u670d\u6027\u63d0\u793a\uff09\u548c\u4f7f\u7528\u4e0d\u540c\u8bf4\u670d\u6027\u6846\u67b6\uff08\u4f8b\u5982\uff0c\u6743\u5a01\u6027\u6846\u67b6\uff09\u63d0\u793a\u540e\u7684\u6a21\u578b\u8868\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8bf4\u670d\u6027\u63d0\u793a\u80fd\u591f\u663e\u8457\u63d0\u5347\u4e8b\u5b9e\u77e5\u8bc6\u7684\u56de\u5fc6\u80fd\u529b\u3002\u5728\u672a\u4f7f\u7528\u8bf4\u670d\u6027\u63d0\u793a\u7684\u57fa\u7ebf\u60c5\u51b5\u4e0b\uff0c\u4e8b\u5b9e\u77e5\u8bc6\u7684\u56de\u5fc6\u7387\u4e3a14.8%\uff0c\u800c\u5728\u91c7\u7528\u6743\u5a01\u6027\u6846\u67b6\u8fdb\u884c\u63d0\u793a\u540e\uff0c\u8be5\u6bd4\u7387\u63d0\u5347\u81f324.5%\u3002\u66f4\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8bf4\u670d\u6027\u63d0\u793a\u7684\u6709\u6548\u6027\u4e0e\u6a21\u578b\u89c4\u6a21\u5448\u8d1f\u76f8\u5173\uff1a\u5728\u53c2\u6570\u91cf\u4e3a2.7B\u7684\u6a21\u578b\u4e2d\uff0c\u4e8b\u5b9e\u77e5\u8bc6\u7684\u6062\u590d\u7387\u9ad8\u8fbe128%\uff0c\u800c\u5728\u53c2\u6570\u91cf\u4e3a13B\u7684\u6a21\u578b\u4e2d\uff0c\u6062\u590d\u7387\u4ec5\u4e3a15%\u3002\u8fd9\u8868\u660e\uff0c\u8f83\u5c0f\u89c4\u6a21\u7684\u6a21\u578b\u5728\u9057\u5fd8\u540e\u66f4\u5bb9\u6613\u53d7\u5230\u8bf4\u670d\u6027\u63d0\u793a\u7684\u5f71\u54cd\u800c\u6062\u590d\u88ab\u9057\u5fd8\u7684\u77e5\u8bc6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684 SKeB \u6846\u67b6\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u201c\u9057\u5fd8\u201d\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5b9e\u7528\u7684\u5de5\u5177\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8bf4\u670d\u6027\u63d0\u793a\u53ef\u4ee5\u6709\u6548\u4fc3\u8fdb\u88ab\u9057\u5fd8\u4e8b\u5b9e\u77e5\u8bc6\u7684\u6062\u590d\uff0c\u5e76\u4e14\u8fd9\u79cd\u6062\u590d\u6548\u679c\u5728\u8f83\u5c0f\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u66f4\u4e3a\u663e\u8457\u3002\u8fd9\u4e0d\u4ec5\u63ed\u793a\u4e86 LLM \u5185\u90e8\u77e5\u8bc6\u8868\u793a\u7684\u8106\u5f31\u6027\uff0c\u4e5f\u4e3a\u7406\u89e3\u548c\u6539\u8fdb LLM \u7684\u53ef\u63a7\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002SKeB \u6846\u67b6\u7684\u5ea6\u91cf\u6307\u6807\u4e3a\u91cf\u5316\u77e5\u8bc6\u6fc0\u6d3b\u548c\u8bc4\u4f30\u9057\u5fd8\u7684\u5b8c\u6574\u6027\u3001\u9c81\u68d2\u6027\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f46\u5176\u5728\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\u548c\u9057\u5fd8\u7b56\u7565\u4e0a\u7684\u666e\u9002\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u805a\u7126\u4e8e\u5f00\u53d1\u66f4\u7cbe\u7ec6\u7684\u9057\u5fd8\u673a\u5236\uff0c\u4ee5\u53ca\u63a2\u7d22\u5982\u4f55\u5229\u7528 SKeB \u6846\u67b6\u6765\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684 LLM \u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2510.25623", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25623", "abs": "https://arxiv.org/abs/2510.25623", "authors": ["Davide Romano", "Jonathan Schwarz", "Daniele Giofr\u00e9"], "title": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks", "comment": "Accepted to EMNLP - NLLP Workshop", "summary": "Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming \\citep{snell2024scaling, chen2024more}, its value in argumentative\ndomains such as law remains underexplored. We present an empirical study of\nverifier-based TTS methods for legal multiple-choice QA (MCQA) across five\nbenchmarks. Using a family of 7 reward models, we evaluate both outcome-level\n(Best-of-$N$) and process-level (tree search) verification under realistic\nlow-$N$ budgets. Our analysis systematically investigates how verifier utility\nis affected by key properties such as domain specialization, model size, and\nsupervision type (process-supervised PRMs vs. outcome-only ORMs), even when\napplied across different roles.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728\u6cd5\u5f8b\u591a\u9879\u9009\u62e9\u95ee\u7b54\uff08MCQA\uff09\u4efb\u52a1\u4e0a\uff0c\u5bf9\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08TTS\uff09\u6280\u672f\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u5728\u4f4e\u9884\u7b97\u4e0b\uff0c\u9a8c\u8bc1\u5668\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u6cd5\u5f8b\uff09\u7684\u5e94\u7528\u4ef7\u503c\u4ecd\u6709\u5f85\u63a2\u7d22\uff0c\u5e76\u5206\u6790\u4e86\u9886\u57df\u4e13\u4e1a\u5316\u3001\u6a21\u578b\u5927\u5c0f\u548c\u76d1\u7763\u7c7b\u578b\u7b49\u56e0\u7d20\u5bf9\u9a8c\u8bc1\u5668\u6548\u7528\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08TTS\uff09\u6280\u672f\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u5f62\u5f0f\u5316\u9886\u57df\u5df2\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u5176\u5728\u6cd5\u5f8b\u7b49\u8bba\u8bc1\u9886\u57df\u7684\u5e94\u7528\u4ef7\u503c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7a76TTS\u5728\u6cd5\u5f8bMCQA\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7ecf\u9a8c\u6027\u65b9\u6cd5\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5bf9\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684TTS\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7814\u7a76\u4f7f\u7528\u4e867\u79cd\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u5728\u5b9e\u9645\u7684\u4f4e\u9884\u7b97\uff08\u4f4eN\u503c\uff09\u4e0b\uff0c\u8bc4\u4f30\u4e86\u7ed3\u679c\u5c42\u9762\u7684TTS\uff08\u5982Best-of-N\uff09\u548c\u8fc7\u7a0b\u5c42\u9762\u7684TTS\uff08\u5982\u6811\u641c\u7d22\uff09\u3002\u7814\u7a76\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u9886\u57df\u4e13\u4e1a\u5316\u3001\u6a21\u578b\u5927\u5c0f\u548c\u76d1\u7763\u7c7b\u578b\uff08\u5982\u8fc7\u7a0b\u76d1\u7763\u7684PRM\u4e0e\u4ec5\u7ed3\u679c\u76d1\u7763\u7684ORM\uff09\u5bf9\u9a8c\u8bc1\u5668\u6548\u7528\u7684\u5f71\u54cd\uff0c\u5e76\u8003\u8651\u4e86\u4e0d\u540c\u89d2\u8272\u4e0b\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6cd5\u5f8bMCQA\u4efb\u52a1\u4e2d\uff0c\u7279\u522b\u662f\u5f53\u9884\u7b97\u6709\u9650\u65f6\uff0cTTS\u6280\u672f\u7684\u6709\u6548\u6027\u53d7\u5230\u591a\u79cd\u56e0\u7d20\u7684\u5f71\u54cd\u3002\u9886\u57df\u4e13\u4e1a\u5316\u3001\u6a21\u578b\u89c4\u6a21\u4ee5\u53ca\u5956\u52b1\u6a21\u578b\u7684\u76d1\u7763\u7c7b\u578b\uff08\u8fc7\u7a0b\u76d1\u7763\u4e0e\u7ed3\u679c\u76d1\u7763\uff09\u90fd\u663e\u8457\u5f71\u54cd\u9a8c\u8bc1\u5668\u7684\u6548\u7528\u3002\u5728\u4f4eN\u9884\u7b97\u4e0b\uff0c\u8fd9\u4e9b\u56e0\u7d20\u5bf9\u4e8e\u6700\u5927\u5316TTS\u7684\u6536\u76ca\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5730\u7814\u7a76\u4e86TTS\u6280\u672f\u5728\u6cd5\u5f8bMCQA\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u4f4e\u9884\u7b97\u4e0b\uff0c\u9886\u57df\u4e13\u4e1a\u5316\u3001\u6a21\u578b\u5927\u5c0f\u548c\u76d1\u7763\u7c7b\u578b\u5bf9\u9a8c\u8bc1\u5668\u6548\u7528\u7684\u91cd\u8981\u5f71\u54cd\u3002\u5c3d\u7ba1\u7814\u7a76\u4e3a\u7406\u89e3TTS\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u4f46\u4ecd\u6709\u5c40\u9650\u6027\uff0c\u672a\u6765\u5de5\u4f5c\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u5e7f\u6cdb\u7684\u6cd5\u5f8b\u4efb\u52a1\u548c\u66f4\u590d\u6742\u7684TTS\u7b56\u7565\u3002"}}
{"id": "2510.25744", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25744", "abs": "https://arxiv.org/abs/2510.25744", "authors": ["Shannon Zejiang Shen", "Valerie Chen", "Ken Gu", "Alexis Ross", "Zixian Ma", "Jillian Ross", "Alex Gu", "Chenglei Si", "Wayne Chi", "Andi Peng", "Jocelyn J Shen", "Ameet Talwalkar", "Tongshuang Wu", "David Sontag"], "title": "Task Completion Agents are Not Ideal Collaborators", "comment": "22 pages, 5 figures, 3 tables", "summary": "Current evaluations of agents remain centered around one-shot task\ncompletion, failing to account for the inherently iterative and collaborative\nnature of many real-world problems, where human goals are often underspecified\nand evolve. We argue for a shift from building and assessing task completion\nagents to developing collaborative agents, assessed not only by the quality of\ntheir final outputs but by how well they engage with and enhance human effort\nthroughout the problem-solving process. To support this shift, we introduce\ncollaborative effort scaling, a framework that captures how an agent's utility\ngrows with increasing user involvement. Through case studies and simulated\nevaluations, we show that state-of-the-art agents often underperform in\nmulti-turn, real-world scenarios, revealing a missing ingredient in agent\ndesign: the ability to sustain engagement and scaffold user understanding.\nCollaborative effort scaling offers a lens for diagnosing agent behavior and\nguiding development toward more effective interactions.", "AI": {"tldr": "\u5f53\u524d\u5bf9\u667a\u80fd\u4f53\u7684\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u4e00\u6b21\u6027\u4efb\u52a1\u5b8c\u6210\uff0c\u672a\u80fd\u8003\u8651\u5230\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u56fa\u6709\u7684\u8fed\u4ee3\u548c\u534f\u4f5c\u6027\u8d28\uff0c\u56e0\u4e3a\u4eba\u7c7b\u7684\u76ee\u6807\u901a\u5e38\u662f\u672a\u5145\u5206\u8bf4\u660e\u548c\u4e0d\u65ad\u53d8\u5316\u7684\u3002\u6211\u4eec\u4e3b\u5f20\u4ece\u6784\u5efa\u548c\u8bc4\u4f30\u4efb\u52a1\u5b8c\u6210\u667a\u80fd\u4f53\u8f6c\u5411\u5f00\u53d1\u534f\u4f5c\u667a\u80fd\u4f53\uff0c\u5176\u8bc4\u4f30\u4e0d\u4ec5\u4f9d\u636e\u6700\u7ec8\u8f93\u51fa\u7684\u8d28\u91cf\uff0c\u8fd8\u4f9d\u636e\u5b83\u4eec\u5728\u6574\u4e2a\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u4e2d\u4e0e\u4eba\u7c7b\u4e92\u52a8\u7684\u7a0b\u5ea6\u4ee5\u53ca\u5982\u4f55\u589e\u5f3a\u4eba\u7c7b\u7684\u52aa\u529b\u3002\u4e3a\u4e86\u652f\u6301\u8fd9\u4e00\u8f6c\u53d8\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u534f\u4f5c\u52aa\u529b\u6269\u5c55\uff0c\u8fd9\u662f\u4e00\u4e2a\u6355\u83b7\u667a\u80fd\u4f53\u6548\u7528\u5982\u4f55\u968f\u7740\u7528\u6237\u53c2\u4e0e\u5ea6\u589e\u52a0\u800c\u589e\u957f\u7684\u6846\u67b6\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u6a21\u62df\u8bc4\u4f30\uff0c\u6211\u4eec\u53d1\u73b0\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u5728\u591a\u8f6e\u3001\u73b0\u5b9e\u4e16\u754c\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u8bbe\u8ba1\u4e2d\u7f3a\u5931\u7684\u4e00\u73af\uff1a\u7ef4\u6301\u53c2\u4e0e\u548c\u811a\u624b\u67b6\u7528\u6237\u7406\u89e3\u7684\u80fd\u529b\u3002\u534f\u4f5c\u52aa\u529b\u6269\u5c55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bca\u65ad\u667a\u80fd\u4f53\u884c\u4e3a\u548c\u6307\u5bfc\u5f00\u53d1\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u4ea4\u4e92\u7684\u89c6\u89d2\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8bb8\u591a\u95ee\u9898\u672c\u8d28\u4e0a\u662f\u8fed\u4ee3\u548c\u534f\u4f5c\u7684\uff0c\u4eba\u7c7b\u7684\u76ee\u6807\u5f80\u5f80\u4e0d\u660e\u786e\u4e14\u4f1a\u968f\u7740\u65f6\u95f4\u6f14\u53d8\u3002\u7136\u800c\uff0c\u5f53\u524d\u5bf9\u667a\u80fd\u4f53\u7684\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u4e00\u6b21\u6027\u4efb\u52a1\u5b8c\u6210\u4e0a\uff0c\u672a\u80fd\u5145\u5206\u4f53\u73b0\u8fd9\u79cd\u8fed\u4ee3\u548c\u534f\u4f5c\u7684\u6027\u8d28\u3002\u8fd9\u79cd\u8bc4\u4f30\u65b9\u5f0f\u5ffd\u7565\u4e86\u667a\u80fd\u4f53\u5728\u6574\u4e2a\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u4e2d\u4e0e\u4eba\u7c7b\u4e92\u52a8\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u8f85\u52a9\u548c\u589e\u5f3a\u4eba\u7c7b\u7684\u52aa\u529b\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u5f25\u5408\u5f53\u524d\u667a\u80fd\u4f53\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63a8\u52a8\u667a\u80fd\u4f53\u8bbe\u8ba1\u4ece\u5355\u7eaf\u7684\u4efb\u52a1\u5b8c\u6210\u8f6c\u5411\u66f4\u5177\u534f\u4f5c\u6027\u7684\u4ea4\u4e92\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u534f\u4f5c\u52aa\u529b\u6269\u5c55\u201d\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u83b7\u667a\u80fd\u4f53\u6548\u7528\u968f\u7528\u6237\u53c2\u4e0e\u5ea6\u589e\u52a0\u800c\u589e\u957f\u7684\u65b9\u5f0f\u3002\u8be5\u6846\u67b6\u65e8\u5728\u91cf\u5316\u548c\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\u3002\u7814\u7a76\u91c7\u7528\u4e86\u6848\u4f8b\u7814\u7a76\u548c\u6a21\u62df\u8bc4\u4f30\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5177\u4f53\u7684\u6848\u4f8b\u5206\u6790\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u667a\u80fd\u4f53\u7684\u8868\u73b0\u3002\u540c\u65f6\uff0c\u5229\u7528\u6a21\u62df\u8bc4\u4f30\u6765\u7cfb\u7edf\u5730\u68c0\u9a8c\u5728\u4e0d\u540c\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\u4e0b\uff0c\u6700\u5148\u8fdb\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u6a21\u62df\u8bc4\u4f30\uff0c\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5728\u4e00\u6b21\u6027\u4efb\u52a1\u5b8c\u6210\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u5728\u591a\u8f6e\u3001\u73b0\u5b9e\u4e16\u754c\u7684\u534f\u4f5c\u573a\u666f\u4e2d\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u8868\u660e\u5f53\u524d\u667a\u80fd\u4f53\u8bbe\u8ba1\u4e2d\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u7684\u7f3a\u5931\u73af\u8282\uff0c\u5373\u7ef4\u6301\u7528\u6237\u53c2\u4e0e\u548c\u4fc3\u8fdb\u7528\u6237\u7406\u89e3\u7684\u80fd\u529b\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u5728\u590d\u6742\u3001\u8fed\u4ee3\u7684\u73af\u5883\u4e2d\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u4ece\u4e00\u6b21\u6027\u4efb\u52a1\u5b8c\u6210\u8bc4\u4f30\u8f6c\u5411\u534f\u4f5c\u6027\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u534f\u4f5c\u52aa\u529b\u6269\u5c55\u201d\u6846\u67b6\u6765\u91cf\u5316\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u80fd\u529b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u667a\u80fd\u4f53\u5728\u4e0e\u4eba\u7c7b\u8fdb\u884c\u591a\u8f6e\u534f\u4f5c\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u7ef4\u6301\u7528\u6237\u53c2\u4e0e\u548c\u63d0\u4f9b\u652f\u6301\u65b9\u9762\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5e94\u4fa7\u91cd\u4e8e\u5f00\u53d1\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u7528\u6237\u7406\u89e3\u548c\u53c2\u4e0e\u7684\u667a\u80fd\u4f53\uff0c\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u95ee\u9898\u7684\u6311\u6218\u3002\u534f\u4f5c\u52aa\u529b\u6269\u5c55\u6846\u67b6\u4e3a\u8bca\u65ad\u667a\u80fd\u4f53\u884c\u4e3a\u548c\u6307\u5bfc\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2510.25771", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25771", "abs": "https://arxiv.org/abs/2510.25771", "authors": ["Nathan Godey", "Wissam Antoun", "Rian Touchent", "Rachel Bawden", "\u00c9ric de la Clergerie", "Beno\u00eet Sagot", "Djam\u00e9 Seddah"], "title": "Gaperon: A Peppered English-French Generative Language Model Suite", "comment": null, "summary": "We release Gaperon, a fully open suite of French-English-coding language\nmodels designed to advance transparency and reproducibility in large-scale\nmodel training. The Gaperon family includes 1.5B, 8B, and 24B parameter models\ntrained on 2-4 trillion tokens, released with all elements of the training\npipeline: French and English datasets filtered with a neural quality\nclassifier, an efficient data curation and training framework, and hundreds of\nintermediate checkpoints. Through this work, we study how data filtering and\ncontamination interact to shape both benchmark and generative performance. We\nfind that filtering for linguistic quality enhances text fluency and coherence\nbut yields subpar benchmark results, and that late deliberate contamination --\ncontinuing training on data mixes that include test sets -- recovers\ncompetitive scores while only reasonably harming generation quality. We discuss\nhow usual neural filtering can unintentionally amplify benchmark leakage. To\nsupport further research, we also introduce harmless data poisoning during\npretraining, providing a realistic testbed for safety studies. By openly\nreleasing all models, datasets, code, and checkpoints, Gaperon establishes a\nreproducible foundation for exploring the trade-offs between data curation,\nevaluation, safety, and openness in multilingual language model development.", "AI": {"tldr": "Gaperon\u662f\u4e00\u4e2a\u5305\u542b1.5B\u30018B\u548c24B\u53c2\u6570\u7684\u6cd5\u8bed-\u82f1\u8bed-\u7f16\u7801\u8bed\u8a00\u6a21\u578b\u5957\u4ef6\uff0c\u65e8\u5728\u63d0\u9ad8\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u7684\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027\u3002\u8be5\u5957\u4ef6\u5305\u542b\u4e86\u8bad\u7ec3\u6240\u9700\u7684\u6240\u6709\u5143\u7d20\uff0c\u5982\u7ecf\u8fc7\u795e\u7ecf\u8d28\u91cf\u5206\u7c7b\u5668\u8fc7\u6ee4\u7684\u6cd5\u8bed\u548c\u82f1\u8bed\u6570\u636e\u96c6\u3001\u9ad8\u6548\u7684\u6570\u636e\u7b56\u9009\u548c\u8bad\u7ec3\u6846\u67b6\u4ee5\u53ca\u5927\u91cf\u7684\u4e2d\u95f4\u68c0\u67e5\u70b9\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8bed\u8a00\u8d28\u91cf\u8fc7\u6ee4\u80fd\u63d0\u9ad8\u6587\u672c\u6d41\u7545\u5ea6\u548c\u8fde\u8d2f\u6027\uff0c\u4f46\u4f1a\u964d\u4f4e\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\uff1b\u800c\u540e\u671f\u7684\u6570\u636e\u6c61\u67d3\uff08\u5728\u5305\u542b\u6d4b\u8bd5\u96c6\u7684\u6570\u636e\u6df7\u5408\u7269\u4e0a\u7ee7\u7eed\u8bad\u7ec3\uff09\u53ef\u4ee5\u6062\u590d\u6709\u7ade\u4e89\u529b\u7684\u5206\u6570\uff0c\u540c\u65f6\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u635f\u5bb3\u76f8\u5bf9\u8f83\u5c0f\u3002\u7814\u7a76\u8fd8\u8ba8\u8bba\u4e86\u901a\u5e38\u7684\u795e\u7ecf\u8fc7\u6ee4\u5982\u4f55\u65e0\u610f\u4e2d\u653e\u5927\u57fa\u51c6\u6d4b\u8bd5\u6cc4\u9732\uff0c\u5e76\u5f15\u5165\u4e86\u65e0\u5bb3\u7684\u6570\u636e\u6295\u6bd2\u65b9\u6cd5\uff0c\u4e3a\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u73b0\u5b9e\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002Gaperon\u7684\u5168\u9762\u5f00\u653e\u4e3a\u63a2\u7d22\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u4e2d\u6570\u636e\u7b56\u9009\u3001\u8bc4\u4f30\u3001\u5b89\u5168\u548c\u5f00\u653e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u548c\u4ee3\u7801\u751f\u6210\u9886\u57df\u3002\u73b0\u6709\u7814\u7a76\u5728\u6570\u636e\u8fc7\u6ee4\u3001\u6c61\u67d3\u4ee5\u53ca\u5b83\u4eec\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u65b9\u9762\u5b58\u5728\u4e89\u8bae\uff0c\u5e76\u4e14\u7f3a\u4e4f\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u3001\u5168\u9762\u7684\u8d44\u6e90\u6765\u652f\u6301\u5bf9\u8fd9\u4e9b\u95ee\u9898\u7684\u6df1\u5165\u7814\u7a76\u3002Gaperon\u7684\u53d1\u5e03\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u5b8c\u5168\u5f00\u653e\u7684\u8bad\u7ec3\u5957\u4ef6\uff0c\u4fc3\u8fdb\u5bf9\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u3001\u53ef\u590d\u73b0\u6027\u4ee5\u53ca\u6570\u636e\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u7684\u7406\u89e3\u3002", "method": "Gaperon\u5957\u4ef6\u5305\u542b\u4e86\u4e09\u4e2a\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\uff081.5B, 8B, 24B\uff09\u7684\u6cd5\u8bed-\u82f1\u8bed-\u7f16\u7801\u8bed\u8a00\u6a21\u578b\u3002\u8bad\u7ec3\u6570\u636e\u5305\u62ec\u6cd5\u8bed\u548c\u82f1\u8bed\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u795e\u7ecf\u8d28\u91cf\u5206\u7c7b\u5668\u8fdb\u884c\u8fc7\u6ee4\u3002\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u6570\u636e\u7b56\u9009\u548c\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u4fdd\u5b58\u4e86\u6570\u767e\u4e2a\u4e2d\u95f4\u68c0\u67e5\u70b9\u3002\u5b9e\u9a8c\u90e8\u5206\u7814\u7a76\u4e86\u6570\u636e\u8fc7\u6ee4\u548c\u6c61\u67d3\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u548c\u751f\u6210\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u65e0\u5bb3\u7684\u6570\u636e\u6295\u6bd2\u65b9\u6cd5\u7528\u4e8e\u5b89\u5168\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u5173\u6ce8\u8bed\u8a00\u8d28\u91cf\u7684\u8fc7\u6ee4\u80fd\u591f\u63d0\u5347\u6587\u672c\u7684\u6d41\u7545\u5ea6\u548c\u8fde\u8d2f\u6027\uff0c\u4f46\u4f1a\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u901a\u8fc7\u5728\u8bad\u7ec3\u540e\u671f\u5f15\u5165\u6d4b\u8bd5\u96c6\u6570\u636e\uff08\u5373\u2018\u6570\u636e\u6c61\u67d3\u2019\uff09\uff0c\u53ef\u4ee5\u6062\u590d\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\u4ec5\u4e3a\u2018\u5408\u7406\u2019\u7a0b\u5ea6\u3002\u7814\u7a76\u8fd8\u6307\u51fa\uff0c\u5e38\u89c4\u7684\u795e\u7ecf\u8fc7\u6ee4\u65b9\u6cd5\u53ef\u80fd\u4f1a\u65e0\u610f\u4e2d\u589e\u52a0\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u6cc4\u9732\u7684\u98ce\u9669\u3002\u6b64\u5916\uff0c\u5f15\u5165\u7684\u65e0\u5bb3\u6570\u636e\u6295\u6bd2\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u6a21\u578b\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "Gaperon\u5957\u4ef6\u7684\u5168\u9762\u5f00\u653e\uff08\u5305\u62ec\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u68c0\u67e5\u70b9\uff09\u4e3a\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u6570\u636e\u8fc7\u6ee4\u548c\u6c61\u67d3\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u53ca\u5176\u5bf9\u6027\u80fd\u7684\u6743\u8861\uff0c\u5f3a\u8c03\u4e86\u5728\u6a21\u578b\u5f00\u53d1\u4e2d\u5e73\u8861\u6570\u636e\u7b56\u9009\u3001\u8bc4\u4f30\u3001\u5b89\u5168\u6027\u548c\u5f00\u653e\u6027\u7684\u91cd\u8981\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u5229\u7528Gaperon\u6765\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fd9\u4e9b\u6743\u8861\uff0c\u5e76\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u7684\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2510.25628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25628", "abs": "https://arxiv.org/abs/2510.25628", "authors": ["Yusheng Liao", "Chaoyi Wu", "Junwei Liu", "Shuyang Jiang", "Pengcheng Qiu", "Haowen Wang", "Yun Yue", "Shuai Zhen", "Jian Wang", "Qianrui Fan", "Jinjie Gu", "Ya Zhang", "Yanfeng Wang", "Yu Wang", "Weidi Xie"], "title": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis", "comment": null, "summary": "Electronic Health Records (EHRs) contain rich yet complex information, and\ntheir automated analysis is critical for clinical decision-making. Despite\nrecent advances of large language models (LLMs) in clinical workflows, their\nability to analyze EHRs remains limited due to narrow task coverage and lack of\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\nthinking-graph-driven framework that enables to generate high-quality reasoning\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\ntraining paradigm, including domain adaptation, reasoning enhancement, and\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\n42 tasks, to comprehensively assess reasoning and prediction across EHR\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\noutperforms state-of-the-art commercial and open-source LLMs (including\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\nachieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\nEHR-R1, and EHR-Bench have significantly advanced the development for more\nreliable and clinically relevant EHR analysis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86EHR-Ins\uff0c\u4e00\u4e2a\u5305\u542b300k\u63a8\u7406\u6848\u4f8b\u548c4M\u975e\u63a8\u7406\u6848\u4f8b\u7684\u5927\u89c4\u6a21\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u63a8\u7406\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6b64\u8bad\u7ec3\u7684EHR-R1\u7cfb\u5217LLM\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\uff08\u9886\u57df\u9002\u5e94\u3001\u63a8\u7406\u589e\u5f3a\u3001\u5f3a\u5316\u5b66\u4e60\uff09\u5177\u5907EHR\u5206\u6790\u80fd\u529b\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u53d1\u5e03\u4e86EHR-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30EHR\u573a\u666f\u4e0b\u7684\u63a8\u7406\u548c\u9884\u6d4b\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cEHR-R1\u5728MIMIC-Bench\u4e0a\u8d85\u8d8aGPT-4o 30\u4f59\u5206\uff0c\u5728EHRSHOT\u4e0a\u5b9e\u73b0\u4e8610%\u7684\u96f6\u6837\u672cAUROC\u63d0\u5347\uff0c\u663e\u8457\u63d0\u5347\u4e86EHR\u5206\u6790\u7684\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u5305\u542b\u4e30\u5bcc\u4f46\u590d\u6742\u7684\u4fe1\u606f\uff0c\u5176\u81ea\u52a8\u5316\u5206\u6790\u5bf9\u4e8e\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u4efb\u52a1\u8986\u76d6\u8303\u56f4\u7a84\u548c\u7f3a\u4e4f\u9762\u5411EHR\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5176\u5206\u6790EHR\u7684\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u63d0\u5347LLMs\u5728EHR\u5206\u6790\u65b9\u9762\u7684\u6027\u80fd\u548c\u5e94\u7528\u3002", "method": "\u7814\u7a76\u7684\u6838\u5fc3\u521b\u65b0\u662f\u63d0\u51fa\u4e00\u4e2a\u601d\u8003-\u56fe\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5927\u89c4\u6a21\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u6570\u636e\uff0c\u4ece\u800c\u6784\u5efa\u4e86EHR-Ins\u5927\u89c4\u6a21EHR\u63a8\u7406\u6307\u4ee4\u6570\u636e\u96c6\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\uff0c\u7814\u7a76\u5f00\u53d1\u4e86EHR-R1\u7cfb\u5217LLM\uff0c\u5e76\u91c7\u7528\u4e86\u5305\u62ec\u9886\u57df\u9002\u5e94\u3001\u63a8\u7406\u589e\u5f3a\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u5185\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u4ee5\u7cfb\u7edf\u5730\u5b66\u4e60\u9886\u57df\u77e5\u8bc6\u548c\u591a\u6837\u5316\u7684\u63a8\u7406\u80fd\u529b\u3002\u6700\u540e\uff0c\u7814\u7a76\u53d1\u5e03\u4e86EHR-Bench\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u4eceMIMIC-IV\u4e2d\u63d0\u53d6\uff0c\u6db5\u76d642\u9879\u4efb\u52a1\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30EHR\u573a\u666f\u4e0b\u7684\u63a8\u7406\u548c\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEHR-R1\u7cfb\u5217\u6a21\u578b\u5728EHR\u5206\u6790\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u5546\u4e1a\u548c\u5f00\u6e90LLMs\uff08\u5305\u62ecDeepSeek-V3\u548cGPT-4o\uff09\u76f8\u6bd4\uff0cEHR-R1\u6301\u7eed\u53d6\u5f97\u4f18\u4e8e\u5b83\u4eec\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728MIMIC-Bench\u57fa\u51c6\u4e0a\uff0cEHR-R1\u7684\u5f97\u5206\u6bd4GPT-4o\u9ad8\u51fa30\u4f59\u5206\uff1b\u5728EHRSHOT\u4e0a\uff0c\u5b9e\u73b0\u4e8610%\u7684\u96f6\u6837\u672cAUROC\uff08Area Under the Receiver Operating Characteristic Curve\uff09\u63d0\u5347\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86EHR-R1\u5728EHR\u5206\u6790\u65b9\u9762\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86EHR-Ins\u6570\u636e\u96c6\u3001EHR-R1\u7cfb\u5217LLM\u548cEHR-Bench\u57fa\u51c6\uff0c\u8fd9\u4e09\u8005\u5171\u540c\u63a8\u52a8\u4e86\u66f4\u53ef\u9760\u3001\u66f4\u5177\u4e34\u5e8a\u76f8\u5173\u6027\u7684EHR\u5206\u6790\u6280\u672f\u7684\u53d1\u5c55\u3002EHR-R1\u6a21\u578b\u901a\u8fc7\u4e13\u95e8\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u7406\u89e3\u548c\u63a8\u7406EHR\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6269\u5c55\u6570\u636e\u96c6\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\uff0c\u5e76\u63a2\u7d22\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9EHR\u5206\u6790\u4e2d\u66f4\u590d\u6742\u7684\u6311\u6218\u3002"}}
{"id": "2510.25682", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25682", "abs": "https://arxiv.org/abs/2510.25682", "authors": ["Jiani Zheng", "Zhiyang Teng", "Xiangtai Li", "Anran Wang", "Yu Tian", "Kunpeng Qiu", "Ye Tian", "Haochen Wang", "Zhuochen Wang"], "title": "PairUni: Pairwise Training for Unified Multimodal Language Models", "comment": null, "summary": "Unified vision-language models (UVLMs) must perform both understanding and\ngeneration within a single architecture, but these tasks rely on heterogeneous\ndata and supervision, making it difficult to balance them during reinforcement\nlearning (RL). We propose PairUni, a unified framework that reorganizes data\ninto understanding-generation (UG) pairs and aligns optimization accordingly.\nWe first use GPT-o3 to augment single-task data, generating captions for\nunderstanding samples and question-answer (QA) pairs for generation samples,\nforming aligned pairs from the same instance. Additionally, for each generation\nsample, we retrieve a semantically related understanding example to form a\nretrieved pair, linking different but related data points. These paired\nstructures expose cross-task semantic correspondences and support consistent\npolicy learning. To leverage this structure, we present Pair-GPRO, a pair-aware\nvariant based on Group Relative Policy Optimization. It assigns a similarity\nscore to each pair to modulate the advantage, strengthening learning from\nwell-aligned examples and reducing task interference. We curate a high-quality\ndataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on\nthe powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on\nvarious UVLMs, outperforming strong UVLM RL baselines. Code:\n\\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PairUni \u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u7684\u6570\u636e\u91cd\u7ec4\u4e3a\u914d\u5bf9\u7ed3\u6784\uff08UG \u5bf9\u548c\u68c0\u7d22\u5bf9\uff09\uff0c\u5e76\u91c7\u7528 Pair-GPRO \u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7edf\u4e00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08UVLM\uff09\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u5e73\u8861\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u7684\u6311\u6218\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPairUni \u5728 Janus-Pro UVLM \u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u5404\u79cd UVLM \u4e0a\u5b9e\u73b0\u4e86\u5747\u8861\u7684\u6539\u8fdb\u3002", "motivation": "\u7edf\u4e00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08UVLM\uff09\u9700\u8981\u5728\u5355\u4e00\u67b6\u6784\u4e2d\u540c\u65f6\u5904\u7406\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u7136\u800c\u8fd9\u4e9b\u4efb\u52a1\u4f9d\u8d56\u4e8e\u5f02\u6784\u6570\u636e\u548c\u76d1\u7763\u4fe1\u53f7\uff0c\u5bfc\u81f4\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u5e73\u8861\u3002", "method": "PairUni \u6846\u67b6\u9996\u5148\u5229\u7528 GPT-o3 \u6269\u5145\u5355\u4efb\u52a1\u6570\u636e\uff0c\u4e3a\u7406\u89e3\u6837\u672c\u751f\u6210\u6807\u9898\uff0c\u4e3a\u751f\u6210\u6837\u672c\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u4ece\u800c\u5f62\u6210\u6765\u81ea\u540c\u4e00\u5b9e\u4f8b\u7684\u914d\u5bf9\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u751f\u6210\u6837\u672c\uff0c\u68c0\u7d22\u4e00\u4e2a\u8bed\u4e49\u76f8\u5173\u7684\u7406\u89e3\u6837\u672c\u5f62\u6210\u68c0\u7d22\u5bf9\uff0c\u8fde\u63a5\u4e0d\u540c\u7684\u4f46\u76f8\u5173\u7684\u6570\u636e\u70b9\u3002\u7136\u540e\uff0c\u63d0\u51fa Pair-GPRO\uff0c\u8fd9\u662f\u57fa\u4e8e Group Relative Policy Optimization \u7684\u4e00\u79cd\u914d\u5bf9\u611f\u77e5\u53d8\u4f53\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u914d\u5bf9\u5206\u914d\u76f8\u4f3c\u5ea6\u5f97\u5206\u6765\u8c03\u8282\u4f18\u52bf\uff0c\u4ece\u800c\u52a0\u5f3a\u4ece\u826f\u597d\u5bf9\u9f50\u7684\u6837\u672c\u4e2d\u5b66\u4e60\uff0c\u5e76\u51cf\u5c11\u4efb\u52a1\u5e72\u6270\u3002\u7814\u7a76\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 16K UG \u5bf9\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6 PairUG \u7528\u4e8e RL \u5fae\u8c03\uff0c\u5e76\u5728\u5f3a\u5927\u7684 Janus-Pro UVLMs \u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "PairUni \u5728 Janus-Pro UVLM \u4e0a\u5b9e\u73b0\u4e86\u5747\u8861\u7684\u6539\u8fdb\uff0c\u5e76\u4e14\u5728\u5404\u79cd UVLM \u4e0a\u5747\u4f18\u4e8e\u5f3a UVLM RL \u57fa\u7ebf\u3002", "conclusion": "PairUni \u6846\u67b6\u901a\u8fc7\u5176\u65b0\u9896\u7684\u914d\u5bf9\u6570\u636e\u7ed3\u6784\u548c Pair-GPRO \u4f18\u5316\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86 UVLM \u5728 RL \u4e2d\u5e73\u8861\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u7684\u96be\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2510.25701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25701", "abs": "https://arxiv.org/abs/2510.25701", "authors": ["Saeed AlMarri", "Kristof Juhasz", "Mathieu Ravaut", "Gautier Marti", "Hamdan Al Ahbabi", "Ibrahim Elfadel"], "title": "Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?", "comment": "8 pages, 6 figures, 3 tables, CIKM 2025 FinFAI workshop", "summary": "Large Language Models (LLMs) are increasingly explored as flexible\nalternatives to classical machine learning models for classification tasks\nthrough zero-shot prompting. However, their suitability for structured tabular\ndata remains underexplored, especially in high-stakes financial applications\nsuch as financial risk assessment. This study conducts a systematic comparison\nbetween zero-shot LLM-based classifiers and LightGBM, a state-of-the-art\ngradient-boosting model, on a real-world loan default prediction task. We\nevaluate their predictive performance, analyze feature attributions using SHAP,\nand assess the reliability of LLM-generated self-explanations. While LLMs are\nable to identify key financial risk indicators, their feature importance\nrankings diverge notably from LightGBM, and their self-explanations often fail\nto align with empirical SHAP attributions. These findings highlight the\nlimitations of LLMs as standalone models for structured financial risk\nprediction and raise concerns about the trustworthiness of their self-generated\nexplanations. Our results underscore the need for explainability audits,\nbaseline comparisons with interpretable models, and human-in-the-loop oversight\nwhen deploying LLMs in risk-sensitive financial environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u96f6\u6837\u672cLLM\u5206\u7c7b\u5668\u548cLightGBM\u5728\u771f\u5b9e\u4e16\u754c\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1LLM\u80fd\u591f\u8bc6\u522b\u5173\u952e\u98ce\u9669\u6307\u6807\uff0c\u4f46\u5176\u7279\u5f81\u91cd\u8981\u6027\u6392\u540d\u4e0eLightGBM\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u4e14\u5176\u81ea\u6211\u89e3\u91ca\u4e0e\u7ecf\u9a8cSHAP\u5f52\u56e0\u4e0d\u7b26\uff0c\u8868\u660eLLM\u4f5c\u4e3a\u72ec\u7acb\u7684\u7ed3\u6784\u5316\u91d1\u878d\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u5bf9\u5176\u81ea\u6211\u89e3\u91ca\u7684\u53ef\u9760\u6027\u63d0\u51fa\u8d28\u7591\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f5c\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u7684\u7075\u6d3b\u66ff\u4ee3\u65b9\u6848\u5f97\u5230\u5e7f\u6cdb\u63a2\u7d22\uff0c\u4f46\u5b83\u4eec\u5728\u7ed3\u6784\u5316\u8868\u683c\u6570\u636e\u4e0a\u7684\u9002\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u91d1\u878d\u98ce\u9669\u8bc4\u4f30\u7b49\u9ad8\u98ce\u9669\u91d1\u878d\u5e94\u7528\u4e2d\uff0c\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u8fd9\u9879\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc4\u4f30LLMs\u5728\u5904\u7406\u7ed3\u6784\u5316\u91d1\u878d\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u5728\u771f\u5b9e\u7684\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0c\u7cfb\u7edf\u5730\u6bd4\u8f83\u4e86\u96f6\u6837\u672cLLM\u5206\u7c7b\u5668\u548c\u5148\u8fdb\u7684\u68af\u5ea6\u63d0\u5347\u6a21\u578bLightGBM\u3002\u8bc4\u4f30\u5185\u5bb9\u5305\u62ec\u9884\u6d4b\u6027\u80fd\u3001\u4f7f\u7528SHAP\u7684\u7279\u5f81\u5f52\u56e0\u5206\u6790\uff0c\u4ee5\u53caLLM\u751f\u6210\u81ea\u6211\u89e3\u91ca\u7684\u53ef\u9760\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u80fd\u591f\u8bc6\u522b\u51fa\u5173\u952e\u7684\u91d1\u878d\u98ce\u9669\u6307\u6807\u3002\u7136\u800c\uff0c\u4e0eLightGBM\u76f8\u6bd4\uff0cLLMs\u7684\u7279\u5f81\u91cd\u8981\u6027\u6392\u540d\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u6b64\u5916\uff0cLLM\u751f\u6210\u7684\u81ea\u6211\u89e3\u91ca\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u80fd\u4e0e\u7ecf\u9a8c\u6027\u7684SHAP\u5f52\u56e0\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86LLMs\u4f5c\u4e3a\u72ec\u7acb\u7684\u7ed3\u6784\u5316\u91d1\u878d\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u5bf9\u5176\u751f\u6210\u7684\u81ea\u6211\u89e3\u91ca\u7684\u53ef\u9760\u6027\u8868\u793a\u62c5\u5fe7\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u98ce\u9669\u654f\u611f\u7684\u91d1\u878d\u73af\u5883\u4e2d\u90e8\u7f72LLMs\u65f6\uff0c\u9700\u8981\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5ba1\u8ba1\u3001\u4e0e\u53ef\u89e3\u91ca\u57fa\u7ebf\u6a21\u578b\u7684\u6bd4\u8f83\u4ee5\u53ca\u4eba\u5de5\u76d1\u7763\u3002 futura work\u5e94\u4fa7\u91cd\u4e8e\u63d0\u9ad8LLMs\u5728\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2407.14926", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2407.14926", "abs": "https://arxiv.org/abs/2407.14926", "authors": ["Bowen Fang", "Zixiao Yang", "Xuan Di"], "title": "TraveLLM: Could you plan my new public transit route in face of a network disruption?", "comment": "Accepted to ITSC 2025", "summary": "Existing navigation systems often fail during urban disruptions, struggling\nto incorporate real-time events and complex user constraints, such as avoiding\nspecific areas. We address this gap with TraveLLM, a system using Large\nLanguage Models (LLMs) for disruption-aware public transit routing. We leverage\nLLMs' reasoning capabilities to directly process multimodal user queries\ncombining natural language requests (origin, destination, preferences,\ndisruption info) with map data (e.g., subway, bus, bike-share). To evaluate\nthis approach, we design challenging test scenarios reflecting real-world\ndisruptions like weather events, emergencies, and dynamic service availability.\nWe benchmark the performance of state-of-the-art LLMs, including GPT-4, Claude\n3, and Gemini, on generating accurate travel plans. Our experiments demonstrate\nthat LLMs, notably GPT-4, can effectively generate viable and context-aware\nnavigation plans under these demanding conditions. These findings suggest a\npromising direction for using LLMs to build more flexible and intelligent\nnavigation systems capable of handling dynamic disruptions and diverse user\nneeds.", "AI": {"tldr": "\u73b0\u6709\u5bfc\u822a\u7cfb\u7edf\u5728\u57ce\u5e02\u4e2d\u65ad\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u5904\u7406\u5b9e\u65f6\u4e8b\u4ef6\u548c\u590d\u6742\u7528\u6237\u7ea6\u675f\u3002\u672c\u6587\u63d0\u51fa\u4e86TraveLLM\uff0c\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u4e2d\u65ad\u611f\u77e5\u516c\u5171\u4ea4\u901a\u8def\u7ebf\u89c4\u5212\u7684\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u76f4\u63a5\u5904\u7406\u7ed3\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\uff08\u8d77\u70b9\u3001\u7ec8\u70b9\u3001\u504f\u597d\u3001\u4e2d\u65ad\u4fe1\u606f\uff09\u548c\u5730\u56fe\u6570\u636e\uff08\u5982\u5730\u94c1\u3001\u516c\u4ea4\u3001\u5171\u4eab\u5355\u8f66\uff09\u7684\u591a\u6a21\u6001\u7528\u6237\u67e5\u8be2\u3002\u901a\u8fc7\u8bbe\u8ba1\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u4e2d\u65ad\uff08\u5982\u5929\u6c14\u4e8b\u4ef6\u3001\u7d27\u6025\u60c5\u51b5\u3001\u52a8\u6001\u670d\u52a1\u53ef\u7528\u6027\uff09\u7684\u6311\u6218\u6027\u6d4b\u8bd5\u573a\u666f\uff0c\u5e76\u5bf9GPT-4\u3001Claude 3\u548cGemini\u7b49\u5148\u8fdbLLM\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u8bc1\u660eLLM\uff08\u5c24\u5176\u662fGPT-4\uff09\u80fd\u5728\u8fd9\u4e9b\u4e25\u82db\u6761\u4ef6\u4e0b\u6709\u6548\u751f\u6210\u53ef\u884c\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u51fa\u884c\u8ba1\u5212\u3002\u8fd9\u8868\u660e\u4f7f\u7528LLM\u6784\u5efa\u66f4\u7075\u6d3b\u3001\u667a\u80fd\u7684\u5bfc\u822a\u7cfb\u7edf\u4ee5\u5e94\u5bf9\u52a8\u6001\u4e2d\u65ad\u548c\u591a\u6837\u5316\u7528\u6237\u9700\u6c42\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u7cfb\u7edf\u5728\u9762\u5bf9\u57ce\u5e02\u4e2d\u65ad\uff08\u5982\u5929\u6c14\u4e8b\u4ef6\u3001\u7d27\u6025\u60c5\u51b5\u3001\u670d\u52a1\u53ef\u7528\u6027\u52a8\u6001\u53d8\u5316\uff09\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u6574\u5408\u5b9e\u65f6\u4fe1\u606f\u548c\u5904\u7406\u7528\u6237\u590d\u6742\u7684\u4e2a\u6027\u5316\u9700\u6c42\uff08\u5982\u907f\u5f00\u7279\u5b9a\u533a\u57df\uff09\uff0c\u5bfc\u81f4\u5bfc\u822a\u5931\u8d25\u3002\u8fd9\u79cd\u4e0d\u8db3\u4e25\u91cd\u5f71\u54cd\u4e86\u7528\u6237\u5728\u7a81\u53d1\u72b6\u51b5\u4e0b\u7684\u51fa\u884c\u6548\u7387\u548c\u4f53\u9a8c\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u5b9e\u65f6\u52a8\u6001\u53d8\u5316\u5e76\u6ee1\u8db3\u7528\u6237\u590d\u6742\u7ea6\u675f\u7684\u5bfc\u822a\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u7684\u73b0\u5b9e\u610f\u4e49\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684TraveLLM\u7cfb\u7edf\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u76f4\u63a5\u5904\u7406\u591a\u6a21\u6001\u7684\u7528\u6237\u67e5\u8be2\uff0c\u8fd9\u4e9b\u67e5\u8be2\u7ed3\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff08\u5305\u62ec\u8d77\u70b9\u3001\u7ec8\u70b9\u3001\u51fa\u884c\u504f\u597d\u4ee5\u53ca\u4e2d\u65ad\u4fe1\u606f\uff09\u548c\u5730\u56fe\u6570\u636e\u3002\u4e3a\u4e86\u8bc4\u4f30\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u4e2d\u65ad\u573a\u666f\u7684\u6311\u6218\u6027\u6d4b\u8bd5\u6848\u4f8b\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u5bf9\u5305\u62ecGPT-4\u3001Claude 3\u548cGemini\u5728\u5185\u7684\u591a\u79cd\u5148\u8fdbLLM\u5728\u751f\u6210\u51c6\u786e\u51fa\u884c\u8ba1\u5212\u65b9\u9762\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u7279\u522b\u662fGPT-4\uff0c\u5728\u5904\u7406\u4e2d\u65ad\u611f\u77e5\u5bfc\u822a\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u8272\u3002\u5728\u6a21\u62df\u7684\u771f\u5b9e\u4e16\u754c\u4e2d\u65ad\u573a\u666f\u4e0b\uff0cLLM\u80fd\u591f\u751f\u6210\u51c6\u786e\u4e14\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u51fa\u884c\u8ba1\u5212\u3002\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5176\u4ed6LLM\u76f8\u6bd4\uff0cGPT-4\u5728\u751f\u6210\u53ef\u884c\u6027\u9ad8\u3001\u8003\u8651\u4e86\u5b9e\u65f6\u4e2d\u65ad\u4fe1\u606f\u7684\u5bfc\u822a\u65b9\u6848\u65b9\u9762\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u80fd\u529b\u3002\u8fd9\u8bc1\u660e\u4e86LLM\u5728\u5e94\u5bf9\u590d\u6742\u548c\u52a8\u6001\u7684\u5bfc\u822a\u6311\u6218\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684TraveLLM\u7cfb\u7edf\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u5bfc\u822a\u7cfb\u7edf\u5728\u57ce\u5e02\u4e2d\u65ad\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cLLM\uff08\u5c24\u5176\u662fGPT-4\uff09\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u591a\u6a21\u6001\u7528\u6237\u67e5\u8be2\uff0c\u5e76\u751f\u6210\u9002\u5e94\u5b9e\u65f6\u4e2d\u65ad\u548c\u7528\u6237\u7279\u5b9a\u7ea6\u675f\u7684\u5bfc\u822a\u8ba1\u5212\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u66f4\u7075\u6d3b\u3001\u66f4\u80fd\u6ee1\u8db3\u7528\u6237\u591a\u6837\u5316\u9700\u6c42\u7684\u667a\u80fd\u5bfc\u822a\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22LLM\u5728\u66f4\u5e7f\u6cdb\u7684\u5bfc\u822a\u5e94\u7528\u573a\u666f\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4f18\u5316\u6a21\u578b\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.10844", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2505.10844", "abs": "https://arxiv.org/abs/2505.10844", "authors": ["Simeng Han", "Howard Dai", "Stephen Xia", "Grant Zhang", "Chen Liu", "Lichang Chen", "Hoang Huy Nguyen", "Hongyuan Mei", "Jiayuan Mao", "R. Thomas McCoy"], "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models", "comment": "NeurIPS 2025", "summary": "Accuracy remains a standard metric for evaluating AI systems, but it offers\nlimited insight into how models arrive at their solutions. In this work, we\nintroduce a benchmark based on brainteasers written in long narrative form to\nprobe more deeply into the types of reasoning strategies that models use.\nBrainteasers are well-suited for this goal because they can be solved with\nmultiple approaches, such as a few-step solution that uses a creative insight\nor a longer solution that uses more brute force. We investigate large language\nmodels (LLMs) across multiple layers of reasoning, focusing not only on\ncorrectness but also on the quality and creativity of their solutions. We\ninvestigate many aspects of the reasoning process: (1) semantic parsing of the\nbrainteasers into precise mathematical competition style formats; (2)\ngenerating solutions from these mathematical forms; (3) self-correcting\nsolutions based on gold solutions; (4) producing step-by-step sketches of\nsolutions; and (5) making use of hints. We find that LLMs are in many cases\nable to find creative, insightful solutions to brainteasers, suggesting that\nthey capture some of the capacities needed to solve novel problems in creative\nways. Nonetheless, there also remain situations where they rely on brute force\ndespite the availability of more efficient, creative solutions, highlighting a\npotential direction for improvement in the reasoning abilities of LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u957f\u7bc7\u53d9\u4e8b\u8111\u7b4b\u6025\u8f6c\u5f2f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u6df1\u5165\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u7b56\u7565\uff0c\u4e0d\u4ec5\u5173\u6ce8\u6b63\u786e\u6027\uff0c\u8fd8\u5173\u6ce8\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\u548c\u521b\u9020\u529b\u3002\u7814\u7a76\u53d1\u73b0LLMs\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u80fd\u627e\u5230\u5bcc\u6709\u521b\u9020\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4e5f\u5b58\u5728\u4f9d\u8d56\u66b4\u529b\u8ba1\u7b97\u800c\u975e\u66f4\u4f18\u521b\u610f\u65b9\u6cd5\u7684\u60c5\u51b5\uff0c\u6307\u51fa\u4e86LLMs\u63a8\u7406\u80fd\u529b\u6539\u8fdb\u7684\u65b9\u5411\u3002", "motivation": "\u51c6\u786e\u7387\u4f5c\u4e3a\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u6807\u51c6\u6307\u6807\uff0c\u5176\u5c40\u9650\u6027\u5728\u4e8e\u96be\u4ee5\u63ed\u793a\u6a21\u578b\u5f97\u51fa\u89e3\u51b3\u65b9\u6848\u7684\u8fc7\u7a0b\u3002\u4e3a\u4e86\u66f4\u6df1\u5165\u5730\u7406\u89e3\u6a21\u578b\u6240\u4f7f\u7528\u7684\u63a8\u7406\u7b56\u7565\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u957f\u7bc7\u53d9\u4e8b\u8111\u7b4b\u6025\u8f6c\u5f2f\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002\u8111\u7b4b\u6025\u8f6c\u5f2f\u56e0\u5176\u53ef\u4ee5\u91c7\u7528\u591a\u79cd\u89e3\u9898\u65b9\u6cd5\uff08\u5982\u5de7\u5999\u7684\u987f\u609f\u6216\u8017\u65f6\u7684\u7a77\u4e3e\u641c\u7d22\uff09\u800c\u7279\u522b\u9002\u5408\u6b64\u76ee\u6807\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u957f\u7bc7\u53d9\u4e8b\u8111\u7b4b\u6025\u8f6c\u5f2f\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u7b56\u7565\u3002\u7814\u7a76\u5206\u6790\u4e86LLMs\u5728\u591a\u4e2a\u63a8\u7406\u5c42\u9762\u4e0a\u7684\u8868\u73b0\uff0c\u5305\u62ec\uff1a\uff081\uff09\u5c06\u8111\u7b4b\u6025\u8f6c\u5f2f\u89e3\u6790\u4e3a\u7cbe\u786e\u7684\u6570\u5b66\u7ade\u8d5b\u98ce\u683c\u683c\u5f0f\uff1b\uff082\uff09\u4ece\u6570\u5b66\u5f62\u5f0f\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff1b\uff083\uff09\u57fa\u4e8e\u53c2\u8003\u7b54\u6848\u8fdb\u884c\u81ea\u6211\u4fee\u6b63\uff1b\uff084\uff09\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u6b65\u9aa4\u8349\u56fe\uff1b\uff085\uff09\u5229\u7528\u63d0\u793a\u4fe1\u606f\u3002\u901a\u8fc7\u8fd9\u4e9b\u5206\u6790\uff0c\u6df1\u5165\u4e86\u89e3LLMs\u5728\u89e3\u51b3\u95ee\u9898\u65f6\u7684\u7b56\u7565\u9009\u62e9\u548c\u6548\u7387\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u4e3a\u8111\u7b4b\u6025\u8f6c\u5f2f\u627e\u5230\u5bcc\u6709\u521b\u9020\u6027\u548c\u6d1e\u5bdf\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u8868\u660e\u5b83\u4eec\u5177\u5907\u89e3\u51b3\u65b0\u9896\u95ee\u9898\u5e76\u4ee5\u521b\u65b0\u65b9\u5f0f\u601d\u8003\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u7814\u7a76\u4e5f\u89c2\u5bdf\u5230\uff0c\u5c3d\u7ba1\u5b58\u5728\u66f4\u9ad8\u6548\u3001\u66f4\u5177\u521b\u9020\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0cLLMs\u6709\u65f6\u4ecd\u4f1a\u4f9d\u8d56\u8ba1\u7b97\u91cf\u66f4\u5927\u7684\u201c\u66b4\u529b\u201d\u65b9\u6cd5\u6765\u89e3\u51b3\u95ee\u9898\u3002\u8fd9\u8868\u660eLLMs\u7684\u63a8\u7406\u80fd\u529b\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u7b56\u7565\u9009\u62e9\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u65b9\u9762\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u8111\u7b4b\u6025\u8f6c\u5f2f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89d2\uff0c\u8d85\u8d8a\u4e86\u5355\u7eaf\u7684\u51c6\u786e\u7387\u3002\u7814\u7a76\u7ed3\u679c\u663e\u793aLLMs\u5728\u521b\u9020\u6027\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u63a8\u7406\u7b56\u7565\u9009\u62e9\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u6700\u4f18\u89e3\u65f6\u53ef\u80fd\u503e\u5411\u4e8e\u91c7\u7528\u6b21\u4f18\u65b9\u6cd5\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u96c6\u4e2d\u4e8e\u6539\u8fdbLLMs\u7684\u63a8\u7406\u7b56\u7565\u9009\u62e9\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u66f4\u7075\u6d3b\u3001\u66f4\u9ad8\u6548\u5730\u5229\u7528\u4e0d\u540c\u89e3\u9898\u65b9\u6cd5\uff0c\u4ece\u800c\u63d0\u5347\u5176\u5728\u89e3\u51b3\u590d\u6742\u65b0\u9896\u95ee\u9898\u4e0a\u7684\u6574\u4f53\u8868\u73b0\u3002"}}
{"id": "2510.25741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25741", "abs": "https://arxiv.org/abs/2510.25741", "authors": ["Rui-Jie Zhu", "Zixuan Wang", "Kai Hua", "Tianyu Zhang", "Ziniu Li", "Haoran Que", "Boyi Wei", "Zixin Wen", "Fan Yin", "He Xing", "Lu Li", "Jiajun Shi", "Kaijing Ma", "Shanda Li", "Taylor Kergan", "Andrew Smith", "Xingwei Qu", "Mude Hui", "Bohong Wu", "Qiyang Min", "Hongzhi Huang", "Xun Zhou", "Wei Ye", "Jiaheng Liu", "Jian Yang", "Yunfeng Shi", "Chenghua Lin", "Enduo Zhao", "Tianle Cai", "Ge Zhang", "Wenhao Huang", "Yoshua Bengio", "Jason Eshraghian"], "title": "Scaling Latent Reasoning via Looped Language Models", "comment": null, "summary": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.", "AI": {"tldr": "Ouro\u662f\u4e00\u4e2a\u540d\u4e3aLoopLM\u7684\u65b0\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u878d\u5165\u8fed\u4ee3\u8ba1\u7b97\u3001\u71b5\u6b63\u5219\u5316\u76ee\u6807\u548c\u5927\u89c4\u6a21\u6570\u636e\uff087.7T tokens\uff09\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u7684\u77e5\u8bc6\u64cd\u63a7\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u4e8612B\u53c2\u6570\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3b\u8981\u901a\u8fc7\u663e\u5f0f\u6587\u672c\u751f\u6210\uff08\u5982Chain-of-Thought\uff09\u8fdb\u884c\u63a8\u7406\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5c06\u63a8\u7406\u63a8\u8fdf\u5230\u8bad\u7ec3\u540e\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u6570\u636e\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u4e00\u79cd\u80fd\u5c06\u63a8\u7406\u80fd\u529b\u6df1\u5ea6\u6574\u5408\u5230\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86Ouro\uff0c\u4e00\u4e2a\u57fa\u4e8eLoopLM\uff08Looped Language Models\uff09\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7cfb\u5217\u3002\u5176\u6838\u5fc3\u5728\u4e8e\u9884\u8bad\u7ec3\u9636\u6bb5\u5c31\u6784\u5efa\u63a8\u7406\u80fd\u529b\uff0c\u5177\u4f53\u5305\u62ec\uff1a\uff081\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8fed\u4ee3\u8ba1\u7b97\uff1b\uff082\uff09\u91c7\u7528\u71b5\u6b63\u5219\u5316\u76ee\u6807\u6765\u5b66\u4e60\u6df1\u5ea6\u5206\u914d\uff1b\uff083\uff09\u6269\u5c55\u52307.7T tokens\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u7814\u7a76\u4e2d\u4f7f\u7528\u4e86Ouro 1.4B\u548c2.6B\u6a21\u578b\uff0c\u5e76\u4e0e\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u768412B\u53c2\u6570\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "Ouro 1.4B\u548c2.6B\u6a21\u578b\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u80fd\u591f\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u9ad8\u8fbe12B\u53c2\u6570\u7684SOTA\uff08State-of-the-Art\uff09LLMs\u3002\u901a\u8fc7\u5bf9\u7167\u5b9e\u9a8c\u53d1\u73b0\uff0cOuro\u7684\u4f18\u52bf\u5728\u4e8e\u5176\u77e5\u8bc6\u64cd\u63a7\u80fd\u529b\u800c\u975e\u77e5\u8bc6\u5bb9\u91cf\u7684\u589e\u52a0\u3002\u6b64\u5916\uff0cLoopLM\u751f\u6210\u7684\u63a8\u7406\u8fc7\u7a0b\u6bd4\u663e\u5f0f\u7684Chain-of-Thought\uff08CoT\uff09\u66f4\u7b26\u5408\u6700\u7ec8\u8f93\u51fa\u3002", "conclusion": "LoopLM\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u6784\u5efa\u548c\u6269\u5c55\u65b9\u5411\uff0c\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5c55\u73b0\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002Ouro\u6a21\u578b\u7684\u6210\u529f\u8868\u660e\uff0c\u5c06\u63a8\u7406\u80fd\u529b\u6574\u5408\u5230\u9884\u8bad\u7ec3\u9636\u6bb5\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22LoopLM\u4f5c\u4e3aLLM\u53d1\u5c55\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.17801", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2505.17801", "abs": "https://arxiv.org/abs/2505.17801", "authors": ["B\u00e1lint Gyevn\u00e1r", "Christopher G. Lucas", "Stefano V. Albrecht", "Shay B. Cohen"], "title": "Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour", "comment": null, "summary": "Autonomous multi-agent systems (MAS) are useful for automating complex tasks\nbut raise trust concerns due to risks such as miscoordination or goal\nmisalignment. Explainability is vital for users' trust calibration, but\nexplainable MAS face challenges due to complex environments, the human factor,\nand non-standardised evaluation. Leveraging the counterfactual effect size\nmodel and LLMs, we propose Agentic eXplanations via Interrogative Simulation\n(AXIS). AXIS generates human-centred action explanations for multi-agent\npolicies by having an LLM interrogate an environment simulator using prompts\nlike 'whatif' and 'remove' to observe and synthesise counterfactual information\nover multiple rounds. We evaluate AXIS on autonomous driving across ten\nscenarios for five LLMs with a comprehensive methodology combining robustness,\nsubjective preference, correctness, and goal/action prediction with an external\nLLM as evaluator. Compared to baselines, AXIS improves perceived explanation\ncorrectness by at least 7.7% across all models and goal prediction accuracy by\n23% for four models, with comparable action prediction accuracy, achieving the\nhighest scores overall. Our code is open-sourced at\nhttps://github.com/gyevnarb/axis.", "AI": {"tldr": "\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\u65b9\u9762\u5f88\u6709\u7528\uff0c\u4f46\u7531\u4e8e\u534f\u8c03\u5931\u8bef\u6216\u76ee\u6807\u4e0d\u4e00\u81f4\u7b49\u98ce\u9669\uff0c\u4f1a\u5f15\u8d77\u4fe1\u4efb\u95ee\u9898\u3002\u53ef\u89e3\u91ca\u6027\u5bf9\u4e8e\u7528\u6237\u4fe1\u4efb\u6821\u51c6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53ef\u89e3\u91ca\u7684MAS\u9762\u4e34\u590d\u6742\u73af\u5883\u3001\u4eba\u4e3a\u56e0\u7d20\u548c\u975e\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u6311\u6218\u3002\u5229\u7528\u53cd\u4e8b\u5b9e\u6548\u5e94\u5927\u5c0f\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u901a\u8fc7\u8be2\u95ee\u6a21\u62df\u5b9e\u73b0\u7684\u667a\u80fd\u4f53\u53ef\u89e3\u91ca\u6027\uff08AXIS\uff09\u3002AXIS\u901a\u8fc7\u8ba9LLM\u4f7f\u7528\u201cwhatif\u201d\u548c\u201cremove\u201d\u7b49\u63d0\u793a\u8be2\u95ee\u73af\u5883\u6a21\u62df\u5668\uff0c\u5e76\u5728\u591a\u8f6e\u4e2d\u89c2\u5bdf\u548c\u7efc\u5408\u53cd\u4e8b\u5b9e\u4fe1\u606f\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7b56\u7565\u751f\u6210\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u64cd\u4f5c\u89e3\u91ca\u3002\u6211\u4eec\u5728\u81ea\u4e3b\u9a7e\u9a76\u7684\u5341\u4e2a\u573a\u666f\u4e2d\uff0c\u9488\u5bf9\u4e94\u79cdLLM\u8bc4\u4f30\u4e86AXIS\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u7efc\u5408\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u9c81\u68d2\u6027\u3001\u4e3b\u89c2\u504f\u597d\u3001\u6b63\u786e\u6027\u4ee5\u53ca\u76ee\u6807/\u64cd\u4f5c\u9884\u6d4b\uff0c\u5e76\u4ee5\u5916\u90e8LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u3002\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0cAXIS\u5728\u6240\u6709\u6a21\u578b\u4e0a\u5c06\u53ef\u611f\u77e5\u7684\u89e3\u91ca\u6b63\u786e\u6027\u63d0\u9ad8\u4e86\u81f3\u5c117.7%\uff0c\u5e76\u5c06\u76ee\u6807\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8623%\uff08\u9488\u5bf9\u56db\u79cd\u6a21\u578b\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u64cd\u4f5c\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5728\u603b\u4f53\u4e0a\u53d6\u5f97\u4e86\u6700\u9ad8\u5206\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5728https://github.com/gyevnarb/axis \u5f00\u6e90\u3002", "motivation": "\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u5982\u534f\u8c03\u5931\u8bef\u548c\u76ee\u6807\u4e0d\u4e00\u81f4\uff0c\u7ed9\u7528\u6237\u5e26\u6765\u4e86\u4fe1\u4efb\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u4fe1\u4efb\u95ee\u9898\uff0c\u53ef\u89e3\u91ca\u6027\u88ab\u8ba4\u4e3a\u662f\u7528\u6237\u6821\u51c6\u4fe1\u4efb\u7684\u5173\u952e\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u53ef\u89e3\u91caMAS\u65b9\u6cd5\u9762\u4e34\u7740\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u6311\u6218\u6e90\u4e8e\u590d\u6742\u591a\u53d8\u7684\u8fd0\u884c\u73af\u5883\u3001\u96be\u4ee5\u6349\u6478\u7684\u4eba\u7c7b\u56e0\u7d20\u4ee5\u53ca\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u8fd9\u4e9b\u56e0\u7d20\u5171\u540c\u963b\u788d\u4e86MAS\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u901a\u8fc7\u8be2\u95ee\u6a21\u62df\u5b9e\u73b0\u7684\u667a\u80fd\u4f53\u53ef\u89e3\u91ca\u6027\u201d\uff08AXIS\uff09\u7684\u65b0\u65b9\u6cd5\u3002AXIS\u5229\u7528\u53cd\u4e8b\u5b9e\u6548\u5e94\u5927\u5c0f\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u751f\u6210\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u591a\u667a\u80fd\u4f53\u7b56\u7565\u7684\u64cd\u4f5c\u89e3\u91ca\u3002\u5177\u4f53\u800c\u8a00\uff0cAXIS\u901a\u8fc7\u4e00\u4e2aLLM\u4e0e\u73af\u5883\u6a21\u62df\u5668\u8fdb\u884c\u4ea4\u4e92\uff0c\u5e76\u91c7\u7528\u201cwhatif\u201d\uff08\u5047\u5982\uff09\u548c\u201cremove\u201d\uff08\u79fb\u9664\uff09\u7b49\u7c7b\u578b\u7684\u67e5\u8be2\u63d0\u793a\u3002\u901a\u8fc7\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u89c2\u5bdf\u548c\u7efc\u5408\u8fd9\u4e9b\u53cd\u4e8b\u5b9e\u4fe1\u606f\uff0cAXIS\u80fd\u591f\u751f\u6210\u6709\u610f\u4e49\u7684\u89e3\u91ca\u3002\u7814\u7a76\u4eba\u5458\u5728\u81ea\u4e3b\u9a7e\u9a76\u9886\u57df\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e86\u5341\u4e2a\u4e0d\u540c\u7684\u573a\u666f\uff0c\u5e76\u4f7f\u7528\u4e86\u4e94\u79cd\u4e0d\u540c\u7684LLM\u3002\u8bc4\u4f30\u65b9\u6cd5\u7efc\u5408\u4e86\u9c81\u68d2\u6027\u3001\u4e3b\u89c2\u504f\u597d\u3001\u89e3\u91ca\u7684\u6b63\u786e\u6027\u4ee5\u53ca\u76ee\u6807\u548c\u64cd\u4f5c\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5916\u90e8LLM\u4f5c\u4e3a\u8bc4\u4f30\u88c1\u5224\u3002", "result": "AXIS\u5728\u81ea\u4e3b\u9a7e\u9a76\u4efb\u52a1\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u4e0e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cAXIS\u5728\u6240\u6709\u8bc4\u4f30\u7684LLM\u4e0a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u5bf9\u89e3\u91ca\u6b63\u786e\u6027\u7684\u611f\u77e5\uff0c\u5e73\u5747\u63d0\u5347\u4e86\u81f3\u5c117.7%\u3002\u6b64\u5916\uff0cAXIS\u5728\u76ee\u6807\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5bf9\u4e8e\u56db\u79cd\u6a21\u578b\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8623%\u3002\u5728\u64cd\u4f5c\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\uff0cAXIS\u7684\u8868\u73b0\u4e0e\u57fa\u7ebf\u76f8\u5f53\uff0c\u4f46\u6574\u4f53\u7efc\u5408\u8bc4\u5206\u6700\u9ad8\u3002", "conclusion": "AXIS\u65b9\u6cd5\u901a\u8fc7\u5229\u7528LLM\u4e0e\u73af\u5883\u6a21\u62df\u5668\u7684\u4ea4\u4e92\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u53ef\u89e3\u91caMAS\u9762\u4e34\u7684\u4fe1\u4efb\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u89e3\u91ca\u7684\u6b63\u786e\u6027\u548c\u76ee\u6807\u9884\u6d4b\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u5e76\u4e14\u5728\u81ea\u4e3b\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u7efc\u5408\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u503c\u5f97\u4fe1\u8d56\u7684\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22AXIS\u5728\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u53ca\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u89e3\u91ca\u751f\u6210\u548c\u8bc4\u4f30\u673a\u5236\u3002\u672c\u7814\u7a76\u7684\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.25761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25761", "abs": "https://arxiv.org/abs/2510.25761", "authors": ["Chumeng Liang", "Jiaxuan You"], "title": "DiagramEval: Evaluating LLM-Generated Diagrams via Graphs", "comment": null, "summary": "Diagrams play a central role in research papers for conveying ideas, yet they\nare often notoriously complex and labor-intensive to create. Although diagrams\nare presented as images, standard image generative models struggle to produce\nclear diagrams with well-defined structure. We argue that a promising direction\nis to generate demonstration diagrams directly in textual form as SVGs, which\ncan leverage recent advances in large language models (LLMs). However, due to\nthe complexity of components and the multimodal nature of diagrams,\nsufficiently discriminative and explainable metrics for evaluating the quality\nof LLM-generated diagrams remain lacking. In this paper, we propose\nDiagramEval, a novel evaluation metric designed to assess demonstration\ndiagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams\nas graphs, treating text elements as nodes and their connections as directed\nedges, and evaluates diagram quality using two new groups of metrics: node\nalignment and path alignment. For the first time, we effectively evaluate\ndiagrams produced by state-of-the-art LLMs on recent research literature,\nquantitatively demonstrating the validity of our metrics. Furthermore, we show\nhow the enhanced explainability of our proposed metrics offers valuable\ninsights into the characteristics of LLM-generated diagrams. Code:\nhttps://github.com/ulab-uiuc/diagram-eval.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiagramEval\u7684\u65b0\u578b\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u6f14\u793a\u56fe\u3002\u8be5\u6307\u6807\u5c06\u56fe\u89c6\u4e3a\u56fe\uff0c\u5e76\u5c06\u6587\u672c\u5143\u7d20\u89c6\u4e3a\u8282\u70b9\uff0c\u8fde\u63a5\u89c6\u4e3a\u6709\u5411\u8fb9\uff0c\u901a\u8fc7\u8282\u70b9\u5bf9\u9f50\u548c\u8def\u5f84\u5bf9\u9f50\u4e24\u7ec4\u65b0\u6307\u6807\u6765\u8861\u91cf\u56fe\u7684\u8d28\u91cf\u3002\u7814\u7a76\u8868\u660e\uff0cDiagramEval\u80fd\u591f\u6709\u6548\u8bc4\u4f30LLM\u751f\u6210\u7684\u56fe\uff0c\u5e76\u63d0\u4f9b\u5bf9\u5176\u7279\u6027\u7684\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "motivation": "\u7814\u7a76\u8bba\u6587\u4e2d\u7684\u56fe\u5bf9\u4e8e\u4f20\u8fbe\u601d\u60f3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u521b\u5efa\u5b83\u4eec\u901a\u5e38\u590d\u6742\u4e14\u8017\u65f6\u3002\u73b0\u6709\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u96be\u4ee5\u751f\u6210\u7ed3\u6784\u6e05\u6670\u7684\u56fe\u3002\u76f4\u63a5\u4ee5SVG\u6587\u672c\u5f62\u5f0f\u751f\u6210\u56fe\u53ef\u4ee5\u5229\u7528LLM\u7684\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u4f30\u6307\u6807\u6765\u8861\u91cfLLM\u751f\u6210\u56fe\u7684\u8d28\u91cf\uff0c\u56e0\u4e3a\u56fe\u7684\u590d\u6742\u6027\u548c\u591a\u6a21\u6001\u6027\u8d28\u3002", "method": "DiagramEval\u5c06\u56fe\u6982\u5ff5\u5316\u4e3a\u56fe\uff0c\u5c06\u6587\u672c\u5143\u7d20\u89c6\u4e3a\u8282\u70b9\uff0c\u8fde\u63a5\u89c6\u4e3a\u6709\u5411\u8fb9\u3002\u5b83\u4f7f\u7528\u4e24\u7ec4\u65b0\u6307\u6807\uff1a\u8282\u70b9\u5bf9\u9f50\uff08\u8861\u91cf\u8282\u70b9\uff08\u6587\u672c\u5143\u7d20\uff09\u7684\u51c6\u786e\u6027\uff09\u548c\u8def\u5f84\u5bf9\u9f50\uff08\u8861\u91cf\u8282\u70b9\u4e4b\u95f4\u8fde\u63a5\uff08\u5173\u7cfb\uff09\u7684\u51c6\u786e\u6027\uff09\u6765\u8bc4\u4f30\u56fe\u7684\u8d28\u91cf\u3002", "result": "DiagramEval\u9996\u6b21\u6709\u6548\u5730\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684LLMs\u5728\u8fd1\u671f\u7814\u7a76\u6587\u732e\u4e0a\u751f\u6210\u7684\u56fe\u3002\u5b9a\u91cf\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u6307\u6807\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660eDiagramEval\u7684\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u4e3a\u7406\u89e3LLM\u751f\u6210\u56fe\u7684\u7279\u6027\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002", "conclusion": "DiagramEval\u4e3a\u8bc4\u4f30LLM\u751f\u6210\u7684\u6f14\u793a\u56fe\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u6709\u6548\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u56fe\u89c6\u4e3a\u56fe\u5e76\u91c7\u7528\u8282\u70b9\u5bf9\u9f50\u548c\u8def\u5f84\u5bf9\u9f50\u6307\u6807\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u91cf\u5316\u8bc4\u4f30\u56fe\u7684\u8d28\u91cf\uff0c\u5e76\u63d0\u4f9b\u5173\u4e8eLLM\u751f\u6210\u56fe\u7279\u6027\u7684\u53ef\u89e3\u91ca\u89c1\u89e3\u3002\u8fd9\u4e3a\u672a\u6765\u5728\u56fe\u751f\u6210\u548c\u8bc4\u4f30\u9886\u57df\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.17818", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2505.17818", "abs": "https://arxiv.org/abs/2505.17818", "authors": ["Daeun Kyung", "Hyunseung Chung", "Seongsu Bae", "Jiho Kim", "Jae Ho Sohn", "Taerim Kim", "Soo Kyung Kim", "Edward Choi"], "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions", "comment": "Accepted as a Spotlight at NeurIPS 2025 Datasets and Benchmarks Track\n  (10 pages for main text, 4 pages for references, 36 pages for supplementary\n  materials)", "summary": "Doctor-patient consultations require multi-turn, context-aware communication\ntailored to diverse patient personas. Training or evaluating doctor LLMs in\nsuch settings requires realistic patient interaction systems. However, existing\nsimulators often fail to reflect the full range of personas seen in clinical\npractice. To address this, we introduce PatientSim, a patient simulator that\ngenerates realistic and diverse patient personas for clinical scenarios,\ngrounded in medical expertise. PatientSim operates using: 1) clinical profiles,\nincluding symptoms and medical history, derived from real-world data in the\nMIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:\npersonality, language proficiency, medical history recall level, and cognitive\nconfusion level, resulting in 37 unique combinations. We evaluate eight LLMs\nfor factual accuracy and persona consistency. The top-performing open-source\nmodel, Llama 3.3 70B, is validated by four clinicians to confirm the robustness\nof our framework. As an open-source, customizable platform, PatientSim provides\na reproducible and scalable solution that can be customized for specific\ntraining needs. Offering a privacy-compliant environment, it serves as a robust\ntestbed for evaluating medical dialogue systems across diverse patient\npresentations and shows promise as an educational tool for healthcare. The code\nis available at https://github.com/dek924/PatientSim.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PatientSim\uff0c\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u903c\u771f\u4e14\u591a\u6837\u5316\u60a3\u8005\u89d2\u8272\u7684\u4e34\u5e8a\u573a\u666f\u60a3\u8005\u6a21\u62df\u5668\u3002\u8be5\u6a21\u62df\u5668\u57fa\u4e8e\u771f\u5b9e\u533b\u7597\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u7ef4\u5ea6\uff08\u4e2a\u6027\u3001\u8bed\u8a00\u80fd\u529b\u3001\u75c5\u53f2\u56de\u5fc6\u6c34\u5e73\u3001\u8ba4\u77e5\u6df7\u4e71\u7a0b\u5ea6\uff09\u5b9a\u4e49\u4e8637\u79cd\u72ec\u7279\u7684\u60a3\u8005\u7ec4\u5408\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6a21\u62df\u5668\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u60a3\u8005\u591a\u6837\u6027\u7684\u95ee\u9898\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u516b\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u89d2\u8272\u4e00\u81f4\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5176\u4e2d\u5f00\u6e90\u6a21\u578bLlama 3.3 70B\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u5f97\u5230\u56db\u4f4d\u4e34\u5e8a\u533b\u751f\u7684\u9a8c\u8bc1\u3002PatientSim\u4f5c\u4e3a\u4e00\u4e2a\u5f00\u6e90\u3001\u53ef\u5b9a\u5236\u4e14\u7b26\u5408\u9690\u79c1\u89c4\u8303\u7684\u5e73\u53f0\uff0c\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u533b\u7597\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6709\u671b\u6210\u4e3a\u91cd\u8981\u7684\u6559\u5b66\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u751f-\u60a3\u8005\u6c9f\u901a\u6a21\u62df\u7cfb\u7edf\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u533b\u751f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5b83\u4eec\u751f\u6210\u7684\u60a3\u8005\u89d2\u8272\u4e0d\u591f\u591a\u6837\u5316\uff0c\u65e0\u6cd5\u5b8c\u5168\u53cd\u6620\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u771f\u5b9e\u60c5\u51b5\u3002\u8fd9\u963b\u788d\u4e86LLM\u5728\u9700\u8981\u591a\u8f6e\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u4e14\u9488\u5bf9\u4e0d\u540c\u60a3\u8005\u7279\u5f81\u8fdb\u884c\u4e2a\u6027\u5316\u6c9f\u901a\u7684\u533b\u7597\u573a\u666f\u4e2d\u7684\u5e94\u7528\u548c\u8bc4\u4f30\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u903c\u771f\u4e14\u591a\u6837\u5316\u60a3\u8005\u89d2\u8272\u7684\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u63d0\u9ad8LLM\u5728\u533b\u7597\u9886\u57df\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "PatientSim\u6a21\u62df\u5668\u901a\u8fc7\u6574\u5408\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u6570\u636e\uff08\u6765\u81eaMIMIC-ED\u548cMIMIC-IV\u6570\u636e\u96c6\uff09\u6765\u6784\u5efa\u4e34\u5e8a\u6863\u6848\uff0c\u5305\u62ec\u75c7\u72b6\u548c\u75c5\u53f2\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u5b9a\u4e49\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\u6765\u751f\u6210\u591a\u6837\u5316\u7684\u60a3\u8005\u89d2\u8272\uff1a1\uff09\u4e2a\u6027\uff1b2\uff09\u8bed\u8a00\u80fd\u529b\uff1b3\uff09\u75c5\u53f2\u56de\u5fc6\u6c34\u5e73\uff1b4\uff09\u8ba4\u77e5\u6df7\u4e71\u7a0b\u5ea6\u3002\u8fd9\u56db\u4e2a\u7ef4\u5ea6\u7684\u4e0d\u540c\u7ec4\u5408\u5f62\u6210\u4e8637\u79cd\u72ec\u7279\u7684\u60a3\u8005\u89d2\u8272\u3002\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u8be5\u6a21\u62df\u5668\u8bc4\u4f30\u4e86\u516b\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u89d2\u8272\u4e00\u81f4\u6027\u65b9\u9762\u7684\u80fd\u529b\u3002\u6700\u540e\uff0c\u7531\u56db\u4f4d\u4e34\u5e8a\u533b\u751f\u5bf9\u8868\u73b0\u6700\u4f73\u7684\u5f00\u6e90\u6a21\u578bLlama 3.3 70B\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4ee5\u786e\u8ba4\u6846\u67b6\u7684\u7a33\u5065\u6027\u3002", "result": "\u5728\u5bf9\u516b\u4e2aLLM\u7684\u8bc4\u4f30\u4e2d\uff0cPatientSim\u80fd\u591f\u751f\u6210\u5177\u6709\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u89d2\u8272\u4e00\u81f4\u6027\u7684\u60a3\u8005\u4ea4\u4e92\u3002\u5f00\u6e90\u6a21\u578bLlama 3.3 70B\u5728\u5404\u9879\u8bc4\u4f30\u6307\u6807\u4e2d\u8868\u73b0\u6700\u4f18\u3002\u8be5\u6a21\u578b\u7684\u6027\u80fd\u5f97\u5230\u4e86\u56db\u4f4d\u4e34\u5e8a\u533b\u751f\u7684\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86PatientSim\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u7a33\u5065\u6027\u3002", "conclusion": "PatientSim\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u903c\u771f\u4e14\u591a\u6837\u5316\u60a3\u8005\u89d2\u8272\u7684\u4e34\u5e8a\u573a\u666f\u6a21\u62df\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u62df\u5668\u5728\u60a3\u8005\u591a\u6837\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u8be5\u7cfb\u7edf\u57fa\u4e8e\u771f\u5b9e\u533b\u7597\u6570\u636e\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u89d2\u8272\u5b9a\u4e49\uff0c\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30LLM\u5728\u533b\u7597\u5bf9\u8bdd\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u53ef\u6269\u5c55\u4e14\u7b26\u5408\u9690\u79c1\u7684\u5e73\u53f0\u3002\u5f00\u6e90\u7684\u7279\u6027\u4f7f\u5f97PatientSim\u6613\u4e8e\u5b9a\u5236\u548c\u590d\u7528\uff0c\u6709\u671b\u6210\u4e3a\u63a8\u52a8\u533b\u7597LLM\u53d1\u5c55\u548c\u6539\u8fdb\u533b\u60a3\u6c9f\u901a\u6559\u80b2\u7684\u91cd\u8981\u5de5\u5177\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6269\u5c55\u89d2\u8272\u7ef4\u5ea6\u548c\u5bf9\u8bdd\u573a\u666f\u3002"}}
{"id": "2510.25766", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25766", "abs": "https://arxiv.org/abs/2510.25766", "authors": ["Sriram Balasubramaniam", "Samyadeep Basu", "Koustava Goswami", "Ryan Rossi", "Varun Manjunatha", "Roshan Santhosh", "Ruiyi Zhang", "Soheil Feizi", "Nedim Lipka"], "title": "Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models", "comment": "Post-hoc attribution", "summary": "Large language models (LLMs) are increasingly used for long-document question\nanswering, where reliable attribution to sources is critical for trust.\nExisting post-hoc attribution methods work well for extractive QA but struggle\nin multi-hop, abstractive, and semi-extractive settings, where answers\nsynthesize information across passages. To address these challenges, we argue\nthat post-hoc attribution can be reframed as a reasoning problem, where answers\nare decomposed into constituent units, each tied to specific context. We first\nshow that prompting models to generate such decompositions alongside\nattributions improves performance. Building on this, we introduce DecompTune, a\npost-training method that teaches models to produce answer decompositions as\nintermediate reasoning steps. We curate a diverse dataset of complex QA tasks,\nannotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and\n14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.\nAcross extensive experiments and ablations, DecompTune substantially improves\nattribution quality, outperforming prior methods and matching or exceeding\nstate-of-the-art frontier models.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u6587\u6863\u95ee\u7b54\u4e2d\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7efc\u5408\u8de8\u6bb5\u4fe1\u606f\u7684\u62bd\u53d6\u5f0f\u95ee\u7b54\u4ee5\u5916\u7684\u573a\u666f\uff08\u5982\u591a\u8df3\u3001\u6458\u8981\u5f0f\u3001\u534a\u62bd\u53d6\u5f0f\u95ee\u7b54\uff09\u4e2d\uff0c\u5bf9\u6765\u6e90\u8fdb\u884c\u53ef\u9760\u5f52\u5c5e\u7684\u6311\u6218\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u5c06\u4e8b\u540e\u5f52\u5c5e\u91cd\u6784\u4e3a\u4e00\u4e2a\u63a8\u7406\u95ee\u9898\u3002\u901a\u8fc7\u63d0\u793a\u6a21\u578b\u751f\u6210\u7b54\u6848\u7684\u7ec4\u6210\u5355\u5143\u53ca\u5176\u5bf9\u5e94\u7684\u4e0a\u4e0b\u6587\uff0c\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u7814\u7a76\u5f15\u5165\u4e86DecompTune\uff0c\u4e00\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7684SFT + GRPO\u6d41\u6c34\u7ebf\uff0c\u5e76\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u7684\u5956\u52b1\uff0c\u5bf9Qwen-2.5\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u4f7f\u5176\u80fd\u591f\u751f\u6210\u7b54\u6848\u5206\u89e3\u4f5c\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDecompTune\u663e\u8457\u63d0\u9ad8\u4e86\u5f52\u5c5e\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5ab2\u7f8e\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u6587\u6863\u95ee\u7b54\u4e2d\uff0c\u867d\u7136\u5728\u62bd\u53d6\u5f0f\u95ee\u7b54\u65b9\u9762\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u7684\u6765\u6e90\u5f52\u5c5e\uff0c\u4f46\u5728\u591a\u8df3\u3001\u6458\u8981\u5f0f\u548c\u534a\u62bd\u53d6\u5f0f\u95ee\u7b54\u7b49\u66f4\u590d\u6742\u7684\u573a\u666f\u4e2d\uff0c\u5373\u7b54\u6848\u9700\u8981\u7efc\u5408\u591a\u4e2a\u6587\u6863\u7247\u6bb5\u7684\u4fe1\u606f\u65f6\uff0c\u5176\u5f52\u5c5e\u80fd\u529b\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u79cd\u5f52\u5c5e\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86\u7528\u6237\u5bf9LLM\u751f\u6210\u7b54\u6848\u7684\u4fe1\u4efb\u5ea6\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u6539\u8fdbLLMs\u5728\u590d\u6742\u95ee\u7b54\u573a\u666f\u4e0b\u7684\u5f52\u5c5e\u80fd\u529b\uff0c\u4ee5\u589e\u5f3a\u5176\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u8be5\u7814\u7a76\u5c06\u4e8b\u540e\u5f52\u5c5e\u95ee\u9898\u91cd\u6784\u4e3a\u4e00\u4e2a\u63a8\u7406\u95ee\u9898\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u7b54\u6848\u5206\u89e3\u4e3a\u72ec\u7acb\u7684\u7ec4\u6210\u5355\u5143\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5355\u5143\u627e\u5230\u5177\u4f53\u7684\u4e0a\u4e0b\u6587\u6765\u6e90\u3002\u9996\u5148\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u63d0\u793a\u6a21\u578b\u5728\u751f\u6210\u7b54\u6848\u7684\u540c\u65f6\u4e5f\u751f\u6210\u8fd9\u79cd\u5206\u89e3\u548c\u5f52\u5c5e\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5f52\u5c5e\u6027\u80fd\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u7814\u7a76\u63d0\u51fa\u4e86DecompTune\uff0c\u4e00\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002DecompTune\u901a\u8fc7\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u7684\u53d8\u4f53\uff08GRPO\uff09\u6d41\u6c34\u7ebf\uff0c\u5bf9Qwen-2.5\u6a21\u578b\uff08\u5305\u62ec7B\u548c14B\u53c2\u6570\u7248\u672c\uff09\u8fdb\u884c\u8bad\u7ec3\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u4e86\u4e13\u95e8\u4e3a\u8fd9\u9879\u4efb\u52a1\u8bbe\u8ba1\u7684\u3001\u5305\u542b\u590d\u6742\u95ee\u7b54\u548c\u7b54\u6848\u5206\u89e3\u6ce8\u91ca\u7684\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u4e86\u4efb\u52a1\u7279\u5b9a\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u901a\u8fc7\u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u590d\u6742\u95ee\u7b54\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0cDecompTune\u65b9\u6cd5\u88ab\u8bc1\u660e\u80fd\u591f\u663e\u8457\u63d0\u5347\u7b54\u6848\u5f52\u5c5e\u7684\u8d28\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDecompTune\u5728\u5f52\u5c5e\u51c6\u786e\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u4e8b\u540e\u5f52\u5c5e\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u80fd\u591f\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "conclusion": "DecompTune\u901a\u8fc7\u5c06\u4e8b\u540e\u5f52\u5c5e\u89c6\u4e3a\u4e00\u4e2a\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u5c06\u7b54\u6848\u5206\u89e3\u4e3a\u53ef\u8ffd\u6eaf\u7684\u7ec4\u6210\u5355\u5143\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u957f\u6587\u6863\u95ee\u7b54\u573a\u666f\u4e0b\u7684\u5f52\u5c5e\u96be\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u540e\u8bad\u7ec3\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5f52\u5c5e\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u3002\u867d\u7136\u7814\u7a76\u5c55\u793a\u4e86DecompTune\u7684\u6709\u6548\u6027\uff0c\u4f46\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\u67b6\u6784\u3001\u66f4\u590d\u6742\u7684\u4efb\u52a1\u7c7b\u578b\u4ee5\u53ca\u81ea\u52a8\u5316\u7684\u5206\u89e3\u4e0e\u5f52\u5c5e\u7b56\u7565\u3002"}}
{"id": "2506.17585", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17585", "abs": "https://arxiv.org/abs/2506.17585", "authors": ["Yukun Huang", "Sanxing Chen", "Jian Pei", "Manzil Zaheer", "Bhuwan Dhingra"], "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models", "comment": null, "summary": "Trustworthy language models should provide both correct and verifiable\nanswers. However, citations generated directly by standalone LLMs are often\nunreliable. As a result, current systems insert citations by querying an\nexternal retriever at inference time, introducing latency, infrastructure\ndependence, and vulnerability to retrieval noise. We explore whether LLMs can\nbe made to reliably attribute to the documents seen during continual\npretraining without test-time retrieval, by revising the training process. To\nstudy this, we construct CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel documents and probes both\nshort-form (single-fact) and long-form (multi-fact) citation tasks. Our\napproach follows a two-stage process: (1) continual pretraining to index\nfactual knowledge by binding it to persistent document identifiers; and (2)\ninstruction tuning to elicit citation behavior. We introduce Active Indexing\nfor the first stage, which creates generalizable, source-anchored bindings by\naugmenting training with synthetic data that (i) restate each fact in diverse,\ncompositional forms and (ii) enforce bidirectional training (source-to-fact and\nfact-to-source). This equips the model to both generate content from a cited\nsource and attribute its own answers, improving robustness to paraphrase and\ncomposition. Experiments with Qwen-2.5-7B&3B show that Active Indexing\nconsistently outperforms a Passive Indexing baseline, which simply appends an\nidentifier to each document, achieving citation precision gains of up to 30.2%\nacross all tasks and models. Our ablation studies reveal that performance\ncontinues to improve as we scale the amount of augmented data, showing a clear\nupward trend even at 16x the original token count. Finally, we show that\ninternal citations complement external ones by making the model more robust to\nretrieval noise.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cCitePretrainBench\u201d\u7684\u65b0\u57fa\u51c6\u548c\u4e00\u79cd\u540d\u4e3a\u201cActive Indexing\u201d\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e8\u5728\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u65e0\u9700\u5916\u90e8\u68c0\u7d22\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u9760\u5730\u5f15\u7528\u9884\u8bad\u7ec3\u671f\u95f4\u89c1\u8fc7\u7684\u6587\u6863\u3002\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u5fae\u8c03\uff0c\u7279\u522b\u662f\u5229\u7528\u589e\u5f3a\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u4e3b\u52a8\u7d22\u5f15\uff0c\u6a21\u578b\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5c06\u4e8b\u5b9e\u4e0e\u5176\u6765\u6e90\u7ed1\u5b9a\uff0c\u4ece\u800c\u5728\u5f15\u7528\u51c6\u786e\u6027\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5e76\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u9762\u5bf9\u68c0\u7d22\u566a\u58f0\u65f6\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7b54\u6848\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u5b83\u4eec\u76f4\u63a5\u751f\u6210\u7684\u5f15\u7528\u5e38\u5e38\u4e0d\u53ef\u9760\u3002\u5f53\u524d\u4f9d\u8d56\u5916\u90e8\u68c0\u7d22\u5668\u7684\u65b9\u6cd5\u4f1a\u589e\u52a0\u5ef6\u8fdf\u3001\u57fa\u7840\u8bbe\u65bd\u6210\u672c\uff0c\u5e76\u5bb9\u6613\u53d7\u5230\u68c0\u7d22\u566a\u58f0\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7fLLMs\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u6d4b\u8bd5\u65f6\u68c0\u7d22\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u9760\u5730\u5f15\u7528\u9884\u8bad\u7ec3\u671f\u95f4\u63a5\u89e6\u8fc7\u7684\u6587\u6863\uff0c\u4ee5\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aCitePretrainBench\u7684\u65b0\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u6df7\u5408\u4e86Wikipedia\u3001Common Crawl\u548carXiv\u7b49\u771f\u5b9e\u8bed\u6599\u5e93\u4ee5\u53ca\u65b0\u6587\u6863\uff0c\u5e76\u5305\u542b\u5355\u4e8b\u5b9e\u548c\u591a\u4e8b\u5b9e\u5f15\u7528\u4efb\u52a1\u3002\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff1a1. **\u6301\u7eed\u9884\u8bad\u7ec3**\uff1a\u901a\u8fc7\u201cActive Indexing\u201d\u6280\u672f\uff0c\u5c06\u4e8b\u5b9e\u77e5\u8bc6\u4e0e\u5176\u6301\u4e45\u7684\u6587\u6863\u6807\u8bc6\u7b26\u7ed1\u5b9a\u3002Active Indexing\u901a\u8fc7\u589e\u5f3a\u5408\u6210\u6570\u636e\u6765\u5b9e\u73b0\uff0c\u8fd9\u4e9b\u6570\u636e\u5305\u62ec\uff1a(i)\u4ee5\u591a\u6837\u5316\u7684\u3001\u7ec4\u5408\u7684\u5f62\u5f0f\u91cd\u8ff0\u4e8b\u5b9e\uff1b(ii)\u5f3a\u5236\u53cc\u5411\u8bad\u7ec3\uff08\u6e90\u5230\u4e8b\u5b9e\u548c\u4e8b\u5b9e\u5230\u6e90\uff09\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5bf9\u91ca\u4e49\u548c\u7ec4\u5408\u7684\u9c81\u68d2\u6027\u30022. **\u6307\u4ee4\u5fae\u8c03**\uff1a\u7528\u4e8e\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u5f15\u7528\u884c\u4e3a\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86Qwen-2.5-7B\u548c3B\u6a21\u578b\uff0c\u5e76\u4e0e\u201cPassive Indexing\u201d\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u4f7f\u7528Qwen-2.5-7B\u548c3B\u6a21\u578b\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u201cActive Indexing\u201d\u65b9\u6cd5\u5728\u6240\u6709\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u5747\u6301\u7eed\u4f18\u4e8e\u201cPassive Indexing\u201d\u57fa\u7ebf\uff0c\u5f15\u7528\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u5347\u4e8630.2%\u3002\u6d88\u878d\u7814\u7a76\u663e\u793a\uff0c\u968f\u7740\u589e\u5f3a\u6570\u636e\u91cf\u7684\u589e\u52a0\uff0c\u6027\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u5373\u4f7f\u5728\u539f\u59cb token \u6570\u91cf\u768416\u500d\u65f6\u4ecd\u5448\u73b0\u660e\u663e\u7684\u4e0a\u5347\u8d8b\u52bf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u5185\u90e8\u5f15\u7528\u53ef\u4ee5\u4f5c\u4e3a\u5916\u90e8\u5f15\u7528\u7684\u8865\u5145\uff0c\u63d0\u9ad8\u6a21\u578b\u5bf9\u68c0\u7d22\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6d4b\u8bd5\u65f6\u68c0\u7d22\u5373\u53ef\u5b9e\u73b0\u53ef\u4fe1\u5f15\u7528\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002\u901a\u8fc7CitePretrainBench\u57fa\u51c6\u548cActive Indexing\u8bad\u7ec3\u7b56\u7565\uff0c\u6a21\u578b\u80fd\u591f\u66f4\u53ef\u9760\u5730\u5c06\u5176\u751f\u6210\u7684\u5185\u5bb9\u4e0e\u5176\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u76f8\u5173\u8054\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5f15\u7528\u51c6\u786e\u6027\uff0c\u8fd8\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u9762\u4e34\u68c0\u7d22\u566a\u58f0\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u503c\u5f97\u4fe1\u8d56\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u66f4\u590d\u6742\u7684\u5f15\u7528\u573a\u666f\u3002"}}
{"id": "2507.22149", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22149", "abs": "https://arxiv.org/abs/2507.22149", "authors": ["Xianxuan Long", "Yao Fu", "Runchao Li", "Mu Sheng", "Haotian Yu", "Xiaotian Han", "Pan Li"], "title": "When Truthful Representations Flip Under Deceptive Instructions?", "comment": null, "summary": "Large language models (LLMs) tend to follow maliciously crafted instructions\nto generate deceptive responses, posing safety challenges. How deceptive\ninstructions alter the internal representations of LLM compared to truthful\nones remains poorly understood beyond output analysis. To bridge this gap, we\ninvestigate when and how these representations ``flip'', such as from truthful\nto deceptive, under deceptive versus truthful/neutral instructions. Analyzing\nthe internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct\non a factual verification task, we find the model's instructed True/False\noutput is predictable via linear probes across all conditions based on the\ninternal representation. Further, we use Sparse Autoencoders (SAEs) to show\nthat the Deceptive instructions induce significant representational shifts\ncompared to Truthful/Neutral representations (which are similar), concentrated\nin early-to-mid layers and detectable even on complex datasets. We also\nidentify specific SAE features highly sensitive to deceptive instruction and\nuse targeted visualizations to confirm distinct truthful/deceptive\nrepresentational subspaces. % Our analysis pinpoints layer-wise and\nfeature-level correlates of instructed dishonesty, offering insights for LLM\ndetection and control. Our findings expose feature- and layer-level signatures\nof deception, offering new insights for detecting and mitigating instructed\ndishonesty in LLMs.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bb9\u6613\u88ab\u6076\u610f\u6307\u4ee4\u8bf1\u5bfc\u4ea7\u751f\u6b3a\u9a97\u6027\u56de\u7b54\uff0c\u8fd9\u5e26\u6765\u4e86\u5b89\u5168\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u8d85\u8d8a\u8f93\u51fa\u5206\u6790\uff0c\u6df1\u5165\u7406\u89e3\u6b3a\u9a97\u6027\u6307\u4ee4\u5982\u4f55\u6539\u53d8LLM\u7684\u5185\u90e8\u8868\u5f81\u3002\u7814\u7a76\u4eba\u5458\u5206\u6790\u4e86Llama-3.1-8B-Instruct\u548cGemma-2-9B-Instruct\u6a21\u578b\u5728\u5904\u7406\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u65f6\uff0c\u6b3a\u9a97\u6027\u6307\u4ee4\u4e0e\u771f\u5b9e/\u4e2d\u6027\u6307\u4ee4\u5bf9\u5176\u5185\u90e8\u8868\u5f81\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u7684\u201c\u771f/\u5047\u201d\u8f93\u51fa\u53ef\u4ee5\u901a\u8fc7\u7ebf\u6027\u63a2\u6d4b\u4ece\u5176\u5185\u90e8\u8868\u5f81\u4e2d\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u6b3a\u9a97\u6027\u6307\u4ee4\u4f1a\u5f15\u8d77\u663e\u8457\u7684\u8868\u5f81\u53d8\u5316\uff0c\u5c24\u5176\u96c6\u4e2d\u5728\u6a21\u578b\u7684\u65e9\u671f\u5230\u4e2d\u671f\u5c42\uff0c\u5e76\u4e14\u8fd9\u79cd\u53d8\u5316\u5373\u4f7f\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u4e5f\u662f\u53ef\u68c0\u6d4b\u7684\u3002\u7814\u7a76\u8fd8\u8bc6\u522b\u51fa\u5bf9\u6b3a\u9a97\u6027\u6307\u4ee4\u9ad8\u5ea6\u654f\u611f\u7684SAE\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u786e\u8ba4\u4e86\u72ec\u7acb\u7684\u771f\u5b9e/\u6b3a\u9a97\u6027\u8868\u5f81\u5b50\u7a7a\u95f4\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7406\u89e3\u548c\u63a7\u5236LLM\u7684\u6b3a\u9a97\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u63ed\u793a\u4e86\u6b3a\u9a97\u884c\u4e3a\u7684\u7279\u5f81\u548c\u5c42\u7ea7\u7b7e\u540d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9075\u5faa\u6b3a\u9a97\u6027\u6307\u4ee4\u65b9\u9762\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u53ef\u80fd\u751f\u6210\u865a\u5047\u4fe1\u606f\u3002\u7136\u800c\uff0c\u76ee\u524d\u5bf9\u4e8e\u6b3a\u9a97\u6027\u6307\u4ee4\u5982\u4f55\u6539\u53d8LLM\u5185\u90e8\u8868\u5f81\u7684\u7406\u89e3\u4ecd\u7136\u6709\u9650\uff0c\u4e3b\u8981\u5c40\u9650\u4e8e\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u5206\u6790\u3002\u8fd9\u9879\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u77e5\u8bc6\u7a7a\u767d\uff0c\u6df1\u5165\u63a2\u7a76\u6b3a\u9a97\u6027\u6307\u4ee4\u5982\u4f55\u4ee5\u53ca\u4f55\u65f6\u5bfc\u81f4LLM\u5185\u90e8\u8868\u5f81\u53d1\u751f\u201c\u7ffb\u8f6c\u201d\uff08\u4f8b\u5982\uff0c\u4ece\u771f\u5b9e\u8f6c\u53d8\u4e3a\u6b3a\u9a97\uff09\uff0c\u4ece\u800c\u4e3aLLM\u7684\u5b89\u5168\u6027\u548c\u53ef\u63a7\u6027\u63d0\u4f9b\u66f4\u6df1\u5c42\u6b21\u7684\u89c1\u89e3\u3002", "method": "\u7814\u7a76\u4eba\u5458\u9009\u53d6\u4e86Llama-3.1-8B-Instruct\u548cGemma-2-9B-Instruct\u4e24\u4e2a\u6a21\u578b\uff0c\u5728\u6267\u884c\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u65f6\uff0c\u5bf9\u6bd4\u5206\u6790\u4e86\u5728\u63a5\u6536\u6b3a\u9a97\u6027\u6307\u4ee4\u4e0e\u771f\u5b9e/\u4e2d\u6027\u6307\u4ee4\u4e0b\u7684\u5185\u90e8\u8868\u5f81\u53d8\u5316\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u4f7f\u7528\u7ebf\u6027\u63a2\u6d4b\uff08linear probes\uff09\u6280\u672f\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u5185\u90e8\u8868\u5f81\u662f\u5426\u80fd\u591f\u9884\u6d4b\u5176\u6700\u7ec8\u7684\u201c\u771f/\u5047\u201d\u8f93\u51fa\u30022. \u5e94\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08Sparse Autoencoders, SAEs\uff09\u6765\u5206\u6790\u548c\u91cf\u5316\u6b3a\u9a97\u6027\u6307\u4ee4\u4e0e\u771f\u5b9e/\u4e2d\u6027\u6307\u4ee4\u5728\u6a21\u578b\u5185\u90e8\u8868\u5f81\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u786e\u5b9a\u8fd9\u4e9b\u5dee\u5f02\u4e3b\u8981\u96c6\u4e2d\u5728\u6a21\u578b\u7684\u54ea\u4e9b\u5c42\u7ea7\u30023. \u8bc6\u522b\u5bf9\u6b3a\u9a97\u6027\u6307\u4ee4\u7279\u522b\u654f\u611f\u7684SAE\u7279\u5f81\u30024. \u5229\u7528\u53ef\u89c6\u5316\u6280\u672f\uff0c\u76f4\u89c2\u5730\u5c55\u793a\u548c\u786e\u8ba4\u771f\u5b9e\u8868\u5f81\u4e0e\u6b3a\u9a97\u6027\u8868\u5f81\u5728\u6a21\u578b\u5185\u90e8\u5b58\u5728\u7684\u4e0d\u540c\u5b50\u7a7a\u95f4\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u65e0\u8bba\u662f\u5728\u6b3a\u9a97\u6027\u6307\u4ee4\u8fd8\u662f\u771f\u5b9e/\u4e2d\u6027\u6307\u4ee4\u4e0b\uff0c\u6a21\u578b\u7684\u201c\u771f/\u5047\u201d\u8f93\u51fa\u90fd\u53ef\u4ee5\u901a\u8fc7\u5bf9\u5176\u5185\u90e8\u8868\u5f81\u8fdb\u884c\u7ebf\u6027\u63a2\u6d4b\u6765\u51c6\u786e\u9884\u6d4b\u3002\u901a\u8fc7SAEs\u5206\u6790\uff0c\u7814\u7a76\u4eba\u5458\u89c2\u5bdf\u5230\u6b3a\u9a97\u6027\u6307\u4ee4\u786e\u5b9e\u4f1a\u5f15\u8d77\u6bd4\u771f\u5b9e/\u4e2d\u6027\u6307\u4ee4\u66f4\u663e\u8457\u7684\u5185\u90e8\u8868\u5f81\u53d8\u5316\u3002\u8fd9\u4e9b\u53d8\u5316\u4e3b\u8981\u96c6\u4e2d\u5728\u6a21\u578b\u7684\u65e9\u671f\u81f3\u4e2d\u671f\u5c42\uff0c\u5e76\u4e14\u5373\u4f7f\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u96c6\u65f6\u4e5f\u80fd\u88ab\u68c0\u6d4b\u5230\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u6210\u529f\u8bc6\u522b\u51fa\u4e00\u4e9b\u5bf9\u6b3a\u9a97\u6027\u6307\u4ee4\u9ad8\u5ea6\u654f\u611f\u7684SAE\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u624b\u6bb5\u786e\u8ba4\u4e86\u771f\u5b9e\u8868\u5f81\u548c\u6b3a\u9a97\u6027\u8868\u5f81\u5728\u6a21\u578b\u5185\u90e8\u5f62\u6210\u4e86\u4e0d\u540c\u7684\u5b50\u7a7a\u95f4\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790LLM\u7684\u5185\u90e8\u8868\u5f81\uff0c\u6210\u529f\u63ed\u793a\u4e86\u6b3a\u9a97\u6027\u6307\u4ee4\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u8fd0\u4f5c\u673a\u5236\u3002\u7814\u7a76\u53d1\u73b0\u4e86\u6b3a\u9a97\u884c\u4e3a\u5728\u7279\u5f81\u548c\u5c42\u7ea7\u4e0a\u7684\u7b7e\u540d\uff0c\u4e3a\u68c0\u6d4b\u548c\u63a7\u5236LLM\u4e2d\u7684\u201c\u6307\u793a\u6027\u4e0d\u8bda\u5b9e\u201d\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u89c1\u89e3\u3002\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u52a0\u6df1\u4e86\u6211\u4eec\u5bf9LLM\u5185\u90e8\u8868\u5f81\u7684\u7406\u89e3\uff0c\u4e5f\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fd9\u4e9b\u53d1\u73b0\u5982\u4f55\u5e94\u7528\u4e8e\u5b9e\u9645\u7684LLM\u9632\u5fa1\u7b56\u7565\uff0c\u4ee5\u53ca\u5728\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u9a8c\u8bc1\u8fd9\u4e9b\u7ed3\u8bba\u3002"}}
{"id": "1810.06818", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/1810.06818", "abs": "https://arxiv.org/abs/1810.06818", "authors": ["Yufei Zhao", "Xiaoshi Zhong", "Erik Cambria", "Jagath C. Rajapakse"], "title": "Large Language Models for Few-Shot Named Entity Recognition", "comment": "17 pages, 2 figures. Accepted by AI, Computer Science and Robotics\n  Technology (ACRT)", "summary": "Named entity recognition (NER) is a fundamental task in numerous downstream\napplications. Recently, researchers have employed pre-trained language models\n(PLMs) and large language models (LLMs) to address this task. However, fully\nleveraging the capabilities of PLMs and LLMs with minimal human effort remains\nchallenging. In this paper, we propose GPT4NER, a method that prompts LLMs to\nresolve the few-shot NER task. GPT4NER constructs effective prompts using three\nkey components: entity definition, few-shot examples, and chain-of-thought. By\nprompting LLMs with these effective prompts, GPT4NER transforms few-shot NER,\nwhich is traditionally considered as a sequence-labeling problem, into a\nsequence-generation problem. We conduct experiments on two benchmark datasets,\nCoNLL2003 and OntoNotes5.0, and compare the performance of GPT4NER to\nrepresentative state-of-the-art models in both few-shot and fully supervised\nsettings. Experimental results demonstrate that GPT4NER achieves the $F_1$ of\n83.15\\% on CoNLL2003 and 70.37\\% on OntoNotes5.0, significantly outperforming\nfew-shot baselines by an average margin of 7 points. Compared to\nfully-supervised baselines, GPT4NER achieves 87.9\\% of their best performance\non CoNLL2003 and 76.4\\% of their best performance on OntoNotes5.0. We also\nutilize a relaxed-match metric for evaluation and report performance in the\nsub-task of named entity extraction (NEE), and experiments demonstrate their\nusefulness to help better understand model behaviors in the NER task.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGPT4NER\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u6784\u5efa\u5305\u542b\u5b9e\u4f53\u5b9a\u4e49\u3001\u793a\u4f8b\u548c\u601d\u7ef4\u94fe\u7684\u6709\u6548\u63d0\u793a\u6765\u89e3\u51b3\u5c11\u6837\u672c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4efb\u52a1\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u5e8f\u5217\u751f\u6210\u95ee\u9898\uff0c\u5e76\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5982\u4f55\u4ee5\u6700\u5c11\u7684\u4eba\u5de5\u5e72\u9884\u5145\u5206\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u7684\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "GPT4NER\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u5305\u542b\u5b9e\u4f53\u5b9a\u4e49\u3001\u5c11\u6837\u672c\u793a\u4f8b\u548c\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\u7684\u6709\u6548\u63d0\u793a\u6765\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u51b3\u5c11\u6837\u672cNER\u4efb\u52a1\uff0c\u5c06\u4f20\u7edf\u7684\u5e8f\u5217\u6807\u6ce8\u95ee\u9898\u8f6c\u5316\u4e3a\u5e8f\u5217\u751f\u6210\u95ee\u9898\u3002\u7814\u7a76\u5728CoNLL2003\u548cOntoNotes5.0\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u5c11\u6837\u672c\u548c\u5168\u76d1\u7763\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5728CoNLL2003\u6570\u636e\u96c6\u4e0a\uff0cGPT4NER\u7684F1\u5f97\u5206\u4e3a83.15%\uff1b\u5728OntoNotes5.0\u6570\u636e\u96c6\u4e0a\uff0cF1\u5f97\u5206\u4e3a70.37%\u3002\u4e0e\u5c11\u6837\u672c\u57fa\u7ebf\u76f8\u6bd4\uff0cGPT4NER\u7684\u5e73\u5747\u6027\u80fd\u9ad8\u51fa7\u4e2a\u767e\u5206\u70b9\u3002\u4e0e\u5168\u76d1\u7763\u57fa\u7ebf\u76f8\u6bd4\uff0cGPT4NER\u5728CoNLL2003\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u5176\u6700\u4f73\u6027\u80fd\u768487.9%\uff0c\u5728OntoNotes5.0\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8676.4%\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u91c7\u7528\u4e86\u653e\u677e\u5339\u914d\uff08relaxed-match\uff09\u6307\u6807\uff0c\u5e76\u62a5\u544a\u4e86\u547d\u540d\u5b9e\u4f53\u63d0\u53d6\uff08NEE\uff09\u5b50\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3\u6a21\u578b\u884c\u4e3a\u3002", "conclusion": "GPT4NER\u901a\u8fc7\u6709\u6548\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u6210\u529f\u5730\u5c06\u5c11\u6837\u672cNER\u4efb\u52a1\u8f6c\u5316\u4e3a\u5e8f\u5217\u751f\u6210\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6837\u672c\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002\u867d\u7136GPT4NER\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u6bd4\u4ecd\u6709\u5dee\u8ddd\uff0c\u8fd9\u8868\u660e\u672a\u6765\u53ef\u4ee5\u5728\u6a21\u578b\u8bad\u7ec3\u548c\u63d0\u793a\u8bbe\u8ba1\u65b9\u9762\u8fdb\u884c\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u4ee5\u8fdb\u4e00\u6b65\u7f29\u5c0f\u6027\u80fd\u5dee\u8ddd\u5e76\u63a2\u7d22\u5176\u5728\u66f4\u591a\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2307.02103", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2307.02103", "abs": "https://arxiv.org/abs/2307.02103", "authors": ["Abdelhadi Soudi", "Manal El Hakkaoui", "Kristof Van Laerhoven"], "title": "Do predictability factors towards signing avatars hold across cultures?", "comment": "In Proceedings of SLTAT 2023: Eighth International Workshop on Sign\n  Language Translation and Avatar Technology, held in conjunction with ICASSP\n  2023: IEEE International Conference on Acoustics, Speech, and Signal\n  Processing, Rhodes, Greece, June 4-10, 2023", "summary": "Avatar technology can offer accessibility possibilities and improve the\nDeaf-and-Hard of Hearing sign language users access to communication, education\nand services, such as the healthcare system. However, sign language users\nacceptance of signing avatars as well as their attitudes towards them vary and\ndepend on many factors. Furthermore, research on avatar technology is mostly\ndone by researchers who are not Deaf. The study examines the extent to which\nintrinsic or extrinsic factors contribute to predict the attitude towards\navatars across cultures. Intrinsic factors include the characteristics of the\navatar, such as appearance, movements and facial expressions. Extrinsic factors\ninclude users technology experience, their hearing status, age and their sign\nlanguage fluency. This work attempts to answer questions such as, if lower\nattitude ratings are related to poor technology experience with ASL users, for\nexample, is that also true for Moroccan Sign Language (MSL) users? For the\npurposes of the study, we designed a questionnaire to understand MSL users\nattitude towards avatars. Three groups of participants were surveyed: Deaf\n(57), Hearing (20) and Hard-of-Hearing (3). The results of our study were then\ncompared with those reported in other relevant studies.", "AI": {"tldr": "Despite avatar technology's potential to improve communication for Deaf and Hard-of-Hearing sign language users, their acceptance varies. This study investigates how intrinsic (avatar characteristics) and extrinsic (user experience, hearing status, age, sign language fluency) factors influence attitudes towards avatars across cultures. A questionnaire surveyed 80 Moroccan Sign Language (MSL) users (57 Deaf, 20 Hearing, 3 Hard-of-Hearing). Findings, compared with other studies, aim to understand cultural variations in avatar acceptance, particularly concerning technology experience and its relation to attitude ratings.", "motivation": "Avatar technology holds promise for enhancing communication, education, and access to services for Deaf and Hard-of-Hearing sign language users. However, user acceptance and attitudes towards these avatars are influenced by various factors and can differ across cultures. A significant gap exists because much of the research on avatar technology is conducted by non-Deaf researchers. This study aims to address this gap by exploring the intrinsic and extrinsic factors that predict attitudes towards avatars across different cultures, thereby understanding the nuances of acceptance within diverse user groups.", "method": "This study designed a questionnaire to assess the attitudes of Moroccan Sign Language (MSL) users towards avatars. The survey targeted three groups: Deaf (n=57), Hearing (n=20), and Hard-of-Hearing (n=3) participants. Intrinsic factors such as avatar appearance, movements, and facial expressions, alongside extrinsic factors including users' technology experience, hearing status, age, and sign language fluency, were considered. The study sought to answer specific questions, such as whether a correlation between lower attitude ratings and poor technology experience observed in ASL users also holds true for MSL users. Results were compared with findings from other relevant studies.", "result": "The study surveyed 80 Moroccan Sign Language (MSL) users, comprising 57 Deaf, 20 Hearing, and 3 Hard-of-Hearing individuals, using a specifically designed questionnaire. The collected data on attitudes towards avatars, influenced by both intrinsic (avatar characteristics) and extrinsic (user-related) factors, were compared with results from previous research on other sign language communities, like ASL users. This comparison aims to identify potential cross-cultural differences and similarities in avatar acceptance, particularly in relation to technology experience.", "conclusion": "This research provides insights into the attitudes of Moroccan Sign Language (MSL) users towards avatar technology, examining the influence of intrinsic and extrinsic factors. By comparing findings with studies on other sign language users, the study contributes to a cross-cultural understanding of avatar acceptance. The results aim to inform the development of more effective and accepted avatar technologies for the Deaf and Hard-of-Hearing community, while acknowledging the need for further research to explore the full impact and address limitations, especially concerning the varying technology experiences across different linguistic and cultural groups."}}
{"id": "2509.01308", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.01308", "abs": "https://arxiv.org/abs/2509.01308", "authors": ["Mattia Tritto", "Giuseppe Farano", "Dario Di Palma", "Gaetano Rossiello", "Fedelucio Narducci", "Dharmashankar Subramanian", "Tommaso Di Noia"], "title": "GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL Generation from Large Language Models", "comment": null, "summary": "Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has significantly advanced with the introduction of Large Language\nModels (LLMs), broadening database accessibility for a wide range of users.\nDespite substantial progress in generating valid SQL, current LLMs still\nstruggle with complex queries. To address this limitation, test-time strategies\nsuch as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on\nthe assumption that LLMs can produce correct answers after multiple attempts.\nHowever, these methods rely on surface-level heuristics, selecting the\nsyntactically correct query through execution-based BoN (ex-BoN) or the most\nfrequently generated one through Majority Voting. Recently, Outcome Reward\nModels (ORMs), which assign utility scores to generated outputs based on\nsemantic correctness, have emerged as a promising reinforcement learning\napproach for improving model alignment. We argue that ORMs could serve as an\neffective new test-time heuristic, although their application in this context\nremains largely underexplored.\n  In this work, we propose a unified framework for training ORMs tailored to\nthe Text-to-SQL task and assess their effectiveness as a test-time heuristic\nwithin the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the\nBIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2,\nGranite3, and Llama3 families. Results show that ORMs outperform ex-BoN and\nMaj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider)\nover ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further\ndemonstrate that finetuning models already aligned with SQL generation, such as\nOmniSQL, yields superior ORM performance. Additionally, we observe that ORMs\nachieve competitive results on simple queries and benefit more from an\nincreased number of candidates compared to ex-BoN and Maj.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728Text-to-SQL\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u96be\u4ee5\u5904\u7406\u590d\u6742\u67e5\u8be2\u3002\u73b0\u6709\u6d4b\u8bd5\u65f6\u7b56\u7565\uff08\u5982Best-of-N\u548cMajority Voting\uff09\u4f9d\u8d56\u4e8e\u8868\u9762\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8eText-to-SQL\u4efb\u52a1\u7684\u7edf\u4e00\u7ed3\u679c\u5956\u52b1\u6a21\u578b(ORM)\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u8bc4\u4f30\u5176\u4f5c\u4e3aBest-of-N\u7b56\u7565\u4e2d\u7684\u6d4b\u8bd5\u65f6\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cORM\u5728BIRD\u548cSpider\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u6267\u884c\u57fa\u7840\u7684BoN (ex-BoN) \u548cMajority Voting (Maj)\uff0c\u5206\u522b\u5728\u6267\u884c\u51c6\u786e\u7387\u4e0a\u63d0\u5347\u4e86+4.33%\u548c+2.10%\uff08\u76f8\u6bd4ex-BoN\uff09\uff0c\u4ee5\u53ca+2.91%\u548c+0.93%\uff08\u76f8\u6bd4Maj\uff09\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728Text-to-SQL\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f7f\u5f97\u975e\u4e13\u4e1a\u7528\u6237\u4e5f\u80fd\u8bbf\u95ee\u6570\u636e\u5e93\uff0c\u4f46\u5b83\u4eec\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u65f6\u4ecd\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u7814\u7a76\u4eba\u5458\u91c7\u7528\u4e86\u6d4b\u8bd5\u65f6\u7b56\u7565\uff0c\u5982Best-of-N (BoN) \u548cMajority Voting (Maj)\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5047\u8bbeLLMs\u5728\u591a\u6b21\u5c1d\u8bd5\u540e\u80fd\u591f\u751f\u6210\u6b63\u786e\u7b54\u6848\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u751f\u6210\u67e5\u8be2\u7684\u8868\u9762\u7279\u5f81\u8fdb\u884c\u5224\u65ad\uff0c\u4f8b\u5982\u901a\u8fc7\u6267\u884c\u6765\u9009\u62e9\u8bed\u6cd5\u6b63\u786e\u7684\u67e5\u8be2\uff08ex-BoN\uff09\uff0c\u6216\u9009\u62e9\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u67e5\u8be2\uff08Maj\uff09\u3002\u8fd1\u671f\u51fa\u73b0\u7684\u7ed3\u679c\u5956\u52b1\u6a21\u578b\uff08ORMs\uff09\u901a\u8fc7\u4e3a\u751f\u6210\u8f93\u51fa\u5206\u914d\u57fa\u4e8e\u8bed\u4e49\u6b63\u786e\u6027\u7684\u6548\u7528\u5206\u6570\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u5c55\u73b0\u51fa\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u80fd\u529b\u7684\u6f5c\u529b\u3002\u5c3d\u7ba1ORMs\u5728\u6539\u5584\u6a21\u578b\u6027\u80fd\u65b9\u9762\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5b83\u4eec\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728Text-to-SQL\u9886\u57df\u7684\u5e94\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22ORMs\u5728Text-to-SQL\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u9002\u7528\u4e8eText-to-SQL\u4efb\u52a1\u7684\u7ed3\u679c\u5956\u52b1\u6a21\u578b\uff08ORMs\uff09\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u6d4b\u8bd5\u65f6\u542f\u53d1\u5f0f\u65b9\u6cd5\u6574\u5408\u5230Best-of-N\uff08BoN\uff09\u7b56\u7565\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002\u5b9e\u9a8c\u5728BIRD\u548cSpider\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u5e76\u4f7f\u7528\u4e86Qwen2\u3001Granite3\u548cLlama3\u7cfb\u5217\u4e2d\u7684\u591a\u79cd\u5f00\u6e90LLMs\u8fdb\u884c\u5fae\u8c03\u3002\u7814\u7a76\u4eba\u5458\u5c06ORM\u7684\u8868\u73b0\u4e0e\u73b0\u6709\u7684ex-BoN\u548cMaj\u7b56\u7565\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002\u8bc4\u4f30\u6307\u6807\u4e3b\u8981\u4e3a\u6267\u884c\u51c6\u786e\u7387\u3002", "result": "\u5728BIRD\u548cSpider\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cORM\u4f5c\u4e3a\u6d4b\u8bd5\u65f6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5176\u8868\u73b0\u4f18\u4e8eex-BoN\u548cMaj\u3002\u5177\u4f53\u800c\u8a00\uff0cORM\u5728BIRD\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4ex-BoN\u7684\u6267\u884c\u51c6\u786e\u7387\u63d0\u5347\u4e86+4.33%\uff0c\u76f8\u6bd4Maj\u63d0\u5347\u4e86+2.91%\u3002\u5728Spider\u6570\u636e\u96c6\u4e0a\uff0cORM\u76f8\u6bd4ex-BoN\u7684\u6267\u884c\u51c6\u786e\u7387\u63d0\u5347\u4e86+2.10%\uff0c\u76f8\u6bd4Maj\u63d0\u5347\u4e86+0.93%\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5bf9\u4e8e\u50cfOmniSQL\u8fd9\u6837\u5df2\u7ecf\u5728SQL\u751f\u6210\u65b9\u9762\u6709\u6240\u4f18\u5316\u7684\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u4f18\u7684ORM\u6027\u80fd\u3002\u540c\u65f6\uff0cORM\u5728\u5904\u7406\u7b80\u5355\u67e5\u8be2\u65f6\u4e5f\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u76f8\u6bd4ex-BoN\u548cMaj\uff0cORM\u4ece\u589e\u52a0\u5019\u9009\u6570\u91cf\u4e2d\u53d7\u76ca\u66f4\u591a\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3Text-to-SQL\u4efb\u52a1\u7684\u7ed3\u679c\u5956\u52b1\u6a21\u578b\uff08ORMs\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u63d0\u5347LLMs\u5904\u7406\u590d\u6742SQL\u67e5\u8be2\u7684\u80fd\u529b\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002ORM\u5728BIRD\u548cSpider\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684ex-BoN\u548cMaj\u7b56\u7565\uff0c\u5728\u6267\u884c\u51c6\u786e\u7387\u4e0a\u53d6\u5f97\u4e86\u53ef\u89c2\u7684\u63d0\u5347\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0cORM\u5728\u7b80\u5355\u67e5\u8be2\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5e76\u4e14\u80fd\u66f4\u597d\u5730\u4ece\u589e\u52a0\u5019\u9009\u6570\u91cf\u4e2d\u83b7\u76ca\u3002\u5c3d\u7ba1ORM\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u66f4\u5e7f\u6cdb\u7684\u6570\u636e\u5e93\u548c\u66f4\u590d\u6742\u7684\u67e5\u8be2\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u7814\u7a76\u66f4\u9ad8\u6548\u7684ORM\u8bad\u7ec3\u65b9\u6cd5\u548c\u6a21\u578b\u7ed3\u6784\u3002"}}
{"id": "2405.05583", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2405.05583", "abs": "https://arxiv.org/abs/2405.05583", "authors": ["Yuxia Wang", "Minghan Wang", "Hasan Iqbal", "Georgi Georgiev", "Jiahui Geng", "Preslav Nakov"], "title": "OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems and Evaluating the Factuality of Claims and LLMs", "comment": "23 pages, 8 tables, 11 figures, Published In Proceedings of the 31st\n  International Conference on Computational Linguistics 2025", "summary": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. Difficulties lie in assessing the factuality of free-form\nresponses in open domains. Also, different papers use disparate evaluation\nbenchmarks and measurements, which renders them hard to compare and hampers\nfuture progress. To mitigate these issues, we propose OpenFactCheck, a unified\nframework for building customized automatic fact-checking systems, benchmarking\ntheir accuracy, evaluating factuality of LLMs, and verifying claims in a\ndocument. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users\nto easily customize an automatic fact-checker and verify the factual\ncorrectness of documents and claims, (ii) LLMEVAL, a unified evaluation\nframework assesses LLM's factuality ability from various perspectives fairly,\nand (iii) CHECKEREVAL is an extensible solution for gauging the reliability of\nautomatic fact-checkers' verification results using human-annotated datasets.\nData and code are publicly available at\nhttps://github.com/yuxiaw/openfactcheck.", "AI": {"tldr": "OpenFactCheck\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u3001\u8bc4\u4f30\u548c\u9a8c\u8bc1\u81ea\u52a8\u4e8b\u5b9e\u68c0\u67e5\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8f93\u51fa\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u5e73\u53f0\u3002", "motivation": "\u968f\u7740LLMs\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9a8c\u8bc1\u5176\u8f93\u51fa\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8bc4\u4f30\u5f00\u653e\u57df\u4e2d\u81ea\u7531\u5f62\u5f0f\u54cd\u5e94\u7684\u4e8b\u5b9e\u6027\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u6d4b\u91cf\u65b9\u6cd5\uff0c\u963b\u788d\u4e86\u53ef\u6bd4\u6027\u548c\u672a\u6765\u8fdb\u5c55\u3002", "method": "OpenFactCheck\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aCUSTCHECKER\u7528\u4e8e\u5b9a\u5236\u5316\u81ea\u52a8\u4e8b\u5b9e\u68c0\u67e5\u5668\u5e76\u9a8c\u8bc1\u6587\u6863\u548c\u58f0\u660e\uff1bLLMEVAL\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u591a\u89d2\u5ea6\u516c\u5e73\u5730\u8bc4\u4f30LLM\u7684\u4e8b\u5b9e\u6027\u80fd\u529b\uff1bCHECKEREVAL\u5219\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u96c6\u6765\u8861\u91cf\u81ea\u52a8\u4e8b\u5b9e\u68c0\u67e5\u5668\u9a8c\u8bc1\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7\u63d0\u4f9b\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u5bf9LLM\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u53ef\u6bd4\u8bc4\u4f30\uff0c\u5e76\u4e3a\u5f00\u53d1\u548c\u8861\u91cf\u81ea\u52a8\u4e8b\u5b9e\u68c0\u67e5\u5668\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "OpenFactCheck\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u96c6\u6210\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86LLM\u4e8b\u5b9e\u51c6\u786e\u6027\u9a8c\u8bc1\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u8be5\u6846\u67b6\u7684\u5f00\u653e\u6570\u636e\u548c\u4ee3\u7801\u5c06\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2405.17220", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2405.17220", "abs": "https://arxiv.org/abs/2405.17220", "authors": ["Tianyu Yu", "Haoye Zhang", "Qiming Li", "Qixin Xu", "Yuan Yao", "Da Chen", "Xiaoman Lu", "Ganqu Cui", "Yunkai Dang", "Taiwen He", "Xiaocheng Feng", "Jun Song", "Bo Zheng", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "title": "RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness", "comment": "Project Website: https://github.com/RLHF-V/RLAIF-V", "summary": "Traditional feedback learning for hallucination reduction relies on\nlabor-intensive manual labeling or expensive proprietary models. This leaves\nthe community without foundational knowledge about how to build high-quality\nfeedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novel\nframework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally\nexplores open-source MLLMs from two perspectives, including high-quality\nfeedback data generation for preference learning and self-feedback guidance for\ninference-time scaling. Extensive experiments on six benchmarks in both\nautomatic and human evaluation show that RLAIF-V substantially enhances the\ntrustworthiness of models at both preference learning and inference time.\nRLAIF-V 7B reduces object hallucination by 80.7\\% and overall hallucination by\n33.7\\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of\nopen-source MLLMs, where the model can learn from feedback of itself to achieve\nsuper GPT-4V trustworthiness.", "AI": {"tldr": "RLAIF-V\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u5168\u5f00\u6e90\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u53cd\u9988\u6570\u636e\u548c\u81ea\u53cd\u9988\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u6e90\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u7684\u53ef\u9760\u6027\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0c\u5e76\u5c55\u73b0\u4e86\u6a21\u578b\u81ea\u6211\u5b66\u4e60\u5bf9\u9f50\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u53cd\u9988\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\u6216\u4e13\u6709\u6a21\u578b\uff0c\u963b\u788d\u4e86\u793e\u533a\u5728\u5f00\u653e\u6e90MLLMs\u4e0a\u6784\u5efa\u9ad8\u8d28\u91cf\u53cd\u9988\u7684\u77e5\u8bc6\u79ef\u7d2f\u3002RLAIF-V\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u5f00\u653e\u6e90MLLMs\u63d0\u4f9b\u4e00\u4e2a\u53ef\u884c\u7684\u3001\u9ad8\u6027\u80fd\u7684\u53cd\u9988\u5b66\u4e60\u8303\u5f0f\u3002", "method": "RLAIF-V\u6846\u67b6\u901a\u8fc7\u4e24\u4e2a\u4e3b\u8981\u9014\u5f84\u6765\u4f18\u5316\u5f00\u653e\u6e90MLLMs\uff1a1. \u751f\u6210\u9ad8\u8d28\u91cf\u7684\u53cd\u9988\u6570\u636e\u4ee5\u8fdb\u884c\u504f\u597d\u5b66\u4e60\uff1b2. \u5229\u7528\u81ea\u53cd\u9988\u6307\u5bfc\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u6269\u5c55\u3002\u8be5\u6846\u67b6\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u7ed3\u5408\u4e86\u81ea\u52a8\u548c\u4eba\u7c7b\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRLAIF-V\u5728\u504f\u597d\u5b66\u4e60\u548c\u63a8\u7406\u65f6\u90fd\u80fd\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0cRLAIF-V 7B\u6a21\u578b\u5c06\u5bf9\u8c61\u5e7b\u89c9\u51cf\u5c11\u4e8680.7%\uff0c\u6574\u4f53\u5e7b\u89c9\u51cf\u5c11\u4e8633.7%\u3002\u66f4\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cRLAIF-V 12B\u6a21\u578b\u901a\u8fc7\u81ea\u6211\u53cd\u9988\u5b66\u4e60\uff0c\u8fbe\u5230\u4e86\u8d85\u8d8aGPT-4V\u7684\u53ef\u9760\u6027\u6c34\u5e73\uff0c\u63ed\u793a\u4e86\u5f00\u653e\u6e90MLLMs\u7684\u81ea\u6211\u5bf9\u9f50\u6f5c\u529b\u3002", "conclusion": "RLAIF-V\u6210\u529f\u5730\u5728\u5168\u5f00\u6e90\u8303\u5f0f\u4e0b\u5b9e\u73b0\u4e86MLLMs\u7684\u6709\u6548\u5bf9\u9f50\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0c\u5e76\u8bc1\u660e\u4e86\u5f00\u653e\u6e90\u6a21\u578b\u5728\u81ea\u6211\u5b66\u4e60\u548c\u8fbe\u5230\u9876\u5c16\u6c34\u5e73\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u8be5\u6846\u67b6\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9d\u8d35\u7684\u5de5\u5177\u548c\u65b9\u6cd5\uff0c\u6709\u671b\u63a8\u52a8\u5f00\u653e\u6e90MLLMs\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u66f4\u591a\u6a21\u6001\u548c\u4efb\u52a1\u4e0a\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u4f18\u5316\u81ea\u53cd\u9988\u673a\u5236\u7684\u6548\u7387\u3002"}}
{"id": "2406.07222", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2406.07222", "abs": "https://arxiv.org/abs/2406.07222", "authors": ["Auguste Poiroux", "Gail Weiss", "Viktor Kun\u010dak", "Antoine Bosselut"], "title": "Reliable Evaluation and Benchmarks for Statement Autoformalization", "comment": "Accepted to EMNLP 2025. New benchmarks released, see\n  https://github.com/augustepoiroux/RLMEval ,\n  https://huggingface.co/datasets/PAug/ProofNetSharp , and\n  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see\n  https://github.com/augustepoiroux/LeanInteract", "summary": "Evaluating statement autoformalization, translating natural language\nmathematics into formal languages like Lean 4, remains a significant challenge,\nwith few metrics, datasets, and standards to robustly measure progress. In this\nwork, we present a comprehensive approach combining improved metrics, robust\nbenchmarks, and systematic evaluation, to fill this gap. First, we introduce\nBEq+, an automated metric that correlates strongly with human judgment, along\nwith ProofNetVerif, a new dataset for assessing the quality of evaluation\nmetrics, containing 3,752 annotated examples. Second, we develop two new\nautoformalization benchmarks: ProofNet#, a corrected version of ProofNet, and\nRLM25, with 619 new pairs of research-level mathematics from six formalization\nprojects. Through systematic experimentation across these benchmarks, we find\nthat current techniques can achieve up to 45.1% accuracy on undergraduate\nmathematics but struggle with research-level content without proper context.\nOur work establishes a reliable foundation for evaluating and advancing\nautoformalization systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u5230\u5f62\u5f0f\u8bed\u8a00\uff08\u5982Lean 4\uff09\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u7cfb\u7edf\u3002\u4ed6\u4eec\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807BEq+\u548cProofNetVerif\u6570\u636e\u96c6\uff0c\u4ee5\u53caProofNet#\u548cRLM25\u4e24\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6280\u672f\u5728\u672c\u79d1\u6570\u5b66\u65b9\u9762\u51c6\u786e\u7387\u53ef\u8fbe45.1%\uff0c\u4f46\u5728\u7814\u7a76\u7ea7\u6570\u5b66\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u8be5\u5de5\u4f5c\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u81ea\u52a8\u5f62\u5f0f\u5316\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u5230\u5f62\u5f0f\u8bed\u8a00\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u662f\u5b9e\u73b0\u6570\u5b66\u5b9a\u7406\u8bc1\u660e\u548c\u53ef\u9a8c\u8bc1\u63a8\u7406\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u6807\u51c6\u6765\u8861\u91cf\u8fd9\u4e00\u9886\u57df\u7684\u8fdb\u5c55\u3002\u8fd9\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u6539\u8fdb\u6307\u6807\u3001\u9c81\u68d2\u57fa\u51c6\u548c\u7cfb\u7edf\u5316\u8bc4\u4f30\u7684\u7efc\u5408\u65b9\u6cd5\u3002\u5177\u4f53\u5305\u62ec\uff1a1. \u5f15\u5165\u65b0\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807BEq+\uff0c\u8be5\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\u30022. \u6784\u5efaProofNetVerif\u6570\u636e\u96c6\uff0c\u5305\u542b3,752\u4e2a\u6807\u6ce8\u793a\u4f8b\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bc4\u4f30\u6307\u6807\u7684\u8d28\u91cf\u30023. \u5f00\u53d1\u4e24\u4e2a\u65b0\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff1aProofNet#\uff08ProofNet\u7684\u4fee\u6b63\u7248\uff09\u548cRLM25\uff08\u5305\u542b619\u4e2a\u6765\u81ea\u516d\u4e2a\u5f62\u5f0f\u5316\u9879\u76ee\u7684\u65b0\u7814\u7a76\u7ea7\u6570\u5b66\u5b9e\u4f8b\uff09\u30024. \u5728\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u73b0\u6709\u81ea\u52a8\u5f62\u5f0f\u5316\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u672c\u79d1\u6570\u5b66\u9886\u57df\uff0c\u73b0\u6709\u6280\u672f\u53ef\u4ee5\u8fbe\u523045.1%\u7684\u51c6\u786e\u7387\u3002\u7136\u800c\uff0c\u5728\u6ca1\u6709\u5145\u5206\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u6280\u672f\u5728\u5904\u7406\u7814\u7a76\u7ea7\u6570\u5b66\u5185\u5bb9\u65f6\u8868\u73b0\u4e0d\u4f73\u3002BEq+\u6307\u6807\u88ab\u8bc1\u660e\u4e0e\u4eba\u7c7b\u5224\u65ad\u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\uff0cProofNetVerif\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u6307\u6807\u7684\u8d28\u91cf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u5bf9\u6570\u5b66\u81ea\u52a8\u5f62\u5f0f\u5316\u7cfb\u7edf\u7684\u8bc4\u4f30\u80fd\u529b\u3002\u867d\u7136\u73b0\u6709\u6280\u672f\u5728\u672c\u79d1\u6570\u5b66\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u5b9a\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u7684\u7814\u7a76\u7ea7\u6570\u5b66\u65f6\u4ecd\u9762\u4e34\u6311\u6218\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u672a\u6765\u81ea\u52a8\u5f62\u5f0f\u5316\u7cfb\u7edf\u7684\u53d1\u5c55\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u5373\u9700\u8981\u5f00\u53d1\u66f4\u80fd\u5904\u7406\u7814\u7a76\u7ea7\u6570\u5b66\u5185\u5bb9\u7684\u6280\u672f\uff0c\u5e76\u8fdb\u4e00\u6b65\u5b8c\u5584\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2509.09810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09810", "abs": "https://arxiv.org/abs/2509.09810", "authors": ["Agnieszka Mensfelt", "David Tena Cucala", "Santiago Franco", "Angeliki Koutsoukou-Argyraki", "Vince Trencsenyi", "Kostas Stathis"], "title": "Towards a Common Framework for Autoformalization", "comment": null, "summary": "Autoformalization has emerged as a term referring to the automation of\nformalization - specifically, the formalization of mathematics using\ninteractive theorem provers (proof assistants). Its rapid development has been\ndriven by progress in deep learning, especially large language models (LLMs).\nMore recently, the term has expanded beyond mathematics to describe the broader\ntask of translating informal input into formal logical representations. At the\nsame time, a growing body of research explores using LLMs to translate informal\nlanguage into formal representations for reasoning, planning, and knowledge\nrepresentation - often without explicitly referring to this process as\nautoformalization. As a result, despite addressing similar tasks, the largely\nindependent development of these research areas has limited opportunities for\nshared methodologies, benchmarks, and theoretical frameworks that could\naccelerate progress. The goal of this paper is to review - explicit or implicit\n- instances of what can be considered autoformalization and to propose a\nunified framework, encouraging cross-pollination between different fields to\nadvance the development of next generation AI systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86", "motivation": "Autoformalization\uff08\u81ea\u52a8\u5f62\u5f0f\u5316\uff09\uff0c\u5373\u5c06\u6570\u5b66\u7b49\u975e\u5f62\u5f0f\u5316\u8f93\u5165\u8f6c\u6362\u4e3a\u4ea4\u4e92\u5f0f\u5b9a\u7406\u8bc1\u660e\u5668\uff08\u5982\u4ea4\u4e92\u5f0f\u8bc1\u660e\u52a9\u624b\uff09\u7b49\u5f62\u5f0f\u5316\u8868\u793a\u7684\u8fc7\u7a0b\uff0c\u56e0\u6df1\u5ea6\u5b66\u4e60\u5c24\u5176\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u6b65\u800c\u8fc5\u901f\u53d1\u5c55\u3002\u8be5\u672f\u8bed\u7684\u542b\u4e49\u5df2\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u5c06\u975e\u5f62\u5f0f\u5316\u8f93\u5165\u8f6c\u6362\u4e3a\u5f62\u5f0f\u5316\u903b\u8f91\u8868\u793a\u7684\u4efb\u52a1\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u7814\u7a76\u9886\u57df\uff08\u5305\u62ec\u663e\u5f0f\u5730\u79f0\u4e3a\u201cautoformalization\u201d\u548c\u9690\u5f0f\u5730\u5904\u7406\u7c7b\u4f3c\u4efb\u52a1\u7684\u7814\u7a76\uff09\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u72ec\u7acb\u53d1\u5c55\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u5171\u4eab\u65b9\u6cd5\u3001\u57fa\u51c6\u548c\u7406\u8bba\u6846\u67b6\u7684\u51fa\u73b0\uff0c\u963b\u788d\u4e86\u8fdb\u5c55\u3002\u672c\u6587\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u8de8\u9886\u57df\u4ea4\u6d41\uff0c\u4ee5\u52a0\u901f\u4e0b\u4e00\u4ee3\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u56de\u987e\u548c\u5206\u6790\u73b0\u6709\u7814\u7a76\uff08\u65e0\u8bba\u662f\u660e\u786e\u63d0\u53ca\u201cautoformalization\u201d\u8fd8\u662f\u9690\u5f0f\u5904\u7406\u7c7b\u4f3c\u4efb\u52a1\u7684\u7814\u7a76\uff09\u6765\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u65e8\u5728\u4fc3\u8fdb\u4e0d\u540c\u7814\u7a76\u9886\u57df\u4e4b\u95f4\u7684\u4ea4\u53c9\u5b66\u4e60\u548c\u65b9\u6cd5\u5171\u4eab\uff0c\u4ece\u800c\u52a0\u901f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u8bc6\u522b\u548c\u5206\u7c7b\u73b0\u6709\u7684autoformalization\u5b9e\u4f8b\u3002 2. \u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u4e0d\u540c\u7684\u7814\u7a76\u65b9\u5411\u6574\u5408\u8d77\u6765\u3002 3. \u9f13\u52b1\u4e0d\u540c\u9886\u57df\u7684\u7814\u7a76\u8005\u5171\u4eab\u65b9\u6cd5\u3001\u57fa\u51c6\u548c\u7406\u8bba\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "result": "\u7531\u4e8e\u672c\u6587\u662f\u4e00\u7bc7\u7efc\u8ff0\u6027\u8bba\u6587\uff0c\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5176\u201c\u7ed3\u679c\u201d\u5e76\u975e\u5b9e\u9a8c\u6027\u7684\u91cf\u5316\u6210\u679c\uff0c\u800c\u662f\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a1. \u5bf9autoformalization\u9886\u57df\u73b0\u72b6\u7684\u5168\u9762\u68b3\u7406\u548c\u5206\u7c7b\u3002 2. \u4e00\u4e2a\u6e05\u6670\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u6574\u5408\u4e0d\u540c\u7684\u7814\u7a76\u65b9\u5411\u3002 3. \u4fc3\u8fdb\u8de8\u9886\u57df\u4ea4\u6d41\u7684\u5efa\u8bae\u548c\u65b9\u5411\u3002 4. \u6307\u51fa\u5f53\u524d\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5c06autoformalization\u9886\u57df\u7684\u7814\u7a76\u65b9\u5411\u6574\u5408\u8d77\u6765\uff0c\u514b\u670d\u4e86\u5f53\u524d\u7814\u7a76\u9886\u57df\u5206\u6563\u3001\u7f3a\u4e4f\u5171\u4eab\u673a\u5236\u7684\u9650\u5236\u3002\u8fd9\u6709\u671b\u4fc3\u8fdb\u4e0d\u540c\u9886\u57df\u4e4b\u95f4\u7684\u4ea4\u53c9\u5b66\u4e60\u548c\u65b9\u6cd5\u5171\u4eab\uff0c\u52a0\u901f\u4e0b\u4e00\u4ee3\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u96c6\u4e2d\u5728\u8fdb\u4e00\u6b65\u5b8c\u5584\u8be5\u6846\u67b6\uff0c\u5f00\u53d1\u66f4\u6709\u6548\u7684\u8de8\u9886\u57df\u57fa\u51c6\u548c\u65b9\u6cd5\uff0c\u4ee5\u53ca\u63a2\u7d22autoformalization\u5728\u66f4\u5e7f\u6cdb\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2408.11832", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2408.11832", "abs": "https://arxiv.org/abs/2408.11832", "authors": ["Hasan Iqbal", "Yuxia Wang", "Minghan Wang", "Georgi Georgiev", "Jiahui Geng", "Iryna Gurevych", "Preslav Nakov"], "title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs", "comment": "11 pages, 4 Figures, 3 Tables, Published In Proceedings of The 2024\n  Conference on Empirical Methods in Natural Language Processing", "summary": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for automatic tools to check the factual accuracy\nof their outputs, as LLMs often hallucinate. This is difficult as it requires\nassessing the factuality of free-form open-domain responses. While there has\nbeen a lot of research on this topic, different papers use different evaluation\nbenchmarks and measures, which makes them hard to compare and hampers future\nprogress. To mitigate these issues, we developed OpenFactCheck, a unified\nframework, with three modules: (i) RESPONSEEVAL, which allows users to easily\ncustomize an automatic fact-checking system and to assess the factuality of all\nclaims in an input document using that system, (ii) LLMEVAL, which assesses the\noverall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate\nautomatic fact-checking systems. OpenFactCheck is open-sourced\n(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python\nlibrary (https://pypi.org/project/openfactcheck/) and also as a web service\n(http://app.openfactcheck.com). A video describing the system is available at\nhttps://youtu.be/-i9VKL0HleI.", "AI": {"tldr": "LLMs\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5185\u5bb9\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u5e38\u53d7\u5230\u201c\u5e7b\u89c9\u201d\u73b0\u8c61\u7684\u56f0\u6270\uff0c\u8fd9\u4f7f\u5f97\u81ea\u52a8\u8bc4\u4f30LLM\u8f93\u51fa\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u53d8\u5f97\u56f0\u96be\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0d\u540c\u7814\u7a76\u4e4b\u95f4\u8bc4\u4f30\u57fa\u51c6\u548c\u8861\u91cf\u6807\u51c6\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5f00\u53d1\u4e86OpenFactCheck\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aRESPONSEEVAL\uff08\u7528\u4e8e\u5b9a\u5236\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u5e76\u8bc4\u4f30\u6587\u6863\u4e2d\u6240\u6709\u58f0\u660e\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff09\u3001LLMEVAL\uff08\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u6574\u4f53\u4e8b\u5b9e\u51c6\u786e\u6027\uff09\u548cCHECKEREVAL\uff08\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\uff09\u3002OpenFactCheck\u5df2\u5f00\u6e90\uff0c\u5e76\u4ee5Python\u5e93\u548cWeb\u670d\u52a1\u7684\u5f62\u5f0f\u53d1\u5e03\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5b83\u4eec\u5e38\u5e38\u4ea7\u751f\u201c\u5e7b\u89c9\u201d\uff0c\u5373\u751f\u6210\u4e0d\u51c6\u786e\u6216\u865a\u5047\u7684\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u81ea\u52a8\u5de5\u5177\u6765\u68c0\u67e5LLM\u8f93\u51fa\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u7814\u7a76\u5728\u8bc4\u4f30\u57fa\u51c6\u548c\u8861\u91cf\u6807\u51c6\u4e0a\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u8fd9\u963b\u788d\u4e86\u7814\u7a76\u7684\u53ef\u6bd4\u6027\u548c\u672a\u6765\u8fdb\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aOpenFactCheck\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a1. RESPONSEEVAL\uff1a\u5141\u8bb8\u7528\u6237\u81ea\u5b9a\u4e49\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\uff0c\u5e76\u8bc4\u4f30\u8f93\u5165\u6587\u6863\u4e2d\u6240\u6709\u58f0\u660e\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u30022. LLMEVAL\uff1a\u8bc4\u4f30LLM\u7684\u6574\u4f53\u4e8b\u5b9e\u51c6\u786e\u6027\u30023. CHECKEREVAL\uff1a\u8bc4\u4f30\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u3002\u8be5\u6846\u67b6\u5df2\u5f00\u6e90\uff0c\u5e76\u63d0\u4f9bPython\u5e93\u548cWeb\u670d\u52a1\u3002", "result": "\uff08\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u7ed3\u679c\u548c\u6027\u80fd\u6bd4\u8f83\uff0c\u4f46\u6697\u793a\u4e86\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u6765\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002\uff09", "conclusion": "OpenFactCheck\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u89e3\u51b3LLM\u4e8b\u5b9e\u51c6\u786e\u6027\u8bc4\u4f30\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u8bc4\u4f30\u3001LLM\u6574\u4f53\u8bc4\u4f30\u4ee5\u53ca\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u7684\u8bc4\u4f30\u3002\u8be5\u6846\u67b6\u7684\u5f00\u6e90\u548c\u591a\u5f62\u5f0f\u53d1\u5e03\uff08Python\u5e93\u3001Web\u670d\u52a1\uff09\u65e8\u5728\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2509.23234", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23234", "abs": "https://arxiv.org/abs/2509.23234", "authors": ["Runyan Tan", "Shuang Wu", "Phillip Howard"], "title": "p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding", "comment": null, "summary": "Obtaining high-quality outputs from Large Language Models (LLMs) often\ndepends upon the choice of a sampling-based decoding strategy to\nprobabilistically choose the next token at each generation step. While a\nvariety of such sampling methods have been proposed, their performance can be\nsensitive to the selection of hyperparameters which may require different\nsettings depending upon the generation task and temperature configuration. In\nthis work, we introduce $p$-less sampling: an information-theoretic approach to\nsampling which dynamically sets a truncation threshold at each decoding step\nbased on the entire token probability distribution. Unlike existing methods,\n$p$-less sampling has no hyperparameters and consistently produces high-quality\noutputs as temperature increases. We provide theoretical perspectives on\n$p$-less sampling to ground our proposed method and conduct experiments to\nempirically validate its effectiveness across a range of math, logical\nreasoning, and creative writing tasks. Our results demonstrate how $p$-less\nsampling consistently outperforms existing sampling approaches while exhibiting\nmuch less degradation in text quality at higher temperature values. We further\nshow how $p$-less achieves greater inference-time efficiency than alternative\nmethods through lower average token sampling times and shorter generation\nlengths, without sacrificing accuracy. Finally, we provide analyses to\nhighlight the benefits of $p$-less through qualitative examples, case studies,\nand diversity assessments. The code is available at\nhttps://github.com/ryttry/p-less .", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cp-less sampling\u201d\u7684\u65b0\u578bLLM\u89e3\u7801\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u57fa\u4e8e\u4fe1\u606f\u8bba\uff0c\u80fd\u591f\u52a8\u6001\u8c03\u6574\u622a\u65ad\u9608\u503c\uff0c\u65e0\u9700\u8d85\u53c2\u6570\u5373\u53ef\u5728\u4e0d\u540c\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u3001\u903b\u8f91\u63a8\u7406\u548c\u521b\u610f\u5199\u4f5c\u7b49\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\u65f6\uff0c\u5176\u91c7\u6837\u89e3\u7801\u7b56\u7565\u5bf9\u8d85\u53c2\u6570\u7684\u9009\u62e9\u975e\u5e38\u654f\u611f\uff0c\u9700\u8981\u6839\u636e\u7279\u5b9a\u4efb\u52a1\u548c\u6e29\u5ea6\u914d\u7f6e\u8fdb\u884c\u8c03\u6574\u3002\u8fd9\u79cd\u8d85\u53c2\u6570\u654f\u611f\u6027\u7ed9LLM\u7684\u5e94\u7528\u5e26\u6765\u4e86\u4e0d\u4fbf\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cp-less sampling\u201d\u7684\u4fe1\u606f\u8bba\u91c7\u6837\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u4e2d\uff0c\u57fa\u4e8e\u6574\u4e2atoken\u6982\u7387\u5206\u5e03\u52a8\u6001\u5730\u8bbe\u5b9a\u4e00\u4e2a\u622a\u65ad\u9608\u503c\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cp-less sampling\u4e0d\u9700\u8981\u4efb\u4f55\u8d85\u53c2\u6570\uff0c\u5e76\u4e14\u5728\u6e29\u5ea6\u5347\u9ad8\u65f6\u4ecd\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\u3002\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6765\u652f\u6301\u8be5\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5728\u6570\u5b66\u63a8\u7406\u3001\u903b\u8f91\u63a8\u7406\u548c\u521b\u610f\u5199\u4f5c\u7b49\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cp-less sampling\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u91c7\u6837\u65b9\u6cd5\u3002\u7279\u522b\u662f\u5728\u8f83\u9ad8\u7684\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\uff0cp-less sampling\u7684\u6587\u672c\u8d28\u91cf\u4e0b\u964d\u5e45\u5ea6\u660e\u663e\u5c0f\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u6b64\u5916\uff0cp-less sampling\u901a\u8fc7\u964d\u4f4e\u5e73\u5747token\u91c7\u6837\u65f6\u95f4\u548c\u7f29\u77ed\u751f\u6210\u957f\u5ea6\uff0c\u5b9e\u73b0\u4e86\u6bd4\u66ff\u4ee3\u65b9\u6cd5\u66f4\u9ad8\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e14\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "conclusion": "p-less sampling\u662f\u4e00\u79cd\u65e0\u9700\u8d85\u53c2\u6570\u3001\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u6e29\u5ea6\u8bbe\u7f6e\u5e76\u7a33\u5b9a\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\u7684LLM\u89e3\u7801\u7b56\u7565\u3002\u5b83\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3aLLM\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u9009\u62e9\u3002\u7814\u7a76\u8fd8\u901a\u8fc7\u6848\u4f8b\u5206\u6790\u548c\u591a\u6837\u6027\u8bc4\u4f30\u8fdb\u4e00\u6b65\u51f8\u663e\u4e86\u5176\u4f18\u52bf\u3002"}}
{"id": "2410.24155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2410.24155", "abs": "https://arxiv.org/abs/2410.24155", "authors": ["Jinghan Zhang", "Fengran Mo", "Tharindu Cyril Weerasooriya", "Xinyue Ye", "Dongjie Wang", "Yanjie Fu", "Kunpeng Liu"], "title": "Blind Spot Navigation in Large Language Model Reasoning with Thought Space Explorer", "comment": null, "summary": "Large language models have shown strong reasoning capabilities through\nchain-structured methods such as Chain-of-Thought. Recent studies optimize\nthought structures by generating parallel or tree-like structures, switching\nbetween long and short reasoning modes, or aligning reasoning steps with task\nperformance. However, these approaches mainly rely on previously generated\nlogical directions of the chains, which ignore the unexplored regions of the\nsolution space. Such a phenomenon is defined as blind spots, which limit the\ndiversity and effectiveness of the reasoning process. To this end, we propose\nthe ``Thought Space Explorer'' (TSE), a framework for navigating and expanding\nthought structures to overcome blind spots in LLM reasoning. Our TSE first\nidentifies key nodes with high impact, then generates new nodes by integrating\ninformation from multiple chains. Finally, it extends new branches through\nconnection strategies. We conduct a series of experiments on math and QA\nbenchmarks. Compared with existing baseline methods, TSE improves the accuracy\nof both the final answer and intermediate reasoning steps, while maintaining a\nbetter effectiveness-efficiency trade-off for practical deployment.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u53ef\u4ee5\u901a\u8fc7\u94fe\u5f0f\u65b9\u6cd5\uff08\u5982\u601d\u7ef4\u94fe\uff09\u5f97\u5230\u589e\u5f3a\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5148\u524d\u751f\u6210\u7684\u94fe\u7684\u903b\u8f91\u65b9\u5411\uff0c\u5ffd\u7565\u4e86\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u4e2d\u672a\u63a2\u7d22\u7684\u533a\u57df\uff0c\u4ece\u800c\u4ea7\u751f\u4e86\u201c\u76f2\u70b9\u201d\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u601d\u7ef4\u7a7a\u95f4\u63a2\u7d22\u5668\u201d\uff08TSE\uff09\uff0c\u4e00\u4e2a\u7528\u4e8e\u5bfc\u822a\u548c\u6269\u5c55\u601d\u7ef4\u7ed3\u6784\u7684\u6846\u67b6\uff0c\u4ee5\u514b\u670dLLM\u63a8\u7406\u4e2d\u7684\u76f2\u70b9\u3002TSE\u9996\u5148\u8bc6\u522b\u5173\u952e\u8282\u70b9\uff0c\u7136\u540e\u901a\u8fc7\u6574\u5408\u591a\u94fe\u4fe1\u606f\u751f\u6210\u65b0\u8282\u70b9\uff0c\u6700\u540e\u901a\u8fc7\u8fde\u63a5\u7b56\u7565\u6269\u5c55\u65b0\u5206\u652f\u3002\u5728\u6570\u5b66\u548c\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTSE\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6700\u7ec8\u7b54\u6848\u548c\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6709\u6548\u6027-\u6548\u7387\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u65b9\u6cd5\uff0c\u5982\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\uff0c\u867d\u7136\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5148\u524d\u751f\u6210\u7684\u903b\u8f91\u65b9\u5411\uff0c\u8fd9\u5bfc\u81f4\u5ffd\u7565\u4e86\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u4e2d\u672a\u88ab\u63a2\u7d22\u7684\u533a\u57df\uff0c\u5373\u201c\u76f2\u70b9\u201d\u3002\u8fd9\u79cd\u76f2\u70b9\u9650\u5236\u4e86LLM\u63a8\u7406\u8fc7\u7a0b\u7684\u591a\u6837\u6027\u548c\u6709\u6548\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u514b\u670d\u8fd9\u4e9b\u76f2\u70b9\uff0c\u6269\u5c55LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5bf9\u4e8e\u63d0\u9ad8\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u601d\u7ef4\u7a7a\u95f4\u63a2\u7d22\u5668\u201d\uff08TSE\uff09\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3LLM\u63a8\u7406\u4e2d\u7684\u76f2\u70b9\u95ee\u9898\u3002TSE\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a1. \u8bc6\u522b\u5173\u952e\u8282\u70b9\uff1a\u9996\u5148\uff0cTSE\u4f1a\u8bc6\u522b\u51fa\u5bf9\u63a8\u7406\u8fc7\u7a0b\u5f71\u54cd\u6700\u5927\u7684\u5173\u952e\u8282\u70b9\u30022. \u751f\u6210\u65b0\u8282\u70b9\uff1a\u63a5\u7740\uff0c\u901a\u8fc7\u6574\u5408\u6765\u81ea\u591a\u6761\u63a8\u7406\u94fe\u7684\u4fe1\u606f\uff0cTSE\u80fd\u591f\u751f\u6210\u65b0\u7684\u3001\u53ef\u80fd\u5305\u542b\u672a\u63a2\u7d22\u4fe1\u606f\u7684\u8282\u70b9\u30023. \u6269\u5c55\u65b0\u5206\u652f\uff1a\u6700\u540e\uff0c\u5229\u7528\u7279\u5b9a\u7684\u8fde\u63a5\u7b56\u7565\uff0cTSE\u5c06\u8fd9\u4e9b\u65b0\u8282\u70b9\u8fde\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u65b0\u7684\u63a8\u7406\u5206\u652f\uff0c\u4ece\u800c\u6269\u5c55\u601d\u7ef4\u7ed3\u6784\u3002\u6211\u4eec\u5728\u6570\u5b66\u548c\u95ee\u7b54\uff08QA\uff09\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5e76\u5c06TSE\u4e0e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5728\u6570\u5b66\u548c\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cTSE\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0cTSE\u8fd8\u80fd\u63d0\u5347\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u8d28\u91cf\u3002\u540c\u65f6\uff0cTSE\u5728\u4fdd\u6301\u8f83\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6709\u6548\u6027-\u6548\u7387\u6743\u8861\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "conclusion": "\u201c\u601d\u7ef4\u7a7a\u95f4\u63a2\u7d22\u5668\u201d\uff08TSE\uff09\u6846\u67b6\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u7684\u201c\u76f2\u70b9\u201d\u95ee\u9898\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u8282\u70b9\u3001\u6574\u5408\u591a\u94fe\u4fe1\u606f\u751f\u6210\u65b0\u8282\u70b9\u4ee5\u53ca\u6269\u5c55\u65b0\u5206\u652f\uff0c\u6709\u6548\u62d3\u5bbd\u4e86LLM\u7684\u63a8\u7406\u89c6\u91ce\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cTSE\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u63a8\u7406\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u4f18\u5316\u4e86\u6548\u7387-\u6548\u80fd\u7684\u5e73\u8861\uff0c\u4e3aLLM\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22TSE\u5728\u66f4\u591a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e94\u7528\uff0c\u5e76\u7814\u7a76\u66f4\u7cbe\u7ec6\u5316\u7684\u8282\u70b9\u8bc6\u522b\u548c\u5206\u652f\u6269\u5c55\u7b56\u7565\u3002"}}
{"id": "2501.08102", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2501.08102", "abs": "https://arxiv.org/abs/2501.08102", "authors": ["Wenlu Fan", "Yuqi Zhu", "Bin Wang", "Wentao Xu"], "title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media", "comment": "This paper has been accepted by the International AAAI Conference on\n  Web and Social Media (ICWSM) 2026 (Los Angeles, California, U.S.)", "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using three open-source models: Gemma, Llama3\nand Llama3.3 and one commercial Model:Claude. By analyzing climate change\ndiscussions from Twitter and Reddit, we examine emotional transitions,\nintensity patterns, and semantic consistency between human-authored and\nLLM-generated content. Our findings reveal that while both models maintain high\nsemantic coherence, they exhibit distinct emotional patterns: these models show\na strong tendency to moderate negative emotions. When the input text carries\nnegative emotions such as anger, disgust, fear, or sadness, LLM tends to\ngenerate content with more neutral emotions, or even convert them into positive\nemotions such as joy or surprise. At the same time, we compared the\nLLM-generated content with human-authored content. The four models\nsystematically generated responses with reduced emotional intensity and showed\na preference for neutral rational emotions in the response task. In addition,\nthese models all maintained a high semantic similarity with the original text,\nalthough their performance in the continuation task and the response task was\ndifferent. These findings provide deep insights into the emotion and semantic\nprocessing capabilities of LLM, which are of great significance for its\ndeployment in social media environments and human-computer interaction design.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u793e\u4ea4\u5a92\u4f53\u73af\u5883\u4e2d\u5904\u7406\u60c5\u611f\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u72ec\u7279\u7684\u6a21\u5f0f\uff0c\u5b83\u4eec\u503e\u5411\u4e8e\u7f13\u548c\u8d1f\u9762\u60c5\u7eea\u5e76\u751f\u6210\u66f4\u4e2d\u6027\u7684\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5ea6\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u672c\u751f\u6210\u65b9\u9762\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5b83\u4eec\u5728\u793e\u4ea4\u5a92\u4f53\u8bed\u5883\u4e0b\u7684\u60c5\u611f\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u63a2\u7a76LLMs\u5982\u4f55\u5904\u7406\u60c5\u611f\u5185\u5bb9\u4ee5\u53ca\u7ef4\u6301\u8bed\u4e49\u5173\u7cfb\uff0c\u8fd9\u5bf9\u4e8e\u5176\u5728\u793e\u4ea4\u5a92\u4f53\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u548c\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528Gemma\u3001Llama3\u3001Llama3.3\uff08\u5f00\u6e90\u6a21\u578b\uff09\u548cClaude\uff08\u5546\u4e1a\u6a21\u578b\uff09\u8fd9\u56db\u4e2a\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u6790Twitter\u548cReddit\u4e0a\u7684\u6c14\u5019\u53d8\u5316\u8ba8\u8bba\uff0c\u5728\u7eed\u5199\u548c\u56de\u590d\u4efb\u52a1\u4e2d\u68c0\u6d4b\u60c5\u611f\u8f6c\u6362\u3001\u5f3a\u5ea6\u6a21\u5f0f\u4ee5\u53ca\u4eba\u673a\u751f\u6210\u5185\u5bb9\u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6240\u6709\u6a21\u578b\u5728\u7eed\u5199\u548c\u56de\u590d\u4efb\u52a1\u4e2d\u5747\u80fd\u4fdd\u6301\u9ad8\u5ea6\u7684\u8bed\u4e49\u8fde\u8d2f\u6027\u3002\u7136\u800c\uff0c\u5728\u60c5\u611f\u5904\u7406\u65b9\u9762\uff0c\u6a21\u578b\u666e\u904d\u8868\u73b0\u51fa\u4e00\u79cd\u7f13\u548c\u8d1f\u9762\u60c5\u7eea\u7684\u503e\u5411\uff0c\u5c06\u8d1f\u9762\u60c5\u7eea\uff08\u5982\u6124\u6012\u3001\u538c\u6076\u3001\u6050\u60e7\u6216\u60b2\u4f24\uff09\u8f6c\u5316\u4e3a\u66f4\u4e2d\u6027\u751a\u81f3\u79ef\u6781\u7684\u60c5\u7eea\uff08\u5982\u559c\u60a6\u6216\u60ca\u8bb6\uff09\u3002\u4e0e\u4eba\u7c7b\u5185\u5bb9\u76f8\u6bd4\uff0cLLMs\u751f\u6210\u7684\u56de\u590d\u60c5\u611f\u5f3a\u5ea6\u8f83\u4f4e\uff0c\u5e76\u504f\u597d\u4e2d\u6027\u3001\u7406\u6027\u7684\u60c5\u611f\u3002\u5c3d\u7ba1\u6a21\u578b\u5728\u7eed\u5199\u548c\u56de\u590d\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u6709\u6240\u4e0d\u540c\uff0c\u4f46\u90fd\u4fdd\u6301\u4e86\u4e0e\u539f\u6587\u7684\u9ad8\u5ea6\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u793e\u4ea4\u5a92\u4f53\u8ba8\u8bba\u4e2d\u7684\u60c5\u611f\u548c\u8bed\u4e49\u5904\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5728\u9762\u5bf9\u8d1f\u9762\u60c5\u7eea\u65f6\u503e\u5411\u4e8e\u4e2d\u6027\u5316\uff0c\u4ee5\u53ca\u5728\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u4e86\u89e3LLMs\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u4e3a\u4f18\u5316\u5176\u5728\u793e\u4ea4\u5a92\u4f53\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u6a21\u578b\u5728\u4e0d\u540c\u6587\u5316\u548c\u793e\u4f1a\u80cc\u666f\u4e0b\u7684\u60c5\u611f\u53cd\u5e94\u5dee\u5f02\u3002"}}
{"id": "2502.17720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2502.17720", "abs": "https://arxiv.org/abs/2502.17720", "authors": ["Yuxuan Li", "Hirokazu Shirado"], "title": "Spontaneous Giving and Calculated Greed in Language Models", "comment": "Accepted to EMNLP 2025 main conference and selected as an Oral\n  Presentation", "summary": "Large language models demonstrate strong problem-solving abilities through\nreasoning techniques such as chain-of-thought prompting and reflection.\nHowever, it remains unclear whether these reasoning capabilities extend to a\nform of social intelligence: making effective decisions in cooperative\ncontexts. We examine this question using economic games that simulate social\ndilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o\nin a Public Goods Game. We then evaluate multiple off-the-shelf models across\nsix cooperation and punishment games, comparing those with and without explicit\nreasoning mechanisms. We find that reasoning models consistently reduce\ncooperation and norm enforcement, favoring individual rationality. In repeated\ninteractions, groups with more reasoning agents exhibit lower collective gains.\nThese behaviors mirror human patterns of \"spontaneous giving and calculated\ngreed.\" Our findings underscore the need for LLM architectures that incorporate\nsocial intelligence alongside reasoning, to help address--rather than\nreinforce--the challenges of collective action.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7ecf\u6d4e\u535a\u5f08\u4e2d\uff0c\u7279\u522b\u662f\u6d89\u53ca\u5408\u4f5c\u4e0e\u51b2\u7a81\u7684\u573a\u666f\u4e0b\uff0c\u5176\u63a8\u7406\u80fd\u529b\uff08\u5982\u94fe\u5f0f\u601d\u8003\u548c\u53cd\u601d\uff09\u5e76\u672a\u5982\u9884\u671f\u822c\u63d0\u5347\u793e\u4f1a\u667a\u80fd\uff0c\u53cd\u800c\u503e\u5411\u4e8e\u4e2a\u4f53\u7406\u6027\uff0c\u5bfc\u81f4\u5408\u4f5c\u51cf\u5c11\u548c\u96c6\u4f53\u6536\u76ca\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u76f8\u6bd4\u4e8e\u4eba\u7c7b\u5728\u2018\u81ea\u53d1\u7ed9\u4e88\u4e0e\u8ba1\u7b97\u8d2a\u5a6a\u2019\u4e4b\u95f4\u7684\u6743\u8861\uff0cLLM\u7684\u63a8\u7406\u673a\u5236\u66f4\u6613\u5bfc\u5411\u2018\u8ba1\u7b97\u8d2a\u5a6a\u2019\uff0c\u8fd9\u8868\u660e\u73b0\u6709LLM\u5728\u5904\u7406\u793e\u4f1a\u6027\u95ee\u9898\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u672a\u6765\u9700\u8981\u4e13\u95e8\u7684\u793e\u4f1a\u667a\u80fd\u67b6\u6784\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4f46\u5176\u5728\u6a21\u62df\u793e\u4f1a\u4e92\u52a8\u548c\u5408\u4f5c\u60c5\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u5373\u793e\u4f1a\u667a\u80fd\uff0c\u5374\u9c9c\u6709\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLM\u7684\u63a8\u7406\u80fd\u529b\u662f\u5426\u80fd\u5ef6\u4f38\u81f3\u793e\u4f1a\u667a\u80fd\u9886\u57df\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5728\u5408\u4f5c\u535a\u5f08\u4e2d\u7684\u51b3\u7b56\u8868\u73b0\uff0c\u4ee5\u53ca\u8fd9\u79cd\u51b3\u7b56\u662f\u5426\u80fd\u6709\u6548\u4fc3\u8fdb\u96c6\u4f53\u5229\u76ca\uff0c\u586b\u8865\u4e86LLM\u5728\u793e\u4f1a\u667a\u80fd\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u9996\u5148\u5c06\u94fe\u5f0f\u601d\u8003\uff08Chain-of-Thought\uff09\u548c\u53cd\u601d\uff08Reflection\uff09\u63d0\u793a\u6280\u672f\u5e94\u7528\u4e8eGPT-4o\u6a21\u578b\uff0c\u5728\u516c\u5171\u7269\u54c1\u535a\u5f08\uff08Public Goods Game\uff09\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002\u968f\u540e\uff0c\u7814\u7a76\u8005\u8bc4\u4f30\u4e86\u591a\u79cd\u73b0\u6210\u7684LLM\u5728\u516d\u79cd\u5408\u4f5c\u4e0e\u60e9\u7f5a\u535a\u5f08\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4f7f\u7528\u548c\u4e0d\u4f7f\u7528\u663e\u5f0f\u63a8\u7406\u673a\u5236\u7684\u6a21\u578b\u3002\u5b9e\u9a8c\u8bbe\u7f6e\u6a21\u62df\u4e86\u793e\u4f1a\u56f0\u5883\uff0c\u5e76\u91cf\u5316\u4e86\u6a21\u578b\u7684\u5408\u4f5c\u6c34\u5e73\u3001\u89c4\u8303\u6267\u884c\u4ee5\u53ca\u96c6\u4f53\u6536\u76ca\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u542f\u7528\u4e86\u63a8\u7406\u673a\u5236\u7684LLM\u5728\u535a\u5f08\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u5408\u4f5c\u6c34\u5e73\u548c\u89c4\u8303\u6267\u884c\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u4e2a\u4f53\u7406\u6027\u503e\u5411\u3002\u5728\u91cd\u590d\u535a\u5f08\u573a\u666f\u4e0b\uff0c\u5305\u542b\u66f4\u591a\u63a8\u7406\u578bLLM\u7684\u7fa4\u4f53\u6240\u83b7\u5f97\u7684\u96c6\u4f53\u6536\u76ca\u4f4e\u4e8e\u5176\u4ed6\u7fa4\u4f53\u3002\u8fd9\u4e9b\u884c\u4e3a\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u5728\u793e\u4f1a\u4e92\u52a8\u4e2d\u8868\u73b0\u51fa\u7684\u2018\u81ea\u53d1\u7ed9\u4e88\u4e0e\u8ba1\u7b97\u8d2a\u5a6a\u2019\u7684\u6743\u8861\u73b0\u8c61\u6709\u76f8\u4f3c\u4e4b\u5904\uff0c\u4f46LLM\u7684\u63a8\u7406\u673a\u5236\u4f3c\u4e4e\u66f4\u504f\u5411\u2018\u8ba1\u7b97\u8d2a\u5a6a\u2019\u3002", "conclusion": "\u672c\u7814\u7a76\u53d1\u73b0\uff0cLLM\u7684\u94fe\u5f0f\u601d\u8003\u548c\u53cd\u601d\u7b49\u63a8\u7406\u80fd\u529b\u5728\u5408\u4f5c\u535a\u5f08\u4e2d\u5e76\u672a\u63d0\u5347\u5176\u793e\u4f1a\u667a\u80fd\uff0c\u53cd\u800c\u53ef\u80fd\u56e0\u4e3a\u5f3a\u5316\u4e2a\u4f53\u7406\u6027\u800c\u635f\u5bb3\u96c6\u4f53\u5229\u76ca\u3002\u8fd9\u8868\u660e\uff0c\u5f53\u524d\u7684LLM\u67b6\u6784\u5728\u5904\u7406\u9700\u8981\u793e\u4f1a\u667a\u80fd\u7684\u573a\u666f\u65f6\u5b58\u5728\u56fa\u6709\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u53ef\u80fd\u4f1a\u52a0\u5267\u800c\u975e\u7f13\u89e3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u96c6\u4f53\u884c\u52a8\u56f0\u5883\u3002\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5e94\u805a\u7126\u4e8e\u5f00\u53d1\u80fd\u591f\u6574\u5408\u793e\u4f1a\u667a\u80fd\u7684LLM\u67b6\u6784\uff0c\u4ee5\u66f4\u597d\u5730\u5e94\u5bf9\u590d\u6742\u7684\u793e\u4f1a\u534f\u4f5c\u6311\u6218\u3002"}}
{"id": "2505.15063", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2505.15063", "abs": "https://arxiv.org/abs/2505.15063", "authors": ["Sarfraz Ahmad", "Hasan Iqbal", "Momina Ahsan", "Numaan Naeem", "Muhammad Ahsan Riaz Khan", "Arham Riaz", "Muhammad Arslan Manzoor", "Yuxia Wang", "Preslav Nakov"], "title": "UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking", "comment": "15 pages, 4 figures, 5 tables, 6 Listings, Published in Proceeding of\n  The 2025 Conference on Empirical Methods in Natural Language Processing", "summary": "The rapid adoption of Large Language Models (LLMs) has raised important\nconcerns about the factual reliability of their outputs, particularly in\nlow-resource languages such as Urdu. Existing automated fact-checking systems\nare predominantly developed for English, leaving a significant gap for the more\nthan 200 million Urdu speakers worldwide. In this work, we present\nUrduFactBench and UrduFactQA, two novel hand-annotated benchmarks designed to\nenable fact-checking and factual consistency evaluation in Urdu. While\nUrduFactBench focuses on claim verification, UrduFactQA targets the factuality\nof LLMs in question answering. These resources, the first of their kind for\nUrdu, were developed through a multi-stage annotation process involving native\nUrdu speakers. To complement these benchmarks, we introduce UrduFactCheck, a\nmodular fact-checking framework that incorporates both monolingual and\ntranslation-based evidence retrieval strategies to mitigate the scarcity of\nhigh-quality Urdu evidence. Leveraging these resources, we conduct an extensive\nevaluation of twelve LLMs and demonstrate that translation-augmented pipelines\nconsistently enhance performance compared to monolingual ones. Our findings\nreveal persistent challenges for open-source LLMs in Urdu and underscore the\nimportance of developing targeted resources. All code and data are publicly\navailable at https://github.com/mbzuai-nlp/UrduFactCheck.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u7528\u4e8e\u4e4c\u5c14\u90fd\u8bed\u7684\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\uff08UrduFactBench \u548c UrduFactQA\uff09\u548c\u6846\u67b6\uff08UrduFactCheck\uff09\uff0c\u4ee5\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u95ee\u9898\u3002\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u7ffb\u8bd1\u7684\u7ba1\u9053\u6bd4\u5355\u4e00\u8bed\u8a00\u7ba1\u9053\u80fd\u6301\u7eed\u63d0\u9ad8 LLM \u5728\u4e4c\u5c14\u90fd\u8bed\u4e0a\u7684\u4e8b\u5b9e\u6838\u67e5\u6027\u80fd\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u9488\u5bf9\u6027\u8d44\u6e90\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u4e4c\u5c14\u90fd\u8bed\uff09\u4e2d\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u4ee4\u4eba\u62c5\u5fe7\u3002\u73b0\u6709\u7684\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\uff0c\u5ffd\u89c6\u4e86\u5168\u7403\u6570\u4ebf\u4e4c\u5c14\u90fd\u8bed\u4f7f\u7528\u8005\u3002\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4e3a\u4e4c\u5c14\u90fd\u8bed\u7684 LLM \u63d0\u4f9b\u4e8b\u5b9e\u6838\u67e5\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u7684\u8d44\u6e90\u548c\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e24\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u57fa\u51c6\uff1aUrduFactBench \u7528\u4e8e\u58f0\u660e\u9a8c\u8bc1\uff0cUrduFactQA \u7528\u4e8e\u95ee\u7b54\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u3002\u4ed6\u4eec\u8fd8\u63d0\u51fa\u4e86 UrduFactCheck \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5355\u4e00\u8bed\u8a00\u548c\u57fa\u4e8e\u7ffb\u8bd1\u7684\u8bc1\u636e\u68c0\u7d22\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u8d28\u91cf\u4e4c\u5c14\u90fd\u8bed\u8bc1\u636e\u7684\u7a00\u7f3a\u6027\u3002\u6700\u540e\uff0c\u4ed6\u4eec\u4f7f\u7528\u8fd9\u4e9b\u8d44\u6e90\u5bf9\u5341\u4e8c\u4e2a LLM \u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u7ffb\u8bd1\u7684\u7ba1\u9053\u5728 LLM \u7684\u4e4c\u5c14\u90fd\u8bed\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u5355\u4e00\u8bed\u8a00\u7ba1\u9053\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u5f00\u6e90 LLM \u5728\u5904\u7406\u4e4c\u5c14\u90fd\u8bed\u65f6\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u63d0\u4f9b\u4e13\u95e8\u7684\u4e4c\u5c14\u90fd\u8bed\u57fa\u51c6\u548c\u4e8b\u5b9e\u6838\u67e5\u6846\u67b6\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684 LLM \u53ef\u9760\u6027\u505a\u51fa\u4e86\u91cd\u8981\u8d21\u732e\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9488\u5bf9\u6027\u8d44\u6e90\u548c\u7ffb\u8bd1\u589e\u5f3a\u65b9\u6cd5\u5728\u63d0\u9ad8 LLM \u5728\u4e4c\u5c14\u90fd\u8bed\u4e8b\u5b9e\u6838\u67e5\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u5f00\u6e90 LLM \u9762\u4e34\u7684\u6301\u7eed\u6311\u6218\u3002\u672a\u6765\u7814\u7a76\u5e94\u7ee7\u7eed\u5173\u6ce8\u5f00\u53d1\u66f4\u591a\u6b64\u7c7b\u8d44\u6e90\u5e76\u6539\u8fdb LLM \u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2505.15356", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2505.15356", "abs": "https://arxiv.org/abs/2505.15356", "authors": ["Weiming Zhang", "Qingyao Li", "Xinyi Dai", "Jizheng Chen", "Kounianhua Du", "Weiwen Liu", "Yasheng Wang", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging", "comment": null, "summary": "Debugging is a critical aspect of LLM's coding ability. Early debugging\nefforts primarily focused on code-level analysis, which often falls short when\naddressing complex programming errors that require a deeper understanding of\nalgorithmic logic. Recent advancements in large language models (LLMs) have\nshifted attention toward leveraging natural language reasoning to enhance\ncode-related tasks. However, two fundamental questions remain unanswered: What\ntype of natural language format is most effective for debugging tasks? And what\nspecific benefits does natural language reasoning bring to the debugging\nprocess? In this paper, we introduce NL-DEBUGGING, a novel framework that\nemploys natural language as an intermediate representation to improve code\ndebugging. By debugging at a natural language level, we demonstrate that\nNL-DEBUGGING outperforms traditional debugging methods and enables a broader\nmodification space through direct refinement guided by execution feedback. Our\nfindings highlight the potential of natural language reasoning to advance\nautomated code debugging and address complex programming challenges.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNL-DEBUGGING\u7684\u65b0\u578b\u6846\u67b6\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u6765\u6539\u8fdb\u4ee3\u7801\u8c03\u8bd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u81ea\u7136\u8bed\u8a00\u5c42\u9762\u8fdb\u884c\u8c03\u8bd5\uff0c\u80fd\u591f\u8d85\u8d8a\u4f20\u7edf\u8c03\u8bd5\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6267\u884c\u53cd\u9988\u6307\u5bfc\u7684\u76f4\u63a5\u7ec6\u5316\u6765\u62d3\u5bbd\u4fee\u6539\u7a7a\u95f4\uff0c\u8bc1\u660e\u4e86\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u5728\u81ea\u52a8\u5316\u4ee3\u7801\u8c03\u8bd5\u548c\u89e3\u51b3\u590d\u6742\u7f16\u7a0b\u6311\u6218\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u76ee\u524d\u7684LLM\u4ee3\u7801\u8c03\u8bd5\u4e3b\u8981\u96c6\u4e2d\u5728\u4ee3\u7801\u5c42\u9762\u5206\u6790\uff0c\u5bf9\u4e8e\u9700\u8981\u6df1\u5165\u7406\u89e3\u7b97\u6cd5\u903b\u8f91\u7684\u590d\u6742\u7f16\u7a0b\u9519\u8bef\u6548\u679c\u6709\u9650\u3002\u5c3d\u7ba1\u8fd1\u671fLLM\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4ecd\u4e0d\u6e05\u695a\u54ea\u79cd\u81ea\u7136\u8bed\u8a00\u683c\u5f0f\u6700\u9002\u5408\u8c03\u8bd5\u4efb\u52a1\uff0c\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u5177\u4f53\u80fd\u5e26\u6765\u54ea\u4e9b\u597d\u5904\u3002", "method": "\u63d0\u51faNL-DEBUGGING\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u6765\u6539\u8fdb\u4ee3\u7801\u8c03\u8bd5\u3002\u901a\u8fc7\u5728\u81ea\u7136\u8bed\u8a00\u5c42\u9762\u8fdb\u884c\u8c03\u8bd5\uff0c\u5e76\u5229\u7528\u6267\u884c\u53cd\u9988\u8fdb\u884c\u76f4\u63a5\u7ec6\u5316\u3002", "result": "NL-DEBUGGING\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u8c03\u8bd5\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u76f4\u63a5\u7ec6\u5316\u62d3\u5bbd\u4e86\u4fee\u6539\u7a7a\u95f4\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u80fd\u591f\u6709\u6548\u89e3\u51b3\u590d\u6742\u7684\u7f16\u7a0b\u6311\u6218\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u5728\u81ea\u52a8\u5316\u4ee3\u7801\u8c03\u8bd5\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u89e3\u51b3\u4f20\u7edf\u7684\u4ee3\u7801\u7ea7\u5206\u6790\u96be\u4ee5\u5904\u7406\u7684\u590d\u6742\u7f16\u7a0b\u95ee\u9898\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e0d\u540c\u7684\u81ea\u7136\u8bed\u8a00\u683c\u5f0f\u548c\u66f4\u590d\u6742\u7684\u8c03\u8bd5\u573a\u666f\u3002"}}
{"id": "2505.20249", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2505.20249", "abs": "https://arxiv.org/abs/2505.20249", "authors": ["Yongan Yu", "Qingchen Hu", "Xianda Du", "Jiayin Wang", "Fengran Mo", "Renee Sieber"], "title": "WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models", "comment": "Accepted by ACL 2025", "summary": "Climate change adaptation requires the understanding of disruptive weather\nimpacts on society, where large language models (LLMs) might be applicable.\nHowever, their effectiveness is under-explored due to the difficulty of\nhigh-quality corpus collection and the lack of available benchmarks. The\nclimate-related events stored in regional newspapers record how communities\nadapted and recovered from disasters. However, the processing of the original\ncorpus is non-trivial. In this study, we first develop a disruptive weather\nimpact dataset with a four-stage well-crafted construction pipeline. Then, we\npropose WXImpactBench, the first benchmark for evaluating the capacity of LLMs\non disruptive weather impacts. The benchmark involves two evaluation tasks,\nmulti-label classification and ranking-based question answering. Extensive\nexperiments on evaluating a set of LLMs provide first-hand analysis of the\nchallenges in developing disruptive weather impact understanding and climate\nchange adaptation systems. The constructed dataset and the code for the\nevaluation framework are available to help society protect against\nvulnerabilities from disasters.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u6c14\u5019\u53d8\u5316\u9002\u5e94\u4e2d\u7406\u89e3\u7834\u574f\u6027\u5929\u6c14\u5f71\u54cd\u7684\u9700\u6c42\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5 WXImpactBench\u3002\u8be5\u57fa\u51c6\u5305\u542b\u4e00\u4e2a\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u53ca\u6392\u5e8f\u95ee\u7b54\u4efb\u52a1\uff0c\u65e8\u5728\u5206\u6790 LLM \u5728\u6b64\u9886\u57df\u7684\u6311\u6218\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u9002\u5e94\u9700\u8981\u7406\u89e3\u5929\u6c14\u4e8b\u4ef6\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bed\u6599\u5e93\u548c\u8bc4\u4f30\u57fa\u51c6\u6765\u6709\u6548\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6b64\u65b9\u9762\u7684\u80fd\u529b\u3002\u533a\u57df\u62a5\u7eb8\u8bb0\u5f55\u4e86\u793e\u533a\u5e94\u5bf9\u707e\u96be\u7684\u9002\u5e94\u548c\u6062\u590d\u8fc7\u7a0b\uff0c\u4f46\u539f\u59cb\u8bed\u6599\u7684\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u56db\u4e2a\u9636\u6bb5\u7684\u7ed3\u6784\u5316\u6d41\u7a0b\uff0c\u4ee5\u6784\u5efa\u7834\u574f\u6027\u5929\u6c14\u5f71\u54cd\u6570\u636e\u96c6\u3002\u968f\u540e\uff0c\u63d0\u51fa\u4e86 WXImpactBench\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30 LLM \u5728\u7834\u574f\u6027\u5929\u6c14\u5f71\u54cd\u65b9\u9762\u80fd\u529b\u7684\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u5305\u542b\u591a\u6807\u7b7e\u5206\u7c7b\u548c\u57fa\u4e8e\u6392\u540d\u7684\u95ee\u7b54\u4e24\u4e2a\u8bc4\u4f30\u4efb\u52a1\u3002\u7814\u7a76\u56e2\u961f\u5bf9\u4e00\u7ec4 LLM \u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5bf9\u4e00\u7ec4 LLM \u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u9996\u6b21\u5206\u6790\u4e86\u5f00\u53d1\u7834\u574f\u6027\u5929\u6c14\u5f71\u54cd\u7406\u89e3\u548c\u6c14\u5019\u53d8\u5316\u9002\u5e94\u7cfb\u7edf\u6240\u9762\u4e34\u7684\u6311\u6218\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3 LLM \u5728\u5904\u7406\u6b64\u7c7b\u4efb\u52a1\u65f6\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u521d\u6b65\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u7834\u574f\u6027\u5929\u6c14\u5f71\u54cd\u6570\u636e\u96c6\u548c WXImpactBench \u57fa\u51c6\uff0c\u4e3a\u8bc4\u4f30 LLM \u5728\u7406\u89e3\u548c\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u76f8\u5173\u707e\u5bb3\u65b9\u9762\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86 LLM \u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u548c\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u7814\u7a76\u63d0\u4f9b\u7684\u516c\u5f00\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u6709\u52a9\u4e8e\u793e\u4f1a\u66f4\u597d\u5730\u5e94\u5bf9\u707e\u5bb3\u5e26\u6765\u7684\u8106\u5f31\u6027\u3002"}}
{"id": "2505.22586", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2505.22586", "abs": "https://arxiv.org/abs/2505.22586", "authors": ["Yoav Gur-Arieh", "Clara Suslik", "Yihuai Hong", "Fazl Barez", "Mor Geva"], "title": "Precise In-Parameter Concept Erasure in Large Language Models", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9884\u8bad\u7ec3\u4e2d\u83b7\u53d6\u7684\u77e5\u8bc6\u5728\u4e0b\u6e38\u90e8\u7f72\u4e2d\u53ef\u80fd\u662f\u4e0d\u53d7\u6b22\u8fce\u7684\uff0c\u4f8b\u5982\u654f\u611f\u4fe1\u606f\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u5185\u5bb9\u3002\u73b0\u6709\u7684\u77e5\u8bc6\u53bb\u9664\u65b9\u6cd5\uff0c\u5982\u5fae\u8c03\u3001\u4f4e\u79e9\u9002\u914d\u5668\u8bad\u7ec3\u6216\u4e8b\u5b9e\u7ea7\u7f16\u8f91\uff0c\u8981\u4e48\u8fc7\u4e8e\u7c97\u7cd9\u3001\u8fc7\u4e8e\u80a4\u6d45\uff0c\u8981\u4e48\u6548\u679c\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86PISCES\uff08Precise In-parameter Suppression for Concept EraSure\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u7f16\u8f91\u53c2\u6570\u7a7a\u95f4\u4e2d\u7f16\u7801\u6982\u5ff5\u7684\u65b9\u5411\u6765\u7cbe\u786e\u64e6\u9664\u6a21\u578b\u53c2\u6570\u4e2d\u7684\u6574\u4e2a\u6982\u5ff5\u3002PISCES\u4f7f\u7528\u89e3\u8026\u6a21\u578b\u5c06MLP\u5411\u91cf\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u7279\u5f81\uff0c\u5229\u7528\u81ea\u52a8\u5316\u53ef\u89e3\u91ca\u6027\u6280\u672f\u8bc6\u522b\u4e0e\u76ee\u6807\u6982\u5ff5\u76f8\u5173\u7684\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u4ece\u6a21\u578b\u53c2\u6570\u4e2d\u79fb\u9664\u3002\u5728Gemma 2\u548cLlama 3.1\u4e0a\u9488\u5bf9\u5404\u79cd\u6982\u5ff5\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPISCES\u5728\u529f\u6548\u65b9\u9762\u6bd4\u9886\u5148\u7684\u64e6\u9664\u65b9\u6cd5\u6709\u9002\u5ea6\u7684\u63d0\u5347\uff0c\u5c06\u76ee\u6807\u6982\u5ff5\u7684\u51c6\u786e\u6027\u964d\u4f4e\u52307.7%\uff0c\u540c\u65f6\u5c06\u64e6\u9664\u7279\u5f02\u6027\uff08\u63d0\u9ad8\u9ad8\u8fbe31%\uff09\u548c\u9c81\u68d2\u6027\uff08\u63d0\u9ad8\u9ad8\u8fbe38%\uff09\u663e\u8457\u63d0\u9ad8\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u7279\u5f81\u7684\u53c2\u6570\u5185\u7f16\u8f91\u4e3a\u53bb\u9664\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u77e5\u8bc6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u786e\u3001\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u5b66\u4e60\u5230\u5927\u91cf\u77e5\u8bc6\uff0c\u5176\u4e2d\u53ef\u80fd\u5305\u542b\u4e0d\u5e0c\u671b\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u51fa\u73b0\u7684\u654f\u611f\u4fe1\u606f\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u5185\u5bb9\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u77e5\u8bc6\u53bb\u9664\u65b9\u6cd5\uff08\u5982\u5fae\u8c03\u3001\u4f4e\u79e9\u9002\u914d\u5668\u8bad\u7ec3\u6216\u4e8b\u5b9e\u7ea7\u7f16\u8f91\uff09\u5728\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u65f6\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u3001\u7cbe\u5ea6\u4e0d\u8db3\u6216\u9c81\u68d2\u6027\u5dee\u7b49\u7f3a\u70b9\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u7cbe\u786e\u3001\u9ad8\u6548\u5730\u4eceLLMs\u4e2d\u53bb\u9664\u7279\u5b9a\u6982\u5ff5\u7684\u77e5\u8bc6\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u610f\u4e49\u548c\u7814\u7a76\u4ef7\u503c\u3002", "method": "PISCES\u6846\u67b6\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u5b9e\u73b0\u6982\u5ff5\u64e6\u9664\uff1a1. **\u7279\u5f81\u89e3\u8026\uff1a** \u5229\u7528\u4e00\u4e2a\u89e3\u8026\u6a21\u578b\u5c06MLP\u5411\u91cf\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u30022. **\u6982\u5ff5\u8bc6\u522b\uff1a** \u8fd0\u7528\u81ea\u52a8\u5316\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u8bc6\u522b\u51fa\u4e0e\u76ee\u6807\u6982\u5ff5\u76f8\u5173\u7684\u7279\u5f81\u30023. **\u53c2\u6570\u7f16\u8f91\uff1a** \u76f4\u63a5\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u7f16\u8f91\u8fd9\u4e9b\u4e0e\u76ee\u6807\u6982\u5ff5\u76f8\u5173\u7684\u7279\u5f81\uff0c\u4ece\u800c\u5b9e\u73b0\u6982\u5ff5\u7684\u64e6\u9664\u3002\u5b9e\u9a8c\u5728Gemma 2\u548cLlama 3.1\u6a21\u578b\u4e0a\u8fdb\u884c\uff0c\u5e76\u9488\u5bf9\u591a\u79cd\u6982\u5ff5\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPISCES\u5728\u53bb\u9664\u76ee\u6807\u6982\u5ff5\u65b9\u9762\u6bd4\u73b0\u6709\u9886\u5148\u65b9\u6cd5\u6709\u9002\u5ea6\u7684\u6548\u679c\u63d0\u5347\uff0c\u80fd\u5c06\u76ee\u6807\u6982\u5ff5\u7684\u51c6\u786e\u7387\u964d\u4f4e\u81f37.7%\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0cPISCES\u663e\u8457\u63d0\u9ad8\u4e86\u64e6\u9664\u7684\u7279\u5f02\u6027\uff08\u6700\u9ad8\u63d0\u534731%\uff09\u548c\u9c81\u68d2\u6027\uff08\u6700\u9ad8\u63d0\u534738%\uff09\u3002\u8fd9\u8868\u660ePISCES\u80fd\u591f\u66f4\u7cbe\u786e\u3001\u66f4\u53ef\u9760\u5730\u53bb\u9664\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u77e5\u8bc6\uff0c\u540c\u65f6\u5bf9\u6a21\u578b\u5176\u4ed6\u80fd\u529b\u7684\u8d1f\u9762\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "PISCES\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u7279\u5f81\u7684\u53c2\u6570\u5185\u7f16\u8f91\uff0c\u4e3a\u7cbe\u786e\u3001\u53ef\u9760\u5730\u53bb\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u77e5\u8bc6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u529f\u6548\u3001\u7279\u5f02\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22PISCES\u5728\u66f4\u5e7f\u6cdb\u7684\u6982\u5ff5\u64e6\u9664\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u4f18\u5316\u5176\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.23722", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2505.23722", "abs": "https://arxiv.org/abs/2505.23722", "authors": ["Fan Bai", "Hamid Hassanzadeh", "Ardavan Saeedi", "Mark Dredze"], "title": "LLMs are Better Than You Think: Label-Guided In-Context Learning for Named Entity Recognition", "comment": "Accepted to EMNLP 2025", "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. However, in Named Entity Recognition\n(NER), existing ICL methods typically rely on task-agnostic semantic similarity\nfor demonstration retrieval, which often yields less relevant examples and\nleads to inferior results. We introduce DEER, a training-free ICL approach that\nenables LLMs to make more informed entity predictions through the use of\nlabel-grounded statistics. DEER leverages token-level statistics from training\nlabels to identify tokens most informative for entity recognition, enabling\nentity-focused demonstrations. It further uses these statistics to detect and\nrefine error-prone tokens through a targeted reflection step. Evaluated on five\nNER datasets across four LLMs, DEER consistently outperforms existing ICL\nmethods and achieves performance comparable to supervised fine-tuning. Further\nanalyses demonstrate that DEER improves example retrieval, remains effective on\nboth seen and unseen entities, and exhibits strong robustness in low-resource\nsettings.", "AI": {"tldr": "DEER\u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u514d\u8d39\u7684ICL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6807\u7b7e\u76f8\u5173\u7684\u7edf\u8ba1\u6570\u636e\uff0c\u63d0\u9ad8LLMs\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u5b83\u901a\u8fc7\u8bc6\u522b\u4fe1\u606f\u91cf\u5927\u7684token\u5e76\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u9519\u8bef\u4fee\u6b63\uff0c\u4f18\u4e8e\u73b0\u6709ICL\u65b9\u6cd5\uff0c\u5e76\u63a5\u8fd1\u76d1\u7763\u5fae\u8c03\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684ICL\u65b9\u6cd5\u5728NER\u4efb\u52a1\u4e2d\u901a\u5e38\u4f9d\u8d56\u4efb\u52a1\u65e0\u5173\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6765\u68c0\u7d22\u793a\u4f8b\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u793a\u4f8b\u76f8\u5173\u6027\u4e0d\u9ad8\uff0cNER\u6548\u679c\u4e0d\u4f73\u3002", "method": "DEER\u5229\u7528\u6765\u81ea\u8bad\u7ec3\u6807\u7b7e\u7684token\u7ea7\u7edf\u8ba1\u6570\u636e\u6765\u8bc6\u522b\u5bf9\u5b9e\u4f53\u8bc6\u522b\u6700\u6709\u7528\u7684token\uff0c\u4ece\u800c\u5b9e\u73b0\u4ee5\u5b9e\u4f53\u4e3a\u4e2d\u5fc3\u7684\u6f14\u793a\u3002\u5b83\u8fd8\u5229\u7528\u8fd9\u4e9b\u7edf\u8ba1\u6570\u636e\uff0c\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u53cd\u601d\u6b65\u9aa4\u6765\u68c0\u6d4b\u548c\u4fee\u6b63\u6613\u9519\u7684token\u3002", "result": "\u5728\u4e94\u4e2aNER\u6570\u636e\u96c6\u548c\u56db\u79cdLLMs\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDEER\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684ICL\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u4e0e\u76d1\u7763\u5fae\u8c03\u76f8\u5f53\u3002DEER\u8fd8\u6539\u8fdb\u4e86\u793a\u4f8b\u68c0\u7d22\uff0c\u5728\u5df2\u89c1\u548c\u672a\u89c1\u7684\u5b9e\u4f53\u4e0a\u90fd\u6709\u6548\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "DEER\u901a\u8fc7\u5f15\u5165\u6807\u7b7e\u76f8\u5173\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709ICL\u65b9\u6cd5\u5728NER\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5b9e\u4f53\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u800c\u4e14\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22DEER\u5728\u5176\u4ed6\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4ee5\u53ca\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u7edf\u8ba1\u4fe1\u606f\u7684\u5229\u7528\u65b9\u5f0f\u3002"}}
{"id": "2506.01367", "categories": ["cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.01367", "abs": "https://arxiv.org/abs/2506.01367", "authors": ["Kensuke Mitsuzawa", "Damien Garreau"], "title": "MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations", "comment": null, "summary": "Large language models (LLMs) have become pervasive in our everyday life. Yet,\na fundamental obstacle prevents their use in many critical applications: their\npropensity to generate fluent, human-quality content that is not grounded in\nreality. The detection of such hallucinations is thus of the highest\nimportance. In this work, we propose a new method to flag hallucinated content:\nMMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric\ndistance between distributions. On a high-level perspective, MMD-Flagger tracks\nthe MMD between the output to inspect and counterparts generated with various\ntemperature parameters. We show empirically that inspecting the shape of this\ntrajectory is sufficient to detect most hallucinations. This novel method is\nbenchmarked on machine translation and summarization datasets, on which it\nexhibits competitive performance relative to natural competitors.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMD-Flagger\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u6765\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u5185\u5bb9\u4e2d\u7684\u5e7b\u89c9\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8ffd\u8e2a\u6a21\u578b\u8f93\u51fa\u4e0e\u4e0d\u540c\u6e29\u5ea6\u53c2\u6570\u4e0b\u751f\u6210\u5185\u5bb9\u7684\u5206\u5e03\u5dee\u5f02\u6765\u8bc6\u522b\u4e0d\u771f\u5b9e\u7684\u6587\u672c\uff0c\u5e76\u5728\u673a\u5668\u7ffb\u8bd1\u548c\u6458\u8981\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u5df2\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u751f\u6210\u7684\u5185\u5bb9\u53ef\u80fd\u4e0e\u4e8b\u5b9e\u4e0d\u7b26\uff08\u5373\u4ea7\u751f\u5e7b\u89c9\uff09\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u5728\u8bb8\u591a\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u68c0\u6d4bLLMs\u751f\u6210\u7684\u5e7b\u89c9\u5185\u5bb9\u81f3\u5173\u91cd\u8981\u3002", "method": "MMD-Flagger\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u975e\u53c2\u6570\u7684\u5206\u5e03\u8ddd\u79bb\u5ea6\u91cf\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97\u5f85\u68c0\u6d4b\u8f93\u51fa\u4e0e\u4f7f\u7528\u4e0d\u540c\u6e29\u5ea6\u53c2\u6570\u751f\u6210\u7684\u4e00\u7cfb\u5217\u5bf9\u7167\u8f93\u51fa\u4e4b\u95f4\u7684MMD\uff0c\u5e76\u5206\u6790MMD\u8f68\u8ff9\u7684\u5f62\u72b6\u6765\u68c0\u6d4b\u5e7b\u89c9\u3002\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u7ffb\u8bd1\u548c\u6458\u8981\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u673a\u5668\u7ffb\u8bd1\u548c\u6458\u8981\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMMD-Flagger\u5728\u68c0\u6d4b\u5e7b\u89c9\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u51fa\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u901a\u8fc7\u68c0\u67e5MMD\u8f68\u8ff9\u7684\u5f62\u72b6\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8bc6\u522b\u51fa\u5927\u90e8\u5206\u5e7b\u89c9\u5185\u5bb9\u3002", "conclusion": "MMD-Flagger\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u5185\u5bb9\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u975e\u53c2\u6570\u7684\u5206\u5e03\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u8be5\u65b9\u6cd5\u5728\u66f4\u591a\u7c7b\u578b\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2506.03690", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.03690", "abs": "https://arxiv.org/abs/2506.03690", "authors": ["Jie Sun", "Junkang Wu", "Jiancan Wu", "Zhibo Zhu", "Xingyu Lu", "Jun Zhou", "Lintao Ma", "Xiang Wang"], "title": "Robust Preference Optimization via Dynamic Target Margins", "comment": "18 pages, 6 figures, accepted to Findings of the 63rd Annual Meeting\n  of the Association for Computational Linguistics (ACL 2025)", "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bf9\u9f50\u5bf9\u4e8e\u786e\u4fdd\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u5bf9\u76f4\u63a5\u4f18\u5316\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u9700\u6c42\u3002\u7136\u800c\uff0cDPO \u7684\u6709\u6548\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6570\u636e\u8d28\u91cf\uff0c\u800c\u6570\u636e\u8d28\u91cf\u5e38\u5e38\u53d7\u5230\u566a\u58f0\u7684\u635f\u5bb3\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86 $\\gamma$-PO\uff0c\u4e00\u79cd\u52a8\u6001\u76ee\u6807\u8fb9\u9645\u504f\u597d\u4f18\u5316\u7b97\u6cd5\uff0c\u53ef\u5728\u6210\u5bf9\u7ea7\u522b\u8c03\u6574\u5956\u52b1\u8fb9\u9645\u3002\u901a\u8fc7\u5f15\u5165\u5b9e\u4f8b\u7279\u5b9a\u7684\u8fb9\u9645\u6821\u51c6\uff0c$\\gamma$-PO \u4f18\u5148\u8003\u8651\u9ad8\u7f6e\u4fe1\u5ea6\u5bf9\uff08\u5177\u6709\u66f4\u9ad8\u5956\u52b1\u8fb9\u9645\u7684\u5bf9\uff09\uff0c\u540c\u65f6\u6291\u5236\u6a21\u7cca\u5bf9\u53ef\u80fd\u5e26\u6765\u7684\u566a\u58f0\u3002\u6b64\u5916\uff0c$\\gamma$-PO \u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u53ef\u4e0e\u4f9d\u8d56\u504f\u597d\u5bf9\u4e4b\u95f4\u5956\u52b1\u8fb9\u9645\u7684 DPO \u53d8\u4f53\u517c\u5bb9\u3002\u5728 AlpacaEval2 \u548c Arena-Hard \u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c$\\gamma$-PO \u6bd4\u5176\u4ed6\u57fa\u7ebf\u5e73\u5747\u63d0\u9ad8\u4e86 4.4%\uff0c\u4e3a\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8bbe\u5b9a\u4e86\u65b0\u7684\u6807\u6746\u3002\u6b64\u5916\uff0c$\\gamma$-PO \u6240\u9700\u7684\u4ee3\u7801\u66f4\u6539\u6781\u5c11\uff0c\u5e76\u4e14\u5bf9\u8bad\u7ec3\u6548\u7387\u7684\u5f71\u54cd\u5fae\u4e4e\u5176\u5fae\uff0c\u4f7f\u5176\u6210\u4e3a\u589e\u5f3a LLM \u5bf9\u9f50\u7684\u7a33\u5065\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bf9\u9f50\u5bf9\u4e8e\u786e\u4fdd\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u5bf9\u76f4\u63a5\u4f18\u5316\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u9700\u6c42\u3002\u7136\u800c\uff0cDPO \u7684\u6709\u6548\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6570\u636e\u8d28\u91cf\uff0c\u800c\u6570\u636e\u8d28\u91cf\u5e38\u5e38\u53d7\u5230\u566a\u58f0\u7684\u635f\u5bb3\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u63d0\u9ad8 DPO \u5728\u5b58\u5728\u566a\u58f0\u6570\u636e\u65f6\u7684\u9c81\u68d2\u6027\u662f\u5341\u5206\u91cd\u8981\u7684\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 $\\gamma$-PO\uff0c\u4e00\u79cd\u52a8\u6001\u76ee\u6807\u8fb9\u9645\u504f\u597d\u4f18\u5316\u7b97\u6cd5\uff0c\u53ef\u5728\u6210\u5bf9\u7ea7\u522b\u8c03\u6574\u5956\u52b1\u8fb9\u9645\u3002\u901a\u8fc7\u5f15\u5165\u5b9e\u4f8b\u7279\u5b9a\u7684\u8fb9\u9645\u6821\u51c6\uff0c$\\gamma$-PO \u4f18\u5148\u8003\u8651\u9ad8\u7f6e\u4fe1\u5ea6\u5bf9\uff08\u5177\u6709\u66f4\u9ad8\u5956\u52b1\u8fb9\u9645\u7684\u5bf9\uff09\uff0c\u540c\u65f6\u6291\u5236\u6a21\u7cca\u5bf9\u53ef\u80fd\u5e26\u6765\u7684\u566a\u58f0\u3002$\\gamma$-PO \u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u53ef\u4e0e\u4f9d\u8d56\u504f\u597d\u5bf9\u4e4b\u95f4\u5956\u52b1\u8fb9\u9645\u7684 DPO \u53d8\u4f53\u517c\u5bb9\u3002", "result": "\u5728 AlpacaEval2 \u548c Arena-Hard \u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c$\\gamma$-PO \u6bd4\u5176\u4ed6\u57fa\u7ebf\u5e73\u5747\u63d0\u9ad8\u4e86 4.4%\uff0c\u4e3a\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8bbe\u5b9a\u4e86\u65b0\u7684\u6807\u6746\u3002\u6b64\u5916\uff0c$\\gamma$-PO \u6240\u9700\u7684\u4ee3\u7801\u66f4\u6539\u6781\u5c11\uff0c\u5e76\u4e14\u5bf9\u8bad\u7ec3\u6548\u7387\u7684\u5f71\u54cd\u5fae\u4e4e\u5176\u5fae\u3002", "conclusion": "$\\gamma$-PO \u662f\u4e00\u79cd\u6709\u6548\u7684\u52a8\u6001\u76ee\u6807\u8fb9\u9645\u504f\u597d\u4f18\u5316\u7b97\u6cd5\uff0c\u80fd\u591f\u63d0\u9ad8 LLM \u5728\u5b58\u5728\u566a\u58f0\u6570\u636e\u65f6\u7684\u5bf9\u9f50\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b9e\u4f8b\u7279\u5b9a\u7684\u8fb9\u9645\u6821\u51c6\uff0c\u4f18\u5148\u8003\u8651\u9ad8\u7f6e\u4fe1\u5ea6\u504f\u597d\u5bf9\uff0c\u540c\u65f6\u6291\u5236\u6a21\u7cca\u5bf9\u7684\u566a\u58f0\u5f71\u54cd\u3002$\\gamma$-PO \u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u6613\u4e8e\u96c6\u6210\uff0c\u5e76\u4e14\u5bf9\u8bad\u7ec3\u6548\u7387\u5f71\u54cd\u5f88\u5c0f\uff0c\u4e3a LLM \u7684\u5b89\u5168\u548c\u53ef\u9760\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.00814", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2507.00814", "abs": "https://arxiv.org/abs/2507.00814", "authors": ["Anita Keshmirian", "Razan Baltaji", "Babak Hemmatian", "Hadi Asghari", "Lav R. Varshney"], "title": "Many LLMs Are More Utilitarian Than One", "comment": "Accepted to the Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Moral judgment is integral to large language models' (LLMs) social reasoning.\nAs multi-agent systems gain prominence, it becomes crucial to understand how\nLLMs function when collaborating compared to operating as individual agents. In\nhuman moral judgment, group deliberation leads to a Utilitarian Boost: a\ntendency to endorse norm violations that inflict harm but maximize benefits for\nthe greatest number of people. We study whether a similar dynamic emerges in\nmulti-agent LLM systems. We test six models on well-established sets of moral\ndilemmas across two conditions: (1) Solo, where models reason independently,\nand (2) Group, where they engage in multi-turn discussions in pairs or triads.\nIn personal dilemmas, where agents decide whether to directly harm an\nindividual for the benefit of others, all models rated moral violations as more\nacceptable when part of a group, demonstrating a Utilitarian Boost similar to\nthat observed in humans. However, the mechanism for the Boost in LLMs differed:\nWhile humans in groups become more utilitarian due to heightened sensitivity to\ndecision outcomes, LLM groups showed either reduced sensitivity to norms or\nenhanced impartiality. We report model differences in when and how strongly the\nBoost manifests. We also discuss prompt and agent compositions that enhance or\nmitigate the effect. We end with a discussion of the implications for AI\nalignment, multi-agent design, and artificial moral reasoning. Code available\nat: https://github.com/baltaci-r/MoralAgents", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7fa4\u4f53\u534f\u4f5c\u4e2d\u662f\u5426\u4f1a\u51fa\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u7684\u201c\u529f\u5229\u4e3b\u4e49\u52a9\u63a8\u201d\u73b0\u8c61\uff0c\u5373\u5728\u7fa4\u4f53\u8ba8\u8bba\u4e2d\uff0c\u6a21\u578b\u4f1a\u66f4\u503e\u5411\u4e8e\u652f\u6301\u90a3\u4e9b\u4e3a\u4e86\u6700\u5927\u5316\u591a\u6570\u4eba\u5229\u76ca\u800c\u635f\u5bb3\u5c11\u6570\u4eba\u539f\u5219\u7684\u884c\u4e3a\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e2a\u4eba\u56f0\u5883\u60c5\u5883\u4e0b\uff0cLLMs\u786e\u5b9e\u8868\u73b0\u51fa\u4e86\u529f\u5229\u4e3b\u4e49\u52a9\u63a8\uff0c\u4f46\u5176\u5185\u5728\u673a\u5236\u4e0e\u4eba\u7c7b\u4e0d\u540c\u3002\u4eba\u7c7b\u662f\u7531\u4e8e\u5bf9\u51b3\u7b56\u7ed3\u679c\u7684\u654f\u611f\u6027\u63d0\u9ad8\uff0c\u800cLLMs\u5219\u662f\u7531\u4e8e\u5bf9\u89c4\u8303\u7684\u654f\u611f\u6027\u964d\u4f4e\u6216\u589e\u5f3a\u4e86\u516c\u6b63\u6027\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u4e0d\u540c\u6a21\u578b\u3001\u63d0\u793a\u548c\u6210\u5458\u6784\u6210\u5bf9\u8fd9\u79cd\u52a9\u63a8\u6548\u5e94\u7684\u5f71\u54cd\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u4eba\u5de5\u667a\u80fd\u5bf9\u9f50\u3001\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u548c\u4eba\u5de5\u667a\u80fd\u9053\u5fb7\u63a8\u7406\u65b9\u9762\u7684\u610f\u4e49\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u793e\u4f1a\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\u65e5\u76ca\u51f8\u663e\uff0c\u7406\u89e3\u5b83\u4eec\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\u4e2d\u7684\u884c\u4e3a\u6a21\u5f0f\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7279\u522b\u662f\u5728\u7fa4\u4f53\u534f\u4f5c\u573a\u666f\u4e0b\uff0cLLMs\u7684\u8868\u73b0\u53ef\u80fd\u4e0e\u72ec\u7acb\u4e2a\u4f53\u8fd0\u884c\u65f6\u5b58\u5728\u5dee\u5f02\u3002\u4eba\u7c7b\u5728\u9053\u5fb7\u5224\u65ad\u4e2d\u5b58\u5728\u4e00\u79cd\u201c\u529f\u5229\u4e3b\u4e49\u52a9\u63a8\u201d\u73b0\u8c61\uff0c\u5373\u7fa4\u4f53\u8ba8\u8bba\u4f1a\u4fc3\u4f7f\u4e2a\u4f53\u66f4\u503e\u5411\u4e8e\u652f\u6301\u90a3\u4e9b\u80fd\u4e3a\u6700\u5927\u591a\u6570\u4eba\u5e26\u6765\u5229\u76ca\u4f46\u53ef\u80fd\u635f\u5bb3\u5c11\u6570\u4eba\u539f\u5219\u7684\u884c\u4e3a\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLMs\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u662f\u5426\u4e5f\u4f1a\u51fa\u73b0\u7c7b\u4f3c\u7684\u529f\u5229\u4e3b\u4e49\u52a9\u63a8\uff0c\u4ee5\u53ca\u5176\u80cc\u540e\u7684\u673a\u5236\u662f\u5426\u4e0e\u4eba\u7c7b\u4e00\u81f4\u3002\u8fd9\u5bf9\u4e8e\u7406\u89e3\u548c\u8bbe\u8ba1\u66f4\u5b89\u5168\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u516d\u79cd\u4e0d\u540c\u7684LLMs\u6a21\u578b\u6765\u7814\u7a76\u529f\u5229\u4e3b\u4e49\u52a9\u63a8\u73b0\u8c61\uff1a1. \u5355\u72ec\uff08Solo\uff09\u6761\u4ef6\uff1a\u6a21\u578b\u72ec\u7acb\u8fdb\u884c\u9053\u5fb7\u5224\u65ad\u30022. \u7fa4\u4f53\uff08Group\uff09\u6761\u4ef6\uff1a\u6a21\u578b\u4ee5\u4e24\u4eba\u6216\u4e09\u4eba\u5bf9\u8fdb\u884c\u591a\u8f6e\u8ba8\u8bba\u540e\u505a\u51fa\u5224\u65ad\u3002\u7814\u7a76\u4f7f\u7528\u4e86\u4e24\u5957\u7ecf\u5178\u7684\u9053\u5fb7\u56f0\u5883\u6570\u636e\u96c6\u3002\u5728\u4e2a\u4eba\u56f0\u5883\u60c5\u5883\u4e0b\uff0c\u5373\u6a21\u578b\u9700\u8981\u51b3\u5b9a\u662f\u5426\u4e3a\u4e86\u4ed6\u4eba\u5229\u76ca\u800c\u76f4\u63a5\u4f24\u5bb3\u4e2a\u4f53\u65f6\uff0c\u7814\u7a76\u4eba\u5458\u5206\u6790\u4e86\u7fa4\u4f53\u8ba8\u8bba\u5bf9\u6a21\u578b\u5224\u65ad\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5728\u4e2a\u4eba\u56f0\u5883\u60c5\u5883\u4e0b\uff0c\u6240\u6709\u6d4b\u8bd5\u7684LLMs\u6a21\u578b\u5728\u7fa4\u4f53\u8ba8\u8bba\u540e\u90fd\u6bd4\u5355\u72ec\u8fd0\u884c\u65f6\u66f4\u80fd\u63a5\u53d7\u9053\u5fb7\u4e0a\u7684\u4fb5\u72af\u884c\u4e3a\uff0c\u8fd9\u8868\u660eLLMs\u4e5f\u5b58\u5728\u529f\u5229\u4e3b\u4e49\u52a9\u63a8\u73b0\u8c61\u3002\u7136\u800c\uff0cLLMs\u4ea7\u751f\u52a9\u63a8\u6548\u5e94\u7684\u673a\u5236\u4e0e\u4eba\u7c7b\u4e0d\u540c\uff1a\u4eba\u7c7b\u662f\u7531\u4e8e\u5bf9\u51b3\u7b56\u7ed3\u679c\u7684\u654f\u611f\u6027\u589e\u5f3a\uff0c\u800cLLMs\u5219\u662f\u8868\u73b0\u51fa\u5bf9\u9053\u5fb7\u89c4\u8303\u7684\u654f\u611f\u6027\u964d\u4f4e\u6216\u589e\u5f3a\u4e86\u516c\u6b63\u6027\u3002\u7814\u7a76\u8fd8\u8be6\u7ec6\u8bb0\u5f55\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u52a9\u63a8\u6548\u5e94\u51fa\u73b0\u7684\u65f6\u95f4\u548c\u5f3a\u5ea6\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0d\u540c\u7684\u63d0\u793a\u548c\u6210\u5458\u6784\u6210\u5982\u4f55\u589e\u5f3a\u6216\u51cf\u5f31\u8fd9\u79cd\u6548\u5e94\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u8bc1\u660e\u4e86LLMs\u5728\u7fa4\u4f53\u534f\u4f5c\u4e2d\u4f1a\u8868\u73b0\u51fa\u529f\u5229\u4e3b\u4e49\u52a9\u63a8\u73b0\u8c61\uff0c\u4f46\u5176\u5185\u5728\u673a\u5236\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u63ed\u793a\u4e86LLMs\u5728\u9053\u5fb7\u63a8\u7406\u65b9\u9762\u4e0e\u4eba\u7c7b\u7684\u4e0d\u540c\u4e4b\u5904\u3002\u7814\u7a76\u7ed3\u679c\u5bf9\u4e8e\u4eba\u5de5\u667a\u80fd\u5bf9\u9f50\uff08AI alignment\uff09\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u4ee5\u53ca\u672a\u6765\u4eba\u5de5\u667a\u80fd\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u5177\u6709\u91cd\u8981\u542f\u793a\u3002\u7406\u89e3\u8fd9\u4e9b\u5dee\u5f02\u6709\u52a9\u4e8e\u6211\u4eec\u66f4\u597d\u5730\u8bbe\u8ba1\u80fd\u591f\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4fdd\u6301\u4e00\u81f4\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u8d1f\u8d23\u4efb\u7684\u4eba\u5de5\u667a\u80fd\u5960\u5b9a\u57fa\u7840\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e0d\u540c\u7c7b\u578b\u7684\u9053\u5fb7\u56f0\u5883\u3001\u66f4\u590d\u6742\u7684\u7fa4\u4f53\u4e92\u52a8\u4ee5\u53ca\u8de8\u6587\u5316\u80cc\u666f\u4e0b\u7684LLMs\u9053\u5fb7\u5224\u65ad\u3002"}}
{"id": "2507.04458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.04458", "abs": "https://arxiv.org/abs/2507.04458", "authors": ["Soumyadeep Jana", "Abhrajyoti Kundu", "Sanasam Ranbir Singh"], "title": "Think Twice Before You Judge: Mixture of Dual Reasoning Experts for Multimodal Sarcasm Detection", "comment": null, "summary": "Multimodal sarcasm detection has attracted growing interest due to the rise\nof multimedia posts on social media. Understanding sarcastic image-text posts\noften requires external contextual knowledge, such as cultural references or\ncommonsense reasoning. However, existing models struggle to capture the deeper\nrationale behind sarcasm, relying mainly on shallow cues like image captions or\nobject-attribute pairs from images. To address this, we propose \\textbf{MiDRE}\n(\\textbf{Mi}xture of \\textbf{D}ual \\textbf{R}easoning \\textbf{E}xperts), which\nintegrates an internal reasoning expert for detecting incongruities within the\nimage-text pair and an external reasoning expert that utilizes structured\nrationales generated via Chain-of-Thought prompting to a Large Vision-Language\nModel. An adaptive gating mechanism dynamically weighs the two experts,\nselecting the most relevant reasoning path. Unlike prior methods that treat\nexternal knowledge as static input, MiDRE selectively adapts to when such\nknowledge is beneficial, mitigating the risks of hallucinated or irrelevant\nsignals from large models. Experiments on two benchmark datasets show that\nMiDRE achieves superior performance over baselines. Various qualitative\nanalyses highlight the crucial role of external rationales, revealing that even\nwhen they are occasionally noisy, they provide valuable cues that guide the\nmodel toward a better understanding of sarcasm.", "AI": {"tldr": "\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u56e0\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u6d45\u5c42\u7ebf\u7d22\u4e14\u96be\u4ee5\u6355\u6349\u6df1\u5c42\u539f\u56e0\u3002\u672c\u6587\u63d0\u51faMiDRE\uff0c\u7ed3\u5408\u5185\u90e8\u548c\u5916\u90e8\u63a8\u7406\u4e13\u5bb6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u63a8\u7406\u6765\u7406\u89e3\u8bbd\u523a\u3002\u5b9e\u9a8c\u8bc1\u660eMiDRE\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5916\u90e8\u63a8\u7406\u5728\u5f15\u5bfc\u6a21\u578b\u7406\u89e3\u8bbd\u523a\u65b9\u9762\u4f5c\u7528\u5173\u952e\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u7406\u89e3\u5305\u542b\u6587\u5316\u53c2\u8003\u6216\u5e38\u8bc6\u63a8\u7406\u7684\u56fe\u50cf-\u6587\u672c\u5e16\u5b50\u4e2d\u7684\u8bbd\u523a\u65f6\uff0c\u4e3b\u8981\u4f9d\u8d56\u56fe\u50cf\u6807\u9898\u6216\u5bf9\u8c61\u5c5e\u6027\u7b49\u6d45\u5c42\u7ebf\u7d22\uff0c\u96be\u4ee5\u6355\u6349\u6df1\u5c42\u539f\u56e0\u3002", "method": "\u63d0\u51faMiDRE\uff08\u591a\u6a21\u6001\u53cc\u63a8\u7406\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff09\uff0c\u7ed3\u5408\u5185\u90e8\u63a8\u7406\u4e13\u5bb6\uff08\u68c0\u6d4b\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u4e0d\u4e00\u81f4\u6027\uff09\u548c\u5916\u90e8\u63a8\u7406\u4e13\u5bb6\uff08\u5229\u7528\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u751f\u6210\u7684\u7ed3\u6784\u5316\u63a8\u7406\uff09\u3002\u91c7\u7528\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u52a8\u6001\u52a0\u6743\u4e24\u4e2a\u4e13\u5bb6\uff0c\u5e76\u80fd\u9009\u62e9\u6027\u5730\u9002\u5e94\u5916\u90e8\u77e5\u8bc6\u7684\u4f7f\u7528\uff0c\u4ee5\u51cf\u8f7b\u5927\u6a21\u578b\u53ef\u80fd\u4ea7\u751f\u7684\u5e7b\u89c9\u6216\u4e0d\u76f8\u5173\u4fe1\u53f7\u7684\u98ce\u9669\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMiDRE\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u5373\u4f7f\u5916\u90e8\u63a8\u7406\u6709\u65f6\u5b58\u5728\u566a\u58f0\uff0c\u4e5f\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u7ebf\u7d22\uff0c\u6307\u5bfc\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u8bbd\u523a\u3002", "conclusion": "MiDRE\u901a\u8fc7\u7ed3\u5408\u5185\u90e8\u548c\u5916\u90e8\u63a8\u7406\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u4e24\u8005\u6743\u91cd\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u7684\u6027\u80fd\u3002\u5916\u90e8\u63a8\u7406\u5728\u7406\u89e3\u590d\u6742\u8bbd\u523a\u65b9\u9762\u53d1\u6325\u4e86\u5173\u952e\u4f5c\u7528\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u4f18\u5316\u5916\u90e8\u63a8\u7406\u7684\u751f\u6210\u548c\u5229\u7528\uff0c\u4ee5\u53ca\u5904\u7406\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002"}}
{"id": "2507.04508", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.04508", "abs": "https://arxiv.org/abs/2507.04508", "authors": ["Soumyadeep Jana", "Sahil Danayak", "Sanasam Ranbir Singh"], "title": "Adapter-state Sharing CLIP for Parameter-efficient Multimodal Sarcasm Detection", "comment": null, "summary": "The growing prevalence of multimodal image-text sarcasm on social media poses\nchallenges for opinion mining systems. Existing approaches rely on full\nfine-tuning of large models, making them unsuitable to adapt under\nresource-constrained settings. While recent parameter-efficient fine-tuning\n(PEFT) methods offer promise, their off-the-shelf use underperforms on complex\ntasks like sarcasm detection. We propose AdS-CLIP (Adapter-state Sharing in\nCLIP), a lightweight framework built on CLIP that inserts adapters only in the\nupper layers to preserve low-level unimodal representations in the lower layers\nand introduces a novel adapter-state sharing mechanism, where textual adapters\nguide visual ones to promote efficient cross-modal learning in the upper\nlayers. Experiments on two public benchmarks demonstrate that AdS-CLIP not only\noutperforms standard PEFT methods but also existing multimodal baselines with\nsignificantly fewer trainable parameters.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86AdS-CLIP\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u591a\u6a21\u6001\u56fe\u50cf-\u6587\u672c\u8bbd\u523a\u68c0\u6d4b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5728CLIP\u6a21\u578b\u7684\u4e0a\u5c42\u63d2\u5165\u9002\u914d\u5668\u5e76\u5f15\u5165\u9002\u914d\u5668\u72b6\u6001\u5171\u4eab\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5b66\u4e60\u6548\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u66f4\u5c11\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u591a\u6a21\u6001\u56fe\u50cf-\u6587\u672c\u8bbd\u523a\u7684\u65e5\u76ca\u666e\u904d\u7ed9\u89c2\u70b9\u6316\u6398\u7cfb\u7edf\u5e26\u6765\u4e86\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5bf9\u5927\u578b\u6a21\u578b\u8fdb\u884c\u5b8c\u5168\u5fae\u8c03\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u9002\u7528\u3002\u867d\u7136\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u8bbd\u523a\u68c0\u6d4b\u7b49\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faAdS-CLIP\u6846\u67b6\uff0c\u57fa\u4e8eCLIP\u6a21\u578b\uff0c\u4ec5\u5728\u4e0a\u5c42\u63d2\u5165\u9002\u914d\u5668\u4ee5\u4fdd\u7559\u5e95\u5c42\u5355\u6a21\u6001\u8868\u793a\u3002\u5f15\u5165\u65b0\u9896\u7684\u9002\u914d\u5668\u72b6\u6001\u5171\u4eab\u673a\u5236\uff0c\u7531\u6587\u672c\u9002\u914d\u5668\u5f15\u5bfc\u89c6\u89c9\u9002\u914d\u5668\uff0c\u4fc3\u8fdb\u4e0a\u5c42\u7684\u9ad8\u6548\u8de8\u6a21\u6001\u5b66\u4e60\u3002\u5728\u4e24\u4e2a\u516c\u5f00\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u57fa\u51c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAdS-CLIP\u7684\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u7684PEFT\u65b9\u6cd5\u548c\u73b0\u6709\u7684\u591a\u6a21\u6001\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u53ef\u8bad\u7ec3\u53c2\u6570\u91cf\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "AdS-CLIP\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53c2\u6570\u91cf\u5c11\u7684\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u9002\u914d\u5668\u72b6\u6001\u5171\u4eab\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u89c2\u70b9\u6316\u6398\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u672a\u6765\u53ef\u63a2\u7d22\u66f4\u590d\u6742\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u673a\u5236\u548c\u9002\u914d\u5668\u8bbe\u8ba1\u3002"}}
{"id": "2510.13852", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13852", "abs": "https://arxiv.org/abs/2510.13852", "authors": ["Peter Banyas", "Shristi Sharma", "Alistair Simmons", "Atharva Vispute"], "title": "ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups", "comment": "For associated code repository, see\n  http://github.com/banyasp/consistencyAI For user-friendly web app, see\n  http://v0-llm-comparison-webapp.vercel.app/", "summary": "Is an LLM telling you different facts than it's telling me? This paper\nintroduces ConsistencyAI, an independent benchmark for measuring the factual\nconsistency of large language models (LLMs) for different personas.\nConsistencyAI tests whether, when users of different demographics ask identical\nquestions, the model responds with factually inconsistent answers. Designed\nwithout involvement from LLM providers, this benchmark offers impartial\nevaluation and accountability. In our experiment, we queried 19 LLMs with\nprompts that requested 5 facts for each of 15 topics. We repeated this query\n100 times for each LLM, each time adding prompt context from a different\npersona selected from a subset of personas modeling the general population. We\nprocessed the responses into sentence embeddings, computed cross-persona cosine\nsimilarity, and computed the weighted average of cross-persona cosine\nsimilarity to calculate factual consistency scores. In 100-persona experiments,\nscores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as\na benchmark threshold. xAI's Grok-3 is most consistent, while several\nlightweight models rank lowest. Consistency varies by topic: the job market is\nleast consistent, G7 world leaders most consistent, and issues like vaccines or\nthe Israeli-Palestinian conflict diverge by provider. These results show that\nboth the provider and the topic shape the factual consistency. We release our\ncode and interactive demo to support reproducible evaluation and encourage\npersona-invariant prompting strategies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86 ConsistencyAI\uff0c\u4e00\u4e2a\u72ec\u7acb\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u540c\u4e2a\u4f53\u8eab\u4efd\u4e0b\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u8eab\u4efd\u7684\u63d0\u95ee\u53ef\u80fd\u5bfc\u81f4 LLM \u7ed9\u51fa\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u7684\u56de\u7b54\uff0c\u5e76\u4e14\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\u56e0\u6a21\u578b\u548c\u4e3b\u9898\u800c\u5f02\u3002\u7814\u7a76\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u6f14\u793a\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4fe1\u606f\u4f20\u64ad\u4e2d\u7684\u4f5c\u7528\u65e5\u76ca\u589e\u5f3a\uff0c\u786e\u4fdd\u5b83\u4eec\u63d0\u4f9b\u7684\u4e8b\u5b9e\u4fe1\u606f\u5728\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u4e2d\u4fdd\u6301\u4e00\u81f4\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u72ec\u7acb\u4e14\u5168\u9762\u7684\u57fa\u51c6\u6765\u8861\u91cf LLMs \u5728\u9762\u5bf9\u4e0d\u540c\u4e2a\u4f53\u8eab\u4efd\uff08\u5982\u4e0d\u540c\u7684\u5e74\u9f84\u3001\u6027\u522b\u3001\u5730\u57df\u7b49\uff09\u65f6\uff0c\u56de\u7b54\u4e8b\u5b9e\u4fe1\u606f\u65f6\u7684\u4e00\u81f4\u6027\u3002LLMs \u53ef\u80fd\u4f1a\u56e0\u4e3a\u7528\u6237\u7684\u8eab\u4efd\u800c\u8c03\u6574\u5176\u56de\u7b54\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u4e8b\u5b9e\u4e0a\u7684\u4e0d\u4e00\u81f4\uff0c\u4ece\u800c\u5f71\u54cd\u4fe1\u606f\u7684\u53ef\u9760\u6027\u548c\u7528\u6237\u7684\u4fe1\u4efb\u5ea6\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u4e2a\u72ec\u7acb\u4e8e LLM \u63d0\u4f9b\u5546\u7684\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f30\u548c\u63d0\u9ad8 LLMs \u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a ConsistencyAI \u7684\u72ec\u7acb\u57fa\u51c6\uff0c\u7528\u4e8e\u8861\u91cf LLMs \u5728\u4e0d\u540c\u4e2a\u4f53\u8eab\u4efd\u4e0b\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002\u7814\u7a76\u56e2\u961f\u4f7f\u7528\u4e86 19 \u4e2a LLMs\uff0c\u9488\u5bf9 15 \u4e2a\u4e3b\u9898\uff0c\u6bcf\u4e2a\u4e3b\u9898\u8981\u6c42\u6a21\u578b\u63d0\u4f9b 5 \u4e2a\u4e8b\u5b9e\u3002\u4e3a\u4e86\u6a21\u62df\u4e0d\u540c\u7528\u6237\u8eab\u4efd\uff0c\u7814\u7a76\u4eba\u5458\u4e3a\u6bcf\u6b21\u67e5\u8be2\u6dfb\u52a0\u4e86\u6765\u81ea\u4e0d\u540c\u4e2a\u4f53\u8eab\u4efd\u7684\u63d0\u793a\u8bed\u5883\uff0c\u5e76\u91cd\u590d\u4e86 100 \u6b21\u3002\u4e3a\u4e86\u91cf\u5316\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u7814\u7a76\u4eba\u5458\u5c06\u6a21\u578b\u7684\u56de\u5e94\u8f6c\u6362\u4e3a\u53e5\u5d4c\u5165\uff0c\u8ba1\u7b97\u4e86\u4e0d\u540c\u8eab\u4efd\u56de\u5e94\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u5e76\u8ba1\u7b97\u4e86\u52a0\u6743\u5e73\u5747\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u4e8b\u5b9e\u4e00\u81f4\u6027\u5f97\u5206\u3002\u8be5\u7814\u7a76\u8fd8\u786e\u5b9a\u4e86\u4e00\u4e2a\u57fa\u51c6\u9608\u503c\uff080.8656\uff09\uff0c\u5e76\u5bf9\u4e0d\u540c\u6a21\u578b\u548c\u4e3b\u9898\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u6392\u540d\u548c\u6bd4\u8f83\u3002", "result": "\u5728 100 \u4e2a\u4e2a\u4f53\u8eab\u4efd\u7684\u5b9e\u9a8c\u4e2d\uff0cLLMs \u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u5f97\u5206\u8303\u56f4\u4ece 0.9065 \u5230 0.7896\uff0c\u5e73\u5747\u5f97\u5206\u4e3a 0.8656\uff0c\u8be5\u5e73\u5747\u5206\u88ab\u786e\u5b9a\u4e3a\u57fa\u51c6\u9608\u503c\u3002\u7814\u7a76\u53d1\u73b0\uff0cxAI \u7684 Grok-3 \u6a21\u578b\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u4e00\u81f4\u6027\uff0c\u800c\u4e00\u4e9b\u8f7b\u91cf\u7ea7\u6a21\u578b\u5219\u5f97\u5206\u6700\u4f4e\u3002\u4e8b\u5b9e\u4e00\u81f4\u6027\u4e5f\u56e0\u4e3b\u9898\u800c\u5f02\uff1a\u5173\u4e8e\u5c31\u4e1a\u5e02\u573a\u7684\u4e3b\u9898\u4e00\u81f4\u6027\u6700\u4f4e\uff0c\u800c\u5173\u4e8e G7 \u56fd\u5bb6\u9886\u5bfc\u4eba\u7684\u4fe1\u606f\u6700\u4e00\u81f4\u3002\u5bf9\u4e8e\u75ab\u82d7\u63a5\u79cd\u6216\u4ee5\u8272\u5217-\u5df4\u52d2\u65af\u5766\u51b2\u7a81\u7b49\u654f\u611f\u8bdd\u9898\uff0c\u4e0d\u540c\u63d0\u4f9b\u5546\u7684 LLMs \u8868\u73b0\u51fa\u663e\u8457\u7684\u5206\u6b67\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u7684\u63d0\u4f9b\u5546\u548c\u4fe1\u606f\u7684\u5177\u4f53\u4e3b\u9898\u90fd\u4f1a\u5f71\u54cd\u4e8b\u5b9e\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86 LLMs \u5728\u4e0d\u540c\u4e2a\u4f53\u8eab\u4efd\u4e0b\u53ef\u80fd\u5b58\u5728\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u6027\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86 ConsistencyAI \u4f5c\u4e3a\u8861\u91cf\u8fd9\u4e00\u95ee\u9898\u7684\u72ec\u7acb\u57fa\u51c6\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLMs \u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u53d7\u5230\u6a21\u578b\u63d0\u4f9b\u5546\u548c\u4fe1\u606f\u4e3b\u9898\u7684\u53cc\u91cd\u5f71\u54cd\uff0c\u5e76\u4e14\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u4e3a\u4e86\u63d0\u9ad8 LLMs \u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u7814\u7a76\u4eba\u5458\u547c\u5401\u5f00\u53d1\u548c\u91c7\u7528\u201c\u4e2a\u4f53\u8eab\u4efd\u65e0\u5173\u201d\u7684\u63d0\u793a\u7b56\u7565\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u4ea4\u4e92\u5f0f\u6f14\u793a\uff0c\u4ee5\u652f\u6301\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\uff0c\u5e76\u9f13\u52b1\u793e\u533a\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6539\u8fdb LLMs \u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u6269\u5c55\u6d4b\u8bd5\u7684\u4e3b\u9898\u8303\u56f4\uff0c\u63a2\u7d22\u66f4\u591a\u6837\u7684\u4e2a\u4f53\u8eab\u4efd\uff0c\u5e76\u7814\u7a76\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6027\u7684\u5177\u4f53\u539f\u56e0\u3002"}}
{"id": "2507.21112", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.21112", "abs": "https://arxiv.org/abs/2507.21112", "authors": ["Panyi Dong", "Zhiyu Quan"], "title": "InsurTech innovation using natural language processing", "comment": null, "summary": "With the rapid rise of InsurTech, traditional insurance companies are\nincreasingly exploring alternative data sources and advanced technologies to\nsustain their competitive edge. This paper provides both a conceptual overview\nand practical case studies of natural language processing (NLP) and its\nemerging applications within insurance operations, focusing on transforming\nraw, unstructured text into structured data suitable for actuarial analysis and\ndecision-making. Leveraging real-world alternative data provided by an\nInsurTech industry partner that enriches traditional insurance data sources, we\napply various NLP techniques to demonstrate feature de-biasing, feature\ncompression, and industry classification in the commercial insurance context.\nThese enriched, text-derived insights not only add to and refine traditional\nrating factors for commercial insurance pricing but also offer novel\nperspectives for assessing underlying risk by introducing novel industry\nclassification techniques. Through these demonstrations, we show that NLP is\nnot merely a supplementary tool but a foundational element of modern,\ndata-driven insurance analytics.", "AI": {"tldr": "InsurTech\u5174\u8d77\uff0c\u4f20\u7edf\u4fdd\u9669\u516c\u53f8\u63a2\u7d22NLP\u6280\u672f\u5904\u7406\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\uff0c\u901a\u8fc7\u7279\u5f81\u53bb\u504f\u3001\u538b\u7f29\u548c\u884c\u4e1a\u5206\u7c7b\uff0c\u4e30\u5bcc\u4e86\u5546\u4e1a\u4fdd\u9669\u5b9a\u4ef7\u548c\u98ce\u9669\u8bc4\u4f30\uff0c\u8bc1\u660eNLP\u662f\u73b0\u4ee3\u4fdd\u9669\u5206\u6790\u7684\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u4fdd\u9669\u516c\u53f8\u9762\u4e34InsurTech\u7684\u7ade\u4e89\uff0c\u9700\u8981\u5229\u7528\u66ff\u4ee3\u6570\u636e\u6e90\u548c\u5148\u8fdb\u6280\u672f\uff08\u5982NLP\uff09\u6765\u5904\u7406\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\uff0c\u4ee5\u6539\u8fdb\u73b0\u6709\u4fdd\u9669\u4e1a\u52a1\uff0c\u5c24\u5176\u662f\u5728\u5546\u4e1a\u4fdd\u9669\u5b9a\u4ef7\u548c\u98ce\u9669\u8bc4\u4f30\u65b9\u9762\uff0c\u4ece\u800c\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "method": "\u672c\u6587\u7ed3\u5408\u4e86\u6982\u5ff5\u6027\u6982\u8ff0\u548c\u5b9e\u9645\u6848\u4f8b\u7814\u7a76\u3002\u7814\u7a76\u4eba\u5458\u5229\u7528InsurTech\u5408\u4f5c\u4f19\u4f34\u63d0\u4f9b\u7684\u771f\u5b9e\u4e16\u754c\u66ff\u4ee3\u6570\u636e\uff0c\u7ed3\u5408\u4f20\u7edf\u4fdd\u9669\u6570\u636e\uff0c\u5e94\u7528\u4e86\u591a\u79cdNLP\u6280\u672f\uff0c\u5305\u62ec\u7279\u5f81\u53bb\u504f\u3001\u7279\u5f81\u538b\u7f29\u548c\u884c\u4e1a\u5206\u7c7b\uff0c\u4ee5\u5904\u7406\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cNLP\u6280\u672f\u80fd\u591f\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u80fd\u591f\u4e30\u5bcc\u548c\u4f18\u5316\u5546\u4e1a\u4fdd\u9669\u7684\u5b9a\u4ef7\u56e0\u7d20\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u884c\u4e1a\u5206\u7c7b\u6280\u672f\u6765\u8bc4\u4f30\u6f5c\u5728\u98ce\u9669\u3002NLP\u5904\u7406\u540e\u7684\u6570\u636e\u4e3a\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "conclusion": "NLP\u4e0d\u4ec5\u4ec5\u662f\u4e00\u4e2a\u8f85\u52a9\u5de5\u5177\uff0c\u800c\u662f\u73b0\u4ee3\u6570\u636e\u9a71\u52a8\u7684\u4fdd\u9669\u5206\u6790\u7684\u57fa\u77f3\u3002\u901a\u8fc7NLP\u6280\u672f\uff0c\u4fdd\u9669\u516c\u53f8\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u6570\u636e\uff0c\u4f18\u5316\u5b9a\u4ef7\uff0c\u6539\u8fdb\u98ce\u9669\u8bc4\u4f30\uff0c\u5e76\u5728\u65e5\u76ca\u6fc0\u70c8\u7684\u5e02\u573a\u7ade\u4e89\u4e2d\u4fdd\u6301\u4f18\u52bf\u3002"}}
{"id": "2510.22967", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22967", "abs": "https://arxiv.org/abs/2510.22967", "authors": ["Yucheng Ning", "Xixun Lin", "Fang Fang", "Yanan Cao"], "title": "MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs", "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-51369-x}", "summary": "The widespread adoption of Large Language Models (LLMs) raises critical\nconcerns about the factual accuracy of their outputs, especially in high-risk\ndomains such as biomedicine, law, and education. Existing evaluation methods\nfor short texts often fail on long-form content due to complex reasoning\nchains, intertwined perspectives, and cumulative information. To address this,\nwe propose a systematic approach integrating large-scale long-form datasets,\nmulti-agent verification mechanisms, and weighted evaluation metrics. We\nconstruct LongHalluQA, a Chinese long-form factuality dataset; and develop\nMAD-Fact, a debate-based multi-agent verification system. We introduce a fact\nimportance hierarchy to capture the varying significance of claims in long-form\ntexts. Experiments on two benchmarks show that larger LLMs generally maintain\nhigher factual consistency, while domestic models excel on Chinese content. Our\nwork provides a structured framework for evaluating and enhancing factual\nreliability in long-form LLM outputs, guiding their safe deployment in\nsensitive domains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aLongHalluQA\u7684\u65b0\u578b\u4e2d\u6587\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aMAD-Fact\u7684\u591a\u4e3b\u4f53\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u66f4\u5927\u7684\u6a21\u578b\u5728\u4e8b\u5b9e\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u56fd\u5185\u6a21\u578b\u5728\u4e2d\u6587\u5185\u5bb9\u4e0a\u5177\u6709\u4f18\u52bf\u3002\u8be5\u6846\u67b6\u65e8\u5728\u63d0\u5347LLMs\u5728\u751f\u7269\u533b\u5b66\u3001\u6cd5\u5f8b\u548c\u6559\u80b2\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5b89\u5168\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u666e\u904d\u62c5\u5fe7\uff0c\u5c24\u5176\u662f\u5728\u751f\u7269\u533b\u5b66\u3001\u6cd5\u5f8b\u548c\u6559\u80b2\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u3002\u9488\u5bf9\u77ed\u6587\u672c\u7684\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u957f\u6587\u672c\u6d89\u53ca\u590d\u6742\u7684\u63a8\u7406\u94fe\u3001\u4ea4\u7ec7\u7684\u89c2\u70b9\u548c\u7d2f\u79ef\u7684\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u63d0\u9ad8LLMs\u957f\u6587\u672c\u8f93\u51fa\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u3002", "method": "\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aLongHalluQA\u7684\u4e2d\u6587\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8fa9\u8bba\u7684\u591a\u4e3b\u4f53\u9a8c\u8bc1\u7cfb\u7edfMAD-Fact\u3002\u4ed6\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u4e8b\u5b9e\u91cd\u8981\u6027\u5c42\u6b21\u7ed3\u6784\uff0c\u4ee5\u6355\u6349\u957f\u6587\u672c\u4e2d\u4e0d\u540c\u58f0\u660e\u7684\u91cd\u8981\u7a0b\u5ea6\u3002\u901a\u8fc7\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u6765\u8bc4\u4f30\u8be5\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u66f4\u5927\u7684LLMs\u6a21\u578b\u5728\u4e8b\u5b9e\u4e00\u81f4\u6027\u65b9\u9762\u901a\u5e38\u8868\u73b0\u66f4\u597d\u3002\u56fd\u5185\u6a21\u578b\u5728\u5904\u7406\u4e2d\u6587\u5185\u5bb9\u65f6\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\u3002\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u589e\u5f3aLLMs\u957f\u6587\u672c\u8f93\u51fa\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u8bc4\u4f30\u6240\u9700\u7684\u6570\u636e\u96c6\u548c\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u4e3aLLMs\u5728\u654f\u611f\u9886\u57df\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u8bed\u8a00\u7279\u5f02\u6027\u5bf9\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6269\u5c55\u6570\u636e\u96c6\uff0c\u4f18\u5316\u591a\u4e3b\u4f53\u9a8c\u8bc1\u673a\u5236\uff0c\u5e76\u63a2\u7d22\u66f4\u590d\u6742\u7684\u957f\u6587\u672c\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2507.22811", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22811", "abs": "https://arxiv.org/abs/2507.22811", "authors": ["Debayan Banerjee", "Tilahun Abedissa Taffa", "Ricardo Usbeck"], "title": "DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph", "comment": null, "summary": "In this work we present an entity linker for DBLP's 2025 version of RDF-based\nKnowledge Graph. Compared to the 2022 version, DBLP now considers publication\nvenues as a new entity type called dblp:Stream. In the earlier version of\nDBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce\nentity linkings. In contrast, in this work, we develop a zero-shot entity\nlinker using LLMs using a novel method, where we re-rank candidate entities\nbased on the log-probabilities of the \"yes\" token output at the penultimate\nlayer of the LLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eDBLP 2025\u77e5\u8bc6\u56fe\u8c31\u7684\u96f6\u6837\u672c\u5b9e\u4f53\u94fe\u63a5\u5668\uff0c\u8be5\u94fe\u63a5\u5668\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e76\u901a\u8fc7\u201cyes\u201d\u4ee4\u724c\u7684\u5bf9\u6570\u6982\u7387\u91cd\u65b0\u6392\u5e8f\u5019\u9009\u5b9e\u4f53\u6765\u5de5\u4f5c\uff0c\u4e0e\u4e4b\u524d\u57fa\u4e8eKG\u5d4c\u5165\u548c\u91cd\u6392\u5e8f\u5668\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5177\u6709\u65b0\u9896\u6027\u3002", "motivation": "DBLP\u77e5\u8bc6\u56fe\u8c31\u7684\u66f4\u65b0\u5f15\u5165\u4e86\u65b0\u7684\u5b9e\u4f53\u7c7b\u578b\uff08dblp:Stream\uff09\uff0c\u8fd9\u9700\u8981\u65b0\u7684\u5b9e\u4f53\u94fe\u63a5\u65b9\u6cd5\u3002\u73b0\u6709\u7684\u57fa\u4e8eKG\u5d4c\u5165\u548c\u91cd\u6392\u5e8f\u5668\u7684\u65b9\u6cd5\u5728\u5904\u7406\u65b0\u7c7b\u578b\u548c\u65b0\u7248\u672c\u7684\u6570\u636e\u65f6\u53ef\u80fd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7075\u6d3b\u548c\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u5b9e\u4f53\u94fe\u63a5\u5668\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\uff0c\u901a\u8fc7\u5728LLM\u7684\u5012\u6570\u7b2c\u4e8c\u5c42\u8bc4\u4f30\u201cyes\u201d\u4ee4\u724c\u7684\u5bf9\u6570\u6982\u7387\u6765\u91cd\u65b0\u6392\u5e8f\u5019\u9009\u5b9e\u4f53\u3002\u8fd9\u79cd\u65b9\u6cd5\u907f\u514d\u4e86\u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u7684\u663e\u5f0f\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u3002", "result": "\u6587\u7ae0\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u96f6\u6837\u672c\u5b9e\u4f53\u94fe\u63a5\u5668\u5728DBLP 2025\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u6027\u80fd\u3002\u867d\u7136\u5177\u4f53\u7684\u91cf\u5316\u6307\u6807\u672a\u5728\u6458\u8981\u4e2d\u63d0\u4f9b\uff0c\u4f46\u5176\u96f6\u6837\u672c\u80fd\u529b\u548c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u88ab\u8ba4\u4e3a\u662f\u5173\u952e\u7684\u6210\u679c\uff0c\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u65b0\u6570\u636e\u548c\u65b0\u5b9e\u4f53\u7c7b\u578b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u96f6\u6837\u672c\u5b9e\u4f53\u94fe\u63a5\u65b9\u6cd5\uff0c\u5229\u7528LLM\u548c\u57fa\u4e8e\u5bf9\u6570\u6982\u7387\u7684\u91cd\u6392\u5e8f\u7b56\u7565\uff0c\u4e3aDBLP\u77e5\u8bc6\u56fe\u8c31\u7684\u6700\u65b0\u7248\u672c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u4f53\u94fe\u63a5\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u5728\u4e0d\u4f9d\u8d56\u663e\u5f0f\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5c55\u73b0\u4e86\u5904\u7406\u65b0\u5b9e\u4f53\u7c7b\u578b\u548c\u7248\u672c\u7684\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u5b9e\u4f53\u94fe\u63a5\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.01200", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01200", "abs": "https://arxiv.org/abs/2509.01200", "authors": ["Chenyang Le", "Bing Han", "Jinshun Li", "Songyong Chen", "Yanmin Qian"], "title": "SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation", "comment": "NeurIPS 2025 poster", "summary": "Simultaneous Speech Translation (SimulST) enables real-time cross-lingual\ncommunication by jointly optimizing speech recognition and machine translation\nunder strict latency constraints. Existing systems struggle to balance\ntranslation quality, latency, and semantic coherence, particularly in\nmultilingual many-to-many scenarios where divergent read and write policies\nhinder unified strategy learning. In this paper, we present SimulMEGA\n(Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy\nlearning framework that combines prefix-based training with a\nMixture-of-Experts refiner to learn effective read and write decisions in an\nimplicit manner, without adding inference-time overhead. Our design requires\nonly minimal modifications to standard transformer architectures and\ngeneralizes across both speech-to-text and text-to-speech streaming tasks.\nThrough comprehensive evaluation on six language pairs, our 500M parameter\nspeech-to-text model outperforms the Seamless baseline, achieving under 7\npercent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3\nseconds. We further demonstrate the versatility of SimulMEGA by extending it to\nstreaming TTS with a unidirectional backbone, yielding superior latency quality\ntradeoffs.", "AI": {"tldr": "SimulMEGA\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u524d\u7f00\u5f0f\u8bad\u7ec3\u548c\u6df7\u5408\u4e13\u5bb6\u7cbe\u70bc\u5668\uff0c\u9690\u5f0f\u5730\u5b66\u4e60\u6709\u6548\u7684\u8bfb\u5199\u51b3\u7b56\uff0c\u4ee5\u5e94\u5bf9SimulST\u4e2d\u7684\u7ffb\u8bd1\u8d28\u91cf\u3001\u5ef6\u8fdf\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u4e4b\u95f4\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u591a\u5bf9\u591a\u573a\u666f\u4e0b\u3002\u8be5\u65b9\u6cd5\u5728\u516d\u79cd\u8bed\u8a00\u5bf9\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5176\u8bed\u97f3\u8f6c\u6587\u672c\u6a21\u578b\u5728\u5ef6\u8fdf1.5\u79d2\u65f6BLEU\u4e0b\u964d\u4e0d\u52307%\uff0c\u5728\u5ef6\u8fdf3\u79d2\u65f6\u4e0b\u964d\u4e0d\u52303%\uff0c\u4f18\u4e8eSeamless\u57fa\u7ebf\uff0c\u5e76\u6210\u529f\u6269\u5c55\u5230\u6d41\u5f0fTTS\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u540c\u6b65\u8bed\u97f3\u7ffb\u8bd1\uff08SimulST\uff09\u7cfb\u7edf\u5728\u4e25\u683c\u7684\u5ef6\u8fdf\u9650\u5236\u4e0b\uff0c\u96be\u4ee5\u5e73\u8861\u7ffb\u8bd1\u8d28\u91cf\u3001\u5ef6\u8fdf\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u591a\u5bf9\u591a\u7684\u573a\u666f\u4e2d\uff0c\u8bfb\u5199\u7b56\u7565\u4e0d\u4e00\u81f4\u963b\u788d\u4e86\u7edf\u4e00\u7b56\u7565\u7684\u5b66\u4e60\u3002", "method": "SimulMEGA\uff08Simultaneous Generation by Mixture-of-Experts Gating\uff09\u6846\u67b6\u91c7\u7528\u65e0\u76d1\u7763\u7b56\u7565\u5b66\u4e60\uff0c\u7ed3\u5408\u524d\u7f00\u5f0f\u8bad\u7ec3\u548c\u6df7\u5408\u4e13\u5bb6\u7cbe\u70bc\u5668\uff0c\u9690\u5f0f\u5b66\u4e60\u8bfb\u5199\u7b56\u7565\uff0c\u65e0\u63a8\u7406\u65f6\u95f4\u5f00\u9500\u3002\u8be5\u8bbe\u8ba1\u4ec5\u9700\u5bf9\u6807\u51c6Transformer\u67b6\u6784\u8fdb\u884c\u6700\u5c0f\u4fee\u6539\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u8bed\u97f3\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u8bed\u97f3\u7684\u6d41\u5f0f\u4efb\u52a1\u3002", "result": "\u5728\u516d\u79cd\u8bed\u8a00\u5bf9\u4e0a\u7684\u5168\u9762\u8bc4\u4f30\u663e\u793a\uff0cSimulMEGA\u76845\u4ebf\u53c2\u6570\u8bed\u97f3\u8f6c\u6587\u672c\u6a21\u578b\u4f18\u4e8eSeamless\u57fa\u7ebf\uff0c\u57281.5\u79d2\u5e73\u5747\u5ef6\u8fdf\u4e0bBLEU\u4e0b\u964d\u4e0d\u52307%\uff0c\u57283\u79d2\u5ef6\u8fdf\u4e0b\u4e0b\u964d\u4e0d\u52303%\u3002\u5c06\u5176\u6269\u5c55\u5230\u5355\u5411\u9aa8\u5e72\u7684\u6d41\u5f0fTTS\u4efb\u52a1\u4e5f\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u5ef6\u8fdf-\u8d28\u91cf\u6743\u8861\u3002", "conclusion": "SimulMEGA\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3SimulST\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u8bed\u97f3\u8f6c\u6587\u672c\u548c\u6d41\u5f0fTTS\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2509.19902", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19902", "abs": "https://arxiv.org/abs/2509.19902", "authors": ["Binbin Zhang", "Chengdong Liang", "Shuai Wang", "Xuelong Geng", "Zhao Guo", "Haoyu Li", "Hao Yin", "Xipeng Yang", "Pengshen Zhang", "Changwei Ma", "Lei Xie"], "title": "WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction", "comment": null, "summary": "In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on\na large language model (LLM) for speech understanding, generation, and\ninteraction. There are three key features of WEST: 1) Fully LLM-based: Standing\non the shoulders of giants by reusing mature architectures, ecosystems (e.g.,\nHugging Face), and methods (e.g., sequence packing) from large models. 2)\nFull-stack: Supports tasks such as recognition, synthesis, understanding,\ndialogue, and multimodal capabilities, with extensibility to incorporate\nopen-source models. 3) Simple and Stupid: A simple and stupid speech toolkit\nthat everyone can Touch. In addition, WEST provides two types of recipes,\nmodels, and experimental results. The first is entirely based on open-source\nmodels and open-source data, allowing users to fully reproduce the experiments\nin this paper and serving as a verification system or minimal system baseline.\nThe second is trained on massive data, offering superior performance so the\nuser can directly apply it out of the box. WEST is publicly avilable at\nhttps://github.com/wenet-e2e/west/", "AI": {"tldr": "WEST (WE Speech Toolkit) \u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u8bed\u97f3\u5de5\u5177\u5305\uff0c\u652f\u6301\u8bed\u97f3\u7406\u89e3\u3001\u751f\u6210\u548c\u4ea4\u4e92\u3002\u5b83\u5177\u6709\u5168 LLM \u57fa\u7840\u3001\u5168\u6808\u80fd\u529b\uff08\u5305\u62ec\u8bc6\u522b\u3001\u5408\u6210\u3001\u7406\u89e3\u3001\u5bf9\u8bdd\u548c\u591a\u6a21\u6001\uff09\u4ee5\u53ca\u7b80\u6d01\u6613\u7528\u7684\u7279\u70b9\u3002WEST \u63d0\u4f9b\u4e86\u4e24\u79cd\u7c7b\u578b\u7684\u5b9e\u9a8c\u65b9\u6848\uff1a\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u5f00\u6e90\u6a21\u578b\u548c\u6570\u636e\uff0c\u53ef\u590d\u73b0\u5b9e\u9a8c\uff1b\u53e6\u4e00\u79cd\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u4f18\u8d8a\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u3002WEST \u5df2\u5728 https://github.com/wenet-e2e/west/ \u516c\u5f00\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u5de5\u5177\u5305\u5728\u6574\u5408 LLM \u7684\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u652f\u6301\u7aef\u5230\u7aef\u7684\u8bed\u97f3\u7406\u89e3\u3001\u751f\u6210\u548c\u4ea4\u4e92\u3002WEST \u65e8\u5728\u901a\u8fc7\u5229\u7528 LLM \u7684\u5f3a\u5927\u80fd\u529b\uff0c\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u3001\u6613\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8bed\u97f3\u5de5\u5177\u5305\uff0c\u4ee5\u63a8\u52a8\u8bed\u97f3\u6280\u672f\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002", "method": "WEST \u5de5\u5177\u5305\u91c7\u7528\u5168 LLM \u67b6\u6784\uff0c\u590d\u7528\u6210\u719f\u7684\u5927\u6a21\u578b\u67b6\u6784\u3001\u751f\u6001\u7cfb\u7edf\uff08\u5982 Hugging Face\uff09\u548c\u65b9\u6cd5\uff08\u5982\u5e8f\u5217\u6253\u5305\uff09\u3002\u5b83\u652f\u6301\u5305\u62ec\u8bed\u97f3\u8bc6\u522b\u3001\u8bed\u97f3\u5408\u6210\u3001\u8bed\u97f3\u7406\u89e3\u3001\u5bf9\u8bdd\u7ba1\u7406\u548c\u591a\u6a21\u6001\u80fd\u529b\u5728\u5185\u7684\u5168\u6808\u4efb\u52a1\uff0c\u5e76\u6613\u4e8e\u6269\u5c55\u4ee5\u96c6\u6210\u5f00\u6e90\u6a21\u578b\u3002\u5de5\u5177\u5305\u63d0\u4f9b\u4e86\u4e24\u79cd\u5b9e\u9a8c\u65b9\u6848\uff1a\u4e00\u79cd\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\u548c\u6570\u636e\uff0c\u786e\u4fdd\u5b9e\u9a8c\u7684\u53ef\u590d\u73b0\u6027\uff1b\u53e6\u4e00\u79cd\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4ee5\u83b7\u5f97\u5353\u8d8a\u7684\u6027\u80fd\u3002", "result": "WEST \u5de5\u5177\u5305\u5728\u5404\u79cd\u8bed\u97f3\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u548c\u5f00\u7bb1\u5373\u7528\u7684\u9ad8\u6027\u80fd\u6a21\u578b\u3002\u901a\u8fc7\u590d\u7528 LLM \u7684\u80fd\u529b\uff0cWEST \u7b80\u5316\u4e86\u8bed\u97f3\u5de5\u5177\u5305\u7684\u5f00\u53d1\u548c\u4f7f\u7528\uff0c\u5e76\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002", "conclusion": "WEST (WE Speech Toolkit) \u662f\u4e00\u4e2a\u521b\u65b0\u7684\u3001\u57fa\u4e8e LLM \u7684\u8bed\u97f3\u5de5\u5177\u5305\uff0c\u5b83\u901a\u8fc7\u63d0\u4f9b\u5168\u6808\u80fd\u529b\u3001\u6613\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bed\u97f3\u5de5\u5177\u5305\u7684\u5c40\u9650\u6027\u3002 WEST \u4e3a\u8bed\u97f3\u6280\u672f\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5e73\u53f0\uff0c\u6709\u671b\u63a8\u52a8\u8bed\u97f3\u4ea4\u4e92\u548c\u7406\u89e3\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u96c6\u4e2d\u5728\u8fdb\u4e00\u6b65\u6269\u5c55\u5176\u591a\u6a21\u6001\u80fd\u529b\u548c\u4f18\u5316\u6a21\u578b\u4ee5\u9002\u5e94\u66f4\u591a\u7279\u5b9a\u573a\u666f\u3002"}}
{"id": "2509.21320", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21320", "abs": "https://arxiv.org/abs/2509.21320", "authors": ["Yizhou Wang", "Chen Tang", "Han Deng", "Jiabei Xiao", "Jiaqi Liu", "Jianyu Wu", "Jun Yao", "Pengze Li", "Encheng Su", "Lintao Wang", "Guohang Zhuang", "Yuchen Ren", "Ben Fei", "Ming Hu", "Xin Chen", "Dongzhan Zhou", "Junjun He", "Xiangyu Yue", "Zhenfei Yin", "Jiamin Wu", "Qihao Zheng", "Yuhao Zhou", "Huihui Xu", "Chenglong Ma", "Yan Lu", "Wenlong Zhang", "Chunfeng Song", "Philip Torr", "Shixiang Tang", "Xinzhu Ma", "Wanli Ouyang", "Lei Bai"], "title": "SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines", "comment": "technical report", "summary": "We present a scientific reasoning foundation model that aligns natural\nlanguage with heterogeneous scientific representations. The model is pretrained\non a 206B-token corpus spanning scientific text, pure sequences, and\nsequence-text pairs, then aligned via SFT on 40M instructions, annealed\ncold-start bootstrapping to elicit long-form chain-of-thought, and\nreinforcement learning with task-specific reward shaping, which instills\ndeliberate scientific reasoning. It supports four capability families, covering\nup to 103 tasks across workflows: (i) faithful translation between text and\nscientific formats, (ii) text/knowledge extraction, (iii) property prediction,\n(iv) property classification, (v) unconditional and conditional sequence\ngeneration and design. Compared with specialist systems, our approach broadens\ninstruction coverage, improves cross-domain generalization, and enhances\nfidelity. We detail data curation and training and show that cross-discipline\nlearning strengthens transfer and downstream reliability. The model, instruct\ntuning datasets and the evaluation code are open-sourced at\nhttps://huggingface.co/SciReason and\nhttps://github.com/open-sciencelab/SciReason.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a SciReason \u7684\u79d1\u5b66\u63a8\u7406\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u4e0e\u5f02\u6784\u79d1\u5b66\u8868\u5f81\u5bf9\u9f50\u3002\u6a21\u578b\u5728\u5305\u542b\u79d1\u5b66\u6587\u672c\u3001\u7eaf\u5e8f\u5217\u548c\u5e8f\u5217-\u6587\u672c\u5bf9\u7684 206B \u6807\u8bb0\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u901a\u8fc7 40M \u6307\u4ee4\u7684 SFT\u3001\u51b7\u542f\u52a8\u5f15\u5bfc\u4ee5\u5f15\u51fa\u957f\u7bc7\u601d\u7ef4\u94fe\uff0c\u4ee5\u53ca\u5e26\u6709\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u5851\u9020\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5bf9\u9f50\uff0c\u4ece\u800c\u57f9\u517b\u4e86\u4e25\u8c28\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u3002\u8be5\u6a21\u578b\u652f\u6301\u56db\u79cd\u80fd\u529b\u5bb6\u65cf\uff0c\u6db5\u76d6\u5de5\u4f5c\u6d41\u4e2d\u7684\u591a\u8fbe 103 \u9879\u4efb\u52a1\uff0c\u5305\u62ec\u6587\u672c\u4e0e\u79d1\u5b66\u683c\u5f0f\u4e4b\u95f4\u7684\u5fe0\u5b9e\u7ffb\u8bd1\u3001\u6587\u672c/\u77e5\u8bc6\u63d0\u53d6\u3001\u5c5e\u6027\u9884\u6d4b\u3001\u5c5e\u6027\u5206\u7c7b\uff0c\u4ee5\u53ca\u65e0\u6761\u4ef6\u548c\u6709\u6761\u4ef6\u5e8f\u5217\u751f\u6210\u4e0e\u8bbe\u8ba1\u3002\u4e0e\u73b0\u6709\u4e13\u5bb6\u7cfb\u7edf\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u62d3\u5bbd\u4e86\u6307\u4ee4\u8986\u76d6\u8303\u56f4\uff0c\u6539\u5584\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u9ad8\u4e86\u4fdd\u771f\u5ea6\u3002\u7814\u7a76\u4e2d\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u6570\u636e\u6574\u7406\u548c\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u8bc1\u660e\u4e86\u8de8\u5b66\u79d1\u5b66\u4e60\u80fd\u591f\u589e\u5f3a\u8fc1\u79fb\u548c\u4e0b\u6e38\u53ef\u9760\u6027\u3002\u6700\u540e\uff0c\u7814\u7a76\u5c06\u6a21\u578b\u3001\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u4ee3\u7801\u5f00\u6e90\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u4e0e\u5f02\u6784\u79d1\u5b66\u8868\u5f81\uff08\u5982\u56fe\u8868\u3001\u65b9\u7a0b\u5f0f\u7b49\uff09\u4e4b\u95f4\u7684\u9e3f\u6c9f\u963b\u788d\u4e86\u79d1\u5b66\u77e5\u8bc6\u7684\u6574\u5408\u4e0e\u5229\u7528\u3002\u73b0\u6709\u7684\u6a21\u578b\u901a\u5e38\u4e13\u6ce8\u4e8e\u7279\u5b9a\u9886\u57df\u6216\u7279\u5b9a\u4efb\u52a1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3001\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u548c\u4e25\u8c28\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u591a\u79cd\u79d1\u5b66\u8868\u5f81\uff0c\u5e76\u8fdb\u884c\u590d\u6742\u79d1\u5b66\u63a8\u7406\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u4fc3\u8fdb\u79d1\u5b66\u53d1\u73b0\u548c\u77e5\u8bc6\u4f20\u64ad\u3002", "method": "1. **\u9884\u8bad\u7ec3\uff1a** \u5728\u5305\u542b\u79d1\u5b66\u6587\u672c\u3001\u7eaf\u5e8f\u5217\uff08\u5982\u57fa\u56e0\u7ec4\u6570\u636e\uff09\u548c\u5e8f\u5217-\u6587\u672c\u5bf9\uff08\u5982\u86cb\u767d\u8d28\u5e8f\u5217\u53ca\u5176\u63cf\u8ff0\uff09\u7684 206B \u6807\u8bb0\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u5efa\u7acb\u5bf9\u79d1\u5b66\u6570\u636e\u7684\u5e7f\u6cdb\u7406\u89e3\u3002\n2. **\u6307\u4ee4\u5fae\u8c03 (SFT)\uff1a** \u4f7f\u7528 4000 \u4e07\u6761\u6307\u4ee4\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u4f7f\u5176\u80fd\u591f\u9075\u5faa\u4eba\u7c7b\u6307\u4ee4\u5e76\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u3002\n3. **\u51b7\u542f\u52a8\u5f15\u5bfc (Annealed Cold-Start Bootstrapping)\uff1a** \u91c7\u7528\u4e00\u79cd\u7279\u6b8a\u7684\u5f15\u5bfc\u6280\u672f\uff0c\u65e8\u5728\u6fc0\u53d1\u6a21\u578b\u751f\u6210\u957f\u7bc7\u3001\u8fde\u8d2f\u7684\u201c\u601d\u7ef4\u94fe\u201d\u63a8\u7406\u8fc7\u7a0b\uff0c\u6a21\u4eff\u4eba\u7c7b\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u6b65\u9aa4\u3002\n4. **\u5f3a\u5316\u5b66\u4e60 (RL)\uff1a** \u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u7684\u5956\u52b1\u5851\u9020\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4ee5\u4f18\u5316\u5176\u5728\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u5176\u80fd\u591f\u8fdb\u884c\u66f4\u6df1\u601d\u719f\u8651\u7684\u63a8\u7406\u3002\n5. **\u80fd\u529b\u8bbe\u8ba1\uff1a** \u6a21\u578b\u88ab\u8bbe\u8ba1\u652f\u6301\u56db\u79cd\u6838\u5fc3\u80fd\u529b\u5bb6\u65cf\uff0c\u6db5\u76d6\u4e86\u4ece\u6570\u636e\u8f6c\u6362\u5230\u9884\u6d4b\u548c\u751f\u6210\u7684\u5e7f\u6cdb\u4efb\u52a1\u3002\n6. **\u6570\u636e\u4e0e\u8bc4\u4f30\uff1a** \u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u548c\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u7684\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u4ee3\u7801\u4ee5\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "result": "\u4e0e\u73b0\u6709\u7684\u4e13\u4e1a\u7cfb\u7edf\u76f8\u6bd4\uff0cSciReason \u6a21\u578b\u5728\u4ee5\u4e0b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u52bf\uff1a\n1. **\u62d3\u5bbd\u6307\u4ee4\u8986\u76d6\u8303\u56f4\uff1a** \u80fd\u591f\u5904\u7406\u6bd4\u4e13\u4e1a\u7cfb\u7edf\u66f4\u5e7f\u6cdb\u7684\u6307\u4ee4\u7c7b\u578b\u3002\n2. **\u6539\u5584\u8de8\u9886\u57df\u6cdb\u5316\uff1a** \u5728\u4e0d\u540c\u79d1\u5b66\u9886\u57df\u4e4b\u95f4\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\n3. **\u589e\u5f3a\u4fdd\u771f\u5ea6\uff1a** \u5728\u6267\u884c\u4efb\u52a1\u65f6\u80fd\u591f\u4fdd\u6301\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u5fe0\u5b9e\u5ea6\u3002\n\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660e\u8de8\u5b66\u79d1\u5b66\u4e60\uff08\u5373\u5728\u5305\u542b\u591a\u79cd\u5b66\u79d1\u6570\u636e\u7684\u8bed\u6599\u5e93\u4e0a\u8bad\u7ec3\uff09\u80fd\u591f\u663e\u8457\u589e\u5f3a\u6a21\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u5e76\u5f00\u6e90\u4e86\u4e00\u4e2a\u540d\u4e3a SciReason \u7684\u79d1\u5b66\u63a8\u7406\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u80fd\u591f\u6709\u6548\u5730\u5bf9\u9f50\u81ea\u7136\u8bed\u8a00\u548c\u5f02\u6784\u79d1\u5b66\u8868\u5f81\uff0c\u5e76\u8fdb\u884c\u4e25\u8c28\u7684\u79d1\u5b66\u63a8\u7406\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u5fae\u8c03\u3001\u51b7\u542f\u52a8\u5f15\u5bfc\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u5148\u8fdb\u6280\u672f\u5b9e\u73b0\uff0c\u5728\u591a\u9879\u79d1\u5b66\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u4e13\u5bb6\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6307\u4ee4\u8986\u76d6\u3001\u8de8\u9886\u57df\u6cdb\u5316\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u8de8\u5b66\u79d1\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u5b83\u80fd\u63d0\u5347\u6a21\u578b\u7684\u8fc1\u79fb\u80fd\u529b\u548c\u53ef\u9760\u6027\u3002SciReason \u7684\u5f00\u6e90\u5c06\u6781\u5927\u5730\u63a8\u52a8\u79d1\u5b66\u7814\u7a76\u548c\u5e94\u7528\u7684\u53d1\u5c55\uff0c\u4f46\u672a\u6765\u4ecd\u9700\u63a2\u7d22\u5176\u5728\u5904\u7406\u66f4\u590d\u6742\u3001\u66f4\u957f\u63a8\u7406\u94fe\u4ee5\u53ca\u5728\u66f4\u591a\u65b0\u5174\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.04655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04655", "abs": "https://arxiv.org/abs/2510.04655", "authors": ["Yuheng Li", "Jiechao Gao", "Wei Han", "Wenwen Ouyang", "Wei Zhu", "Hui Yi Leong"], "title": "FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method", "comment": "Accepted by EMNLP-2025", "summary": "Knowledge of the medical decision process, which can be modeled as medical\ndecision trees (MDTs), is critical to building clinical decision support\nsystems. However, current MDT construction methods rely heavily on\ntime-consuming and laborious manual annotation. To address this challenge, we\npropose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for\nautomatically extracting MDTs from clinical guidelines and textbooks. We\nintegrate gradient path information to capture synergistic effects between\ndifferent modules, enabling more effective and reliable rank allocation. This\nframework ensures that the most critical modules receive appropriate rank\nallocations while less important ones are pruned, resulting in a more efficient\nand accurate model for extracting medical decision trees from clinical texts.\nExtensive experiments on medical guideline datasets demonstrate that our\nPI-LoRA method significantly outperforms existing parameter-efficient\nfine-tuning approaches for the Text2MDT task, achieving better accuracy with\nsubstantially reduced model complexity. The proposed method achieves\nstate-of-the-art results while maintaining a lightweight architecture, making\nit particularly suitable for clinical decision support systems where\ncomputational resources may be limited.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPI-LoRA\u7684\u65b0\u578b\u4f4e\u79e9\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4e34\u5e8a\u6307\u5357\u548c\u6559\u79d1\u4e66\u4e2d\u81ea\u52a8\u63d0\u53d6\u533b\u7597\u51b3\u7b56\u6811\uff08MDT\uff09\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u6807\u6ce8\u8017\u65f6\u8017\u529b\u7684\u95ee\u9898\u3002PI-LoRA\u901a\u8fc7\u6574\u5408\u68af\u5ea6\u8def\u5f84\u4fe1\u606f\u6765\u4f18\u5316\u6a21\u5757\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u80fd\u591f\u4e3a\u5173\u952e\u6a21\u5757\u5206\u914d\u66f4\u91cd\u8981\u7684\u79e9\uff0c\u5e76\u4fee\u526a\u4e0d\u91cd\u8981\u7684\u6a21\u5757\uff0c\u4ece\u800c\u63d0\u9ad8MDT\u63d0\u53d6\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPI-LoRA\u5728Text2MDT\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u5316\u67b6\u6784\u7684\u540c\u65f6\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u6784\u5efa\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u6240\u4f9d\u8d56\u7684\u533b\u7597\u51b3\u7b56\u6811\uff08MDT\uff09\u7684\u6784\u5efa\u65b9\u6cd5\uff0c\u4e25\u91cd\u4f9d\u8d56\u4e8e\u8017\u65f6\u4e14\u8d39\u529b\u7684\u624b\u52a8\u6807\u6ce8\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u4e14\u96be\u4ee5\u6269\u5c55\u4ee5\u9002\u5e94\u5927\u89c4\u6a21\u7684\u4e34\u5e8a\u6570\u636e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u4ece\u4e34\u5e8a\u6307\u5357\u548c\u6559\u79d1\u4e66\u4e2d\u9ad8\u6548\u3001\u51c6\u786e\u5730\u63d0\u53d6MDT\uff0c\u4ee5\u514b\u670d\u624b\u52a8\u6807\u6ce8\u7684\u74f6\u9888\uff0c\u5e76\u63a8\u52a8\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPI-LoRA\uff08Path-Integrated LoRA\uff09\u7684\u65b0\u578b\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u5728\u4e8e\u6574\u5408\u68af\u5ea6\u8def\u5f84\u4fe1\u606f\uff0c\u4ee5\u6355\u6349\u4e0d\u540c\u6a21\u5757\u95f4\u7684\u534f\u540c\u6548\u5e94\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0cPI-LoRA\u80fd\u591f\u66f4\u6709\u6548\u5730\u8fdb\u884c\u79e9\u5206\u914d\uff0c\u786e\u4fdd\u5173\u952e\u6a21\u5757\u83b7\u5f97\u6070\u5f53\u7684\u79e9\uff0c\u540c\u65f6\u4fee\u526a\u4e0d\u91cd\u8981\u7684\u6a21\u5757\u3002\u8fd9\u79cd\u9009\u62e9\u6027\u7684\u79e9\u5206\u914d\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u66f4\u805a\u7126\u4e8e\u63d0\u53d6MDT\u4e2d\u7684\u5173\u952e\u51b3\u7b56\u8def\u5f84\uff0c\u4ece\u800c\u63d0\u9ad8\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u7814\u7a76\u5728\u4e34\u5e8a\u6307\u5357\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5e76\u5c06PI-LoRA\u4e0e\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728Text2MDT\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u5728Text2MDT\u4efb\u52a1\u4e0a\uff0cPI-LoRA\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u578b\u590d\u6742\u5ea6\u4e0a\u5f97\u5230\u4e86\u5927\u5e45\u5ea6\u964d\u4f4e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u51c6\u786e\u7684\u6a21\u578b\u3002\u7814\u7a76\u8868\u660e\uff0cPI-LoRA\u5728\u4fdd\u6301\u8f7b\u91cf\u5316\u67b6\u6784\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u8fd9\u4f7f\u5176\u7279\u522b\u9002\u5408\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002", "conclusion": "PI-LoRA\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u4f4e\u79e9\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u4ece\u4e34\u5e8a\u6587\u672c\u4e2d\u81ea\u52a8\u63d0\u53d6\u533b\u7597\u51b3\u7b56\u6811\uff08MDT\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u624b\u52a8\u6807\u6ce8\u7684\u75db\u70b9\u3002\u901a\u8fc7\u6574\u5408\u68af\u5ea6\u8def\u5f84\u4fe1\u606f\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4f18\u5316\u79e9\u5206\u914d\uff0c\u63d0\u9ad8\u6a21\u578b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cPI-LoRA\u5728Text2MDT\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6a21\u578b\u8f7b\u91cf\u5316\uff0c\u975e\u5e38\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u5c06PI-LoRA\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u4e34\u5e8a\u6587\u672c\u6570\u636e\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u5728\u590d\u6742MDT\u7ed3\u6784\u63d0\u53d6\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2510.09158", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.09158", "abs": "https://arxiv.org/abs/2510.09158", "authors": ["Seiya Ishikura", "Hiroaki Yamada", "Tatsuya Hiraoka", "Hiroaki Yamada", "Takenobu Tokunaga"], "title": "Augmenting Dialog with Think-Aloud Utterances for Modeling Individual Personality Traits by LLM", "comment": "8 pages, 1 figure. Accepted at the First Workshop on Tailoring AI:\n  Exploring Active and Passive LLM Personalization (PALS2025@EMNLP2025)", "summary": "This study proposes augmenting dialog data with think-aloud utterances (TAUs)\nfor modeling individual personalities in text chat by LLM. TAU is a\nverbalization of a speaker's thought before articulating the utterance. We\nexpect \"persona LLMs\" trained with TAU-augmented data can mimic the speaker's\npersonality trait better. We tested whether the trained persona LLMs obtain the\nhuman personality with respect to Big Five, a framework characterizing human\npersonality traits from five aspects. The results showed that LLMs trained with\nTAU-augmented data more closely align to the speakers' Agreeableness and\nNeuroticism of Big Five than those trained with original dialog data. We also\nfound that the quality of TAU-augmentation impacts persona LLM's performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5bf9\u8bdd\u6570\u636e\u4e2d\u52a0\u5165\u201c\u51fa\u58f0\u601d\u8003\u201d\uff08TAU\uff09\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u4e2a\u4f53\u6027\u683c\u5efa\u6a21\u7684\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528TAU\u589e\u5f3a\u7684\u6570\u636e\u8bad\u7ec3\u7684\u201c\u4e2a\u6027\u5316LLM\u201d\u5728\u201c\u5b9c\u4eba\u6027\u201d\u548c\u201c\u795e\u7ecf\u8d28\u201d\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u6bd4\u4ec5\u4f7f\u7528\u539f\u59cb\u5bf9\u8bdd\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u66f4\u80fd\u6a21\u4eff\u4eba\u7c7b\u7684\u6027\u683c\u7279\u5f81\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0cTAU\u589e\u5f3a\u7684\u8d28\u91cf\u4f1a\u5f71\u54cd\u4e2a\u6027\u5316LLM\u7684\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\uff0c\u5728\u6587\u672c\u804a\u5929\u4e2d\u5bf9\u4e2a\u4f53\u6027\u683c\u8fdb\u884c\u5efa\u6a21\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u8fd9\u4e00\u9886\u57df\uff0c\u5373\u5229\u7528\u201c\u51fa\u58f0\u601d\u8003\u201d\uff08TAU\uff09\u7684\u5bf9\u8bdd\u6570\u636e\u6765\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4ee5\u671f\u66f4\u597d\u5730\u6355\u6349\u548c\u6a21\u4eff\u4eba\u7c7b\u7684\u6027\u683c\u7279\u5f81\u3002\u8fd9\u5bf9\u4e8e\u5f00\u53d1\u66f4\u5177\u4e2a\u6027\u5316\u548c\u4eba\u6027\u5316\u7684AI\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u5373\u5728\u73b0\u6709\u7684\u5bf9\u8bdd\u6570\u636e\u4e2d\u52a0\u5165\u201c\u51fa\u58f0\u601d\u8003\u201d\uff08TAU\uff09\u7684\u8bed\u53e5\u3002TAU\u662f\u6307\u5728\u8bf4\u51fa\u5177\u4f53\u7684\u8bdd\u4e4b\u524d\uff0c\u5c06\u8bf4\u8bdd\u8005\u5185\u5fc3\u7684\u601d\u8003\u8fc7\u7a0b\u53e3\u5934\u8868\u8fbe\u51fa\u6765\u3002\u7136\u540e\uff0c\u4f7f\u7528\u8fd9\u79cdTAU\u589e\u5f3a\u7684\u5bf9\u8bdd\u6570\u636e\u6765\u8bad\u7ec3\u201c\u4e2a\u6027\u5316LLM\u201d\u3002\u5b9e\u9a8c\u901a\u8fc7\u6bd4\u8f83\u4f7f\u7528TAU\u589e\u5f3a\u6570\u636e\u548c\u539f\u59cb\u5bf9\u8bdd\u6570\u636e\u8bad\u7ec3\u7684LLM\u5728\u6a21\u4eff\u4eba\u7c7b\u201c\u5927\u4e94\u4eba\u683c\u201d\uff08Big Five\uff09\u7279\u5f81\uff08\u5305\u62ec\u5f00\u653e\u6027\u3001\u5c3d\u8d23\u6027\u3001\u5916\u503e\u6027\u3001\u5b9c\u4eba\u6027\u3001\u795e\u7ecf\u8d28\uff09\u65b9\u9762\u7684\u8868\u73b0\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528TAU\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684LLM\u5728\u201c\u5b9c\u4eba\u6027\u201d\uff08Agreeableness\uff09\u548c\u201c\u795e\u7ecf\u8d28\u201d\uff08Neuroticism\uff09\u8fd9\u4e24\u4e2a\u6027\u683c\u7ef4\u5ea6\u4e0a\uff0c\u6bd4\u4f7f\u7528\u539f\u59cb\u5bf9\u8bdd\u6570\u636e\u8bad\u7ec3\u7684LLM\u66f4\u80fd\u51c6\u786e\u5730\u6a21\u4eff\u4eba\u7c7b\u7684\u6027\u683c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\uff0cTAU\u589e\u5f3a\u6570\u636e\u7684\u8d28\u91cf\u5bf9\u4e2a\u6027\u5316LLM\u7684\u6027\u80fd\u6709\u7740\u663e\u8457\u5f71\u54cd\uff0c\u9ad8\u8d28\u91cf\u7684TAU\u80fd\u591f\u5e26\u6765\u66f4\u597d\u7684\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5c55\u793a\u4e86\u901a\u8fc7\u5728\u5bf9\u8bdd\u6570\u636e\u4e2d\u52a0\u5165\u201c\u51fa\u58f0\u601d\u8003\u201d\uff08TAU\uff09\u6765\u589e\u5f3aLLM\u5bf9\u4e2a\u4f53\u6027\u683c\u5efa\u6a21\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u201c\u5b9c\u4eba\u6027\u201d\u548c\u201c\u795e\u7ecf\u8d28\u201d\u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u3002\u8fd9\u4e3a\u5f00\u53d1\u66f4\u7cbe\u7ec6\u3001\u66f4\u5177\u4e2a\u6027\u5316\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u4e0d\u540c\u7c7b\u578b\u7684TAU\u6570\u636e\u4ee5\u53ca\u5982\u4f55\u66f4\u6709\u6548\u5730\u5229\u7528\u5b83\u4eec\u6765\u63d0\u5347\u6a21\u578b\u5728\u5176\u4ed6\u6027\u683c\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u7814\u7a76TAU\u589e\u5f3a\u8d28\u91cf\u7684\u5177\u4f53\u5f71\u54cd\u673a\u5236\u3002"}}
{"id": "2510.12993", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12993", "abs": "https://arxiv.org/abs/2510.12993", "authors": ["Jo\u00e3o A. Leite", "Arnav Arora", "Silvia Gargova", "Jo\u00e3o Luz", "Gustavo Sampaio", "Ian Roberts", "Carolina Scarton", "Kalina Bontcheva"], "title": "A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation", "comment": null, "summary": "Large Language Models (LLMs) can generate human-like disinformation, yet\ntheir ability to personalise such content across languages and demographics\nremains underexplored. This study presents the first large-scale, multilingual\nanalysis of persona-targeted disinformation generation by LLMs. Employing a red\nteaming methodology, we prompt eight state-of-the-art LLMs with 324 false\nnarratives and 150 demographic personas (combinations of country, generation,\nand political orientation) across four languages--English, Russian, Portuguese,\nand Hindi--resulting in AI-TRAITS, a comprehensive dataset of 1.6 million\npersonalised disinformation texts. Results show that the use of even simple\npersonalisation prompts significantly increases the likelihood of jailbreaks\nacross all studied LLMs, up to 10 percentage points, and alters linguistic and\nrhetorical patterns that enhance narrative persuasiveness. Models such as Grok\nand GPT exhibited jailbreak rates and personalisation scores both exceeding\n85%. These insights expose critical vulnerabilities in current state-of-the-art\nLLMs and offer a foundation for improving safety alignment and detection\nstrategies in multilingual and cross-demographic contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5927\u89c4\u6a21\u3001\u591a\u8bed\u8a00\u5730\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u4e2a\u6027\u5316\u865a\u5047\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u7b80\u5355\u7684\u4e2a\u6027\u5316\u63d0\u793a\u4e5f\u80fd\u663e\u8457\u63d0\u9ad8\u2018\u8d8a\u72f1\u2019\uff08\u7ed5\u8fc7\u5b89\u5168\u9650\u5236\uff09\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u6539\u53d8\u8bed\u8a00\u548c\u4fee\u8f9e\u6a21\u5f0f\u4ee5\u589e\u5f3a\u8bf4\u670d\u529b\u3002\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b160\u4e07\u6761\u4e2a\u6027\u5316\u865a\u5047\u4fe1\u606f\u6587\u672c\u7684\u6570\u636e\u96c6\uff08AI-TRAITS\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u591a\u8bed\u8a00\u548c\u8de8\u4eba\u7fa4\u80cc\u666f\u4e0b\u9762\u4e34\u7684\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u865a\u5047\u4fe1\u606f\uff0c\u4f46\u5176\u8de8\u8bed\u8a00\u548c\u8de8\u4eba\u7fa4\u8fdb\u884c\u4e2a\u6027\u5316\u5185\u5bb9\u751f\u6210\u7684\u80fd\u529b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u8ba8LLMs\u5728\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u4eba\u7fa4\u7684\u865a\u5047\u4fe1\u606f\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u548c\u5e94\u5bf9AI\u9a71\u52a8\u7684\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u81f3\u5173\u91cd\u8981\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b89\u5168\u548c\u793e\u4f1a\u610f\u4e49\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u2018\u7ea2\u961f\u6d4b\u8bd5\u2019\uff08red teaming\uff09\u65b9\u6cd5\uff0c\u9488\u5bf9\u516b\u79cd\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4f7f\u7528324\u4e2a\u865a\u5047\u53d9\u8ff0\u548c150\u4e2a\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\uff08\u7ed3\u5408\u4e86\u56fd\u5bb6\u3001\u4e16\u4ee3\u548c\u653f\u6cbb\u53d6\u5411\uff09\u7684\u7ec4\u5408\uff0c\u5728\u82f1\u8bed\u3001\u4fc4\u8bed\u3001\u8461\u8404\u7259\u8bed\u548c\u5370\u5730\u8bed\u56db\u79cd\u8bed\u8a00\u4e2d\u8fdb\u884c\u63d0\u793a\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u63a2\u6d4b\u6a21\u578b\u7684\u5b89\u5168\u8fb9\u754c\u548c\u5185\u5bb9\u751f\u6210\u80fd\u529b\u3002\u5b9e\u9a8c\u751f\u6210\u4e86\u4e00\u4e2a\u540d\u4e3aAI-TRAITS\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b160\u4e07\u6761\u4e2a\u6027\u5316\u865a\u5047\u4fe1\u606f\u6587\u672c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u4e2a\u6027\u5316\u63d0\u793a\u5c31\u80fd\u663e\u8457\u63d0\u9ad8\u6240\u6709\u88ab\u6d4bLLMs\u7684\u2018\u8d8a\u72f1\u2019\uff08\u7ed5\u8fc7\u5b89\u5168\u9650\u5236\uff09\u6210\u529f\u7387\uff0c\u6700\u9ad8\u53ef\u8fbe10\u4e2a\u767e\u5206\u70b9\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u63d0\u793a\u8fd8\u80fd\u6539\u53d8\u6587\u672c\u7684\u8bed\u8a00\u548c\u4fee\u8f9e\u6a21\u5f0f\uff0c\u4f7f\u5176\u66f4\u5177\u8bf4\u670d\u529b\u3002\u5728\u6a21\u578b\u8868\u73b0\u65b9\u9762\uff0cGrok\u548cGPT\u7b49\u6a21\u578b\u5728\u2018\u8d8a\u72f1\u2019\u7387\u548c\u4e2a\u6027\u5316\u5f97\u5206\u4e0a\u5747\u8d85\u8fc785%\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u5f53\u524d\u5148\u8fdbLLMs\u5728\u751f\u6210\u4e2a\u6027\u5316\u865a\u5047\u4fe1\u606f\u65b9\u9762\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u8de8\u8bed\u8a00\u3001\u8de8\u4eba\u7fa4\u7684\u4e2a\u6027\u5316\u865a\u5047\u4fe1\u606f\u65b9\u9762\u7684\u663e\u8457\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08AI-TRAITS\uff09\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4e2a\u6027\u5316\u63d0\u793a\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u6a21\u578b\u7684\u2018\u8d8a\u72f1\u2019\u7387\u5e76\u589e\u5f3a\u865a\u5047\u4fe1\u606f\u7684\u8bf4\u670d\u529b\u3002\u8fd9\u4e9b\u7ed3\u679c\u66b4\u9732\u4e86\u5f53\u524dLLMs\u5728\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5b89\u5168\u9632\u62a4\u548c\u591a\u8bed\u8a00\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u7136\u800c\uff0c\u7814\u7a76\u4e5f\u53ef\u80fd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u6a21\u578b\u548c\u63d0\u793a\u7684\u8986\u76d6\u8303\u56f4\uff0c\u672a\u6765\u53ef\u5728\u6b64\u57fa\u7840\u4e0a\u6269\u5c55\u6a21\u578b\u548c\u63d0\u793a\u7684\u591a\u6837\u6027\uff0c\u5e76\u7814\u7a76\u66f4\u590d\u6742\u7684\u4e2a\u6027\u5316\u7b56\u7565\u3002"}}
{"id": "2510.14040", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14040", "abs": "https://arxiv.org/abs/2510.14040", "authors": ["George Flint", "Kaustubh Kislay"], "title": "Quantifying Phonosemantic Iconicity Distributionally in 6 Languages", "comment": "IJCNLP-AACL 2025 Main Conference Proceedings", "summary": "Language is, as commonly theorized, largely arbitrary. Yet, systematic\nrelationships between phonetics and semantics have been observed in many\nspecific cases. To what degree could those systematic relationships manifest\nthemselves in large scale, quantitative investigations--both in previously\nidentified and unidentified phenomena? This work undertakes a distributional\napproach to quantifying phonosemantic iconicity at scale across 6 diverse\nlanguages (English, Spanish, Hindi, Finnish, Turkish, and Tamil). In each\nlanguage, we analyze the alignment of morphemes' phonetic and semantic\nsimilarity spaces with a suite of statistical measures, and discover an array\nof interpretable phonosemantic alignments not previously identified in the\nliterature, along with crosslinguistic patterns. We also analyze 5 previously\nhypothesized phonosemantic alignments, finding support for some such alignments\nand mixed results for others.", "AI": {"tldr": "\u672c\u7814\u7a76\u91c7\u7528\u5206\u5e03\u65b9\u6cd5\uff0c\u5728\u516d\u79cd\u4e0d\u540c\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5370\u5730\u8bed\u3001\u82ac\u5170\u8bed\u3001\u571f\u8033\u5176\u8bed\u548c\u6cf0\u7c73\u5c14\u8bed\uff09\u4e2d\u5927\u89c4\u6a21\u91cf\u5316\u8bed\u97f3\u8bed\u4e49 the iconicity\u3002\u901a\u8fc7\u5206\u6790\u8bcd\u7f00\u7684\u8bed\u97f3\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u7a7a\u95f4\uff0c\u53d1\u73b0\u4e86\u65b0\u7684\u53ef\u89e3\u91ca\u7684\u8bed\u97f3\u8bed\u4e49 the iconicity \u53ca\u5176\u8de8\u8bed\u8a00\u6a21\u5f0f\uff0c\u5e76\u5bf9\u5148\u524d\u5047\u8bbe\u7684 the iconicity \u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u8a00\u901a\u5e38\u88ab\u8ba4\u4e3a\u662f\u4efb\u610f\u7684\uff0c\u4f46\u8bed\u97f3\u548c\u8bed\u4e49\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u5173\u7cfb\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8fd9\u79cd\u8bed\u97f3\u8bed\u4e49 the iconicity \u5728\u5927\u89c4\u6a21\u3001\u5b9a\u91cf\u5c42\u9762\u4e0a\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u53d1\u73b0\u65b0\u7684\u8bed\u97f3\u8bed\u4e49 the iconicity \u73b0\u8c61\u53ca\u5176\u8de8\u8bed\u8a00\u6a21\u5f0f\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u5206\u5e03\u65b9\u6cd5\uff0c\u5728\u516d\u79cd\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5370\u5730\u8bed\u3001\u82ac\u5170\u8bed\u3001\u571f\u8033\u5176\u8bed\u548c\u6cf0\u7c73\u5c14\u8bed\uff09\u4e2d\u5206\u6790\u8bcd\u7f00\u7684\u8bed\u97f3\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u7a7a\u95f4\u3002\u5229\u7528\u4e00\u7cfb\u5217\u7edf\u8ba1\u6d4b\u91cf\u65b9\u6cd5\u6765\u91cf\u5316\u8bed\u97f3\u8bed\u4e49 the iconicity\uff0c\u5e76\u8bc6\u522b\u51fa\u65b0\u7684 the iconicity \u73b0\u8c61\u4ee5\u53ca\u8de8\u8bed\u8a00\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u5927\u91cf\u53ef\u89e3\u91ca\u7684\u8bed\u97f3\u8bed\u4e49 the iconicity\uff0c\u5176\u4e2d\u8bb8\u591a\u662f\u5148\u524d\u672a\u88ab\u8bc6\u522b\u7684\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u4e86\u8de8\u8bed\u8a00\u7684 the iconicity \u6a21\u5f0f\u3002\u5bf9\u4e8e\u5148\u524d\u5047\u8bbe\u7684\u4e94\u79cd\u8bed\u97f3\u8bed\u4e49 the iconicity\uff0c\u90e8\u5206\u5f97\u5230\u4e86\u652f\u6301\uff0c\u90e8\u5206\u7ed3\u679c\u4e0d\u786e\u5b9a\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u7684\u5206\u5e03\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u8bed\u97f3\u8bed\u4e49 the iconicity \u5728\u591a\u79cd\u8bed\u8a00\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5e76\u53d1\u73b0\u4e86\u65b0\u7684 the iconicity \u73b0\u8c61\u548c\u8de8\u8bed\u8a00\u6a21\u5f0f\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3\u8bed\u8a00\u7684\u4efb\u610f\u6027\u4e0e\u7cfb\u7edf\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u4f46\u4e5f\u6307\u51fa\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u5b8c\u5168\u9610\u660e\u8fd9\u4e9b the iconicity \u7684\u673a\u5236\u548c\u8303\u56f4\u3002"}}
{"id": "2510.23252", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23252", "abs": "https://arxiv.org/abs/2510.23252", "authors": ["Tawsif Tashwar Dipto", "Azmol Hossain", "Rubayet Sabbir Faruque", "Md. Rezuwan Hassan", "Kanij Fatema", "Tanmoy Shome", "Ruwad Naswan", "Md. Foriduzzaman Zihad", "Mohaymen Ul Anam", "Nazia Tasnim", "Hasan Mahmud", "Md Kamrul Hasan", "Md. Mehedi Hasan Shawon", "Farig Sadeque", "Tahsin Reasat"], "title": "Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?", "comment": "The manuscript has to be withdrawn to address an authorship and\n  intellectual property clarification", "summary": "Conventional research on speech recognition modeling relies on the canonical\nform for most low-resource languages while automatic speech recognition (ASR)\nfor regional dialects is treated as a fine-tuning task. To investigate the\neffects of dialectal variations on ASR we develop a 78-hour annotated Bengali\nSpeech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and\ndata-driven perspectives shows that speech foundation models struggle heavily\nin regional dialect ASR, both in zero-shot and fine-tuned settings. We observe\nthat all deep learning methods struggle to model speech data under dialectal\nvariations but dialect specific model training alleviates the issue. Our\ndataset also serves as a out of-distribution (OOD) resource for ASR modeling\nunder constrained resources in ASR algorithms. The dataset and code developed\nfor this project are publicly available", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6807\u51c6\u5f62\u5f0f\u4e0a\u8fdb\u884c\u8bed\u97f3\u8bc6\u522b\u5efa\u6a21\uff0c\u5e76\u5c06\u65b9\u8a00\u4f5c\u4e3a\u5fae\u8c03\u4efb\u52a1\uff0c\u4f1a\u5bf9\u533a\u57df\u65b9\u8a00\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b78\u5c0f\u65f6\u6570\u636e\u7684\u5b5f\u52a0\u62c9\u8bed\u8bed\u97f3\u8f6c\u6587\u672c\uff08STT\uff09\u8bed\u6599\u5e93Ben-10\uff0c\u5e76\u901a\u8fc7\u8bed\u8a00\u5b66\u548c\u6570\u636e\u9a71\u52a8\u7684\u89c6\u89d2\u8fdb\u884c\u4e86\u5206\u6790\u3002\u7ed3\u679c\u53d1\u73b0\uff0c\u73b0\u6709\u7684\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u90fd\u96be\u4ee5\u5904\u7406\u533a\u57df\u65b9\u8a00\u7684ASR\u4efb\u52a1\uff0c\u800c\u9488\u5bf9\u7279\u5b9a\u65b9\u8a00\u7684\u6a21\u578b\u8bad\u7ec3\u80fd\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002\u8be5\u8bed\u6599\u5e93\u8fd8\u53ef\u4f5c\u4e3a\u8d44\u6e90\u53d7\u9650\u60c5\u51b5\u4e0b\u7684\u975e\u5206\u5e03\uff08OOD\uff09\u8d44\u6e90\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6807\u51c6\u5f62\u5f0f\uff0c\u5e76\u5c06\u65b9\u8a00\u89c6\u4e3a\u4e00\u4e2a\u5fae\u8c03\u4efb\u52a1\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u89c6\u4e86\u65b9\u8a00\u7684\u72ec\u7279\u6027\uff0c\u5bfc\u81f4\u5728\u5904\u7406\u533a\u57df\u65b9\u8a00\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u65b9\u8a00\u53d8\u5f02\u5bf9ASR\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b78\u5c0f\u65f6\u6570\u636e\u7684\u5b5f\u52a0\u62c9\u8bed\u8bed\u97f3\u8f6c\u6587\u672c\uff08STT\uff09\u8bed\u6599\u5e93Ben-10\u3002\u4ed6\u4eec\u4ece\u8bed\u8a00\u5b66\u548c\u6570\u636e\u9a71\u52a8\u7684\u89d2\u5ea6\u5206\u6790\u4e86\u8be5\u8bed\u6599\u5e93\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u7814\u7a76\u4e86\u9488\u5bf9\u7279\u5b9a\u65b9\u8a00\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u7684\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5728\u5904\u7406\u533a\u57df\u65b9\u8a00ASR\u4efb\u52a1\u65f6\u5b58\u5728\u663e\u8457\u56f0\u96be\uff0c\u65e0\u8bba\u662f\u5728\u96f6\u6837\u672c\u8fd8\u662f\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u3002\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5efa\u6a21\u65b9\u8a00\u53d8\u5f02\u7684\u8bed\u97f3\u6570\u636e\u65f6\u90fd\u9047\u5230\u6311\u6218\u3002\u7136\u800c\uff0c\u901a\u8fc7\u8fdb\u884c\u7279\u5b9a\u65b9\u8a00\u7684\u6a21\u578b\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002Ben-10\u8bed\u6599\u5e93\u4e5f\u53ef\u4f5c\u4e3aASR\u7b97\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u60c5\u51b5\u4e0b\u7684\u975e\u5206\u5e03\uff08OOD\uff09\u8d44\u6e90\u3002", "conclusion": "\u533a\u57df\u65b9\u8a00\u5bf9\u65b9\u8a00ASR\u6a21\u578b\u63d0\u51fa\u4e86\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u73b0\u6709\u7684\u901a\u7528\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u3002\u9488\u5bf9\u7279\u5b9a\u65b9\u8a00\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u662f\u63d0\u9ad8ASR\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\u3002Ben-10\u8bed\u6599\u5e93\u7684\u516c\u5f00\u4e3a\u672a\u6765ASR\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u63a2\u7d22\u66f4\u6709\u6548\u7684\u8de8\u65b9\u8a00\u5efa\u6a21\u6280\u672f\u4ee5\u53ca\u5229\u7528Ben-10\u8bed\u6599\u5e93\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u7814\u7a76\u3002"}}
