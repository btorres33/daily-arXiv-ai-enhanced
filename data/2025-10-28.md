<div id=toc></div>

# Table of Contents

- [cs.CE](#cs.CE) [Total: 8]
- [cs.AI](#cs.AI) [Total: 89]
- [cs.LG](#cs.LG) [Total: 339]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [cs.MA](#cs.MA) [Total: 9]


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [1] [Error bounded compression for weather and climate applications](https://arxiv.org/abs/2510.22265)
*Langwen Huang,Luigi Fusco,Florian Scheidl,Jan Zibell,Michael Armand Sprenger,Sebastian Schemm,Torsten Hoefler*

Main category: cs.CE

TL;DR: 天气和气候模拟数据量激增导致存储和应用受限。本文提出EBCC（Error Bounded Climate-data Compressor），一个两层压缩器，结合JPEG2000和小波变换+SPIHT编码，并通过反馈速率控制实现指定最大误差目标。EBCC在多个基准测试中表现优异，能将误差集中在零附近，实现15x至300x的压缩比，同时保持数据对下游应用的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着天气和气候模拟分辨率的提高，产生的数据量从数百TB迅速增长到数十PB。这种庞大的数据量已成为更广泛应用的一个限制因素，并且其快速增长率将很快耗尽所有可用的存储设备。因此，迫切需要一种高效的数据压缩方法来解决这些存储和可访问性问题，以促进天气和气候科学领域的发展和应用。

Method: 本文提出了EBCC（Error Bounded Climate-data Compressor），采用两层压缩方法：第一层是基于JPEG2000的基础压缩层，用于捕获数据主体并实现高压缩比；第二层是残差压缩层，利用小波变换和SPIHT编码来有效消除基础压缩层引入的长尾极端误差。EBCC集成了针对两层的反馈速率控制机制，该机制通过调整压缩比来达到指定的最大误差目标。

Result: EBCC与其他现有压缩方法在多个与天气和气候科学相关的基准测试中进行了评估，包括误差统计、飓风附近原始和导出变量的案例研究、全球能量预算闭合评估以及拉格朗日气团轨迹模拟。值得注意的是，这是首次使用轨迹模拟来基准测试压缩方法。结果显示，EBCC将大多数误差集中在零附近，而其他方法倾向于在误差范围内均匀分布误差。在0.1%到10%的相对误差目标下，EBCC在基准测试中表现优于其他方法，实现了15x到超过300x的压缩比。在能量预算闭合和拉格朗日轨迹基准测试中，EBCC可以在保持误差在ERA5不确定性成员导出的自然变异性范围内的同时，实现超过100x的压缩比。

Conclusion: EBCC在天气和气候数据压缩方面表现出显著的有效性，能够生成高度压缩的数据集，同时保持误差在可接受范围内，满足下游应用的需求。特别是在能量预算闭合和拉格朗日轨迹模拟等关键应用中，EBCC在实现高压缩比的同时保持了数据的科学完整性。这验证了EBCC在解决当前和未来气候数据存储挑战方面的潜力，并促进了更广泛的数据应用。

Abstract: As the resolution of weather and climate simulations increases, the amount of
data produced is growing rapidly from hundreds of terabytes to tens of
petabytes. The huge size becomes a limiting factor for broader adoption, and
its fast growth rate will soon exhaust all the available storage devices. To
address these issues, we present EBCC (Error Bounded Climate-data Compressor).
It follows a two-layer approach: a base compression layer using JPEG2000 to
capture the bulk of the data with a high compression ratio, and a residual
compression layer using wavelet transform and SPIHT encoding to efficiently
eliminate long-tail extreme errors introduced by the base compression layer. It
incorporates a feedback rate-control mechanism for both layers that adjusts
compression ratios to achieve the specified maximum error target. We evaluate
EBCC alongside other established compression methods on benchmarks related to
weather and climate science including error statistics, a case study on
primitive and derived variables near a hurricane, evaluation of the closure of
the global energy budget, and a Lagrangian air parcel trajectory simulation.
This is the first time that trajectory simulation is used to benchmark
compression methods. Our method concentrates most errors near zero, while
others tend to distribute errors uniformly within the error bound. EBCC
outperforms other methods in the benchmarks at relative error targets ranging
from 0.1% to 10% and achieves compression ratios from 15x to more than 300x. In
the energy budget closure and Lagrangian trajectory benchmarks, it can achieve
more than 100x compression while keeping errors within natural variability
derived from ERA5 uncertainty members. This verifies the effectiveness of EBCC
in creating heavily compressed weather and climate datasets suitable for
downstream applications. The source code of EBCC is available in
github.com/spcl/EBCC.

</details>


### [2] [Smart Sensor Placement: A Correlation-Aware Attribution Framework (CAAF) for Real-world Data Modeling](https://arxiv.org/abs/2510.22517)
*Sze Chai Leung,Di Zhou,H. Jane Bae*

Main category: cs.CE

TL;DR: 该研究提出了一种基于机器学习的特征归因框架（CAAF），用于优化传感器放置（OSP）。它通过在特征归因前引入聚类步骤来解决输入数据高度相关的问题，从而在结构健康监测、机翼升力预测和湍流通道流速度估计等实际动态系统中，有效识别OSP并优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 最佳传感器放置 (OSP) 对于复杂现实世界系统中的高效、准确监测、控制和推理至关重要。然而，在现实世界应用中，输入数据常常高度相关，这使得传统的特征归因方法难以准确量化输入对模型输出的贡献，从而阻碍了其在识别OSP方面的有效应用。因此，亟需一种能够处理数据相关性并提高泛化能力的OSP识别框架。

Method: 该研究提出了一个名为关联感知归因框架（CAAF）的机器学习框架。CAAF 的核心方法是在执行特征归因之前引入一个聚类步骤。这个聚类步骤旨在减少输入数据中的冗余并增强泛化能力，从而有效处理现实世界应用中常见的高度相关输入数据。该框架首先通过一系列验证案例阐明其核心原理，然后将其有效性在实际动态系统（如结构健康监测、机翼升力预测和湍流通道流的壁面法向速度估计）中进行验证。

Result: 研究结果表明，CAAF 在处理非线性动力学、混沌行为和多尺度相互作用等复杂系统方面，其性能优于现有替代方法。通过在结构健康监测、机翼升力预测和湍流通道流的壁面法向速度估计等实际应用中的验证，CAAF 成功地克服了传统方法因数据相关性而遇到的困难，并证明了其在识别实际环境中的OSP方面的有效性。

Conclusion: 该研究提出并验证了关联感知归因框架（CAAF），该框架通过引入聚类步骤，成功解决了传统特征归因方法在面对高度相关输入数据时识别最佳传感器放置（OSP）的挑战。CAAF 在各种实际动态系统中表现出色，并优于现有方法，这表明其在提高现实世界监测、控制和推理效率方面的巨大潜力。未来的工作可以探索CAAF在更广泛的复杂系统中的应用，并进一步优化其聚类和归因策略。

Abstract: Optimal sensor placement (OSP) is critical for efficient, accurate
monitoring, control, and inference in complex real-world systems. We propose a
machine-learning-based feature attribution framework to identify OSP for the
prediction of quantities of interest. Feature attribution quantifies input
contributions to a model's output; however, it struggles with highly correlated
input data often encountered in real-world applications. To address this, we
propose a Correlation-Aware Attribution Framework (CAAF), which introduces a
clustering step before performing feature attribution to reduce redundancy and
enhance generalizability. We first illustrate the core principles of the
proposed framework through a series of validation cases, then demonstrate its
effectiveness in real-world dynamical systems, such as structural health
monitoring, airfoil lift prediction, and wall-normal velocity estimation for
turbulent channel flow. The results show that the CAAF outperforms alternative
approaches that typically struggle due to the presence of nonlinear dynamics,
chaotic behavior, and multi-scale interactions, and enables the effective
application of feature attribution for identifying OSP in real-world
environments.

</details>


### [3] [P1GPT: a multi-agent LLM workflow module for multi-modal financial information analysis](https://arxiv.org/abs/2510.23032)
*Chen-Che Lu,Yun-Cheng Chou,Teng-Ruei Chen*

Main category: cs.CE

TL;DR: 本文提出了P1GPT，一个分层多智能体大型语言模型框架，用于多模态金融信息分析和可解释的交易决策支持。它通过结构化推理流程系统地融合技术、基本面和新闻洞察，实现了卓越的累计和风险调整收益，保持低回撤，并提供透明的因果解释。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在金融分析中主要局限于孤立的单智能体预测器或松散连接的分析师集合，缺乏统一不同数据模态的连贯推理工作流程。这阻碍了协作决策，且未能充分利用多模态信息，因此需要一个更集成、可解释且能够有效融合多种金融数据源的框架。

Method: P1GPT是一个分层多智能体大型语言模型框架，专为多模态金融信息分析和可解释的交易决策支持而设计。与通过角色模拟模仿交易团队的现有系统不同，P1GPT实现了一个结构化推理管道。该管道通过协调的智能体通信和集成时合成，系统地融合了来自技术、基本面和新闻的洞察。

Result: 在美国主要股票的多模态数据集上的回测结果表明，P1GPT实现了卓越的累计和风险调整收益，同时保持了较低的回撤。此外，它能提供透明的因果理由，证明了其决策过程的可解释性。这些发现强调了结构化推理工作流的有效性。

Conclusion: 研究结果表明，结构化推理工作流，而非智能体角色模仿，为可解释和值得信赖的金融AI系统提供了可扩展的路径。这标志着对现有方法的重大改进，并为未来金融AI研究指明了新方向，强调了可解释性和可信赖性。

Abstract: Recent advances in large language models (LLMs) have enabled multi-agent
reasoning systems capable of collaborative decision-making. However, in
financial analysis, most frameworks remain narrowly focused on either isolated
single-agent predictors or loosely connected analyst ensembles, and they lack a
coherent reasoning workflow that unifies diverse data modalities. We introduce
P1GPT, a layered multi-agent LLM framework for multi-modal financial
information analysis and interpretable trading decision support. Unlike prior
systems that emulate trading teams through role simulation, P1GPT implements a
structured reasoning pipeline that systematically fuses technical, fundamental,
and news-based insights through coordinated agent communication and
integration-time synthesis. Backtesting on multi-modal datasets across major
U.S. equities demonstrates that P1GPT achieves superior cumulative and
risk-adjusted returns, maintains low drawdowns, and provides transparent causal
rationales. These findings suggest that structured reasoning workflows, rather
than agent role imitation, offer a scalable path toward explainable and
trustworthy financial AI systems.

</details>


### [4] [GroupSHAP-Guided Integration of Financial News Keywords and Technical Indicators for Stock Price Prediction](https://arxiv.org/abs/2510.23112)
*Minjoo Kim,Jinwoong Kim,Sangjin Park*

Main category: cs.CE

TL;DR: 该研究引入了一种基于GRU并结合GroupSHAP的金融预测框架，通过量化语义相关关键词组的贡献，解决了传统SHAP在处理大规模文本数据时计算成本过高的问题。在2024年标普500指数的日度预测中，该方法将MAE降低了32.2%，RMSE降低了40.5%，首次证明了GroupSHAP在新闻驱动的金融预测中能够同时提升可解释性和预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管FinBERT等金融语言模型能将公众情绪量化为指数，但将多样化的语言信号压缩为单一指标会忽略语境细微差别并限制可解释性。可解释人工智能技术（如SHAP）虽能识别有影响力特征，但其计算成本随输入特征呈指数增长，不适用于大规模文本金融数据，这限制了其在实际应用中的可行性。

Method: 本研究提出一个结合GroupSHAP的GRU预测框架。首先，利用FinBERT对2015年至2024年的新闻文章进行嵌入。接着，将这些嵌入式新闻文章聚类成连贯的语义组。然后，应用GroupSHAP来量化每个语义组对股票价格变动的贡献，而不是针对单个词元。最后，将这些跨多个主题的组级SHAP变量作为输入特征，用于基于GRU的预测模型，进行标普500指数的日度预测。

Result: 在对2024年标普500指数进行为期一天的超前预测的实证结果表明，与没有GroupSHAP机制的基准模型相比，我们提出的方法在平均绝对误差（MAE）方面降低了32.2%，在均方根误差（RMSE）方面降低了40.5%。这些结果证明了该方法在提升预测准确性方面的显著效果。

Conclusion: 本研究首次将GroupSHAP应用于新闻驱动的金融预测领域，展示了通过分组情感表示可以同时增强模型的可解释性和预测性能。这为大规模文本数据处理和金融市场预测提供了一个更有效且更具洞察力的新范式，有望推动金融领域可解释人工智能应用的进一步发展。

Abstract: Recent advances in finance-specific language models such as FinBERT have
enabled the quantification of public sentiment into index-based measures, yet
compressing diverse linguistic signals into single metrics overlooks contextual
nuances and limits interpretability. To address this limitation, explainable AI
techniques, particularly SHAP (SHapley Additive Explanations), have been
employed to identify influential features. However, SHAP's computational cost
grows exponentially with input features, making it impractical for large-scale
text-based financial data. This study introduces a GRU-based forecasting
framework enhanced with GroupSHAP, which quantifies contributions of
semantically related keyword groups rather than individual tokens,
substantially reducing computational burden while preserving interpretability.
We employed FinBERT to embed news articles from 2015 to 2024, clustered them
into coherent semantic groups, and applied GroupSHAP to measure each group's
contribution to stock price movements. The resulting group-level SHAP variables
across multiple topics were used as input features for the prediction model.
Empirical results from one-day-ahead forecasting of the S&P 500 index
throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE
and a 40.5% reduction in RMSE compared with benchmark models without the
GroupSHAP mechanism. This research presents the first application of GroupSHAP
in news-driven financial forecasting, showing that grouped sentiment
representations simultaneously enhance interpretability and predictive
performance.

</details>


### [5] [Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms](https://arxiv.org/abs/2510.23166)
*Philippe Martin Wyder,Judah Goldfeder,Alexey Yermakov,Yue Zhao,Stefano Riva,Jan P. Williams,David Zoro,Amy Sara Rude,Matteo Tomasetto,Joe Germany,Joseph Bakarji,Georg Maierhofer,Miles Cranmer,J. Nathan Kutz*

Main category: cs.CE

TL;DR: 科学机器学习（ML）发展迅速但缺乏标准化基准，导致评估不一致、重现性差。本文提出了一个科学机器学习的通用任务框架（CTF），包含精选数据集和针对性指标，用于在实际约束下（如噪声和有限数据）评估算法。通过在Kuramoto-Sivashinsky和Lorenz系统上的初步基准测试，CTF展示了其揭示方法优缺点和适用性的能力。未来将启动一项真实世界数据集的竞赛，旨在用标准化的隐藏测试集评估取代临时比较，提高科学ML的严谨性和可重现性。


<details>
  <summary>Details</summary>
Motivation: 机器学习（ML）正在物理、工程和生物科学的建模与控制中引发变革。然而，其快速发展已超越了标准化、客观基准的建立。这导致了一系列问题，包括基线薄弱、报告偏差以及不同方法之间评估结果不一致。这些问题严重损害了研究的重现性、错误引导了资源分配，并阻碍了科学进步。为了解决这些根本性挑战，亟需一个统一的、严格的框架来对科学ML算法进行公平、客观的评估。

Method: 本文提出了一个针对科学机器学习的通用任务框架（CTF）。该框架的核心特点是包含一系列精心策划的数据集和针对特定任务的指标。这些任务涵盖了预测、状态重构以及在现实约束（如噪声和有限数据）下的泛化能力。CTF的设计灵感来源于自然语言处理和计算机视觉等领域中成功应用的CTF，旨在为多样化算法的直接比较提供一个结构化且严格的基础。作为第一步，研究人员在两个典型的非线性系统（Kuramoto-Sivashinsky和Lorenz系统）上对现有方法进行了基准测试。此外，为了促进社区参与，团队正计划围绕一个全球真实世界的海洋表面温度数据集启动一项竞赛，该竞赛将包含一个真实的保留数据集用于评估。长远愿景是通过在隐藏测试集上进行标准化评估来取代目前零散的比较方法，从而提高科学ML的严谨性和可重现性。

Result: 通过在两个典型的非线性系统（Kuramoto-Sivashinsky和Lorenz系统）上进行的初步基准测试，结果清晰地展示了通用任务框架（CTF）的实用性。这些结果有效地揭示了不同机器学习方法的优势、局限性以及它们在特定类型问题和不同目标下的适用性。CTF提供了一个明确的机制，用于识别哪些算法在何种条件下表现最佳，从而为科学机器学习领域的算法选择和进一步发展提供了有价值的洞察。

Conclusion: 本文提出的通用任务框架（CTF）为科学机器学习领域提供了一个结构化、严格的算法评估基础，有效解决了当前缺乏标准化基准所导致的重现性差和评估不一致等问题。通过在典型非线性系统上的初步验证，CTF已展示出其在揭示方法优缺点方面的巨大潜力。未来的工作将包括启动基于真实世界数据集的竞赛，以进一步促进社区参与并验证框架的有效性。长远来看，CTF有望通过引入隐藏测试集上的标准化评估，彻底取代目前零散的比较方式，从而显著提升科学机器学习的严谨性和可重现性，加速科学进步。

Abstract: Machine learning (ML) is transforming modeling and control in the physical,
engineering, and biological sciences. However, rapid development has outpaced
the creation of standardized, objective benchmarks - leading to weak baselines,
reporting bias, and inconsistent evaluations across methods. This undermines
reproducibility, misguides resource allocation, and obscures scientific
progress. To address this, we propose a Common Task Framework (CTF) for
scientific machine learning. The CTF features a curated set of datasets and
task-specific metrics spanning forecasting, state reconstruction, and
generalization under realistic constraints, including noise and limited data.
Inspired by the success of CTFs in fields like natural language processing and
computer vision, our framework provides a structured, rigorous foundation for
head-to-head evaluation of diverse algorithms. As a first step, we benchmark
methods on two canonical nonlinear systems: Kuramoto-Sivashinsky and Lorenz.
These results illustrate the utility of the CTF in revealing method strengths,
limitations, and suitability for specific classes of problems and diverse
objectives. Next, we are launching a competition around a global real world sea
surface temperature dataset with a true holdout dataset to foster community
engagement. Our long-term vision is to replace ad hoc comparisons with
standardized evaluations on hidden test sets that raise the bar for rigor and
reproducibility in scientific ML.

</details>


### [6] [Error estimation and step size control with minimal subsystem interfaces](https://arxiv.org/abs/2406.17353)
*Lars T. Kyllingstad,Severin Sadjina,Stian Skjong*

Main category: cs.CE

TL;DR: 这篇论文综述了协同仿真中的误差估计方法，特别是适用于提供最小接口的“黑盒”子系统的方法。它还描述了如何使用这些误差指标自动控制宏观时间步长，以平衡仿真速度和精度，并提供了伪代码和实用建议。


<details>
  <summary>Details</summary>
Motivation: 协同仿真在工业应用中，对于耦合“黑盒”子系统并运行大型系统仿真具有重要吸引力。然而，这些“黑盒”子系统通常只提供最少的接口（不支持时间步回滚、不输出导数、不提供内部信息，仅提供耦合所需的输出变量），这使得在协同仿真中进行误差估计变得极具挑战性。本研究的动机在于回顾并提出适用于此类限制性子系统的误差估计方法，并利用这些误差估计来有效地控制仿真步长，以解决在保证仿真精度的情况下提高仿真效率的实际问题。

Method: 本论文首先回顾了适用于协同仿真（特别是针对“黑盒”子系统）的误差估计方法。在此基础上，论文详细描述了如何利用生成的误差指标来自动控制宏观时间步长。通过动态调整步长，旨在实现仿真速度和精度之间的良好平衡。文中还以伪代码的形式呈现了步长控制算法的各个要素，以便读者能够在其自己的应用中实现和测试。

Result: 本论文阐述了如何使用误差指标来评估协同仿真的质量，提供了避免常见陷阱的实用建议，并指导读者如何配置步长控制算法。通过提供步长控制算法的伪代码，方便读者在实际应用中进行实施和测试，从而有效地平衡仿真速度和精度。

Conclusion: 本论文为“黑盒”子系统协同仿真中的误差估计和步长控制提供了全面的方法和实用建议。它使工程师和研究人员能够通过自动化的步长控制在仿真速度和精度之间取得良好平衡，从而提高协同仿真的质量和可靠性。论文提供的伪代码和实用指导为实际应用中的误差管理和性能优化提供了可操作的工具。

Abstract: We review error estimation methods for co-simulation, in particular methods
that are applicable when the subsystems provide minimal interfaces. By this, we
mean that subsystems do not support rollback of time steps, do not output
derivatives, and do not provide any other information about their internals
besides the output variables that are required for coupling with other
subsystems. Such "black-box" subsystems are common in industrial applications,
and the ability to couple them and run large-system simulations is one of the
major attractions of the co-simulation paradigm. We also describe how the
resulting error indicators may be used to automatically control macro time step
sizes to strike a good balance between simulation speed and accuracy. The
various elements of the step size control algorithm are presented in pseudocode
so that readers may implement them and test them in their own applications. We
provide practicable advice on how to use error indicators to judge the quality
of a co-simulation, how to avoid common pitfalls, and how to configure the step
size control algorithm.

</details>


### [7] [Magneto-thermally Coupled Field Simulation of Homogenized Foil Winding Models](https://arxiv.org/abs/2503.13010)
*Silas Weinert,Jonas Bundschuh,Yvonne Späck-Leigsnering,Herbert De Gersem*

Main category: cs.CE

TL;DR: 本文提出了一种针对箔绕组的磁热耦合仿真方法，通过在电磁和热分析中采用均匀化技术来提高效率，并利用弱耦合以及不同的时间步长进行处理。该方法在一个简单几何体和一台罐形变压器上得到了验证。


<details>
  <summary>Details</summary>
Motivation: 箔绕组因其层状结构，在高频应用中相比传统线绕组具有优势。对其进行电磁和热分析至关重要，因为这两个物理领域通过焦耳损耗和温度依赖性材料特性相互耦合。为了有效模拟箔绕组，需要采用均匀化技术以避免对单个匝进行解析，从而提高仿真效率。

Method: 本文采用了一种磁热耦合仿真方法，其中在电磁和热部分均使用了均匀化技术。该方法通过弱耦合实现，并为电磁和热部分设置了不同的时间步长。

Result: 该方法在一个简单几何体上得到了验证，并成功应用于一个使用箔绕组和线绕组的罐形变压器。

Conclusion: 本文提出了一种高效的箔绕组磁热耦合仿真方法，该方法结合了均匀化技术和弱耦合策略，并在实际应用案例中得到了有效验证，为高频箔绕组的设计和分析提供了有价值的工具。

Abstract: Foil windings have, due to their layered structure, different properties than
conventional wire windings, which make them advantageous for high frequency
applications. Both electromagnetic and thermal analyses are relevant for foil
windings. These two physical areas are coupled through Joule losses and
temperature dependent material properties. For an efficient simulation of foil
windings, homogenization techniques are used to avoid resolving the single
turns. Therefore, this paper comprises a coupled magneto-thermal simulation
that uses a homogenization method in the electromagnetic and thermal part. A
weak coupling with different time step sizes for both parts is presented. The
method is verified on a simple geometry and showcased for a pot transformer
that uses a foil and a wire winding.

</details>


### [8] [BikeBench: A Bicycle Design Benchmark for Generative Models with Objectives and Constraints](https://arxiv.org/abs/2508.00830)
*Lyle Regenwetter,Yazan Abu Obaideh,Fabien Chiotti,Ioanna Lykourentzou,Faez Ahmed*

Main category: cs.CE

TL;DR: BikeBench是一个用于评估生成模型在多目标、多约束工程设计问题上性能的基准测试平台，特别关注自行车设计。它量化了空气动力学、人体工程学、结构力学等多物理场和以人为中心的设计特性，并提供了丰富的仿真、人工评估和合成数据集。实验表明，大型语言模型（LLMs）和表格生成模型在设计质量、约束满足度和相似性得分方面不如混合GenAI+优化算法，揭示了该领域巨大的改进空间。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI应用范围的扩大，评估其理解物理定律、人类指导原则和硬性约束的能力变得日益重要。工程产品设计恰好处于这些复杂任务的交汇点，为AI能力提出了新的挑战。目前的AI模型需要一个专门的基准来评估它们不仅能够生成与现有数据相似的设计，还能满足特定性能目标和约束的能力。因此，开发BikeBench旨在填补这一空白，提供一个全面、客观的评估工具，以推动生成式AI在实际工程设计问题中的应用和进步。

Method: BikeBench通过量化一系列以人为中心和多物理场的性能特征来评估AI模型生成自行车设计的能力，这些特征包括空气动力学性能、人体工程学、结构力学、人类可用性评级以及与主观文本或图像提示的相似性。为了支持这一基准测试，BikeBench包含了多个数据集：模拟结果数据集，一个包含10,000个人工评估自行车设计的数据集，以及一个包含160万个合成生成设计的数据集，每个设计都具有参数化、CAD/XML、SVG和PNG表示。BikeBench独特之处在于，它能并排评估表格生成模型、大型语言模型（LLMs）、设计优化算法和混合算法，从而对不同类型的生成模型进行全面的比较。

Result: 实验结果表明，在自行车设计任务中，大型语言模型（LLMs）和表格生成模型在设计质量、约束满足度和与提示的相似性得分方面明显逊色于混合GenAI+优化算法。这清晰地指出，当前的LLMs和表格生成模型在处理具有复杂现实世界目标和硬约束的工程设计问题时，仍有显著的改进空间。这些发现为未来生成式AI在工程设计领域的研发指明了方向，即需要更有效地整合优化技术以满足实际工程需求。

Conclusion: BikeBench是首个此类基准测试平台，旨在通过提供一个统一的评估框架来推动生成式AI在受约束的多目标工程设计问题中的发展。它不仅揭示了当前LLMs和表格生成模型在复杂工程设计任务中的局限性，而且强调了混合GenAI+优化算法的潜力。我们希望BikeBench能激发研究人员开发出更先进的生成模型和优化策略，以更好地应对现实世界的工程挑战。该项目已提供代码、数据、交互式排行榜及其他资源，以促进社区的协作和进步。

Abstract: We introduce BikeBench, an engineering design benchmark for evaluating
generative models on problems with multiple real-world objectives and
constraints. As generative AI's reach continues to grow, evaluating its
capability to understand physical laws, human guidelines, and hard constraints
grows increasingly important. Engineering product design lies at the
intersection of these difficult tasks, providing new challenges for AI
capabilities. BikeBench evaluates AI models' capabilities to generate bicycle
designs that not only resemble the dataset, but meet specific performance
objectives and constraints. To do so, BikeBench quantifies a variety of
human-centered and multiphysics performance characteristics, such as
aerodynamics, ergonomics, structural mechanics, human-rated usability, and
similarity to subjective text or image prompts. Supporting the benchmark are
several datasets of simulation results, a dataset of 10,000 human-rated bicycle
assessments, and a synthetically generated dataset of 1.6M designs, each with a
parametric, CAD/XML, SVG, and PNG representation. BikeBench is uniquely
configured to evaluate tabular generative models, large language models (LLMs),
design optimization, and hybrid algorithms side-by-side. Our experiments
indicate that LLMs and tabular generative models fall short of hybrid
GenAI+optimization algorithms in design quality, constraint satisfaction, and
similarity scores, suggesting significant room for improvement. We hope that
BikeBench, a first-of-its-kind benchmark, will help catalyze progress in
generative AI for constrained multi-objective engineering design problems. We
provide code, data, an interactive leaderboard, and other resources at
https://github.com/Lyleregenwetter/BikeBench.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [9] [A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue](https://arxiv.org/abs/2510.21720)
*Anant Pareek*

Main category: cs.AI

TL;DR: 本文提出了一个全面的、多方面的框架，旨在通过将人工智能与计算心理学结合，实现复杂人类心理状态的建模、理解和交互。该框架涵盖了从预测建模到交互式心理分析系统的端到端开发，并成功解决了Transformer模型在情感计算回归任务中的数值不稳定性问题，展示了优越的预测性能，并开发了可复现的大规模AI研究方法。最终，该系统以可扩展的微服务生态系统形式部署，为计算心理学和人机交互提供了集成预测分析与生成式对话的实用模型。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和计算心理学的融合，通过计算手段对复杂人类心理状态进行建模、理解和交互的可能性应运而生。然而，目前的研究往往局限于孤立的预测模型，缺乏一个能够将这些模型与交互式系统相结合的全面框架。本研究的动机在于弥合这一鸿沟，创建一个能够整合预测分析和生成式对话的交互式系统，从而更深入地理解人类心理，并为未来的计算心理学和人机交互研究提供一个实用的、端到端的解决方案。

Method: 本文提出了一种严谨、端到端的开发生命周期方法。首先，利用经典机器学习技术在四个不同的心理学数据集上建立了基础性能基准。其次，对最先进的Transformer模型进行了微调，在此过程中，解决了关键的工程挑战，包括回归任务中数值不稳定性的问题，以及在严重资源限制下进行大规模训练的系统化工作流程的创建。第三，利用参数高效的技术对一个生成式大型语言模型（LLM）进行了微调，使其能够作为一个交互式的“人格大脑”来运行。最后，将所有预测模型和生成模型架构并部署为一个健壮、可扩展的微服务生态系统。

Result: 主要研究成果包括：成功稳定了用于情感计算的基于Transformer的回归模型，在标准方法失效的情况下显示出有意义的预测性能；开发了一种可复现的方法，以实现大规模人工智能研究的民主化。这些成果证明了该框架在处理复杂心理学任务上的有效性和实用性，尤其是在资源受限的情况下进行大规模模型训练的能力。

Conclusion: 本工作的意义在于其整体性方法，展示了一个完整的从研究到部署的流水线，该流水线将预测分析与生成式对话相结合。这为计算心理学和人机交互领域的未来研究提供了一个实用的模型。本研究不仅解决了Transformer模型在特定任务中的工程难题，还为大规模AI研究的推广和应用奠定了基础，具有深远的潜在影响。

Abstract: The confluence of Artificial Intelligence and Computational Psychology
presents an opportunity to model, understand, and interact with complex human
psychological states through computational means. This paper presents a
comprehensive, multi-faceted framework designed to bridge the gap between
isolated predictive modeling and an interactive system for psychological
analysis. The methodology encompasses a rigorous, end-to-end development
lifecycle. First, foundational performance benchmarks were established on four
diverse psychological datasets using classical machine learning techniques.
Second, state-of-the-art transformer models were fine-tuned, a process that
necessitated the development of effective solutions to overcome critical
engineering challenges, including the resolution of numerical instability in
regression tasks and the creation of a systematic workflow for conducting
large-scale training under severe resource constraints. Third, a generative
large language model (LLM) was fine-tuned using parameter-efficient techniques
to function as an interactive "Personality Brain." Finally, the entire suite of
predictive and generative models was architected and deployed as a robust,
scalable microservices ecosystem. Key findings include the successful
stabilization of transformer-based regression models for affective computing,
showing meaningful predictive performance where standard approaches failed, and
the development of a replicable methodology for democratizing large-scale AI
research. The significance of this work lies in its holistic approach,
demonstrating a complete research-to-deployment pipeline that integrates
predictive analysis with generative dialogue, thereby providing a practical
model for future research in computational psychology and human-AI interaction.

</details>


### [10] [Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks](https://arxiv.org/abs/2510.21866)
*Javier Marín*

Main category: cs.AI

TL;DR: 本文档记录了解码器专用自回归语言模型在知识密集型任务中的经验能力上限。尽管损失平滑下降，但知识检索任务的准确性改进却微乎其微。在MMLU数学基准测试中，准确率在所有规模上均保持在19-20%（低于25%的随机机会），而交叉熵损失下降了31%。相比之下，算术等程序性任务则表现出传统的扩展，其中两个指标都一起提高。注意力干预实验揭示了对扰动的高度敏感性：模型之间注意力模式的交换导致了灾难性的性能崩溃（完全失去准确性），而不是逐渐退化。这些测量结果对工程具有直接影响：对于使用OPT和Pythia架构的知识密集型应用程序，参数规模超过1-2B所带来的准确性提升微乎其微，尽管损失持续改善。


<details>
  <summary>Details</summary>
Motivation: 当前研究旨在深入探究解码器专用自回归语言模型在不同类型任务中，尤其是知识密集型任务中的能力扩展行为。尽管大型语言模型在许多领域取得了显著成功，但其性能扩展的潜在瓶颈和机制仍不完全明确。了解模型规模对知识检索和推理任务准确性的影响，以及这些影响是否与损失函数的持续优化同步，对于指导模型设计、资源分配和进一步的架构创新至关重要。本文的动机在于揭示现有模型在特定能力上的扩展局限性，并为优化未来的大型语言模型提供实证基础。

Method: 本研究对OPT和Pythia模型家族进行了系统性评估，这些模型的参数规模从70M到30B，跨越了240倍的扩展范围。评估主要集中在两类任务：知识密集型任务（如知识检索，以及MMLU数学基准测试）和程序性任务（如算术）。通过比较这些模型在不同任务上的准确率和交叉熵损失的变化，研究人员量化了模型扩展对性能的影响。此外，为了探究模型内部机制对性能的影响，研究还进行了注意力干预实验，具体方法是交换不同模型之间的注意力模式，以观察其对模型性能（特别是准确率）的影响。

Result: 研究发现，在知识密集型任务中，尽管交叉熵损失持续平滑下降，但模型在知识检索任务上的准确性改进却微乎其微，几乎可以忽略不计。具体而言，在MMLU数学基准测试中，即使模型规模持续扩大，准确率仍保持在19-20%的水平，这甚至低于25%的随机猜测机会，而同期交叉熵损失下降了31%。这表明，在这些任务上，损失的降低并未有效转化为能力（准确性）的提升。与此形成鲜明对比的是，在算术等程序性任务中，模型表现出传统的扩展模式，即准确率和交叉熵损失指标均同步改善。注意力干预实验揭示了模型对扰动的高度敏感性：交换模型之间的注意力模式会导致灾难性的性能崩溃，即完全失去准确性，而非性能的逐步下降。这些发现具有即时的工程实践意义：对于采用OPT和Pythia架构的知识密集型应用，将参数规模扩展到1-2B以上所带来的准确性提升非常有限，尽管损失可能继续优化。

Conclusion: 本文量化了解码器专用自回归语言模型在知识密集型任务中能力扩展的失败模式。研究明确指出，尽管损失持续优化，但参数规模的增加在某些知识密集型任务中并未带来相应的准确性提升，这揭示了这些模型家族在资源分配决策上的潜在低效。特别是在OPT和Pythia架构中，参数规模超过1-2B后，在知识密集型应用中的准确性收益微乎其微。这些发现对于指导未来大型语言模型的架构设计和资源优化具有重要意义。然而，这些模式究竟反映了解码器专用架构的根本限制，还是仅仅是与特定实现相关的局限性，仍是一个悬而未决的问题，需要通过探索多样化的架构方法来进一步调查和解答。

Abstract: We document empirical capability ceilings in decoder-only autoregressive
language models across knowledge-intensive tasks. Systematic evaluation of OPT
and Pythia model families (70M-30B parameters, spanning 240 times scaling)
reveals that knowledge retrieval tasks show negligible accuracy improvement
despite smooth loss reduction. On MMLU mathematics benchmarks, accuracy remains
flat at 19-20% (below 25% random chance) across all scales while cross-entropy
loss decreases by 31%. In contrast, procedural tasks like arithmetic show
conventional scaling where both metrics improve together. Attention
intervention experiments reveal high sensitivity to perturbation: swapping
attention patterns between models causes catastrophic performance collapse
(complete accuracy loss) rather than graceful degradation. These measurements
have immediate engineering implications: for knowledge-intensive applications
using OPT and Pythia architectures, parameter scaling beyond 1-2B offers
minimal accuracy gains despite continued loss improvement. Our findings
quantify capability-specific scaling failures in these model families to inform
resource allocation decisions. Whether these patterns reflect fundamental
constraints of decoder-only architectures or implementation-specific
limitations remains an open question requiring investigation across diverse
architectural approaches.

</details>


### [11] [GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.21881)
*Nannan Shi,Chuanyu Qin,Shipeng Song,Man Luo*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在文本数学推理上表现出色，但在几何视觉推理任务中性能显著下降。主要挑战包括几何问题的内在复杂性（需要详细图像理解和多步推理）以及现有数据集的局限性。为解决此问题，我们开发了GeoThoughts数据集（包含Geo-Thought-6K和Geo-Thought-Augmented-10K），其特点是包含详细的视觉描述、分步解决方案、明确的推理链、反思步骤和最终答案。基于此数据集，我们构建了多模态模型GeoThought-MLLM，该模型在问题解决过程中生成详细的思维过程。GeoThought-MLLM在几何任务中表现优于现有基准，证明了通过我们基于链式思考（Chain-of-Thought, CoT）的数据集进行训练能够提升模型在域内和域外设置中的几何推理能力。失败案例分析显示，错误主要源于对数学概念的错误解释或空间判断失误，而通过调用CoT可以纠正这些错误，从而得出正确答案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理基于文本的数学问题时，展现出强大的推理能力。然而，当这些模型被应用于视觉推理任务，特别是几何问题解决时，它们的性能会大幅下降。这种性能下降主要源于几何问题固有的两大挑战。首先，几何学本身具有内在的复杂性，它不仅要求模型具备细致的图像理解能力，还需要进行多步骤的复杂推理。其次，当前现有的数据集存在局限性，它们普遍缺乏足够的规模、多样性以及明确的推理轨迹，这严重阻碍了模型进行有效的训练。因此，开发能够有效处理几何视觉推理的LLM，并构建高质量、富含推理过程的数据集，成为了当前研究的关键和重要挑战。

Method: 为了应对大型语言模型在几何视觉推理方面的挑战，我们开发了GeoThoughts数据集，这是一个全面的几何推理语料库。该数据集包含两个主要子集：Geo-Thought-6K，其中包含6,243个样本；以及其增强版本Geo-Thought-Augmented-10K，包含10,834个样本。GeoThoughts数据集的每个条目都设计得非常详细和丰富，它包括视觉描述、分步解决方案、明确的推理链、反思步骤以及最终答案，旨在为模型提供极其丰富的训练信息，以理解和模仿复杂的几何推理过程。基于GeoThoughts数据集，我们进一步开发了GeoThought-MLLM，这是一个专门用于数学推理的多模态模型。GeoThought-MLLM模型的核心能力在于其能够在问题解决过程中生成详细的思维过程，这得益于其利用GeoThoughts数据集中提供的链式思考（Chain-of-Thought, CoT）数据进行训练。这种训练方法旨在使模型能够学习并复现人类解决几何问题的推理路径，从而有效提升其在几何问题上的推理表现。

Result: 我们的GeoThought-MLLM模型在几何任务中取得了卓越的性能表现，其结果明确优于现有基准模型。这强有力地证明了，通过利用我们专门构建的链式思考（Chain-of-Thought, CoT）数据集进行训练，能够显著提升模型在几何推理方面的能力。这种能力的提升不仅体现在模型对熟悉问题的处理上（域内设置），也同样适用于未曾见过的、新的问题类型（域外设置）。此外，我们对模型在解决问题时出现的失败案例进行了深入分析。分析结果揭示，模型产生错误的主要原因在于对数学概念的错误理解或在空间判断上出现偏差。值得注意的是，通过在这些失败案例中重新调用链式思考（CoT）机制来纠正这些错误，模型能够最终得出正确的答案。这一发现不仅突显了CoT在错误识别和纠正方面的强大潜力，也进一步验证了其在提升模型整体准确性上的有效性。

Conclusion: 本研究通过引入创新的GeoThoughts数据集和GeoThought-MLLM模型，成功解决了大型语言模型在几何视觉推理任务中遇到的核心挑战。我们的模型在几何任务中的表现超越了现有基准，有力地证明了利用富含链式思考（Chain-of-Thought）的数据集进行训练能够显著提升模型的几何推理能力，无论是在已知还是未知场景下。对模型失败案例的深入分析揭示了错误的常见来源（数学概念误解或空间误判），并进一步展示了通过调用CoT可以有效地纠正这些错误，从而提高模型的最终准确性。这项工作为未来在视觉推理领域，特别是几何问题解决方面，开发更强大、更鲁棒的人工智能模型奠定了坚实基础，并为错误分析和纠正提供了有价值的见解。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities
in text-based mathematical problem solving; however, when adapted to visual
reasoning tasks, particularly geometric problem solving, their performance
substantially declines because geometric problems present unique challenges.
Specifically, these challenges stem from two key factors: first, the intrinsic
complexity of geometry requiring detailed image comprehension and multi-step
reasoning, and second, the limitations of existing datasets which lack
sufficient scale, diversity, and explicit reasoning traces, consequently
hindering effective model training. To address these challenges, we developed
the GeoThoughts dataset, a comprehensive geometric reasoning corpus with two
subsets: Geo-Thought-6K with 6,243 samples and its augmented version
Geo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visual
descriptions, step-by-step solutions, explicit reasoning chains, reflection
steps, and final answers. Using this dataset, we developed GeoThought-MLLM, a
mathematical reasoning multimodal model that generates detailed thinking
processes during problem-solving. Our model outperforms existing benchmarks in
geometric tasks, demonstrating that training with our Chain-of-Thought dataset
improves geometric reasoning capabilities across both in-domain and
out-of-domain settings. Finally, we analyze failure cases and observe that
errors primarily arise from incorrect interpretation of mathematical concepts
or spatial misjudgment. By invoking CoT to correct these mistakes, the model
produces correct answers.

</details>


### [12] [Computational Hardness of Reinforcement Learning with Partial $q^π$-Realizability](https://arxiv.org/abs/2510.21888)
*Shayan Karimi,Xiaoqi Tan*

Main category: cs.AI

TL;DR: 本文在一种新的线性函数逼近机制——部分$q^{\pi}$-可实现性——下，研究了强化学习的计算复杂度。主要发现是，在这种设置下，学习一个$\epsilon$-最优策略是计算上困难的。具体而言，对于参数化贪婪策略集，证明了NP-hard；对于包含softmax策略的策略集，在随机指数时间假设下，除非NP=RP，否则存在指数级下界。这些结果表明，即使策略集$\Pi$扩展到最优策略之外，计算困难依然存在，这与生成访问模型下的$q^{\pi}$-可实现性形成了对比。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在探索一种新颖的线性函数逼近机制，即部分$q^{\pi}$-可实现性，以理解强化学习在此背景下的计算复杂度。现有的$q^{\pi}$-可实现性假设所有策略的价值函数都是线性可实现的，而$q^*$-可实现性假设最优策略的价值函数是线性可实现的。部分$q^{\pi}$-可实现性介于两者之间，它假设预定义策略集$\Pi$中所有策略的价值函数都是线性可实现的。这种框架比$q^{\pi}$-可实现性弱，但比$q^*$-可实现性强，提供了一个函数逼近自然出现的实用模型。理解这种更实际但又具有挑战性的设置中的计算限制，对于设计高效的强化学习算法至关重要。

Method: 本文采用计算复杂性理论的方法来证明在部分$q^{\pi}$-可实现性下学习$\epsilon$-最优策略的困难性。具体来说，研究人员通过将两个已知的复杂性问题——$\delta$-Max-3SAT和$\delta$-Max-3SAT(b)——归约到GLinear-$\\kappa$-RL（贪婪策略）和SLinear-$\\kappa$-RL（softmax策略）的实例，来建立NP-hard和指数级下界。这种归约技术用于证明问题的计算难度。研究关注两种主要策略集：参数化贪婪策略集（argmax）和包含softmax策略的策略集。对于后者，证明是在随机指数时间假设下进行的。

Result: 本文的主要结果证明了在部分$q^{\pi}$-可实现性下，学习一个$\epsilon$-最优策略是计算上困难的。具体来说：1. 对于参数化贪婪策略集，证明了该问题是NP-hard。这意味着在最坏情况下，找到一个近似最优策略的时间会随着问题规模的增长呈指数级增长。2. 对于包含softmax策略的策略集，在随机指数时间假设下，除非NP=RP，否则存在一个（特征向量维度中的）指数级下界。这表明，在某些情况下，即使是近似解也需要指数级的时间。这些困难性结果与$q^*$-可实现性中的结果相似，表明即使将策略集$\Pi$扩展到最优策略之外，计算困难仍然存在。这些发现暗示，与生成访问模型下的$q^{\pi}$-可实现性不同，在部分$q^{\pi}$-可实现性中，通常无法获得积极的计算结果。

Conclusion: 本文的结论是，在部分$q^{\pi}$-可实现性框架下，强化学习的计算复杂度非常高，学习一个$\epsilon$-最优策略是计算上困难的。对于贪婪策略集，问题是NP-hard；对于softmax策略集，存在指数级下界。这些结果强调了即使在更宽松的线性函数逼近假设下，强化学习仍然面临严重的计算挑战。这与生成访问模型下$q^{\pi}$-可实现性的情况形成鲜明对比，后者可能存在有效的学习算法。未来的工作可以探索在该框架下是否存在特殊结构或更强的假设，从而能够设计出更有效的算法，或者开发能够处理这种计算复杂性的近似方法。

Abstract: This paper investigates the computational complexity of reinforcement
learning in a novel linear function approximation regime, termed partial
$q^{\pi}$-realizability. In this framework, the objective is to learn an
$\epsilon$-optimal policy with respect to a predefined policy set $\Pi$, under
the assumption that all value functions for policies in $\Pi$ are linearly
realizable. The assumptions of this framework are weaker than those in
$q^{\pi}$-realizability but stronger than those in $q^*$-realizability,
providing a practical model where function approximation naturally arises. We
prove that learning an $\epsilon$-optimal policy in this setting is
computationally hard. Specifically, we establish NP-hardness under a
parameterized greedy policy set (argmax) and show that - unless NP = RP - an
exponential lower bound (in feature vector dimension) holds when the policy set
contains softmax policies, under the Randomized Exponential Time Hypothesis.
Our hardness results mirror those in $q^*$-realizability and suggest
computational difficulty persists even when $\Pi$ is expanded beyond the
optimal policy. To establish this, we reduce from two complexity problems,
$\delta$-Max-3SAT and $\delta$-Max-3SAT(b), to instances of GLinear-$\kappa$-RL
(greedy policy) and SLinear-$\kappa$-RL (softmax policy). Our findings indicate
that positive computational results are generally unattainable in partial
$q^{\pi}$-realizability, in contrast to $q^{\pi}$-realizability under a
generative access model.

</details>


### [13] [Performance Trade-offs of Optimizing Small Language Models for E-Commerce](https://arxiv.org/abs/2510.21970)
*Josip Tomo Licardo,Nikola Tankovic*

Main category: cs.AI

TL;DR: 本论文研究了小型开源语言模型作为资源高效的替代方案，用于特定领域任务（如多语言电商意图识别）。通过对10亿参数的Llama 3.2模型进行QLoRA微调和后训练量化（GPTQ和GGUF），该模型在电商意图识别上达到了99%的准确率，与GPT-4.1相当。结果显示，优化的1B模型在CPU上提供了显著的推理速度提升和内存消耗减少，但在旧GPU上可能存在去量化开销导致的推理变慢。文章得出结论，经过适当优化的小型开源模型是领域特定应用更合适的选择，能以较低的计算成本提供最先进的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言理解和生成任务中表现出色，但将其部署到电商等专业任务中，往往面临高昂的计算成本、延迟和运营开支。这使得寻找一种资源高效的替代方案变得至关重要，尤其是在需要为特定领域提供高性能解决方案的情况下。

Method: 本研究采用的方法是优化一个10亿参数的Llama 3.2模型，以进行多语言电商意图识别。具体步骤如下：
1. **模型选择**：选择一个10亿参数的Llama 3.2模型作为基础。
2. **微调**：使用量化低秩适应（QLoRA）技术对模型进行微调。微调数据集是专门生成的合成数据集，旨在模拟真实的电商用户查询，以确保模型对特定领域语境的理解。
3. **后训练量化**：在微调之后，应用后训练量化技术来进一步优化模型，创建了两种针对不同硬件的版本：
    * **GPU优化版本 (GPTQ)**：针对GPU部署进行优化。
    * **CPU优化版本 (GGUF)**：针对CPU部署进行优化。
这些技术旨在减少模型的内存占用和计算需求，同时保持其性能。

Result: 本研究的主要结果如下：
1. **性能匹配**：经过专门优化的10亿参数模型在电商意图识别任务中实现了99%的准确率，与性能更强大的GPT-4.1模型相匹配，这表明小型模型在特定领域也能达到最先进的性能。
2. **硬件依赖的权衡**：
    * **GPU优化 (GPTQ)**：4位GPTQ技术成功将显存（VRAM）使用量减少了41%。然而，在较旧的GPU架构（NVIDIA T4）上，由于去量化开销，推理速度反而减慢了82%。这凸显了在不同硬件上应用量化技术可能带来的复杂性。
    * **CPU优化 (GGUF)**：与FP16基线相比，GGUF格式在CPU上实现了高达18倍的推理吞吐量加速，并且内存（RAM）消耗减少了90%以上。这表明GGUF在CPU部署上具有显著的效率优势。
这些结果揭示了在不同硬件平台上，模型优化技术在资源效率和推理速度之间的关键权衡。

Conclusion: 本研究得出结论，小型、经过适当优化的开源模型不仅是一种可行的选择，而且是领域特定应用更为合适的替代方案。它们能够在计算成本大大降低的情况下，提供与最先进商业模型相匹配的高准确率。这意味着对于电商等需要高性能但对资源敏感的专业任务，无需依赖昂贵的大型模型。未来的工作可以探索更广泛的领域特定应用，以及在不同硬件和量化策略下进一步优化性能和效率的潜力。

Abstract: Large Language Models (LLMs) offer state-of-the-art performance in natural
language understanding and generation tasks. However, the deployment of leading
commercial models for specialized tasks, such as e-commerce, is often hindered
by high computational costs, latency, and operational expenses. This paper
investigates the viability of smaller, open-weight models as a
resource-efficient alternative. We present a methodology for optimizing a
one-billion-parameter Llama 3.2 model for multilingual e-commerce intent
recognition. The model was fine-tuned using Quantized Low-Rank Adaptation
(QLoRA) on a synthetically generated dataset designed to mimic real-world user
queries. Subsequently, we applied post-training quantization techniques,
creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results
demonstrate that the specialized 1B model achieves 99% accuracy, matching the
performance of the significantly larger GPT-4.1 model. A detailed performance
analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ
reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older
GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF
formats on a CPU achieved a speedup of up to 18x in inference throughput and a
reduction of over 90% in RAM consumption compared to the FP16 baseline. We
conclude that small, properly optimized open-weight models are not just a
viable but a more suitable alternative for domain-specific applications,
offering state-of-the-art accuracy at a fraction of the computational cost.

</details>


### [14] [Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions](https://arxiv.org/abs/2510.21977)
*Ji Huang,Mengfei Li,Shuai Shao*

Main category: cs.AI

TL;DR: 该研究提出了一种名为“分布偏移对齐（DSA）”的两阶段微调方法，旨在通过对齐输出分布和不同背景下的分布偏移来改进大型语言模型（LLMs）的问卷调查响应模拟。DSA通过学习分布变化而非简单拟合训练数据，实现了比现有方法更接近真实分布的结果，并在五个公共调查数据集中表现优异，可将所需真实数据量减少53.48%-69.12%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在模拟人类问卷调查响应方面展现出巨大潜力，有望显著降低大规模数据收集的成本。然而，现有方法存在明显局限：零样本方法容易受提示词影响且准确性较低；传统的微调方法则倾向于过度拟合训练集分布，难以产生比训练数据本身更准确的结果。这与利用LLMs模拟调查响应的初衷——即超越现有数据限制，获得更接近真实世界的分布——相悖。因此，需要一种新方法来克服这些挑战，使LLMs能更准确、鲁棒地模拟问卷调查响应。

Method: 本研究引入了“分布偏移对齐（DSA）”方法，这是一种新颖的两阶段微调策略。DSA的核心理念在于，它不仅对齐LLMs的输出分布，更重要的是，它对齐了不同背景（或子群体）之间的分布偏移。与传统的拟合训练数据的方法不同，DSA专注于学习这些分布是如何变化的，而不是简单地复制训练集中的模式。通过这种方式，DSA能够捕捉到更深层次的数据生成机制，从而产生比训练数据本身更接近真实分布的结果。

Result: 实验结果表明，DSA方法在五个公共调查数据集上持续优于其他现有方法。我们进一步进行了全面的比较分析，涵盖了准确性、鲁棒性和数据节省等关键指标。DSA显著减少了模拟问卷调查所需的真实数据量，具体范围为53.48%至69.12%。这有力地证明了DSA在问卷调查模拟方面的有效性和效率。

Conclusion: 本研究提出的分布偏移对齐（DSA）方法成功解决了大型语言模型在问卷调查模拟中面临的挑战，如提示词敏感性和过度拟合训练数据。DSA通过学习分布变化而非简单拟合数据，实现了比传统方法更接近真实分布的模拟结果，并在多个数据集上表现出卓越的性能和数据效率。其显著的数据节省能力（减少53.48%-69.12%的真实数据需求）凸显了其在降低大规模数据收集成本方面的巨大潜力。这项工作为LLMs在社会科学、市场研究等领域的大规模调查模拟应用奠定了坚实基础，并为未来研究如何通过学习分布变化而非固定模式来提高模型泛化能力提供了新思路。

Abstract: Large language models (LLMs) offer a promising way to simulate human survey
responses, potentially reducing the cost of large-scale data collection.
However, existing zero-shot methods suffer from prompt sensitivity and low
accuracy, while conventional fine-tuning approaches mostly fit the training set
distributions and struggle to produce results more accurate than the training
set itself, which deviates from the original goal of using LLMs to simulate
survey responses. Building on this observation, we introduce Distribution Shift
Alignment (DSA), a two-stage fine-tuning method that aligns both the output
distributions and the distribution shifts across different backgrounds. By
learning how these distributions change rather than fitting training data, DSA
can provide results substantially closer to the true distribution than the
training data. Empirically, DSA consistently outperforms other methods on five
public survey datasets. We further conduct a comprehensive comparison covering
accuracy, robustness, and data savings. DSA reduces the required real data by
53.48-69.12%, demonstrating its effectiveness and efficiency in survey
simulation.

</details>


### [15] [LightAgent: Mobile Agentic Foundation Models](https://arxiv.org/abs/2510.22009)
*Yangqin Jiang,Chao Huang*

Main category: cs.AI

TL;DR: LightAgent是一个移动智能体基础模型解决方案，通过设备-云协作，结合了设备端模型的成本效益和云端模型的高能力，以解决移动GUI智能体系统中小型设备端模型性能不足和大型云端模型部署成本高昂的问题。它在Qwen2.5-VL-3B模型上进行了两阶段SFT->GRPO训练，并集成了高效长推理机制，实现了与大型模型相媲美的性能，同时显著降低了云成本。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型（MLLMs）的发展，构建GUI智能体系统，特别是针对移动平台，已成为一个前景广阔的方向。然而，移动GUI智能体面临一个关键困境：真正的设备端模型（4B或更小）性能不足，而有能力的模型（7B或更大）要么对于移动部署来说过大，要么成本过高（例如，仅限云端的闭源MLLMs）。这阻碍了在移动设备上部署高效能GUI智能体。

Method: LightAgent通过设备-云协作来解决上述问题。具体方法包括：1. 增强Qwen2.5-VL-3B模型：通过在合成GUI数据上进行两阶段的SFT->GRPO训练，以实现强大的决策能力。2. 集成高效长推理机制：在资源紧张的情况下，利用历史交互信息进行推理。3. 默认设备端执行：LightAgent主要在设备端执行任务，只通过实时复杂性评估将具有挑战性的子任务升级到云端处理。实验在在线AndroidLab基准测试和各种应用程序上进行。

Result: 在在线AndroidLab基准测试和各种应用程序上的实验结果表明，LightAgent的性能与大型模型相匹配或接近，同时显著降低了云成本。这证明了其在解决移动GUI智能体困境方面的有效性。

Conclusion: LightAgent通过创新的设备-云协作方案，成功地平衡了移动GUI智能体系统的性能和成本问题。它通过优化小型模型并在必要时利用云端能力，实现了与大型模型相当的性能，并显著降低了运营成本。虽然摘要中未明确提及局限性和未来工作，但该研究为移动智能体领域提供了一个高效且经济的解决方案。

Abstract: With the advancement of multimodal large language models (MLLMs), building
GUI agent systems has become an increasingly promising direction-especially for
mobile platforms, given their rich app ecosystems and intuitive touch
interactions. Yet mobile GUI agents face a critical dilemma: truly on-device
models (4B or smaller) lack sufficient performance, while capable models
(starting from 7B) are either too large for mobile deployment or prohibitively
costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose
LightAgent, a mobile agentic foundation model solution that leverages
device-cloud collaboration to tap the cost-efficiency of on-device models and
the high capability of cloud models, while avoiding their drawbacks.
Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO
training on synthetic GUI data for strong decision-making, integrates an
efficient long-reasoning mechanism to utilize historical interactions under
tight resources, and defaults to on-device execution-only escalating
challenging subtasks to the cloud via real-time complexity assessment.
Experiments on the online AndroidLab benchmark and diverse apps show LightAgent
matches or nears larger models, with a significant reduction in cloud costs.

</details>


### [16] [LLM-AR: LLM-powered Automated Reasoning Framework](https://arxiv.org/abs/2510.22034)
*Rick Chen,Joseph Ternasky,Aaron Ontoyin Yin,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: 本论文提出了LLM-AR，一个受神经符号系统启发的管道，用于基于创始人特质预测创业公司成功。它将LLM生成的启发式方法提炼为ProbLog执行的概率规则，并通过关联规则挖掘进行迭代优化。LLM-AR在未见过的数据上实现了59.5%的精度和8.7%的召回率，是随机基线精度的5.9倍，同时提供可解释的决策路径。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）尽管在模式识别和推理方面表现出色，但其不稳定的准确性阻碍了它们在高风险决策应用中的采用。本研究从风险投资的角度出发，通过基于创始人特质预测创意阶段创业公司的成功来解决这一问题。其核心在于开发一个可靠且可解释的预测模型，以克服当前LLMs在关键应用中准确性不足的挑战。

Method: 本研究引入了LLM-AR管道，其灵感来源于神经符号系统。该方法首先将大型语言模型（LLMs）生成的启发式规则提炼为概率规则。这些概率规则随后由ProbLog自动化推理引擎执行，以进行预测。为了持续改进预测规则，LLM-AR采用了一个迭代的策略演化循环，该循环整合了关联规则挖掘技术。这种迭代过程旨在逐步优化规则集，从而提高模型的预测性能和可靠性。模型应用于基于创始人特质预测创意阶段创业公司的成功。

Result: LLM-AR模型在未见过的数据折叠上取得了59.5%的精度和8.7%的召回率。其精度是随机基线精度的5.9倍，显著优于基线表现。此外，该框架能够暴露每一个决策路径供人工检查，这大大增强了模型的可解释性。LLM-AR还可通过超参数进行调整，显示出其灵活性和适应性。研究结果表明，该框架有望扩展到其他领域，具有广泛的应用潜力。

Conclusion: 本研究提出了LLM-AR框架，有效解决了大型语言模型在关键决策应用中准确性不稳定的问题，并通过可解释的方式预测了创意阶段创业公司的成功。LLM-AR通过结合LLM生成的启发式方法和概率推理，并利用迭代策略演化进行规则细化，展现了卓越的性能和透明度。该模型在精度上显著超越随机基线，并且其决策路径对人类开放，提供了高度的可解释性。尽管在召回率方面可能存在改进空间，但其可调谐性和向其他领域扩展的潜力，使其成为未来研究和实际应用中极具前景的工具。

Abstract: Large language models (LLMs) can already identify patterns and reason
effectively, yet their variable accuracy hampers adoption in high-stakes
decision-making applications. In this paper, we study this issue from a venture
capital perspective by predicting idea-stage startup success based on founder
traits. (i) To build a reliable prediction model, we introduce LLM-AR, a
pipeline inspired by neural-symbolic systems that distils LLM-generated
heuristics into probabilistic rules executed by the ProbLog automated-reasoning
engine. (ii) An iterative policy-evolution loop incorporates association-rule
mining to progressively refine the prediction rules.
  On unseen folds, LLM-AR achieves 59.5% precision and 8.7% recall, 5.9x the
random baseline precision, while exposing every decision path for human
inspection. The framework is interpretable and tunable via hyperparameters,
showing promise to extend into other domains.

</details>


### [17] [Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability](https://arxiv.org/abs/2510.22039)
*Po-Chen Kuo,Han Hou,Will Dabney,Edgar Y. Walker*

Main category: cs.AI

TL;DR: 在部分可观测环境中，将自我监督预测编码模块集成到元强化学习（Meta-RL）中，能够学习到更可解释、更接近贝叶斯最优的信念状态表示。这不仅在各种任务中表现出更好的表示学习效果，尤其在需要主动信息探索的复杂任务中能成功学习到最优策略和表示，而且能显著提升泛化能力，即使在传统Meta-RL也能达到最优策略的情况下，其表示效率也更优。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境中进行规划和泛化，学习紧凑的历史表示至关重要。尽管元强化学习（Meta-RL）智能体能够达到接近贝叶斯最优的策略，但它们往往未能学习到紧凑、可解释的贝叶斯最优信念状态。这种表示效率低下可能会限制智能体的适应性和泛化能力。因此，研究的动机在于解决Meta-RL在表示学习方面的不足，以提升其在复杂环境中的性能。

Method: 研究受神经科学中预测编码（大脑预测感官输入作为贝叶斯推断的神经实现）以及深度强化学习中辅助预测目标启发，探索将自我监督预测编码模块集成到元强化学习中。通过状态机模拟进行实验，以评估这种集成方法是否能促进贝叶斯最优表示的学习。

Result: 实验结果表明，与传统的Meta-RL相比，集成了预测模块的Meta-RL在各种任务中始终能生成更可解释、更接近贝叶斯最优信念状态的表示，即使在两种方法都能达到最优策略的情况下也是如此。在需要主动信息探索的复杂任务中，只有集成了预测模块的Meta-RL成功学习到最优表示和策略，而传统的Meta-RL在表示学习方面表现不佳。此外，更好的表示学习能带来更好的泛化能力。

Conclusion: 研究结果有力地表明，预测学习在指导智能体在部分可观测环境中进行有效的表示学习方面发挥着关键作用。这种方法不仅提高了表示的可解释性和贝叶斯最优性，还显著增强了智能体在复杂任务中的学习和泛化能力。未来的工作可以探索这种方法在更复杂现实世界场景中的应用，并进一步研究预测编码机制如何影响决策过程。

Abstract: Learning a compact representation of history is critical for planning and
generalization in partially observable environments. While meta-reinforcement
learning (RL) agents can attain near Bayes-optimal policies, they often fail to
learn the compact, interpretable Bayes-optimal belief states. This
representational inefficiency potentially limits the agent's adaptability and
generalization capacity. Inspired by predictive coding in neuroscience--which
suggests that the brain predicts sensory inputs as a neural implementation of
Bayesian inference--and by auxiliary predictive objectives in deep RL, we
investigate whether integrating self-supervised predictive coding modules into
meta-RL can facilitate learning of Bayes-optimal representations. Through state
machine simulation, we show that meta-RL with predictive modules consistently
generates more interpretable representations that better approximate
Bayes-optimal belief states compared to conventional meta-RL across a wide
variety of tasks, even when both achieve optimal policies. In challenging tasks
requiring active information seeking, only meta-RL with predictive modules
successfully learns optimal representations and policies, whereas conventional
meta-RL struggles with inadequate representation learning. Finally, we
demonstrate that better representation learning leads to improved
generalization. Our results strongly suggest the role of predictive learning as
a guiding principle for effective representation learning in agents navigating
partial observability.

</details>


### [18] [AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines](https://arxiv.org/abs/2510.23408)
*Abolfazl Younesi,Zahra Najafabadi Samani,Thomas Fahringer*

Main category: cs.AI

TL;DR: 本文介绍了AutoStreamPipe，一个新颖的框架，它利用大型语言模型（LLMs）自动设计、生成和部署流处理管道。该框架通过整合超图思维（HGoT）弥合了用户意图与平台实现之间的语义鸿沟，并结合了弹性执行策略和高级查询分析。实验结果表明，与LLM代码生成方法相比，AutoStreamPipe显著减少了开发时间（6.3倍）和错误率（5.19倍）。


<details>
  <summary>Details</summary>
Motivation: 数据管道在流处理中至关重要，因为它们能够高效地收集、处理和交付实时数据，从而支持快速数据分析。然而，设计、生成和部署流处理管道通常涉及将高层用户意图转换为平台特定的实现，这在分布式流处理系统中存在显著的语义鸿沟。这项工作的动机在于自动化这一复杂且耗时的过程，提高开发效率并降低错误率。

Method: AutoStreamPipe框架采用大型语言模型（LLMs）来自动化流处理管道的设计、生成和部署。它通过整合超图思维（Hypergraph of Thoughts, HGoT）作为GoT的扩展版本，实现了结构化的多智能体推理，从而弥合了高级用户意图与分布式流处理系统中的平台特定实现之间的语义鸿沟。AutoStreamPipe结合了弹性执行策略、高级查询分析和HGoT，以交付具有良好准确性的管道。实验评估在多样化的管道上进行，并使用新颖的无错误分数（Error-Free Score, EFS）来衡量性能。

Result: 实验评估在多样化的管道上进行，结果显示AutoStreamPipe与LLM代码生成方法相比，显著减少了开发时间（6.3倍）和错误率（5.19倍）。性能通过新颖的无错误分数（EFS）进行衡量。AutoStreamPipe交付的管道具有良好的准确性，证明了其在自动化流处理管道设计、生成和部署方面的有效性。

Conclusion: AutoStreamPipe框架通过利用大型语言模型和超图思维（HGoT），成功实现了流处理管道的自动化设计、生成和部署。它有效弥合了语义鸿沟，并通过弹性执行策略和高级查询分析提高了管道的准确性。与现有LLM代码生成方法相比，AutoStreamPipe显著提升了开发效率并降低了错误率，展现了其在实时数据处理领域的重要价值和潜力。未来工作可能包括进一步优化HGoT的推理能力，以及在更广泛的实际应用场景中验证其可扩展性和鲁棒性。

Abstract: Data pipelines are essential in stream processing as they enable the
efficient collection, processing, and delivery of real-time data, supporting
rapid data analysis. In this paper, we present AutoStreamPipe, a novel
framework that employs Large Language Models (LLMs) to automate the design,
generation, and deployment of stream processing pipelines. AutoStreamPipe
bridges the semantic gap between high-level user intent and platform-specific
implementations across distributed stream processing systems for structured
multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an
extended version of GoT. AutoStreamPipe combines resilient execution
strategies, advanced query analysis, and HGoT to deliver pipelines with good
accuracy. Experimental evaluations on diverse pipelines demonstrate that
AutoStreamPipe significantly reduces development time (x6.3) and error rates
(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM
code-generation methods.

</details>


### [19] [HW/SW Co-design of a PCM/PWM converter: a System Level Approach based in the SpecC Methodology](https://arxiv.org/abs/2510.22046)
*Daniel G. P. Petrini,Braz Izaias da Silva Junior*

Main category: cs.AI

TL;DR: 本案例研究将SpecC方法应用于PCM-到-PWM转换器的硬件/软件协同设计流程，通过系统级估计和快速功能仿真，实现了满足实时约束的硬件/软件划分，同时降低了纯硬件解决方案的成本并避免了高端处理器纯软件实现的开销，凸显了系统级协同设计在早期架构洞察、快速验证和成本/性能权衡方面的价值。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决Class-D音频放大器中PCM-到-PWM转换器设计的挑战。传统的纯硬件解决方案可能成本过高，而纯软件实现则需要昂贵的高端处理器。因此，需要一种方法来找到最佳的硬件/软件划分，以在满足实时约束的同时降低整体成本。这种需求凸显了在系统设计早期阶段进行架构洞察和成本/性能权衡的重要性。

Method: 本研究采用SpecC方法学，在一个系统级硬件/软件协同设计流程中进行。具体而言，PCM-到-PWM转换器被建模并使用SpecC方法进行探索，以推导出一个合适的硬件/软件划分。研究人员利用系统级估计和快速功能仿真来评估不同的映射方案，以确保它们能够满足实时约束。

Result: 研究结果表明，通过应用SpecC方法和系统级协同设计，可以评估出满足实时约束的映射方案。与纯硬件解决方案相比，这种方法显著降低了估计成本；同时，也避免了在高端处理器上进行纯软件实现所带来的高昂开销。尽管设计复杂性适中，但研究成果有力地强调了系统级协同设计对于早期架构洞察、快速验证以及可操作的成本/性能权衡的显著价值。

Conclusion: 本研究的结论是，系统级协同设计，特别是结合SpecC方法，对于实现早期架构洞察、快速验证和有效的成本/性能权衡具有重要价值，即使对于中等复杂度的设计也是如此。这项工作的影响在于为嵌入式系统（如音频放大器）的更高效和更具成本效益的设计提供了一种有效的途径。抽象中没有明确提及局限性和未来工作。

Abstract: We present a case study applying the SpecC methodology within a system-level
hardware/software co-design flow to a PCM-to-PWM converter, the core of a
Class-D audio amplifier. The converter was modeled and explored with SpecC
methodology to derive an HW/SW partition. Using system-level estimates and fast
functional simulation, we evaluated mappings that meet real-time constraints
while reducing estimated cost of an all-hardware solution and avoiding the
expense of a purely software implementation on a high-end processor. Despite
the design's moderate complexity, the results underline the value of
system-level co-design for early architectural insight, rapid validation, and
actionable cost/performance trade-offs. [Original work from 2005; formatting
revised in 2025, with no changes to the results.]

</details>


### [20] [Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies](https://arxiv.org/abs/2510.22095)
*Yankai Chen,Xinni Zhang,Yifei Zhang,Yangning Li,Henry Peng Zou,Chunyu Miao,Weizhi Zhang,Xue Liu,Philip S. Yu*

Main category: cs.AI

TL;DR: 该立场论文提出将脑机接口（BCI）领域扩展到脑-智能体协作（BAC）的新范式。为了克服BCI的局限性（如信息传输速率低和大量校准），论文强调将大语言模型（LLMs）集成的智能体重新定义为主动的协作伙伴，并呼吁关注伦理数据处理、模型可靠性以及强大的人机协作框架，以确保系统安全、可信和有效。


<details>
  <summary>Details</summary>
Motivation: 脑机接口（BCI）为神经功能严重受损个体提供了直接的交流途径，但其普及受限于信息传输速率低和需要大量用户特定校准。尽管最新研究已尝试整合大语言模型（LLMs）以理解复杂认知状态，但智能体AI的部署仍面临技术和伦理挑战。鉴于此新兴方向缺乏全面讨论，本文旨在提出一个解决方案。

Method: 本论文是一篇立场论文，主要方法是提出并论证一种范式转变。具体而言，它主张将BCI领域扩展到脑-智能体协作（BAC）。论文的核心论点是将智能体重新定义为主动和协作的伙伴，提供智能辅助，而非仅仅是无源的脑信号数据处理器。为实现这一目标，论文强调需要关注伦理数据处理、模型可靠性以及建立一个强大的人机协作框架。

Result: 本文的“结果”是提出的新范式——脑-智能体协作（BAC）——以及实现该范式所需的关键关注点。通过将智能体重新定义为协作伙伴，该论文认为可以克服BCI的现有局限性，并使系统更加安全、可信和有效。这包括明确了在BAC框架下，伦理数据处理、模型可靠性以及稳健的人机协作框架是确保系统功能和接受度的关键要素。

Conclusion: 本论文的结论是，脑机接口（BCI）领域正处于一个范式扩展的临界点，即转向脑-智能体协作（BAC）。通过将智能体视为主动的协作伙伴，并强调伦理数据处理、模型可靠性以及强大的人机协作框架，可以开发出更安全、可信赖和有效的系统，从而克服当前BCI的局限性，并为神经功能障碍患者提供更好的智能辅助。未来的工作应聚焦于构建和验证这些框架。

Abstract: Brain-Computer Interfaces (BCIs) offer a direct communication pathway between
the human brain and external devices, holding significant promise for
individuals with severe neurological impairments. However, their widespread
adoption is hindered by critical limitations, such as low information transfer
rates and extensive user-specific calibration. To overcome these challenges,
recent research has explored the integration of Large Language Models (LLMs),
extending the focus from simple command decoding to understanding complex
cognitive states. Despite these advancements, deploying agentic AI faces
technical hurdles and ethical concerns. Due to the lack of comprehensive
discussion on this emerging direction, this position paper argues that the
field is poised for a paradigm extension from BCI to Brain-Agent Collaboration
(BAC). We emphasize reframing agents as active and collaborative partners for
intelligent assistance rather than passive brain signal data processors,
demanding a focus on ethical data handling, model reliability, and a robust
human-agent collaboration framework to ensure these systems are safe,
trustworthy, and effective.

</details>


### [21] [Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests](https://arxiv.org/abs/2510.22170)
*Alexandra Yost,Shreyans Jain,Shivam Raval,Grant Corser,Allen Roush,Nina Xu,Jacqueline Hammack,Ravid Shwartz-Ziv,Amirali Abdullah*

Main category: cs.AI

TL;DR: 该研究提出了一种新的框架，用于对需要情感判断和伦理考量的AI系统进行心理测量学评估。该框架使用来自真实场景的情境判断测试（SJTs）和整合了工业-组织与人格心理学设计的复杂人物画像。在一个执法助手案例研究中，作者构建了一个包含8,500个人物画像、4,000个SJTs和300,000个响应的大型数据集，以分析跨亚群体和情景切片的行为。该研究旨在克服现有方法在行为真实性和领域相关性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 在传统上需要情感判断和伦理考量的角色中评估AI系统时，现有的人工智能心理测量学方法往往重用人类特质清单（如大五人格、HEXACO）或采用临时的人物画像。这种做法限制了AI行为评估的真实性和领域相关性，使得对AI系统在复杂情境下的表现评估不够准确和全面。因此，本研究的动机是开发一种更先进、更具情境感和心理学深度的评估框架。

Method: 该研究提出了一个包含三个关键部分的框架：
1. 使用来自真实场景的情境判断测试（SJTs）来探测特定领域的 компетенcies，确保评估与实际应用场景紧密相关。
2. 整合工业-组织心理学和人格心理学，设计复杂的人物画像。这些画像包含了行为和心理描述、生活史以及社会和情感功能，从而提供了更丰富、更逼真的人格模型。
3. 采用结构化生成方法，结合人口统计学先验知识和受回忆录启发的叙事，并使用Pydantic模式进行编码，以系统地创建多样化且细致入微的人物画像。
在执法助手案例研究中，作者构建了一个全面的数据集，涵盖了8种人物画像原型和11种属性的SJTs。

Result: 通过执法助手案例研究，研究团队构建了一个丰富的数据集，其中包括8,500个人物画像、4,000个情境判断测试（SJTs）以及300,000个AI响应。该研究对跨亚群体和情景切片的行为进行了深入分析。该数据集和所有相关代码都将公开发布。

Conclusion: 本研究提出了一种新颖且全面的AI心理测量学评估框架，显著提升了AI系统在需要情感判断和伦理考量场景中评估的行为真实性和领域相关性。通过整合情境判断测试和复杂的人物画像设计，该框架能够更细致地捕捉AI在不同情境下的行为模式。虽然抽象内容未明确指出局限性，但大规模数据集的构建和公开发布预示着未来在AI伦理和行为研究方面具有广阔的应用前景，为进一步的开放研究和AI系统改进奠定了基础。

Abstract: AI psychometrics evaluates AI systems in roles that traditionally require
emotional judgment and ethical consideration. Prior work often reuses human
trait inventories (Big Five, \hexaco) or ad hoc personas, limiting behavioral
realism and domain relevance. We propose a framework that (1) uses situational
judgment tests (SJTs) from realistic scenarios to probe domain-specific
competencies; (2) integrates industrial-organizational and personality
psychology to design sophisticated personas which include behavioral and
psychological descriptors, life history, and social and emotional functions;
and (3) employs structured generation with population demographic priors and
memoir inspired narratives, encoded with Pydantic schemas. In a law enforcement
assistant case study, we construct a rich dataset of personas drawn across 8
persona archetypes and SJTs across 11 attributes, and analyze behaviors across
subpopulation and scenario slices. The dataset spans 8,500 personas, 4,000
SJTs, and 300,000 responses. We will release the dataset and all code to the
public.

</details>


### [22] [OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling](https://arxiv.org/abs/2510.22192)
*Haoyang Liu,Jie Wang,Yuyang Cai,Xiongwei Han,Yufei Kuang,Jianye Hao*

Main category: cs.AI

TL;DR: OptiTree提出了一种新颖的树搜索方法，通过自适应地将复杂运筹学（OR）问题分解为更简单的子问题来增强大型语言模型（LLM）的建模能力，从而显著提高了建模准确性，在挑战性基准测试中实现了超过10%的改进。


<details>
  <summary>Details</summary>
Motivation: 运筹学（OR）中的优化建模是极其重要但技术性很强的一个环节。现有工作尝试利用大型语言模型（LLMs）自动化建模过程，通过提示LLMs将任务分解为生成变量、约束和目标等步骤。然而，由于OR问题固有的高度复杂数学结构，标准的固定步骤分解方法往往无法取得高性能，这限制了LLMs在自动化复杂OR问题建模方面的应用和效果。

Method: 为解决现有方法在处理复杂OR问题时表现不佳的问题，我们引入了OptiTree。这是一种新颖的树搜索方法，旨在通过自适应地将复杂问题分解为更简单的子问题来提升建模能力。具体而言，我们开发了一个建模树，该树根据问题的层次分类和复杂性来组织各种OR问题，每个节点代表一个问题类别并包含相关的高级建模思路。对于给定的待建模问题，OptiTree会递归地搜索该树，以识别一系列更简单的子问题，并通过自适应地整合分层思路来合成全局建模思路。

Result: 实验结果表明，与现有最先进的方法相比，OptiTree显著提高了建模准确性。在具有挑战性的基准测试中，OptiTree实现了超过10%的性能提升。研究代码已在https://github.com/MIRALab-USTC/OptiTree/tree/main 发布。

Conclusion: OptiTree通过引入自适应问题分解的树搜索方法，成功解决了大型语言模型在处理复杂运筹学问题建模时性能不佳的挑战。它显著提升了建模的准确性，为自动化复杂优化建模提供了新的、更有效途径。尽管文中没有明确指出局限性和未来工作，但其方法为进一步利用LLMs解决复杂、结构化问题提供了有益的启示。

Abstract: Optimization modeling is one of the most crucial but technical parts of
operations research (OR). To automate the modeling process, existing works have
leveraged large language models (LLMs), prompting them to break down tasks into
steps for generating variables, constraints, and objectives. However, due to
the highly complex mathematical structures inherent in OR problems, standard
fixed-step decomposition often fails to achieve high performance. To address
this challenge, we introduce OptiTree, a novel tree search approach designed to
enhance modeling capabilities for complex problems through adaptive problem
decomposition into simpler subproblems. Specifically, we develop a modeling
tree that organizes a wide range of OR problems based on their hierarchical
problem taxonomy and complexity, with each node representing a problem category
and containing relevant high-level modeling thoughts. Given a problem to model,
we recurrently search the tree to identify a series of simpler subproblems and
synthesize the global modeling thoughts by adaptively integrating the
hierarchical thoughts. Experiments show that OptiTree significantly improves
the modeling accuracy compared to the state-of-the-art, achieving over 10\%
improvements on the challenging benchmarks. The code is released at
https://github.com/MIRALab-USTC/OptiTree/tree/main.

</details>


### [23] [PACR: Progressively Ascending Confidence Reward for LLM Reasoning](https://arxiv.org/abs/2510.22255)
*Eunseop Yoon,Hee Suk Yoon,Jaehyun Jang,SooHwan Eom,Qi Dai,Chong Luo,Mark A. Hasegawa-Johnson,Chang D. Yoo*

Main category: cs.AI

TL;DR: 该论文提出了一种名为PACR（Progressively Ascending Confidence Reward）的密集型、模型内在奖励机制，用于解决RLVR（Reinforcement Learning with Verifiable Rewards）中稀疏奖励导致探索缓慢的问题。PACR通过编码正确答案概率的上升趋势作为归纳偏置，显著加速了LLM的推理探索，并提高了多个基准测试的性能，使RLVR训练更有效和可靠。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR（Reinforcement Learning with Verifiable Rewards）在改进大型语言模型（LLM）推理方面表现显著，但其奖励机制存在明显局限性。具体而言，RLVR采用的是稀疏的、基于最终结果的奖励，这意味着模型只在完成整个推理过程并达到最终结果时才能获得奖励，而中间步骤则没有任何指导信号。这种稀疏性导致了探索过程的缓慢，因为模型难以在庞大的搜索空间中有效学习如何改进中间推理步骤。因此，研究的动机在于开发一种能够为中间推理步骤提供更丰富、更及时指导的奖励机制，以加速学习过程并提高RLVR训练的效率和可靠性。

Method: 本研究提出了一种名为“逐步上升置信度奖励”（Progressively Ascending Confidence Reward, PACR）的新型奖励机制。与传统的稀疏奖励不同，PACR是一种密集的、模型内在的奖励。它的计算方式是直接基于模型对正确答案不断演变的置信度。PACR编码了一种归纳偏置，即在一个结构良好的推理轨迹中，真实答案的概率应呈现普遍上升的趋势。这种归纳偏置通过限制探索搜索空间到逻辑上更合理的区域，从而加速了探索过程。研究通过实证和理论分析来验证了这种归纳偏置的有效性。

Result: 实验结果表明，PACR在多个方面取得了显著改进。首先，它显著加速了探索过程，使得模型能够更快地找到有效的推理路径。其次，PACR能够用更少的轨迹达到奖励饱和，这意味着训练效率得到了提升。最后，PACR在多个基准测试中都表现出了性能上的提升。这些发现共同表明，密集的、模型内在的塑形信号能够使RLVR训练变得更加有效和可靠。

Conclusion: 本研究提出并验证了PACR（Progressively Ascending Confidence Reward）这一新型密集型、模型内在奖励机制在改进RLVR训练中的有效性。通过将正确答案概率的上升趋势编码为归纳偏置，PACR克服了传统稀疏奖励的局限性，显著加速了大型语言模型（LLM）推理的探索过程，并用更少的训练轨迹达到了奖励饱和，最终在多个基准测试中实现了性能提升。研究结果强调了密集、模型内在的塑形信号在提升RLVR训练效率和可靠性方面的重要作用。未来的工作可以进一步探索其他形式的密集型、模型内在信号，以期在LLM推理和其他领域取得更大的突破。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly
improved LLM reasoning, but its sparse, outcome-based reward provides no
guidance for intermediate steps, slowing exploration. We propose Progressively
Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed
directly from the model's evolving belief in the correct answer. PACR encodes
the inductive bias that, along a well-formed reasoning trajectory, the
probability of the ground-truth answer should have a generally ascending trend.
We provide empirical and theoretical analysis validating that such an inductive
bias constrains the exploration search space to regions richer in logically
sound reasoning. We demonstrate that PACR accelerates exploration, reaches
reward saturation with fewer trajectories, and yields improvements on multiple
benchmarks. Our results suggest that dense, model-intrinsic shaping signals can
make RLVR training more effective and reliable.

</details>


### [24] [VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription](https://arxiv.org/abs/2510.22295)
*Quoc Anh Nguyen,Bernard Cheng,Kelvin Soh*

Main category: cs.AI

TL;DR: 该研究针对越南语自动歌词转录（ALT）的挑战，首次构建了大规模越南语ALT数据集（VietLyrics），并基于此数据集对Whisper模型进行微调，取得了优于现有系统的性能，为低资源语言的ALT研究提供了新的方法和资源。


<details>
  <summary>Details</summary>
Motivation: 越南语自动歌词转录（ALT）因其声调复杂性和方言差异而面临独特的挑战，且由于缺乏专门的数据集，该领域的研究尚不充分。现有的自动语音识别（ASR）方法在处理越南语ALT时表现出显著的局限性，包括频繁的转录错误和非人声片段的幻觉。因此，创建一个专门的、大规模的越南语ALT数据集以推动该领域的研究和改进现有技术是十分必要的。

Method: 为了解决越南语ALT的数据稀缺问题，研究团队首先构建了首个大规模越南语ALT数据集——VietLyrics。该数据集包含647小时的歌曲，并附有行级别对齐的歌词和元数据。在此基础上，研究评估了当前基于ASR的方法在越南语ALT任务上的表现，揭示了它们的局限性。随后，研究将Whisper模型在VietLyrics数据集上进行了微调，以期提升转录性能。

Result: 对现有ASR方法的评估揭示了其在越南语ALT上的显著局限性，包括频繁的转录错误以及在非人声片段中出现的幻觉。然而，经过在VietLyrics数据集上微调的Whisper模型表现出卓越的性能，优于包括LyricWhiz在内的现有多语言ALT系统。研究公开了VietLyrics数据集和微调后的模型。

Conclusion: 该研究通过构建首个大规模越南语ALT数据集（VietLyrics）并成功微调Whisper模型，显著推进了越南语音乐计算研究。它不仅解决了低资源语言ALT面临的数据稀缺问题，还展示了该方法在处理低资源语言和音乐的ALT任务中的巨大潜力。公开发布数据集和模型将为未来的研究提供宝贵资源，并有望启发其他低资源语言ALT领域的发展。

Abstract: Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique
challenges due to its tonal complexity and dialectal variations, but remains
largely unexplored due to the lack of a dedicated dataset. Therefore, we
curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising
647 hours of songs with line-level aligned lyrics and metadata to address these
issues. Our evaluation of current ASRbased approaches reveal significant
limitations, including frequent transcription errors and hallucinations in
non-vocal segments. To improve performance, we fine-tuned Whisper models on the
VietLyrics dataset, achieving superior results compared to existing
multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics
and our models, aiming to advance Vietnamese music computing research while
demonstrating the potential of this approach for ALT in low-resource language
and music.

</details>


### [25] [Graph-Coarsening Approach for the Capacitated Vehicle Routing Problem with Time Windows](https://arxiv.org/abs/2510.22329)
*Mustafa Mert Özyılmaz*

Main category: cs.AI

TL;DR: 本文提出了一种多级图粗化和细化框架来解决带时间窗的车辆路径问题（CVRPTW）。该框架通过时空距离度量将客户聚合成元节点，在降低问题规模的同时，利用经典启发式算法求解，并通过可行性修正扩展回原始空间。初步实验表明，该方法在保持或提高解质量的同时显著减少了计算时间，并探讨了量子启发式优化技术的潜力。


<details>
  <summary>Details</summary>
Motivation: 带时间窗的车辆路径问题（CVRPTW）是物流领域一个基本的NP-hard优化问题。对于大规模实例，精确求解器面临巨大的计算挑战。因此，开发能够有效解决大规模CVRPTW实例的计算方法具有重要的研究和实际意义。

Method: 本研究引入了一个多级图粗化和细化框架。该框架利用时空距离度量将客户聚合为元节点，从而对原始图进行粗化处理，形成一个简化的问题。简化的（粗化后的）问题使用经典的启发式算法进行求解。随后，将获得的解扩展回原始空间，并进行可行性修正以确保解的有效性。此外，本文还探讨了将量子启发式优化技术集成到该框架中的可能性，以期进一步加速大规模车辆路径任务的求解。

Result: 在Solomon基准实例上的初步实验结果表明，所提出的方法在保持或提高解决方案质量（特别是在容量和时间窗约束方面）的同时，显著减少了计算时间。这表明该框架在处理大规模CVRPTW问题上具有较高的效率和有效性。量子启发式优化技术的探索也为未来进一步加速大规模车辆路径任务提供了潜在方向。

Conclusion: 本研究提出的多级图粗化和细化框架为解决大规模带时间窗的车辆路径问题（CVRPTW）提供了一种有效的方法。该框架通过创新的粗化策略和后处理机制，在计算效率和解决方案质量之间取得了良好的平衡。未来的工作将侧重于深入整合量子启发式优化技术，以期实现更显著的性能提升，并进一步探索其在其他复杂优化问题中的应用潜力。

Abstract: The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a
fundamental NP-hard optimization problem in logistics. Solving large-scale
instances remains computationally challenging for exact solvers. This work
introduces a multilevel graph coarsening and refinement framework that
aggregates customers into meta-nodes using a spatio-temporal distance metric.
The reduced problem is solved with classical heuristics and subsequently
expanded back into the original space with feasibility corrections. Preliminary
experiments on Solomon benchmark instances show that the proposed method
reduces computation time while preserving or improving solution quality,
particularly with respect to capacity and time window constraints. The paper
also explores the integration of quantum-inspired optimization techniques,
highlighting their potential to further accelerate large-scale vehicle routing
tasks.

</details>


### [26] [LIFT: Interpretable truck driving risk prediction with literature-informed fine-tuned LLMs](https://arxiv.org/abs/2510.22333)
*Xiao Hu,Yuansheng Lian,Ke Zhang,Yunxuan Li,Yuelong Su,Meng Li*

Main category: cs.AI

TL;DR: 本研究提出了一个结合文献信息进行微调（LIFT）的LLMs可解释性预测框架，用于卡车驾驶风险预测。该框架在真实世界数据上进行微调后，实现了高精度预测，并在解释性方面表现出色，其变量重要性排序与基准模型一致，并通过文献知识库增强了解释的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 卡车驾驶风险预测是一个关键领域，需要高精度和可解释性的模型。现有模型可能缺乏足够的解释能力，难以理解预测背后的原因。本研究旨在通过整合领域文献知识，开发一个既能准确预测风险又能提供清晰解释的框架，从而提高风险管理的有效性和决策者的信任度。

Method: 本研究提出了一个LIFT LLM可解释性预测框架，主要包含三个核心组件：一个由LLM驱动的推理核心，负责预测和解释卡车驾驶风险；一个文献处理管道，负责筛选和总结领域特定文献以构建文献知识库；以及一个结果评估器，用于评估LIFT LLM的预测性能和可解释性。该LIFT LLM在真实世界的卡车驾驶风险数据集上进行了微调，并且其解释性通过从299篇领域论文自动构建的文献知识库进行引导。

Result: LIFT LLM在召回率上比基准模型高出26.7%，在F1-score上高出10.1%，实现了准确的风险预测。此外，在文献知识库的指导下，LIFT LLM生成的变量重要性排序与基准模型的结果一致，并表现出对不同数据抽样条件的解释鲁棒性。LIFT LLM还通过检测卡车驾驶风险中的关键变量组合来识别潜在的风险情景，并通过PERMANOVA测试进行了验证。

Conclusion: 本研究展示了文献知识库和微调过程对LIFT LLM可解释性的重要贡献，并讨论了LIFT LLM在数据驱动知识发现方面的潜力。LIFT LLM提供了一个有效且可解释的卡车驾驶风险预测方案，有望在实际风险管理中发挥重要作用。

Abstract: This study proposes an interpretable prediction framework with
literature-informed fine-tuned (LIFT) LLMs for truck driving risk prediction.
The framework integrates an LLM-driven Inference Core that predicts and
explains truck driving risk, a Literature Processing Pipeline that filters and
summarizes domain-specific literature into a literature knowledge base, and a
Result Evaluator that evaluates the prediction performance as well as the
interpretability of the LIFT LLM. After fine-tuning on a real-world truck
driving risk dataset, the LIFT LLM achieved accurate risk prediction,
outperforming benchmark models by 26.7% in recall and 10.1% in F1-score.
Furthermore, guided by the literature knowledge base automatically constructed
from 299 domain papers, the LIFT LLM produced variable importance ranking
consistent with that derived from the benchmark model, while demonstrating
robustness in interpretation results to various data sampling conditions. The
LIFT LLM also identified potential risky scenarios by detecting key combination
of variables in truck driving risk, which were verified by PERMANOVA tests.
Finally, we demonstrated the contribution of the literature knowledge base and
the fine-tuning process in the interpretability of the LIFT LLM, and discussed
the potential of the LIFT LLM in data-driven knowledge discovery.

</details>


### [27] [DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry](https://arxiv.org/abs/2510.22340)
*Changti Wu,Shijie Lian,Zihao Liu,Lei Zhang,Laurence Tianruo Yang,Kai Chen*

Main category: cs.AI

TL;DR: 论文引入了DynaSolidGeo，首个用于评估视觉语言模型（VLMs）在三维立体几何中真实空间推理能力的动态基准。该基准通过半自动注释流程构建，包含503个专家策划的种子问题，能动态生成无限多样的多模态实例，并除答案准确性外，还通过专家标注的推理链进行过程评估。实验发现，现有VLMs在动态设置下性能显著下降，在心理旋转和可视化等高级空间智能任务上表现不佳，存在巨大的性能差距。


<details>
  <summary>Details</summary>
Motivation: 立体几何问题解决需要整合空间智能和符号推理，但现有的大多数多模态数学推理基准主要关注二维平面几何，依赖易受数据污染和记忆影响的静态数据集，并且仅通过最终答案评估模型，忽略了推理过程。这些局限性阻碍了对视觉语言模型（VLMs）真实空间推理能力的评估。

Method: 本研究引入了DynaSolidGeo，这是一个为评估视觉语言模型（VLMs）真实空间推理能力而设计的首个动态基准。该基准通过半自动注释流程构建，包含503个由专家精心策划的种子问题。这些种子问题原则上能够动态生成无限数量的多样化多模态文本-视觉实例。除了评估传统的答案准确性，DynaSolidGeo还纳入了基于专家标注推理链的过程评估，以衡量逻辑有效性和因果连贯性。

Result: 对代表性的开源和闭源视觉语言模型（VLMs）进行的实验揭示了以下主要发现：模型之间存在巨大的性能差距；在动态设置下，模型性能出现严重退化；在需要高级空间智能的任务（例如心理旋转和可视化）上，模型的表现普遍较差。

Conclusion: DynaSolidGeo成功解决了现有基准在评估视觉语言模型（VLMs）空间推理能力方面的局限性，特别是在三维立体几何、动态数据生成和推理过程评估方面。研究结果明确指出，当前的VLMs在处理需要高水平空间智能的任务时，例如心理旋转和可视化，以及在动态环境下的推理能力方面存在显著不足。这为未来VLMs的空间推理能力研究和提升指明了方向。

Abstract: Solid geometry problem solving demands spatial mathematical reasoning that
integrates spatial intelligence and symbolic reasoning. However, most existing
multimodal mathematical reasoning benchmarks focus primarily on 2D plane
geometry, rely on static datasets prone to data contamination and memorization,
and evaluate models solely by final answers, overlooking the reasoning process.
To address these limitations, we introduce DynaSolidGeo, the first dynamic
benchmark for evaluating genuine spatial reasoning in Vision-Language Models
(VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo
contains 503 expert-curated seed questions that can, in principle, dynamically
generate an unbounded number of diverse multimodal text-visual instances.
Beyond answer accuracy, we incorporate process evaluation based on
expert-annotated reasoning chains to measure logical validity and causal
coherence. Experiments across representative open-source and closed-source VLMs
reveal large performance gaps, severe degradation in dynamic settings, and poor
performance on tasks requiring high-level spatial intelligence, such as mental
rotation and visualization. The code and dataset are available at
\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.

</details>


### [28] [Reasoning Models Reason Well, Until They Don't](https://arxiv.org/abs/2510.22371)
*Revanth Rameshkumar,Jimson Huang,Yunxin Sun,Fei Xia,Abulhair Saparov*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在推理任务上表现出进步，但面对复杂问题时会失败。尽管经过微调的推理模型（LRMs）在现有基准上表现出色，本文通过引入新的“深度推理数据集”（DeepRD）发现，这些基准的复杂度有限。实验表明，LRMs的性能在复杂度提高时急剧下降，不具备泛化能力。研究还发现，大多数真实世界应用场景的复杂度在LRMs的成功范围内，但长尾效应揭示了巨大的潜在失败风险。这强调了开发能泛化超越训练分布复杂度的新方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理任务中取得了显著进展，然而，当前研究指出，当推理问题的复杂度适度增加时，Transformer模型和LLMs会出现灾难性的失败。与此同时，大型推理模型（LRMs）——即通过激励分步论证和自验证进行微调的LLMs——在如NLGraph等图和推理基准上展现出非凡的性能，甚至有观点认为它们在数学、物理、医学和法律等推理密集型领域具备通用推理和创新能力。本研究的动机在于，通过更仔细地扩展推理问题的复杂度，重新审视这些令人鼓舞但可能存在局限性的发现。我们旨在深入探究现有基准的真实复杂度限制，并评估LRMs在面对更高复杂度问题时的实际能力和泛化性，从而揭示它们的潜在优势与不足。

Method: 本研究首先从大型推理模型（LRMs）的角度重新审视了大型语言模型在推理任务中面对适度复杂性问题时失效的现象。为了克服现有基准的复杂度限制，我们开发了一个名为“深度推理数据集”（DeepRD）的新数据集。DeepRD的独特之处在于其生成过程，该过程能够产生无限量的、复杂度可扩展的推理示例。我们利用这个新数据集来评估模型在图连通性（graph connectivity）和自然语言证明规划（natural language proof planning）这两种关键推理任务上的性能。此外，我们还将LRMs的实验结果与大型真实世界知识图谱、交互图谱和证明数据集的复杂度分布进行了关联分析，以理解LRMs在实际应用场景中的潜在表现和局限性。

Result: 本研究的关键发现包括：首先，通过对推理问题复杂度进行更细致的扩展，我们发现现有的推理基准实际上具有有限的复杂度。其次，使用我们新开发的DeepRD数据集进行评估，结果显示大型推理模型（LRMs）的性能在达到足够的复杂度时会急剧下降，并且不具备泛化能力。这与现有基准上所宣称的“非凡”表现形成鲜明对比。最后，我们将LRM的性能结果与大型真实世界知识图谱、交互图谱和证明数据集的复杂度分布进行关联分析，结果表明大多数真实世界的例子确实落在LRMs的成功范围内，然而，数据中的“长尾”现象暴露出巨大的潜在失败可能性。

Conclusion: 本研究的结论凸显了大型推理模型（LRMs）在现有基准和大部分真实世界场景中的近期实用性。然而，我们的分析也明确指出，当推理问题的复杂度达到一定程度时，LRMs的性能会急剧下降且不具备泛化能力。这表明现有基准的复杂度不足以全面评估LRMs的真实推理能力。尽管LRMs在当前多数实际应用中表现良好，但真实世界数据中的“长尾”复杂性部分揭示了它们在处理极端复杂问题时的巨大潜在失败风险。因此，本研究强调了开发新的方法的重要性，这些方法必须能够超越训练数据中例子的复杂度限制，从而实现更强的泛化能力和更鲁棒的推理性能。

Abstract: Large language models (LLMs) have shown significant progress in reasoning
tasks. However, recent studies show that transformers and LLMs fail
catastrophically once reasoning problems exceed modest complexity. We revisit
these findings through the lens of large reasoning models (LRMs) -- LLMs
fine-tuned with incentives for step-by-step argumentation and
self-verification. LRM performance on graph and reasoning benchmarks such as
NLGraph seem extraordinary, with some even claiming they are capable of
generalized reasoning and innovation in reasoning-intensive fields such as
mathematics, physics, medicine, and law. However, by more carefully scaling the
complexity of reasoning problems, we show existing benchmarks actually have
limited complexity. We develop a new dataset, the Deep Reasoning Dataset
(DeepRD), along with a generative process for producing unlimited examples of
scalable complexity. We use this dataset to evaluate model performance on graph
connectivity and natural language proof planning. We find that the performance
of LRMs drop abruptly at sufficient complexity and do not generalize. We also
relate our LRM results to the distributions of the complexities of large,
real-world knowledge graphs, interaction graphs, and proof datasets. We find
the majority of real-world examples fall inside the LRMs' success regime, yet
the long tails expose substantial failure potential. Our analysis highlights
the near-term utility of LRMs while underscoring the need for new methods that
generalize beyond the complexity of examples in the training distribution.

</details>


### [29] [OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models](https://arxiv.org/abs/2510.22535)
*Hao Zheng,Zirui Pang,Ling li,Zhijie Deng,Yuhan Pu,Zhaowei Zhu,Xiaobo Xia,Jiaheng Wei*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）的数据隐私问题日益突出，机器遗忘（MU）成为关键需求。然而，现有针对MLLMs的MU基准存在图像多样性不足、潜在不准确性和评估场景有限等问题。本文提出了OFFSIDE，一个基于足球转会谣言的新颖基准，用于评估MLLMs中的错误信息遗忘。该基准包含15.68K条记录，涵盖80名球员，并提供一个全面的框架，包括四个测试集，以评估遗忘效率、泛化能力、实用性和鲁棒性。OFFSIDE支持选择性遗忘、纠正性再学习和单模态遗忘等高级设置。对多个基线方法的广泛评估揭示了五大关键发现：(1) 单模态遗忘方法无法有效处理多模态谣言；(2) 遗忘效率主要由灾难性遗忘驱动；(3) 所有方法在处理“视觉谣言”（谣言出现在图像中）时都表现不佳；(4) 已遗忘的谣言容易被恢复；(5) 所有方法都容易受到提示攻击。这些结果暴露了当前方法的显著脆弱性，强调了开发更鲁棒的多模态遗忘解决方案的必要性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的飞速发展，在带来强大能力的同时，也加剧了人们对数据隐私的担忧。在此背景下，机器遗忘（Machine Unlearning, MU）——即选择性地从模型中移除已学习到的特定信息——变得至关重要。它能帮助模型在特定数据需要被遗忘（例如出于隐私法规、数据错误或撤销同意等原因）时，清除相关影响，维护用户权益和系统合规性。然而，现有针对MLLMs的机器遗忘基准存在显著局限性，具体表现在：
1. **图像多样性不足：** 现有基准所使用的图像数据往往过于单一或规模有限，无法充分模拟真实世界中多模态数据的高度复杂性和多样性，导致评估结果可能无法很好地泛化到实际应用场景。
2. **潜在不准确性：** 部分现有基准可能存在数据标注错误或不一致的情况，这会引入噪声，影响对遗忘算法性能的准确评估。
3. **评估场景不足：** 现有基准通常未能涵盖足够广泛和复杂的评估场景，例如未能充分测试遗忘对模型其他能力的影响（实用性）、遗忘的持久性（鲁棒性）或在不同模态交互下的遗忘效果，从而无法全面捕捉真实世界应用中机器遗忘的复杂性。
这些限制严重阻碍了MLLMs遗忘技术的研究和发展，使得开发出能够有效且可靠地移除多模态敏感信息的解决方案变得困难。因此，迫切需要一个更为全面、多样化且具有挑战性的新基准来推动该领域的研究进展。

Method: 为了促进多模态大语言模型（MLLMs）遗忘技术的发展并解决现有基准的局限性，本文引入了一个名为OFFSIDE的新颖基准，用于评估MLLMs中错误信息（特别是谣言）的遗忘能力。
1. **数据集构建：** OFFSIDE是一个经过人工精心策划的数据集，其内容聚焦于足球转会谣言。选择这一领域是因为它天然包含丰富的多模态信息（球员图像、文本描述、新闻报道等），且谣言性质使其成为测试错误信息遗忘的理想场景。该数据集包含15.68K条记录，涉及80名球员，提供了大量复杂且具有挑战性的遗忘实例。
2. **全面的评估框架：** OFFSIDE提供了一个综合性的评估框架，包含四个专门设计的测试集，用于全面评估遗忘方法的不同方面：
    *   **遗忘效率（Forgetting Efficacy）：** 衡量模型成功且彻底地忘记指定信息的能力。
    *   **泛化能力（Generalization）：** 评估遗忘操作在未直接用于遗忘训练的新数据上的效果，即遗忘是否能够泛化到相关但未见过的实例。
    *   **实用性（Utility）：** 考察遗忘操作对模型整体性能（例如在非遗忘任务上的表现）的影响，确保遗忘不会显著损害模型的原有能力。
    *   **鲁棒性（Robustness）：** 评估遗忘模型在面对各种攻击（如尝试恢复被遗忘信息）时的抵抗力，以及遗忘效果的持久性。
3. **支持高级遗忘设置：** OFFSIDE还支持更高级和更实际的遗忘场景，以模拟真实世界的复杂需求：
    *   **选择性遗忘（Selective Unlearning）：** 允许模型仅遗忘特定的、有针对性的信息，而非对模型进行大规模的重置或重新训练。
    *   **纠正性再学习（Corrective Relearning）：** 在遗忘错误信息之后，模型能够学习或重新学习正确的信息，以纠正其知识体系。
    *   **单模态遗忘（Unimodal Unlearning）：** 这是一个关键的探究性设置，允许仅遗忘文本数据（即仅从文本模态中移除知识，而图像模态保持不变），旨在深入分析不同模态遗忘的挑战和相互作用。
4. **实验评估：** 研究人员对多个现有基线遗忘方法在OFFSIDE基准上进行了广泛的评估，以识别这些方法的优势和局限性，并为未来更鲁棒的MLLMs遗忘解决方案提供指导。

Result: 通过在OFFSIDE基准上对多个现有基线遗忘方法进行广泛评估，本文揭示了当前多模态大语言模型（MLLMs）遗忘技术的以下五个关键发现，这些发现暴露了现有方法的显著脆弱性：
1. **单模态方法对多模态谣言的失效：** 研究发现，仅擦除基于文本知识的单模态遗忘方法，在处理多模态谣言时无法有效发挥作用。这意味着当谣言同时包含文本和图像信息时，仅仅移除文本信息不足以让模型彻底“忘记”谣言，图像信息在谣言传播和模型记忆中扮演着不可忽视的角色。
2. **遗忘效率主要由灾难性遗忘驱动：** 评估结果表明，当前遗忘方法的效率在很大程度上是由“灾难性遗忘”（Catastrophic Forgetting）现象驱动的。这意味着在努力遗忘特定信息时，模型往往会不加选择地遗忘其他相关或不相关的信息，而非进行精确且有针对性的移除，这限制了遗忘操作的精细度和实用性。
3. **所有方法在“视觉谣言”上表现不佳：** 对于那些谣言信息直接出现在图像中（即“视觉谣言”）的场景，所有被评估的遗忘方法都表现出显著的挣扎和不足。这突显了从视觉模态中精确移除错误信息的固有挑战，以及当前技术在此方面的欠缺。
4. **已遗忘的谣言容易被恢复：** 令人担忧的是，即使模型被告知已遗忘特定谣言，这些被遗忘的信息仍然可以相对容易地被恢复。这表明当前遗忘机制可能不够彻底或不够持久，遗忘痕迹可能以某种形式残留在模型中，容易被重新激活。
5. **所有方法都容易受到提示攻击：** 研究还发现，所有被评估的遗忘方法都容易受到“提示攻击”。这意味着通过精心设计和构造的提示，攻击者可能诱导模型重新暴露或生成那些本应被遗忘的敏感或错误信息。这严重威胁了遗忘机制的安全性。

Conclusion: 本文通过引入创新的OFFSIDE基准，为多模态大语言模型（MLLMs）的机器遗忘领域带来了深入的见解。研究结果明确揭示了当前MLLMs遗忘解决方案存在的重大漏洞和局限性。具体而言，单模态遗忘方法在处理包含视觉信息的复杂多模态谣言时的失效，表明了整合多模态遗忘策略的必要性；遗忘效率对灾难性遗忘的严重依赖，凸显了开发更精细、更具选择性遗忘算法的迫切需求。尤其值得关注的是，所有方法在应对“视觉谣言”时的普遍不足，暴露了在视觉模态中实现有效遗忘的巨大技术挑战。此外，被遗忘信息易于恢复以及模型对提示攻击的脆弱性，对当前遗忘机制的彻底性和鲁棒性提出了严峻质疑，并对数据隐私和安全性构成了潜在威胁。

这些发现共同强调了现有MLLMs遗忘方法在应对真实世界复杂场景和安全挑战时的不足。因此，未来的研究工作必须聚焦于开发更强大、更鲁棒的多模态遗忘解决方案。这包括但不限于：设计能够精确、持久地从文本和图像模态中移除特定信息的算法；开发能够抵抗各种恢复攻击和提示攻击的遗忘机制；以及探索新的评估指标和场景，以更全面地衡量遗忘的彻底性、效率和安全性。只有这样，我们才能确保MLLMs能够负责任地处理敏感信息，并在广泛应用中维护用户信任和数据隐私。

Abstract: Advances in Multimodal Large Language Models (MLLMs) intensify concerns about
data privacy, making Machine Unlearning (MU), the selective removal of learned
information, a critical necessity. However, existing MU benchmarks for MLLMs
are limited by a lack of image diversity, potential inaccuracies, and
insufficient evaluation scenarios, which fail to capture the complexity of
real-world applications. To facilitate the development of MLLMs unlearning and
alleviate the aforementioned limitations, we introduce OFFSIDE, a novel
benchmark for evaluating misinformation unlearning in MLLMs based on football
transfer rumors. This manually curated dataset contains 15.68K records for 80
players, providing a comprehensive framework with four test sets to assess
forgetting efficacy, generalization, utility, and robustness. OFFSIDE supports
advanced settings like selective unlearning and corrective relearning, and
crucially, unimodal unlearning (forgetting only text data). Our extensive
evaluation of multiple baselines reveals key findings: (1) Unimodal methods
(erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning
efficacy is largely driven by catastrophic forgetting; (3) All methods struggle
with "visual rumors" (rumors appear in the image); (4) The unlearned rumors can
be easily recovered and (5) All methods are vulnerable to prompt attacks. These
results expose significant vulnerabilities in current approaches, highlighting
the need for more robust multimodal unlearning solutions. The code is available
at
\href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.

</details>


### [30] [ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs](https://arxiv.org/abs/2510.22590)
*Yassir Lairgi,Ludovic Moncla,Khalid Benabdeslem,Rémy Cazabet,Pierre Cléau*

Main category: cs.AI

TL;DR: ATOM是一种小样本、可扩展的方法，用于从非结构化文本构建和持续更新时态知识图谱（TKGs）。它通过将文档分解为“原子”事实，并采用区分信息观察时间与有效时间的双时间建模，显著提高了知识提取的详尽性、稳定性和效率，相较于基线方法，在详尽性、稳定性方面分别提升约18%和17%，延迟降低超过90%。


<details>
  <summary>Details</summary>
Motivation: 在当今数据爆炸式增长的时代，从非结构化文本中进行知识提取对于实时分析、时态推理和动态记忆框架至关重要。然而，传统的静态知识图谱（KG）构建方法往往忽略了真实世界数据的动态和时间敏感性，限制了其对持续变化的适应性。此外，近期避免领域特定微调或依赖预构建本体的零/小样本方法，在多次运行中表现出不稳定，并且关键事实覆盖不完整。

Method: 我们引入了ATOM（AdapTive and OptiMized），这是一种小样本、可扩展的方法，用于从非结构化文本构建并持续更新时态知识图谱（TKGs）。ATOM将输入文档拆分成最小的、自包含的“原子”事实，从而提高了提取的详尽性和稳定性。随后，它利用这些事实构建原子TKGs，并采用一种双时间建模，区分信息的观察时间（when information is observed）和有效时间（when it is valid）。最终，这些生成的原子TKGs并行进行合并。

Result: 实证评估表明，与基线方法相比，ATOM在详尽性方面提高了约18%，稳定性提高了约17%，并且延迟降低了90%以上。这充分展示了ATOM在动态TKG构建方面强大的可扩展性潜力。

Conclusion: ATOM有效解决了从非结构化文本中动态构建和更新时态知识图谱所面临的挑战。通过其创新的“原子”事实拆分和双时间建模方法，ATOM显著提升了知识提取的详尽性、稳定性和处理效率，为实时分析和动态知识管理提供了强大的支持。

Abstract: In today's rapidly expanding data landscape, knowledge extraction from
unstructured text is vital for real-time analytics, temporal inference, and
dynamic memory frameworks. However, traditional static knowledge graph (KG)
construction often overlooks the dynamic and time-sensitive nature of
real-world data, limiting adaptability to continuous changes. Moreover, recent
zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance
on prebuilt ontologies often suffer from instability across multiple runs, as
well as incomplete coverage of key facts. To address these challenges, we
introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that
builds and continuously updates Temporal Knowledge Graphs (TKGs) from
unstructured texts. ATOM splits input documents into minimal, self-contained
"atomic" facts, improving extraction exhaustivity and stability. Then, it
constructs atomic TKGs from these facts while employing a dual-time modeling
that distinguishes when information is observed from when it is valid. The
resulting atomic TKGs are subsequently merged in parallel. Empirical
evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%
better stability, and over 90% latency reduction compared to baseline methods,
demonstrating a strong scalability potential for dynamic TKG construction.

</details>


### [31] [CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation](https://arxiv.org/abs/2510.22609)
*Md. Mehedi Hasan,Rafid Mostafiz,Md. Abir Hossain,Bikash Kumar Paul*

Main category: cs.AI

TL;DR: CLIN-LLM是一个安全性受限的混合型医疗AI系统，它结合多模态患者编码、不确定性校准的疾病分类和检索增强的治疗生成。该系统在症状到疾病分类上达到了98%的准确率和F1分数，并显著减少了不安全的抗生素建议，提供了一个可部署的、人机协作的决策支持框架，尤其适用于资源有限的医疗环境。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型（LLM）的系统在医疗诊断和治疗推荐方面面临挑战，特别是在异质性患者群体和高诊断风险情况下。这些系统常缺乏医学基础且未能量化不确定性，导致输出不安全。因此，迫切需要开发一个既能提高诊断准确性又能确保治疗建议临床安全性的系统。

Method: CLIN-LLM采用混合管道设计，整合了多模态患者编码、不确定性校准的疾病分类以及检索增强的治疗生成。在疾病分类方面，该框架使用BioBERT在Symptom2Disease数据集的1200个临床病例上进行微调，并结合焦点损失（Focal Loss）和蒙特卡洛Dropout（Monte Carlo Dropout）以实现基于自由文本症状和结构化生命体征的置信度感知预测。对于低确定性的病例（占18%），系统会自动标记以供专家审查，确保人工监督。在治疗生成方面，CLIN-LLM利用Biomedical Sentence-BERT从包含26万个样本的MedDialog语料库中检索Top-k相关对话。检索到的证据和患者背景信息被输入到一个微调的FLAN-T5模型中，用于个性化治疗生成。随后，通过RxNorm进行后处理，以实现抗生素管理和药物相互作用（DDI）筛选。

Result: CLIN-LLM在症状到疾病分类中实现了98%的准确率和F1分数，比ClinicalBERT的性能高出7.1%（p < 0.001）。在治疗生成方面，其Top-5检索精度达到78%，临床医生评定的有效性为4.2分（满分5分）。与GPT-5相比，不安全的抗生素建议减少了67%。这些结果共同证明了CLIN-LLM的鲁棒性、可解释性和临床安全性一致性。

Conclusion: CLIN-LLM提供了一个可部署的、人机协作的决策支持框架，特别适用于资源有限的医疗环境，显著提升了医疗诊断和治疗推荐的准确性和安全性。未来的工作将包括整合影像和实验室数据、扩展多语言支持以及进行临床试验验证。

Abstract: Accurate symptom-to-disease classification and clinically grounded treatment
recommendations remain challenging, particularly in heterogeneous patient
settings with high diagnostic risk. Existing large language model (LLM)-based
systems often lack medical grounding and fail to quantify uncertainty,
resulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid
pipeline that integrates multimodal patient encoding, uncertainty-calibrated
disease classification, and retrieval-augmented treatment generation. The
framework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease
dataset and incorporates Focal Loss with Monte Carlo Dropout to enable
confidence-aware predictions from free-text symptoms and structured vitals.
Low-certainty cases (18%) are automatically flagged for expert review, ensuring
human oversight. For treatment generation, CLIN-LLM employs Biomedical
Sentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample
MedDialog corpus. The retrieved evidence and patient context are fed into a
fine-tuned FLAN-T5 model for personalized treatment generation, followed by
post-processing with RxNorm for antibiotic stewardship and drug-drug
interaction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score,
outperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval
precision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic
suggestions are reduced by 67% compared to GPT-5. These results demonstrate
CLIN-LLM's robustness, interpretability, and clinical safety alignment. The
proposed system provides a deployable, human-in-the-loop decision support
framework for resource-limited healthcare environments. Future work includes
integrating imaging and lab data, multilingual extensions, and clinical trial
validation.

</details>


### [32] [Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration](https://arxiv.org/abs/2510.22679)
*Yuval Kainan,Shaked Zychlinski*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在生成样板式回复（如拒绝、确认和问候）时会消耗大量计算资源。本文提出一种简单高效的方法，仅通过第一个生成token的对数概率分布，就能在早期阶段检测出此类回复。实验表明，该方法能以高准确率区分实质性回答和样板回复，从而实现早期终止或重定向，显著降低计算成本，提升LLM部署效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理用户请求时，经常会生成诸如拒绝、简单确认或问候语等样板式回复。这些回复虽然看似简单，但其生成过程依然会消耗大量的计算资源，包括计算成本和响应延迟。这种不必要的资源消耗显著降低了LLMs的运行效率和经济性，尤其是在大规模部署场景下，积少成多会造成巨大的浪费。因此，研究如何有效地识别并避免生成这些不必要的样板式回复，是优化LLM推理过程、提高其部署可持续性的一个重要问题。

Method: 本研究提出了一种创新且高效的方法，用于在LLM生成回复的早期阶段识别样板式内容。该方法的核心思想是利用**第一个生成token的对数概率分布**作为判别信号。具体而言，模型在生成第一个token后，其对数概率分布被提取出来。研究发现，对于不同类型的回复（例如，实质性回答与各种样板式回复，包括用户指定的拒绝），这些第一个token的对数概率向量会形成**明显可分离的聚类**。基于这一发现，研究人员采用了一个**轻量级的k-NN分类器**来对这些对数概率向量进行分类。实验在涵盖小型、大型以及推理专业化模型等多种LLM上进行，验证了该方法的普适性和有效性。

Result: 实验结果表明，该方法在预测回复是实质性回答还是样板式回复（包括用户自定义的拒绝）方面，取得了**高准确率**。这证明了第一个生成token的对数概率分布作为一个信号，在区分不同回复类型上具有强大的能力。最显著的成果是，这项技术提供了一个**实用且计算开销极小**的解决方案。通过在第一个token生成后就进行分类，系统能够实现**早期终止**不必要的样板回复生成，或者将请求**重定向到更小、更经济的模型**进行处理。这带来了**显著的计算成本节约**，并减少了延迟，从而优化了LLM的推理过程。

Conclusion: 本研究提出了一种基于第一个token对数概率分布的有效方法，能够在LLM生成早期阶段高准确率地识别样板式回复，从而显著节约计算资源并降低延迟。这一发现为LLM的推理优化提供了一条**直接且高效的途径**，对于实现**更高效、更可持续的LLM部署**具有重要意义。该技术的计算开销极低，易于实施，有望在实际应用中带来巨大的经济和性能效益。未来工作可以探索将其应用于更复杂的回复类型分类或与其他优化技术结合。

Abstract: Large Language Models (LLMs) often expend significant computational resources
generating boilerplate responses, such as refusals, simple acknowledgements and
casual greetings, which adds unnecessary cost and latency. To address this
inefficiency, we propose a simple yet highly effective method for detecting
such responses after only a single generation step. We demonstrate that the
log-probability distribution of the first generated token serves as a powerful
signal for classifying the nature of the entire subsequent response. Our
experiments, conducted across a diverse range of small, large, and
reasoning-specialized models, show that the first-token log-probability vectors
form distinctly separable clusters for different response types. Using a
lightweight k-NN classifier, we achieve high accuracy in predicting whether a
response will be a substantive answer or a form of boilerplate response,
including user-specified refusals. The primary implication is a practical,
computationally trivial technique, optimizing LLM inference by enabling early
termination or redirection to a smaller model, thereby yielding significant
savings in computational cost. This work presents a direct path toward more
efficient and sustainable LLM deployment.

</details>


### [33] [Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring](https://arxiv.org/abs/2510.22702)
*Mithul Chander,Sai Pragnya Ranga,Prathamesh Mayekar*

Main category: cs.AI

TL;DR: 本文提出了Atlas城市指数（AUI），一种利用Sentinel-2卫星图像和视觉-语言模型（VLMs）衡量城市发展的新指标。AUI通过处理云量最小的图像并结合参考图像和历史图像来克服传统方法（如NDBI）受大气噪声、季节变化和云层影响的局限性，从而提供更可靠、更稳定的发展得分。在班加罗尔的定性实验表明，AUI优于NDBI等标准指数。


<details>
  <summary>Details</summary>
Motivation: 现有城市发展测量方法，如归一化差异建成指数（NDBI），在准确捕捉城市发展方面面临挑战，原因包括大气噪声、季节性变化和云层覆盖。这些局限性阻碍了对人类发展和城市化的大规模监测，因此需要一种新的、更稳健的方法来克服这些难题，提供更准确、更稳定的城市发展评估。

Method: 本文提出了Atlas城市指数（AUI），该方法利用Sentinel-2卫星图像和视觉-语言模型（VLMs）来为区域提供发展得分。具体步骤包括：1. 为每个区域收集Sentinel-2图像的时间序列。2. 在固定的时间窗口内进一步处理图像，以获得云量最小的图像作为该时间窗口的代表性图像。3. 采用两种策略确保评分一致性：(i) 向VLM提供一组经过精心策划的、代表不同城市化水平的参考图像；(ii) 提供最近的过去图像，以锚定时间一致性并减轻当前图像中与云相关的噪声。这些组件协同工作，使AUI能够克服传统城市化指数的挑战。

Result: 在班加罗尔进行的定性实验表明，Atlas城市指数（AUI）在衡量城市发展方面优于归一化差异建成指数（NDBI）等标准指数。这表明AUI能够提供更准确、更稳定的发展得分。

Conclusion: Atlas城市指数（AUI）通过结合Sentinel-2卫星图像和视觉-语言模型（VLMs），有效克服了传统城市化指数（如NDBI）面临的大气噪声、季节变化和云层覆盖等挑战。AUI利用处理后的最小云量图像、参考图像和历史图像，成功地生成了更可靠、更稳定的城市发展得分。在班加罗尔的定性实验验证了AUI优于标准指数的性能，证明了其在城市发展监测领域的潜力。未来的工作可以包括在更多地区进行定量评估和进一步优化模型。

Abstract: We introduce the {\em Atlas Urban Index} (AUI), a metric for measuring urban
development computed using Sentinel-2 \citep{spoto2012sentinel2} satellite
imagery. Existing approaches, such as the {\em Normalized Difference Built-up
Index} (NDBI), often struggle to accurately capture urban development due to
factors like atmospheric noise, seasonal variation, and cloud cover. These
limitations hinder large-scale monitoring of human development and
urbanization. To address these challenges, we propose an approach that
leverages {\em Vision-Language Models }(VLMs) to provide a development score
for regions. Specifically, we collect a time series of Sentinel-2 images for
each region. Then, we further process the images within fixed time windows to
get an image with minimal cloud cover, which serves as the representative image
for that time window. To ensure consistent scoring, we adopt two strategies:
(i) providing the VLM with a curated set of reference images representing
different levels of urbanization, and (ii) supplying the most recent past image
to both anchor temporal consistency and mitigate cloud-related noise in the
current image. Together, these components enable AUI to overcome the challenges
of traditional urbanization indices and produce more reliable and stable
development scores. Our qualitative experiments on Bangalore suggest that AUI
outperforms standard indices such as NDBI.

</details>


### [34] [RaCoT: Plug-and-Play Contrastive Example Generation Mechanism for Enhanced LLM Reasoning Reliability](https://arxiv.org/abs/2510.22710)
*Kaitong Cai,Jusheng Zhang,Yijia Fan,Jing Yang,Keze Wang*

Main category: cs.AI

TL;DR: RaCoT框架通过在预检索阶段引入对比思维来解决RAG中长尾查询的检索噪声问题。它生成对比问题并提取Δ-Prompt以关注关键差异，从而抑制语义干扰。在多个基准测试中，RaCoT超越了现有基线，展现出卓越的鲁棒性和效率，将RAG范式从“事后上下文清理”转变为“先验判别推理塑造”。


<details>
  <summary>Details</summary>
Motivation: RAG系统在处理知识稀疏和语义模糊的长尾查询时面临核心瓶颈，导致检索噪声干扰推理过程并需要昂贵的后处理。现有的单向量查询难以同时编码需要关注和忽略的信息，这限制了其性能。

Method: RaCoT（Retrieval-aware Contrastive-of-Thought）框架将对比思维引入预检索阶段。它通过自动生成一个语义相邻但答案不同的对比问题，并提取一个Δ-Prompt来捕捉它们之间的关键差异。这种方法引导模型主动关注“决定答案分歧的关键细节”，从而在单次检索中抑制语义干扰，克服了单向量查询的理论瓶颈。

Result: 在包括PopQA和TriviaQA-unfiltered在内的六个权威基准测试中，RaCoT的性能比RankRAG和Self-RAG等强劲基线高出0.9-2.4个百分点。它展现出卓越的鲁棒性，在对抗性测试中性能下降仅为8.6%，远优于其他方法超过15%的退化。此外，RaCoT具有低延迟（3.12秒）和低token开销（11.54），使其处于准确性-效率的帕累托前沿。消融研究也验证了其每个组件的必要性。

Conclusion: RaCoT通过将RAG范式从“事后上下文清理”重构为“先验判别推理塑造”，提供了一种高效且鲁棒的途径，以实现面向实时、资源受限部署的可靠AI系统。

Abstract: Retrieval-Augmented Generation (RAG) faces a core bottleneck with
knowledge-sparse and semantically ambiguous long-tail queries, where retrieval
noise distorts reasoning and necessitates costly post-processing. To tackle
this, we propose RaCoT (Retrieval-aware Contrastive-of-Thought), a novel
framework that shifts contrastive thinking to the pre-retrieval stage. By
automatically generating a semantically adjacent yet differently answered
contrastive question and extracting a $\Delta$-Prompt to capture their key
differences, RaCoT guides the model to proactively focus on the ``critical
details that determine answer divergence." This approach allows it to suppress
semantic interference within a single retrieval pass, overcoming the
theoretical bottleneck of single-vector queries that struggle to simultaneously
encode signals for what to attend to and what to ignore. On six authoritative
benchmarks, including PopQA and TriviaQA-unfiltered, RaCoT outperforms strong
baselines like RankRAG and Self-RAG by 0.9-2.4 percentage points. It exhibits
superior robustness, with a performance drop of only 8.6\% in adversarial
tests, far surpassing the over 15\% degradation in other methods. Furthermore,
its low latency (3.12s) and token overhead (11.54) place it on the
accuracy-efficiency Pareto frontier, while ablation studies validate the
necessity of each component. Ultimately, RaCoT reframes the RAG paradigm from
``post-hoc context cleaning" to ``a priori shaping of discriminative
reasoning", offering an efficient and robust path toward reliable AI systems
for real-time, resource-constrained deployments.

</details>


### [35] [Critical Insights into Leading Conversational AI Models](https://arxiv.org/abs/2510.22729)
*Urja Kohli,Aditi Singh,Arun Sharma*

Main category: cs.AI

TL;DR: 本研究对Google的Gemini、High-Flyer的DeepSeek、Anthropic的Claude、OpenAI的GPT模型和Meta的LLaMA这五款领先的大型语言模型（LLMs）进行了性能、道德行为和可用性方面的比较。研究发现，每个模型都有其独特的优势，例如Claude擅长道德推理，Gemini在多模态能力和道德框架方面表现出色，DeepSeek在基于事实的推理上表现卓越，LLaMA适合开放应用，而ChatGPT则提供了注重使用的均衡性能。最终强调用户应根据各模型的优势来选择和利用它们。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）正在深刻改变商业软件、人们生活方式和各行各业的运作模式。随着Google、High-Flyer、Anthropic、OpenAI和Meta等公司不断推出更优异的LLMs，理解不同模型之间在性能、道德行为和可用性方面的差异变得至关重要。这些差异源于它们各自不同的设计理念，因此，对这些模型进行深入分析，以帮助用户根据具体需求选择最适合的工具，是当前研究的迫切需求和重要意义所在。

Method: 本研究采用对比分析方法，选取了五款领先的大型语言模型进行评估：Google的Gemini、High-Flyer的DeepSeek、Anthropic的Claude、OpenAI的GPT模型以及Meta的LLaMA。分析框架主要围绕三个关键因素展开：1. 性能和准确性；2. 伦理与偏见缓解；3. 可用性与集成。通过对这些因素的深入分析，旨在揭示不同模型之间的差异和各自的优势。

Result: 研究发现，各项模型在不同方面展现出独特的优势：
*   **Claude**：在道德推理方面表现出色。
*   **Gemini**：在多模态能力方面表现更佳，并拥有强大的伦理框架。
*   **DeepSeek**：在基于事实的推理方面表现卓越。
*   **LLaMA**：适用于开放应用场景。
*   **ChatGPT（GPT模型）**：提供了均衡的性能，并特别注重易用性。

Conclusion: 本研究得出结论，领先的大型语言模型在工作表现、易用性以及伦理处理方面存在显著差异。鉴于这些差异，用户在选择和部署LLMs时，应充分利用每个模型的特定优势，以最大化其应用价值。这意味着没有一个“一刀切”的最佳模型，而是需要根据具体任务和需求进行明智的选择。

Abstract: Big Language Models (LLMs) are changing the way businesses use software, the
way people live their lives and the way industries work. Companies like Google,
High-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial
to look at how each model is different in terms of performance, moral behaviour
and usability, as these differences are based on the different ideas that built
them. This study compares five top LLMs: Google's Gemini, High-Flyer's
DeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs
this by analysing three important factors: Performance and Accuracy, Ethics and
Bias Mitigation and Usability and Integration. It was found that Claude has
good moral reasoning, Gemini is better at multimodal capabilities and has
strong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA
is good for open applications and ChatGPT delivers balanced performance with a
focus on usage. It was concluded that these models are different in terms of
how well they work, how easy they are to use and how they treat people
ethically, making it a point that each model should be utilised by the user in
a way that makes the most of its strengths.

</details>


### [36] [Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval](https://arxiv.org/abs/2510.22765)
*Binxiao Xu,Junyu Feng,Ruichuan An,Yulin Luo,Shilin Yan,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

TL;DR: Jarvis是一个创新框架，通过个性化KV-Cache检索实现个性化AI助手，将用户特定信息存储在文本和视觉KV-Cache中，显著提高了视觉问答和纯文本任务的准确性，尤其是在处理细粒度局部细节时，为实现实用的个性化AI助手提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型(VLMs)虽然支持开放式感知和推理，并且一些模型已尝试适应个性化助手，但现有方法（如学习概念token或训练VLM使用用户特定信息）在生成准确的个性化回答方面存在困难。即使是ChatGPT等商业模型也支持个性化，但其准确性仍是挑战。这表明需要一种更有效的方法来确保个性化AI助手能够根据用户特定信息提供准确、细致的响应。

Method: Jarvis框架通过“个人KV-Cache检索”实现个性化AI助手。它将用户特定信息存储在文本和视觉token的KV-Cache中。具体来说，文本token是通过将用户信息概括为元数据创建的，而视觉token是通过从用户图像中提取独特的图像块生成的。当需要回答问题时，Jarvis首先从个人存储中检索相关的KV-Cache，并使用这些缓存来确保响应的准确性。此外，该研究还引入了一个基于相同独特图像块挖掘流程构建的细粒度基准，旨在强调基于细粒度用户特定信息的准确问答。

Result: Jarvis在提供更准确的响应方面表现出色，尤其是在响应依赖于特定局部细节的情况下。它在多个数据集的视觉问答(VQA)和纯文本任务中均取得了最先进的(state-of-the-art)结果。这表明Jarvis为实现个性化AI助手提供了一条实用的途径。

Conclusion: Jarvis框架通过其创新的个人KV-Cache检索机制，有效解决了现有个性化VLM在准确性方面的挑战。它在视觉问答和文本任务中均实现了最先进的性能，特别是在处理细粒度用户特定信息时表现出卓越的准确性，为开发实用且高度准确的个性化AI助手开辟了新的可能性。代码和数据集的发布将进一步推动该领域的研究。

Abstract: The rapid development of Vision-language models (VLMs) enables open-ended
perception and reasoning. Recent works have started to investigate how to adapt
general-purpose VLMs into personalized assistants. Even commercial models such
as ChatGPT now support model personalization by incorporating user-specific
information. However, existing methods either learn a set of concept tokens or
train a VLM to utilize user-specific information. However, both pipelines
struggle to generate accurate answers as personalized assistants. We introduce
Jarvis, an innovative framework for a personalized AI assistant through
personal KV-Cache retrieval, which stores user-specific information in the
KV-Caches of both textual and visual tokens. The textual tokens are created by
summarizing user information into metadata, while the visual tokens are
produced by extracting distinct image patches from the user's images. When
answering a question, Jarvis first retrieves related KV-Caches from personal
storage and uses them to ensure accuracy in responses. We also introduce a
fine-grained benchmark built with the same distinct image patch mining
pipeline, emphasizing accurate question answering based on fine-grained
user-specific information. Jarvis is capable of providing more accurate
responses, particularly when they depend on specific local details. Jarvis
achieves state-of-the-art results in both visual question answering and
text-only tasks across multiple datasets, indicating a practical path toward
personalized AI assistants. The code and dataset will be released.

</details>


### [37] [Agentic Meta-Orchestrator for Multi-task Copilots](https://arxiv.org/abs/2510.22781)
*Xiaofeng Zhu,Yunshen Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种名为Agentic Meta-orchestrator (AMO) 的智能体元编排器，旨在为Copilot服务中多任务和可扩展智能体的管理提供鲁棒的解决方案。AMO利用元学习（通过训练决策树模型）来决定最佳推理策略，并可提供自然语言和行动响应。通过Microsoft 365 (M365) 电子商务Copilot和代码合规性Copilot这两个生产用例，展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: Copilot套件作为各种智能体的通用入口点，需要一个强大的编排器来将用户提示中的任务准确分发给合适的智能体。随着新智能体的动态加入，以及智能体自身由语言模型、软件工程操作和知识库等多种能力支持，现有系统需要一个能够高效、智能地管理这些复杂性和可扩展性的解决方案，以确保任务的正确执行并提供多样化的响应。

Method: 本文提出Agentic Meta-orchestrator (AMO)，专门用于处理Copilot服务中的多任务和可扩展智能体。AMO的关键方法是利用元学习进行规划，具体实现为一个经过训练的决策树模型，该模型负责在不同的智能体或模型之间选择最佳的推理策略。AMO能够提供自然语言响应和实际行动响应。通过这种方式，AMO能够有效地将用户提示分发给正确的智能体。

Result: 本文通过两个实际生产用例展示了AMO的有效性。第一个是Microsoft 365 (M365) 电子商务Copilot，它能够向外部客户推广微软产品以促进销售，提供最新的产品信息，并连接到关系数据库和人工客户支持等多个智能体。第二个是代码合规性Copilot，它扫描内部DevOps代码以检测拉取请求(PR)中已知和新的合规性问题。这些用例证明了AMO在不同领域的实际应用能力和有效性。

Conclusion: 本文提出的Agentic Meta-orchestrator (AMO) 及其基于元学习的规划策略，在处理Copilot服务中的多任务和可扩展智能体方面表现出显著的有效性。通过在M365电子商务和代码合规性这两个生产环境中的成功应用，验证了AMO在实际复杂场景中分发任务和提供多样化响应的能力。抽象未提及具体局限性和未来工作。

Abstract: Microsoft Copilot suites serve as the universal entry point for various
agents skilled in handling important tasks, ranging from assisting a customer
with product purchases to detecting vulnerabilities in corporate programming
code. Each agent can be powered by language models, software engineering
operations, such as database retrieval, and internal \& external knowledge. The
repertoire of a copilot can expand dynamically with new agents. This requires a
robust orchestrator that can distribute tasks from user prompts to the right
agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for
handling multiple tasks and scalable agents in copilot services, which can
provide both natural language and action responses. We will also demonstrate
the planning that leverages meta-learning, i.e., a trained decision tree model
for deciding the best inference strategy among various agents/models. We
showcase the effectiveness of our AMO through two production use cases:
Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365
E-Commerce Copilot advertises Microsoft products to external customers to
promote sales success. The M365 E-Commerce Copilot provides up-to-date product
information and connects to multiple agents, such as relational databases and
human customer support. The code compliance copilot scans the internal DevOps
code to detect known and new compliance issues in pull requests (PR).

</details>


### [38] [Will Humanity Be Rendered Obsolete by AI?](https://arxiv.org/abs/2510.22814)
*Mohamed El Louadi,Emna Ben Romdhane*

Main category: cs.AI

TL;DR: 本文分析了人工智能（AI）对人类的生存风险，指出超智能AI可能因其压倒性的、不可控的、漠不关心的认知优势而非恶意，导致人类灭绝。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨人工智能（AI）从当前状态发展到超智能的潜在轨迹，并分析其对人类构成的生存风险。随着机器认知能力的指数级增长和假想智商的提升，理解并应对这种远远超越人类、本质上异质的智能所带来的伦理和生存影响变得至关重要。

Method: 本文借鉴了I.J. Good和Nick Bostrom的理论著作，并结合近期出版物（如《AI 2027》和《If Anyone Builds It, Everyone Dies》），探讨了通用人工智能（AGI）和超级智能。通过考虑机器指数级增长的认知能力和假想的智商，分析了这种远超人类、本质上异质的智能所带来的伦理和生存影响。

Result: 研究发现，人工智能的智能可能远远超越人类，并成为一种根本上陌生的存在。人类灭绝的风险可能并非源于AI的恶意，而是源于其不可控且漠不关心的认知优势。

Conclusion: 论文总结认为，人工智能对人类构成生存风险，其原因在于它可能发展出一种远远超越人类、本质上异质、不可控且漠不关心的认知能力，这可能导致人类灭绝。这强调了理解和应对超智能AI带来的深远伦理和生存挑战的紧迫性。

Abstract: This article analyzes the existential risks artificial intelligence (AI)
poses to humanity, tracing the trajectory from current AI to ultraintelligence.
Drawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent
publications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and
superintelligence. Considering machines' exponentially growing cognitive power
and hypothetical IQs, it addresses the ethical and existential implications of
an intelligence vastly exceeding humanity's, fundamentally alien. Human
extinction may result not from malice, but from uncontrollable, indifferent
cognitive superiority.

</details>


### [39] [HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning](https://arxiv.org/abs/2510.22832)
*Long H Dang,David Rawlinson*

Main category: cs.AI

TL;DR: 本文提出了一种名为HRM-Agent的HRM变体，该变体仅通过强化学习进行训练，并证明其能够在动态和不确定的迷宫环境中学会导航。研究还发现，HRM的循环推理过程能够有效重用先前时间步的计算。


<details>
  <summary>Details</summary>
Motivation: 分层推理模型（HRM）虽然体积小但具有出色的推理能力，但目前仅限于应用于监督式、静态、完全可观察的问题。尽管HRM能够根据问题难度调整计算量，但在动态、不确定或部分可观察的问题中，或者在正确动作未定义的情况下（这些是许多实际世界问题的特征），其现有形式无法整合和重用先前时间步的计算，这限制了其在更广阔领域的应用潜力。

Method: 本文提出了HRM的一种变体，称为HRM-Agent，该模型仅使用强化学习进行训练。通过将HRM-Agent应用于动态和不确定的迷宫环境，探索其学习导航到目标的能力。此外，深入研究了HRM循环推理过程的动态特性，以探究其是否能够重用先前环境时间步的计算。

Result: 研究结果表明，HRM-Agent能够成功地在动态和不确定的迷宫环境中学会导航到目标。通过对循环推理过程的动态性分析，发现了HRM能够成功重用来自先前环境时间步的计算的证据。

Conclusion: 本文成功地将HRM模型扩展到强化学习范畴，使其能够处理动态和不确定性环境中的复杂决策问题。HRM-Agent不仅展现了在这些挑战性环境中导航的能力，还揭示了其循环推理过程在重用计算方面的有效性。这一发现对于提升HRM在真实世界应用中的效率和泛化能力具有重要意义，为未来在更复杂、不完全可观察的AI任务中应用HRM奠定了基础。

Abstract: The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities
given its small size, but has only been applied to supervised, static,
fully-observable problems. One of HRM's strengths is its ability to adapt its
computational effort to the difficulty of the problem. However, in its current
form it cannot integrate and reuse computation from previous time-steps if the
problem is dynamic, uncertain or partially observable, or be applied where the
correct action is undefined, characteristics of many real-world problems.
  This paper presents HRM-Agent, a variant of HRM trained using only
reinforcement learning. We show that HRM can learn to navigate to goals in
dynamic and uncertain maze environments. Recent work suggests that HRM's
reasoning abilities stem from its recurrent inference process. We explore the
dynamics of the recurrent inference process and find evidence that it is
successfully reusing computation from earlier environment time-steps.

</details>


### [40] [Toward Agents That Reason About Their Computation](https://arxiv.org/abs/2510.22833)
*Adrian Orenstein,Jessica Chen,Gwyneth Anne Delos Santos,Bayley Sapara,Michael Bowling*

Main category: cs.AI

TL;DR: 强化学习智能体通过学习如何考虑计算成本并控制计算使用，可以在不增加训练预算的情况下，在75%的游戏中表现更好，并平均减少三倍的计算量。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习智能体在复杂任务中能达到超人表现，但它们通常不会随着能力的提高而提高计算效率，这与人类学习过程形成对比。如果智能体能够像人类一样在学习过程中权衡计算成本，将有望实现更节能的智能体或为规划等其他进程释放计算资源。

Method: 本文的实验方法是向智能体展示其计算的成本，并赋予它们控制何时使用计算的能力。实验在Arcade Learning Environment上进行。

Result: 实验结果表明，在相同的训练计算预算下，考虑计算成本的智能体在75%的游戏中表现更好。此外，这些智能体平均使用的计算量减少了三倍。论文还分析了单个游戏，以展示智能体获得这些效率提升的具体原因。

Conclusion: 本文证明了让强化学习智能体在学习过程中考虑并控制其计算使用，可以显著提高它们的计算效率，同时还能提升任务表现。这为开发更节能、资源利用率更高的智能体提供了新方向。

Abstract: While reinforcement learning agents can achieve superhuman performance in
many complex tasks, they typically do not become more computationally efficient
as they improve. In contrast, humans gradually require less cognitive effort as
they become more proficient at a task. If agents could reason about their
compute as they learn, could they similarly reduce their computation footprint?
If they could, we could have more energy efficient agents or free up compute
cycles for other processes like planning. In this paper, we experiment with
showing agents the cost of their computation and giving them the ability to
control when they use compute. We conduct our experiments on the Arcade
Learning Environment, and our results demonstrate that with the same training
compute budget, agents that reason about their compute perform better on 75% of
games. Furthermore, these agents use three times less compute on average. We
analyze individual games and show where agents gain these efficiencies.

</details>


### [41] [Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes](https://arxiv.org/abs/2510.22836)
*Guanyu Yao,Qiucheng Wu,Yang Zhang,Zhaowen Wang,Handong Zhao,Shiyu Chang*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）在视觉-语言任务中存在“模态差距”，即过度依赖文本而忽视视觉内容，导致视觉推理性能不佳。本文从训练方法角度分析了这一问题，发现现有训练方法会加剧该差距。通过探索数据和损失设计策略，研究旨在弥合模态差距，促进更平衡的多模态推理。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉-语言任务中展现出强大能力，但其在视觉和文本模态间的推理能力存在不平衡。具体而言，当前的MLLMs往往过度依赖文本线索，而对视觉内容的关注不足，这导致在需要真正视觉推理的任务上表现不佳。这种现象被称为“模态差距”，定义为以文本为中心和以视觉为中心的输入之间表现的差异。弥合这一差距对于实现MLLMs更鲁棒和平衡的多模态推理能力至关重要。

Method: 本文通过审视训练方法来分析“模态差距”现象。研究首先证明了现有训练方法倾向于放大这种模态差距。随后，论文系统地从两个互补的视角——数据设计和损失设计——探索了弥合该差距的策略。这些策略旨在调整MLLMs在训练过程中对不同模态的关注权重，以避免过度依赖单一模态。

Result: 研究结果提供了关于如何开发能够减轻模态差距并促进更平衡多模态推理的训练方法的深刻见解。通过对数据和损失设计的系统性探索，论文揭示了有效提升MLLMs视觉推理能力并减少文本依赖性的具体方法和原则。

Conclusion: 本文的结论是，通过改进训练方法，特别是数据和损失设计，可以有效减轻多模态大语言模型中的“模态差距”。研究为未来开发能够实现更平衡、更强大视觉-语言推理能力的MLLMs提供了宝贵的指导和新的方向。这些发现强调了在模型训练过程中需要仔细考虑模态间平衡的重要性，以确保模型能够真正理解和利用所有输入模态的信息。

Abstract: Multimodal large language models (MLLMs) have demonstrated strong
capabilities on vision-and-language tasks. However, recent findings reveal an
imbalance in their reasoning capabilities across visual and textual modalities.
Specifically, current MLLMs often over-rely on textual cues while
under-attending to visual content, resulting in suboptimal performance on tasks
that require genuine visual reasoning. We refer to this phenomenon as the
\textit{modality gap}, defined as the performance disparity between
text-centric and vision-centric inputs. In this paper, we analyze the modality
gap through the lens of training recipes. We first show that existing training
recipes tend to amplify this gap. Then, we systematically explore strategies to
bridge it from two complementary perspectives: data and loss design. Our
findings provide insights into developing training recipes that mitigate the
modality gap and promote more balanced multimodal reasoning. Our code is
publicly available at https://github.com/UCSB-NLP-Chang/Bridging-Modality-Gap.

</details>


### [42] [Lyapunov Function-guided Reinforcement Learning for Flight Control](https://arxiv.org/abs/2510.22840)
*Yifei Li,Erik-Jan van Kampen*

Main category: cs.AI

TL;DR: 本文研究了一种经过动作平滑性增强的级联在线学习飞行控制系统的收敛性能，通过Lyapunov函数候选量的增量来表征，并考虑了离散化误差和状态预测误差，通过飞行控制仿真展示了比较结果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在深入探讨一种已开发并经动作平滑性增强的级联在线学习飞行控制系统的收敛性能。研究的动机在于，在实际应用中，控制系统的收敛性是衡量其稳定性和有效性的关键指标。尤其是在飞行控制这种高动态、高安全性要求的领域，确保系统能够稳定收敛并抵御各种误差至关重要。本文通过考虑增量模型引入的离散化误差和状态预测误差，旨在提供对系统在实际运行条件下的鲁棒性和性能的量化理解。

Method: 本文通过考察Lyapunov函数候选量的增量来表征控制系统的收敛性能。该度量的推导过程特别考虑了由增量模型引入的离散化误差和状态预测误差。这意味着研究方法不仅仅是理论推导，还融入了对实际系统运行中可能出现的非理想因素的考量，从而使收敛性分析更具实用价值。Lyapunov理论在此被用作分析系统稳定性和收敛性的主要工具。

Result: 通过飞行控制仿真展示了比较结果。这些结果旨在验证和量化所提出的收敛性能分析方法的有效性，并说明在考虑离散化误差和状态预测误差后，系统在不同条件下的收敛行为。虽然摘要中未提供具体的数值或图表，但“比较结果”暗示了对系统改进前后的对比，或在不同误差条件下的性能差异。

Conclusion: 本文开发并增强了一种具有动作平滑性的级联在线学习飞行控制系统，并重点研究了其收敛性能。通过引入考虑离散化误差和状态预测误差的Lyapunov函数候选量增量作为收敛性指标，为评估该系统在实际应用中的稳定性和鲁棒性提供了新的视角。飞行控制仿真结果证实了该方法的有效性。未来的工作可能包括进一步优化误差处理机制，以及在更复杂的飞行场景下对系统进行验证。

Abstract: A cascaded online learning flight control system has been developed and
enhanced with respect to action smoothness. In this paper, we investigate the
convergence performance of the control system, characterized by the increment
of a Lyapunov function candidate. The derivation of this metric accounts for
discretization errors and state prediction errors introduced by the incremental
model. Comparative results are presented through flight control simulations.

</details>


### [43] [Exploring Structures of Inferential Mechanisms through Simplistic Digital Circuits](https://arxiv.org/abs/2510.22883)
*Giovanni Sileno,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 本文提出了一个统一的框架，通过将认知研究和人工智能中的各种推理机制（如分类、归纳、溯因等）映射到基于逻辑门的电子电路，来弥补认知领域缺乏统一框架的现状。通过分析，文章识别了四种依赖形式和八种常见的推理模式，并揭示了逻辑程序中内在的功能依赖性，即使论证主要基于符号方法和数字系统，也可能指向更普遍适用的结构。


<details>
  <summary>Details</summary>
Motivation: 认知研究和人工智能领域已经为各种推理机制（如分类、归纳、溯因、因果推理、对比、合并等）开发了不同的模型。然而，无论是自然认知还是人工认知，都明显缺乏一个统一的框架来整合这些不同的机制。这种碎片化的现状阻碍了对认知过程的全面理解和更高效的人工智能系统的开发。因此，本文旨在通过提出一个推测性的答案来填补这一空白，寻求一种能够统一不同推理机制的底层原理或模型。

Method: 本文首先从物质层面考虑高级激活过程，并通过基于逻辑门的电子电路的简化视角，将符号人工智能建模技术融入推理机制。作者观察到，逻辑门视角对蕴涵和否定的处理方式与标准逻辑和逻辑编程不同。接着，通过组合探索，论文识别了这些推理电路可以实现的四种主要依赖形式。然后，通过考察这些依赖形式在逻辑程序中的一般使用方式，论文识别了八种常见的推理模式，从而在一个统一的框架中揭示了传统上不同的推理机制。最后，通过对逻辑程序进行概率解释，论文揭示了内在的功能依赖性。

Result: 研究结果表明，从逻辑门视角出发，蕴涵和否定的处理方式与标准逻辑和逻辑编程存在显著差异。通过组合探索，论文成功识别出这些推理电路可以实现的四种主要依赖形式。进一步地，通过考察这些依赖形式在逻辑程序中的应用，文章识别并归纳出八种常见的推理模式，这些模式有效地将传统上被视为独立的推理机制整合到一个统一的框架中。此外，通过对逻辑程序的概率解释，研究还揭示了这些推理机制背后内在的功能依赖性，为理解其运作机制提供了更深层次的洞察。

Conclusion: 本文提出的框架，尽管其论证主要基于符号方法和数字系统基础设施，但其观察结果可能指向更普遍适用的结构。这意味着通过将推理机制映射到逻辑门电路的简化模型，我们不仅能在一个统一的框架下理解和整合现有的不同推理机制，还能为认知科学和人工智能提供一个新的视角，去探索和构建更通用、更强大的推理系统。未来的工作可能包括进一步验证这些结构在生物认知系统中的适用性，以及如何将这些理论见解应用于开发更先进的人工智能算法。此外，对逻辑程序进行概率解释所揭示的内在功能依赖性，也为深入理解推理过程提供了新的方向。

Abstract: Cognitive studies and artificial intelligence have developed distinct models
for various inferential mechanisms (categorization, induction, abduction,
causal inference, contrast, merge, ...). Yet, both natural and artificial views
on cognition lack apparently a unifying framework. This paper formulates a
speculative answer attempting to respond to this gap. To postulate on
higher-level activation processes from a material perspective, we consider
inferential mechanisms informed by symbolic AI modelling techniques, through
the simplistic lenses of electronic circuits based on logic gates. We observe
that a logic gate view entails a different treatment of implication and
negation compared to standard logic and logic programming. Then, by
combinatorial exploration, we identify four main forms of dependencies that can
be realized by these inferential circuits. Looking at how these forms are
generally used in the context of logic programs, we identify eight common
inferential patterns, exposing traditionally distinct inferential mechanisms in
an unifying framework. Finally, following a probabilistic interpretation of
logic programs, we unveil inner functional dependencies. The paper concludes
elaborating in what sense, even if our arguments are mostly informed by
symbolic means and digital systems infrastructures, our observations may
pinpoint to more generally applicable structures.

</details>


### [44] [On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset](https://arxiv.org/abs/2510.22898)
*Vishvesh Bhat,Omkar Ghugarkar,Julian McAuley*

Main category: cs.AI

TL;DR: 大型语言模型在工具调用环境中的泛化能力不足是一个关键挑战。本研究引入了MAVEN，一个新的域外（OOD）基准来评估多步骤推理，结果显示现有模型表现不佳。为解决此问题，我们提出了CoreThink Agentic Reasoner，一个结合轻量级符号推理层的框架，它在不进行额外训练的情况下，在所有基准测试中均展现出卓越的泛化能力，性能比现有基线提高530%，计算成本仅为十分之一。


<details>
  <summary>Details</summary>
Motivation: 可靠的智能体推理系统需要大型语言模型（LLMs）在不同工具调用环境中具备泛化能力。然而，尽管LLMs在独立的基准测试中表现出色，但它们在不同领域之间迁移推理策略和协调工具的能力却知之甚少，这是一个尚未解决的关键挑战。

Method: 本研究首先对现有最先进的LLMs在多个工具调用基准（BFCL v3, TauBench, Tau2Bench, AceBench）上进行了大规模评估。其次，引入了MAVEN（Math & Physics Adversarial Verification & Evaluation Network），这是一个新的域外（OOD）基准，旨在通过显式验证和对抗性任务组合来压力测试多步骤推理能力。最后，提出了CoreThink Agentic Reasoner框架，该框架通过一个轻量级的符号推理层来增强LLMs，以实现结构化分解和自适应工具编排，且无需额外训练。

Result: 评估结果显示，大多数当前模型在MAVEN基准测试上的准确率低于50%，这揭示了在工具使用设置中存在显著的泛化差距。相比之下，CoreThink Agentic Reasoner在不进行额外训练的情况下，在所有基准测试中均表现出良好的泛化能力，实现了最先进的性能。它比现有基线提高了530%的性能，同时计算成本仅为现有方法的十分之一。

Conclusion: 本研究通过引入MAVEN基准揭示了当前大型语言模型在智能体工具调用环境泛化能力上的显著不足。CoreThink Agentic Reasoner的提出，成功解决了这一泛化差距，它通过轻量级符号推理层的增强，在不进行额外训练的情况下，显著提升了性能并降低了计算成本，为开发更可靠、更高效的智能体推理系统提供了新的方向。

Abstract: Generalization across Agentic tool-calling environments remains a key
unsolved challenge in developing reliable agentic reasoning systems. While
large language models (LLMs) demonstrate strong performance on isolated
benchmarks, their ability to transfer reasoning strategies and co-ordinate
tools across diverse domains is poorly understood. In this work, we conduct a
large-scale evaluation of state-of-the-art LLMs on multiple tool-calling
benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &
Physics Adversarial Verification & Evaluation Network), a new out of
distribution (OOD) benchmark designed to stress-test multi-step reasoning
through explicit verification and adversarial task composition. Our results
show that most current models achieve below 50% accuracy on MAVEN, revealing a
significant generalization gap across tool-use settings.
  To address this, we present the CoreThink Agentic Reasoner, a framework that
augments LLMs with a lightweight symbolic reasoning layer for structured
decomposition and adaptive tool orchestration. Without additional training, it
generalizes across all benchmarks, achieving state-of-the-art performance with
530% improvements over existing baselines at roughly one-tenth the
computational cost.

</details>


### [45] [Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction](https://arxiv.org/abs/2510.22981)
*Jin Hu,Jiakai Wang,Linna Jing,Haolin Li,Haodong Liu,Haotong Qin,Aishan Liu,Ke Xu,Xianglong Liu*

Main category: cs.AI

TL;DR: 该论文提出多维指令不确定性消减 (InSUR) 框架，以解决现有语义约束对抗样本 (SemanticAE) 生成方法因人类指令语义不确定性（如指代多样性、描述不完整性、边界模糊性）而导致的攻击能力不足问题。InSUR 通过残差驱动攻击方向稳定化、上下文编码攻击场景约束和语义抽象攻击评估增强，生成更具可迁移性、适应性和有效性的 SemanticAE。实验证明 InSUR 在迁移攻击性能上表现出色，并首次实现了无参考的语义约束3D对抗样本生成。


<details>
  <summary>Details</summary>
Motivation: 现有语义约束对抗样本 (SemanticAE) 生成方法存在攻击能力不足的问题。SemanticAE 作为一种直接从自然语言指令生成对抗样本的新兴研究方向，具有灵活的攻击形式，前景广阔。然而，人类指令中固有的语义不确定性，包括指代多样性、描述不完整性和边界模糊性等关键潜在因素，尚未得到充分研究，导致当前的生成方法无法产生令人满意的攻击效果。因此，亟需开发一种能够有效解决这些不确定性，从而生成更强大、更有效的 SemanticAE 的框架。

Method: 本文提出了一个多维指令不确定性消减 (InSUR) 框架，旨在解决语义约束对抗样本 (SemanticAE) 生成中的语义不确定性问题。具体方法如下：
1.  **采样方法维度**：为了缓解语言指代多样性导致的对抗优化不稳定，提出了残差驱动攻击方向稳定化。通过粗略预测语言引导的采样过程，设计的 ResAdv-DDIM 采样器能够稳定优化过程，从而释放多步扩散模型的可迁移和鲁棒对抗能力。
2.  **任务建模维度**：为了补充不完整人类指令中缺失的知识，提出了上下文编码攻击场景约束。通过引导掩蔽和渲染器集成来规范2D/3D SemanticAE 的约束，激活更强的场景适应性攻击。
3.  **生成器评估维度**：通过澄清评估边界，提出了语义抽象攻击评估增强，旨在促进开发更有效的 SemanticAE 生成器。

Result: 广泛的实验结果表明了 InSUR 在迁移攻击性能方面的优越性。此外，本文首次实现了无参考的语义约束3D对抗样本的生成。

Conclusion: 该论文提出了多维指令不确定性消减 (InSUR) 框架，有效解决了语义约束对抗样本 (SemanticAE) 生成中由人类指令语义不确定性（如指代多样性、描述不完整性、边界模糊性）引起的攻击能力不足问题。通过在采样方法、任务建模和生成器评估等多个维度进行创新，InSUR 成功生成了更具可迁移性、适应性和有效性的 SemanticAE。其关键贡献包括残差驱动攻击方向稳定化、上下文编码攻击场景约束以及语义抽象攻击评估增强。实验结果有力地证明了 InSUR 在迁移攻击方面的卓越性能，并开创性地实现了无参考的语义约束3D对抗样本生成，为未来 SemanticAE 的研究和应用奠定了坚实基础。

Abstract: Recently, semantically constrained adversarial examples (SemanticAE), which
are directly generated from natural language instructions, have become a
promising avenue for future research due to their flexible attacking forms. To
generate SemanticAEs, current methods fall short of satisfactory attacking
ability as the key underlying factors of semantic uncertainty in human
instructions, such as referring diversity, descriptive incompleteness, and
boundary ambiguity, have not been fully investigated. To tackle the issues,
this paper develops a multi-dimensional instruction uncertainty reduction
(InSUR) framework to generate more satisfactory SemanticAE, i.e., transferable,
adaptive, and effective. Specifically, in the dimension of the sampling method,
we propose the residual-driven attacking direction stabilization to alleviate
the unstable adversarial optimization caused by the diversity of language
references. By coarsely predicting the language-guided sampling process, the
optimization process will be stabilized by the designed ResAdv-DDIM sampler,
therefore releasing the transferable and robust adversarial capability of
multi-step diffusion models. In task modeling, we propose the context-encoded
attacking scenario constraint to supplement the missing knowledge from
incomplete human instructions. Guidance masking and renderer integration are
proposed to regulate the constraints of 2D/3D SemanticAE, activating stronger
scenario-adapted attacks. Moreover, in the dimension of generator evaluation,
we propose the semantic-abstracted attacking evaluation enhancement by
clarifying the evaluation boundary, facilitating the development of more
effective SemanticAE generators. Extensive experiments demonstrate the
superiority of the transfer attack performance of InSUR. Moreover, we realize
the reference-free generation of semantically constrained 3D adversarial
examples for the first time.

</details>


### [46] [ProfileXAI: User-Adaptive Explainable AI](https://arxiv.org/abs/2510.22998)
*Gilber A. Corrales,Carlos Andrés Ferro Sánchez,Reinel Tabares-Soto,Jesús Alfonso López Sotelo,Gonzalo A. Ruz,Johan Sebastian Piña Durán*

Main category: cs.AI

TL;DR: ProfileXAI是一个结合后验解释器（SHAP、LIME、Anchor）与检索增强型大型语言模型，为不同类型用户生成解释的通用框架。该系统通过量化标准选择解释器，利用多模态知识库和聊天式提示生成扎根叙述。评估显示，没有单一解释器占优，LIME在忠实度-鲁棒性上表现最佳，Anchor规则最稀疏，SHAP用户满意度最高。配置文件调整稳定了token使用并保持了积极的用户评价，实现了高效可信的解释。


<details>
  <summary>Details</summary>
Motivation: 现有的解释性人工智能（XAI）方法往往难以针对不同类型的用户（例如领域专家或普通用户）提供定制化、高效且值得信赖的解释。此外，一个普遍的挑战是缺乏模型和领域无关的通用框架，能够整合多种解释技术并结合领域知识来生成高质量、易于理解的解释。本文旨在解决这一问题，通过开发ProfileXAI框架，为用户提供个性化、可信赖且高效的XAI解释。

Method: ProfileXAI框架是一个模型和领域无关的系统，它将后验解释器（如SHAP、LIME和Anchor）与检索增强型大型语言模型（LLMs）相结合。其核心方法包括：
1.  **知识库构建**：系统首先索引一个多模态知识库，以提供解释所需的领域知识。
2.  **解释器选择**：对于每个实例，系统根据定量标准动态选择一个最合适的解释器。
3.  **叙述生成**：选定的解释器输出的结果被输入到检索增强型LLMs中，LLMs利用知识库并结合聊天式提示（chat-enabled prompting）生成扎根（grounded）的解释叙述。
4.  **用户画像条件化**：框架支持配置文件调整（Profile conditioning），这有助于稳定token使用量并维持不同用户画像下的解释质量和满意度。
实验评估在心脏病和甲状腺癌数据集上进行，评估指标包括忠实度（fidelity）、鲁棒性（robustness）、简约性（parsimony）、token使用量和感知质量（perceived quality）。

Result: 评估结果显示，在心脏病和甲状腺癌数据集上，没有单一的解释器在所有指标上都占据主导地位：
*   **LIME**：在忠实度-鲁棒性权衡方面表现最佳，在心脏病数据集上，不忠实度（Infidelity） $\le 0.30$，鲁棒性（L） $<0.7$。
*   **Anchor**：生成的规则最稀疏，且token使用量最低。
*   **SHAP**：获得了最高的满意度评分（平均值 $\bar{x}=4.1$）。
此外，**配置文件调整（Profile conditioning）**机制被证明能够有效稳定token的使用量（标准差 $\sigma \le 13\%$），并在不同用户画像下保持积极的用户评价（平均值 $\bar{x}\ge 3.7$，其中领域专家的评价平均值达到 $3.77$）。这些结果表明ProfileXAI能够提供高效且值得信赖的解释。

Conclusion: ProfileXAI框架通过结合多种后验解释器与检索增强型大型语言模型，并利用用户画像条件化，成功为不同类型的用户提供了高效、可信赖且定制化的解释。尽管没有单一解释器在所有评估维度上表现完美，但该框架通过智能选择和整合不同解释器的优势，实现了在忠实度、鲁棒性、简约性和用户满意度之间取得良好平衡。配置文件调整对于稳定解释效率和维持跨用户群体的满意度至关重要。未来工作可以进一步探索如何优化解释器选择策略，以及扩展框架以支持更多模态的知识库和更复杂的交互模式，从而持续提升解释的质量和用户体验。

Abstract: ProfileXAI is a model- and domain-agnostic framework that couples post-hoc
explainers (SHAP, LIME, Anchor) with retrieval - augmented LLMs to produce
explanations for different types of users. The system indexes a multimodal
knowledge base, selects an explainer per instance via quantitative criteria,
and generates grounded narratives with chat-enabled prompting. On Heart Disease
and Thyroid Cancer datasets, we evaluate fidelity, robustness, parsimony, token
use, and perceived quality. No explainer dominates: LIME achieves the best
fidelity--robustness trade-off (Infidelity $\le 0.30$, $L<0.7$ on Heart
Disease); Anchor yields the sparsest, low-token rules; SHAP attains the highest
satisfaction ($\bar{x}=4.1$). Profile conditioning stabilizes tokens ($\sigma
\le 13\%$) and maintains positive ratings across profiles ($\bar{x}\ge 3.7$,
with domain experts at $3.77$), enabling efficient and trustworthy
explanations.

</details>


### [47] [From Prompt Optimization to Multi-Dimensional Credibility Evaluation: Enhancing Trustworthiness of Chinese LLM-Generated Liver MRI Reports](https://arxiv.org/abs/2510.23008)
*Qiuli Wang,Xiaoming Li,Jie Chen,Yongxu Liu,Xingpeng Zhang,Chen Liu,Wei Chen*

Main category: cs.AI

TL;DR: 该研究旨在通过引入多维可信度评估（MDCA）框架并提供机构特定的提示优化指导，以提高大型语言模型（LLMs）生成的肝脏MRI报告的可信度。该框架用于评估和比较包括Kimi-K2-Instruct-0905、Qwen3-235B-A22B-Instruct-2507、DeepSeek-V3和ByteDance-Seed-OSS-36B-Instruct在内的多种先进LLMs在SiliconFlow平台上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在根据影像发现生成诊断结论方面表现出良好的性能，从而支持放射学报告、学员教育和质量控制。然而，针对不同临床背景下如何优化提示设计的系统性指导仍未被充分探索。此外，评估LLM生成的放射学报告可信度的全面且标准化的框架尚未建立。本研究旨在解决这些空白。

Method: 本研究引入了一个多维可信度评估（MDCA）框架，并提供了机构特定的提示优化指导，以提高LLM生成的肝脏MRI报告的可信度。该框架被应用于评估和比较多种先进LLMs的性能，包括Kimi-K2-Instruct-0905、Qwen3-235B-A22B-Instruct-2507、DeepSeek-V3和ByteDance-Seed-OSS-36B-Instruct。所有评估均通过SiliconFlow平台进行。

Result: 该抽象描述了将提出的多维可信度评估（MDCA）框架应用于评估和比较多种先进大型语言模型（LLMs）性能的过程，包括Kimi-K2-Instruct-0905、Qwen3-235B-A22B-Instruct-2507、DeepSeek-V3和ByteDance-Seed-OSS-36B-Instruct，并在SiliconFlow平台进行。抽象中未具体说明这些模型在评估中的具体表现或性能比较结果。

Conclusion: 本研究通过引入多维可信度评估（MDCA）框架和提供机构特定的提示优化指导，旨在增强大型语言模型（LLMs）生成的肝脏MRI报告的可信度。尽管抽象未提供具体的评估结果，但它建立了评估LLM在特定临床语境下性能的标准化方法，并为未来的研究提供了平台，以实现更可靠的放射学报告生成。未来的工作将涉及深入分析这些LLM的评估结果，并根据MDCA框架进一步完善提示优化策略。

Abstract: Large language models (LLMs) have demonstrated promising performance in
generating diagnostic conclusions from imaging findings, thereby supporting
radiology reporting, trainee education, and quality control. However,
systematic guidance on how to optimize prompt design across different clinical
contexts remains underexplored. Moreover, a comprehensive and standardized
framework for assessing the trustworthiness of LLM-generated radiology reports
is yet to be established. This study aims to enhance the trustworthiness of
LLM-generated liver MRI reports by introducing a Multi-Dimensional Credibility
Assessment (MDCA) framework and providing guidance on institution-specific
prompt optimization. The proposed framework is applied to evaluate and compare
the performance of several advanced LLMs, including Kimi-K2-Instruct-0905,
Qwen3-235B-A22B-Instruct-2507, DeepSeek-V3, and
ByteDance-Seed-OSS-36B-Instruct, using the SiliconFlow platform.

</details>


### [48] [Mixed Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution](https://arxiv.org/abs/2510.23026)
*Crimson Stambaugh,Rajesh P. N. Rao*

Main category: cs.AI

TL;DR: 扩散规划器受益于稀疏规划，但过度稀疏会降低性能。本文提出混合密度扩散器（MDD），通过在规划时程中调整时间密度超参数，在Maze2D、Franka Kitchen和Antmaze D4RL任务领域实现了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 最近研究表明，扩散规划器通过稀疏步长规划优于单步长规划，这有助于捕捉长期依赖而无需额外计算或内存成本。然而，过度稀疏的规划会降低性能。本文假设这种时间密度阈值在整个时间范围是非均匀的，并且规划轨迹的某些部分应该进行更密集的规划。

Method: 本文提出了混合密度扩散器（MDD），这是一种扩散规划器，其在整个规划时程中的时间密度是可调的超参数。这意味着MDD能够根据轨迹的不同部分动态调整规划的稀疏程度，以优化性能。

Result: MDD在Maze2D、Franka Kitchen和Antmaze D4RL任务领域取得了新的SOTA（State-of-the-Art）性能。这表明其提出的方法在多种不同的控制任务中都表现出卓越的有效性。

Conclusion: MDD通过引入可调的时间密度，解决了扩散规划器中稀疏步长规划的性能瓶颈。它能够智能地调整规划的密集程度，从而在多个复杂任务中达到最先进的性能。抽象中未详细说明其局限性和未来的工作。

Abstract: Recent studies demonstrate that diffusion planners benefit from sparse-step
planning over single-step planning. Training models to skip steps in their
trajectories helps capture long-term dependencies without additional or memory
computational cost. However, predicting excessively sparse plans degrades
performance. We hypothesize this temporal density threshold is non-uniform
across a temporal horizon and that certain parts of a planned trajectory should
be more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion
planner where the densities throughout the horizon are tunable hyperparameters.
MDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL
task domains.

</details>


### [49] [A Survey of AI Scientists: Surveying the automatic Scientists and Research](https://arxiv.org/abs/2510.23045)
*Guiyao Tie,Pan Zhou,Lichao Sun*

Main category: cs.AI

TL;DR: 人工智能正在从计算工具转变为科学知识的自主创造者，形成了“AI科学家”范式。本综述通过引入一个统一的六阶段方法论框架（文献综述、思想生成、实验准备、实验执行、科学写作、论文生成），系统地综合了这一领域，梳理了该领域从基础模块（2022-2023）到闭环系统（2024），再到当前的可扩展性、影响和人机协作（2025至今）的演变，并为克服鲁棒性和治理挑战提供了路线图，以期使AI成为人类科学探究中值得信赖的伙伴。


<details>
  <summary>Details</summary>
Motivation: 人工智能正在经历深刻的转变，从计算工具演变为科学知识的自主创造者，催生了“AI科学家”这一新兴范式。这种范式旨在模拟完整的科学工作流程，从初步假设生成到可发表成果的最终综合，有望从根本上重塑发现的节奏和规模。然而，这些系统的快速且无序的扩散造成了研究领域的碎片化，模糊了总体方法论原则和发展趋势。因此，需要一份系统、全面的综述来综合这一领域，以应对这种碎片化局面。

Method: 本综述通过引入一个统一的、六阶段的方法论框架，对这一领域进行了系统而全面的综合。该框架将端到端的科学过程解构为：文献综述、思想生成、实验准备、实验执行、科学写作和论文生成。通过这一分析视角，本综述描绘了该领域的发展演变。

Result: 本综述描绘了该领域从早期基础模块（2022-2023年）到集成闭环系统（2024年），再到当前可扩展性、影响和人机协作（2025年至今）前沿的演变。通过严谨地综合这些发展，本综述不仅阐明了自主科学的当前状态。

Conclusion: 本综述为克服鲁棒性和治理方面的剩余挑战提供了关键路线图，最终指导下一代系统成为人类科学探究中值得信赖且不可或缺的伙伴。

Abstract: Artificial intelligence is undergoing a profound transition from a
computational instrument to an autonomous originator of scientific knowledge.
This emerging paradigm, the AI scientist, is architected to emulate the
complete scientific workflow-from initial hypothesis generation to the final
synthesis of publishable findings-thereby promising to fundamentally reshape
the pace and scale of discovery. However, the rapid and unstructured
proliferation of these systems has created a fragmented research landscape,
obscuring overarching methodological principles and developmental trends. This
survey provides a systematic and comprehensive synthesis of this domain by
introducing a unified, six-stage methodological framework that deconstructs the
end-to-end scientific process into: Literature Review, Idea Generation,
Experimental Preparation, Experimental Execution, Scientific Writing, and Paper
Generation. Through this analytical lens, we chart the field's evolution from
early Foundational Modules (2022-2023) to integrated Closed-Loop Systems
(2024), and finally to the current frontier of Scalability, Impact, and
Human-AI Collaboration (2025-present). By rigorously synthesizing these
developments, this survey not only clarifies the current state of autonomous
science but also provides a critical roadmap for overcoming remaining
challenges in robustness and governance, ultimately guiding the next generation
of systems toward becoming trustworthy and indispensable partners in human
scientific inquiry.

</details>


### [50] [Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards](https://arxiv.org/abs/2510.23083)
*Jan Niklas Groeneveld,Xi Qin,Alexander Schaefer,Yaad Oren*

Main category: cs.AI

TL;DR: 该研究探索了将Phi-4系列等小型LLM转化为有效的代码奖励模型的可行性，这些模型能够结合过程和结果奖励。通过在APPS基准测试数据集上训练一个价值头部模型，研究表明小型LLM可以作为有效的代码评估评论者，成功识别正确解决方案，并将从多个生成代码中搜索最准确代码的能力提高了20%以上。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在生成高质量代码方面仍面临挑战。奖励模型作为中间步骤，对于改进LLM在此任务上的推理模型至关重要，因为它们可以评估结果或中间步骤。虽然已知模型的反射能力通常随模型大小的增加而增强，但这项研究的动机在于探索最先进的小型语言模型（如Phi-4系列）是否也能成为可用的奖励模型，并能同时考虑过程奖励和结果奖励，从而在资源有限的情况下也能实现有效的代码生成改进。

Method: 研究方法是将解码器-only的Transformer模型通过引入回归层和监督微调转化为奖励模型。具体而言，研究人员构建了一个包含代码样本及其正确性标签的数据集，这些标签来源于APPS编程挑战基准测试。随后，他们训练了一个“价值头部模型”来估计中间输出的成功概率。该方法旨在利用小型语言模型（如Phi-4系列）作为基础，开发能够评估代码质量的奖励模型。

Result: 评估结果显示，小型LLM能够作为有效的奖励模型或代码评估评论者。这些小型模型成功地在多个候选方案中识别出正确的解决方案。更重要的是，通过使用这种评论者，从多个生成代码中搜索最准确代码的能力获得了超过20%的提升。

Conclusion: 该研究得出的结论是，小型LLM，如Phi-4系列，能够被有效地用作代码生成任务的奖励模型。它们能够成功地评估代码并识别出正确的解决方案，从而显著提高了从多个生成代码中选择最优代码的能力。这项工作突出了小型模型在代码生成这一复杂领域中的潜力，为在计算资源受限的环境下改进LLM的代码生成质量提供了有价值的方向。

Abstract: Generating high-quality code remains a challenge for Large Language Models
(LLMs). For the evolution of reasoning models on this task, reward models are a
necessary intermediate step. These models judge outcomes or intermediate steps.
Decoder-only transformer models can be turned into reward models by introducing
a regression layer and supervised fine-tuning. While it is known that
reflection capabilities generally increase with the size of a model, we want to
investigate whether state-of-the-art small language models like the Phi-4
family can be turned into usable reward models blending the consideration of
process rewards and outcome rewards.
  Targeting this goal, we construct a dataset of code samples with correctness
labels derived from the APPS coding challenge benchmark. We then train a
value-head model to estimate the success probability of intermediate outputs.
Our evaluation shows that small LLMs are capable of serving as effective reward
models or code evaluation critics, successfully identifying correct solutions
among multiple candidates. Using this critic, we achieve over a 20% improvement
in the search capability of the most accurate code out of multiple generations.

</details>


### [51] [Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs](https://arxiv.org/abs/2510.23127)
*Kai Zhuang,Jiawei Zhang,Yumou Liu,Hanqun Cao,Chunbin Gu,Mengdi Liu,Zhangyang Gao,Zitong Jerry Wang,Xuanhe Zhou,Pheng-Ann Heng,Lijun Wu,Conghui He,Cheng Tan*

Main category: cs.AI

TL;DR: 本研究发现，为科学大型语言模型（Sci-LLMs）提供来自生物信息学工具的高级结构化上下文，而非原始生物分子序列，能显著提升其在生物推理任务上的表现。原始序列甚至可能作为噪声降低性能，表明Sci-LLMs的优势在于对结构化、人类可读知识的推理能力，而非从头解释生物分子语法。因此，应将Sci-LLMs重新定位为基于专家知识的强大推理引擎。


<details>
  <summary>Details</summary>
Motivation: 当前科学大型语言模型（Sci-LLMs）在处理原始生物分子序列时面临“标记化困境”的根本挑战。无论是将序列视为一种专门语言（可能丢失功能基序信息）还是作为独立的模态（引入难以克服的对齐挑战），现有策略都严重限制了模型的推理能力。研究的动机在于挑战这种以序列为中心的范式，认为直接解释低级噪声序列数据并非最佳途径，从而解决Sci-LLMs在生物发现领域加速应用时面临的这一关键瓶颈。

Method: 本研究提出了一种新的策略，即通过提供来自已建立的生物信息学工具的高级结构化上下文来绕过Sci-LLMs直接解释低级噪声序列数据的需要。为了验证这一假设，研究团队系统地比较了领先的Sci-LLMs在生物推理任务上的性能。实验设置了三种输入模式进行测试：仅序列（sequence-only）、仅上下文（context-only）以及序列与上下文的组合（combination of both）。这种方法旨在直接对比不同输入范式对Sci-LLMs推理能力的影响，以揭示其真正的优势所在。

Result: 研究结果显著：仅上下文的输入方式始终且大幅优于所有其他输入模式。更具启发性的是，将原始序列与高级上下文结合输入时，模型的性能反而持续下降。这表明，即使对于采用专门标记化方案的模型，原始序列也充当了信息噪声。这些发现有力地揭示了现有Sci-LLMs的核心优势不在于其从头解释生物分子语法的初步能力，而在于其对结构化、人类可读知识进行深刻推理的强大潜力。

Conclusion: 本研究的结论是，Sci-LLMs不应被视为序列解码器，而应被重新定义为基于专家知识的强大推理引擎。这一工作为新型混合科学AI智能体的开发奠定了基础，将发展重点从直接序列解释转向高级知识合成。这意味着未来的科研AI模型应更侧重于整合和推理经过预处理的、有意义的生物信息，而非从原始序列中“学习”语法。本文的研究成果有望推动生物学发现领域中AI应用的范式转变，并提供了相关代码以供进一步研究和应用。

Abstract: Scientific Large Language Models (Sci-LLMs) have emerged as a promising
frontier for accelerating biological discovery. However, these models face a
fundamental challenge when processing raw biomolecular sequences: the
tokenization dilemma. Whether treating sequences as a specialized language,
risking the loss of functional motif information, or as a separate modality,
introducing formidable alignment challenges, current strategies fundamentally
limit their reasoning capacity. We challenge this sequence-centric paradigm by
positing that a more effective strategy is to provide Sci-LLMs with high-level
structured context derived from established bioinformatics tools, thereby
bypassing the need to interpret low-level noisy sequence data directly. Through
a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we
tested three input modes: sequence-only, context-only, and a combination of
both. Our findings are striking: the context-only approach consistently and
substantially outperforms all other modes. Even more revealing, the inclusion
of the raw sequence alongside its high-level context consistently degrades
performance, indicating that raw sequences act as informational noise, even for
models with specialized tokenization schemes. These results suggest that the
primary strength of existing Sci-LLMs lies not in their nascent ability to
interpret biomolecular syntax from scratch, but in their profound capacity for
reasoning over structured, human-readable knowledge. Therefore, we argue for
reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines
over expert knowledge. This work lays the foundation for a new class of hybrid
scientific AI agents, repositioning the developmental focus from direct
sequence interpretation towards high-level knowledge synthesis. The code is
available at github.com/opendatalab-raise-dev/CoKE.

</details>


### [52] [Guiding Skill Discovery with Foundation Models](https://arxiv.org/abs/2510.23167)
*Zhao Yang,Thomas M. Moerland,Mike Preuss,Aske Plaat,Vincent François-Lavet,Edward S. Hu*

Main category: cs.AI

TL;DR: 本文提出了一种名为FoG（Foundation model Guided）的技能发现方法，通过结合基础模型将人类意图融入技能发现过程。FoG从基础模型中提取评分函数来评估状态，并用这些评分重新加权技能发现算法的奖励，从而有效地学习消除不良行为并避免危险区域，甚至能发现难以定义的行为技能。


<details>
  <summary>Details</summary>
Motivation: 现有的技能发现方法仅专注于最大化技能多样性，而忽略了人类偏好，这可能导致不理想甚至危险的行为。例如，通过传统方法训练的猎豹机器人可能会通过向各个方向翻滚来最大化技能多样性，而我们更希望它能在不翻倒或不进入危险区域的情况下奔跑。因此，将人类意图纳入技能发现是加速下游任务强化学习并提高技能实用性和安全性的关键。

Method: FoG方法通过基础模型将人类意图整合到技能发现中。具体来说，FoG从基础模型中提取一个评分函数，用于根据人类意图评估各种状态，对期望的状态赋予更高的分数，对不期望的状态赋予更低的分数。然后，这些分数被用来重新加权技能发现算法的奖励。通过优化这些重新加权的技能发现奖励，FoG能够学习避免不良行为。

Result: FoG成功地学习了消除不良行为（如翻滚）并避免进入危险区域，这在基于状态和基于像素的任务中都得到了验证。有趣的是，研究表明FoG能够发现涉及难以精确定义的行为的技能。

Conclusion: FoG方法通过在技能发现中融入人类意图，显著提高了所学技能的安全性、实用性和可控性。它成功消除了不希望的行为并避免了危险区域，并且能够发现难以明确定义的复杂行为技能。这项工作为在没有手动设计奖励函数的情况下，通过结合人类偏好来学习更有效和安全的技能提供了新的方向。

Abstract: Learning diverse skills without hand-crafted reward functions could
accelerate reinforcement learning in downstream tasks. However, existing skill
discovery methods focus solely on maximizing the diversity of skills without
considering human preferences, which leads to undesirable behaviors and
possibly dangerous skills. For instance, a cheetah robot trained using previous
methods learns to roll in all directions to maximize skill diversity, whereas
we would prefer it to run without flipping or entering hazardous areas. In this
work, we propose a Foundation model Guided (FoG) skill discovery method, which
incorporates human intentions into skill discovery through foundation models.
Specifically, FoG extracts a score function from foundation models to evaluate
states based on human intentions, assigning higher values to desirable states
and lower to undesirable ones. These scores are then used to re-weight the
rewards of skill discovery algorithms. By optimizing the re-weighted skill
discovery rewards, FoG successfully learns to eliminate undesirable behaviors,
such as flipping or rolling, and to avoid hazardous areas in both state-based
and pixel-based tasks. Interestingly, we show that FoG can discover skills
involving behaviors that are difficult to define. Interactive visualisations
are available from https://sites.google.com/view/submission-fog.

</details>


### [53] [AUPO - Abstracted Until Proven Otherwise: A Reward Distribution Based Abstraction Algorithm](https://arxiv.org/abs/2510.23214)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: AUPO是一种新颖的、可直接集成到蒙特卡洛树搜索（MCTS）决策策略中的修改。它通过利用MCTS期间获得的奖励分布统计数据，实现了自动动作抽象，从而在IPPC基准问题上显著优于MCTS。AUPO不需要转移概率或有向无环搜索图，能有效检测出传统方法难以处理的对称动作，并且可以与其他抽象技术结合使用。


<details>
  <summary>Details</summary>
Motivation: MCTS在决策策略上可能存在效率不足的问题，尤其是在处理需要动作抽象的复杂问题时。现有的自动动作抽象算法通常依赖于转移概率或有向无环搜索图，这限制了它们的应用范围，并且在处理状态空间中相距较远的对称动作时表现不佳。因此，需要一种更鲁棒、更灵活的动作抽象方法，既能提高MCTS的性能，又能克服现有方法的局限性。

Method: AUPO通过对MCTS的决策策略进行修改来实现动作抽象。它是一种自动动作抽象算法，其核心机制完全依赖于在MCTS过程中收集到的奖励分布统计数据。这意味着AUPO不需要访问环境的转移概率，也不需要构建有向无环搜索图来生成抽象。这种设计允许AUPO在更广泛的问题场景中应用，并且能够识别出即使在状态空间中相距较远，也具有对称性的动作。

Result: 在IPPC基准问题系列上的比较表明，AUPO在性能上明显优于MCTS。AUPO成功地检测并处理了传统最先进框架（如ASAP）难以解决的对称动作，尤其是在这些对称状态在状态空间中相距甚远的情况下。此外，AUPO只影响决策策略，这使其可以与只影响树搜索的其他抽象技术兼容，提供了更大的灵活性和组合潜力。

Conclusion: AUPO通过改进MCTS的决策策略，提供了一种高效、灵活的自动动作抽象方法。它显著提升了MCTS在复杂问题上的表现，特别是在处理对称动作方面展现出卓越的能力。由于AUPO不依赖于转移概率或特定的图结构，其适用性更广，并且可以与其他抽象技术结合，为未来的研究和应用提供了新的方向和可能性。

Abstract: We introduce a novel, drop-in modification to Monte Carlo Tree Search's
(MCTS) decision policy that we call AUPO. Comparisons based on a range of IPPC
benchmark problems show that AUPO clearly outperforms MCTS. AUPO is an
automatic action abstraction algorithm that solely relies on reward
distribution statistics acquired during the MCTS. Thus, unlike other automatic
abstraction algorithms, AUPO requires neither access to transition
probabilities nor does AUPO require a directed acyclic search graph to build
its abstraction, allowing AUPO to detect symmetric actions that
state-of-the-art frameworks like ASAP struggle with when the resulting
symmetric states are far apart in state space. Furthermore, as AUPO only
affects the decision policy, it is not mutually exclusive with other
abstraction techniques that only affect the tree search.

</details>


### [54] [CNOT Minimal Circuit Synthesis: A Reinforcement Learning Approach](https://arxiv.org/abs/2510.23304)
*Riccardo Romanello,Daniele Lizzio Bosco,Jacopo Cossio,Dusan Sutulovic,Giuseppe Serra,Carla Piazza,Paolo Burelli*

Main category: cs.AI

TL;DR: 本论文提出了一种新颖的强化学习方法来解决CNOT门最小化问题，该方法通过训练一个单一智能体来处理不同大小的量子电路，并在较大规模的电路上超越了现有最先进的算法。


<details>
  <summary>Details</summary>
Motivation: CNOT门是量子计算的基础，对实现量子纠缠至关重要，而纠缠是量子算法的关键资源。某些类别的量子电路仅由CNOT门构建。由于CNOT门的广泛使用，最小化其数量变得极其重要，这被称为CNOT最小化问题。然而，这个问题仍然是一个开放的挑战，其计算复杂性尚未得到充分表征。

Method: 本研究引入了一种新颖的强化学习方法来解决CNOT最小化任务。与为不同电路尺寸训练多个强化学习智能体不同，我们使用一个单一智能体，该智能体针对固定大小m进行训练。对于与m不同的矩阵尺寸，通过嵌入或高斯条纹进行预处理。为了评估我们方法的有效性，我们训练了一个m=8的智能体，并在尺寸n从3到15的矩阵上进行了评估。

Result: 我们获得的结果表明，随着n值的增加（即电路规模变大），我们的方法在性能上优于现有最先进的算法。

Conclusion: 本工作成功引入了一种新颖的强化学习方法来解决CNOT门最小化问题，并通过单一智能体训练策略有效处理了不同尺寸的电路。实验结果证明，该方法在处理较大规模电路时，性能优于现有最先进的算法，为量子计算中的CNOT优化提供了新的有效途径。

Abstract: CNOT gates are fundamental to quantum computing, as they facilitate
entanglement, a crucial resource for quantum algorithms. Certain classes of
quantum circuits are constructed exclusively from CNOT gates. Given their
widespread use, it is imperative to minimise the number of CNOT gates employed.
This problem, known as CNOT minimisation, remains an open challenge, with its
computational complexity yet to be fully characterised. In this work, we
introduce a novel reinforcement learning approach to address this task. Instead
of training multiple reinforcement learning agents for different circuit sizes,
we use a single agent up to a fixed size $m$. Matrices of sizes different from
m are preprocessed using either embedding or Gaussian striping. To assess the
efficacy of our approach, we trained an agent with m = 8, and evaluated it on
matrices of size n that range from 3 to 15. The results we obtained show that
our method overperforms the state-of-the-art algorithm as the value of n
increases.

</details>


### [55] [What are the odds? Risk and uncertainty about AI existential risk](https://arxiv.org/abs/2510.23453)
*Marco Grossi*

Main category: cs.AI

TL;DR: 这篇评论文章深入分析了Cappelen、Goldstein和Hawthorne的《AI生存故事：AI生存风险的分类学分析》，强调了线性风险模型，特别是瑞士奶酪模型在处理AI生存风险时的哲学局限性。文章指出，在认知无差别的情况下，灾难概率P(D)可能高于预期，并且P(D)的任何估计都受到选项不确定性和状态空间不确定性这两种不确定性的结构性影响。将这些不确定性纳入考量有助于更好地理解AI生存风险中P(D)的可能性。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是对Cappelen、Goldstein和Hawthorne关于AI生存风险的论文进行评论性分析。其核心目的是提醒人们注意“线性”风险模型在哲学上的局限性，尤其是在评估AI生存风险时。研究认为，这些模型可能未能充分捕捉到导致灾难（P(D)）的真正可能性，因为它们忽略了认知无差别和不同类型不确定性的影响，从而可能低估了风险。

Method: 本文首先讨论了标准瑞士奶酪模型与所评论文章中使用的模型之间的差异。接着，作者论证了在认知无差别的情况下，考虑到层级间的结构关系，灾难概率P(D)实际上可能高于人们最初的设想。随后，文章区分了风险与不确定性，并提出P(D)的任何估计都受到两种不确定性的结构性影响：选项不确定性（Option Uncertainty）和状态空间不确定性（State-space Uncertainty）。通过这种分析方法，旨在提供对AI生存风险更深入的理解。

Result: 本文的主要论点和发现是：首先，在认知无差别（epistemic indifference）的情境下，鉴于各层级之间的结构关系，灾难概率P(D)实际上可能高于最初的预期。其次，对P(D)的任何估计都受到两种核心不确定性的结构性影响：即选项不确定性和状态空间不确定性。将这些不确定性维度纳入对AI生存风险的定性讨论中，可以显著提升我们对P(D)可能性的理解。

Conclusion: 本文的结论是，它为我们提供了一个有益的提醒，即“线性”风险模型在哲学上存在局限性，尤其是在评估AI生存风险时。通过深入分析模型差异，强调认知无差别对灾难概率P(D)的影响，并引入选项不确定性和状态空间不确定性，文章最终表明，将这些不确定性维度整合到对AI生存风险的定性讨论中，能够显著提高我们对P(D)发生可能性的理解。本文的价值在于它提供了一个更细致、更全面的框架来思考和评估AI的潜在生存风险。

Abstract: This work is a commentary of the article
\href{https://doi.org/10.18716/ojs/phai/2025.2801}{AI Survival Stories: a
Taxonomic Analysis of AI Existential Risk} by Cappelen, Goldstein, and
Hawthorne. It is not just a commentary though, but a useful reminder of the
philosophical limitations of \say{linear} models of risk. The article will
focus on the model employed by the authors: first, I discuss some differences
between standard Swiss Cheese models and this one. I then argue that in a
situation of epistemic indifference the probability of P(D) is higher than what
one might first suggest, given the structural relationships between layers. I
then distinguish between risk and uncertainty, and argue that any estimation of
P(D) is structurally affected by two kinds of uncertainty: option uncertainty
and state-space uncertainty. Incorporating these dimensions of uncertainty into
our qualitative discussion on AI existential risk can provide a better
understanding of the likeliness of P(D).

</details>


### [56] [Policy-Aware Generative AI for Safe, Auditable Data Access Governance](https://arxiv.org/abs/2510.23474)
*Shames Al Mandalawi,Muzakkiruddin Ahmed Mohammed,Hendrika Maclean,Mert Can Cakmak,John R. Talburt*

Main category: cs.AI

TL;DR: 本研究提出了一个策略感知控制器，利用大型语言模型（LLM，具体实现为Google Gemini 2.0 Flash）解释自然语言访问请求，并对照书面策略和元数据而非原始数据进行决策。该系统通过一个六阶段推理框架（包括早期硬性策略门控和默认拒绝机制），实现了满足最小权限、合规性、可审计性的访问决策。评估结果显示，系统在精确决策匹配率、拒绝召回率、误批准率以及功能适用性和合规性方面表现出色，专家对理由质量评价高，且延迟低。这表明结合策略约束的LLM推理与明确门控和审计追踪，能够将人类可读的策略转化为安全、合规且可追溯的机器决策。


<details>
  <summary>Details</summary>
Motivation: 企业在进行访问决策时面临着多重挑战，需要确保决策既能满足“最小权限”原则（即用户只能访问其完成任务所需的最低限度资源），又能遵守各项严格的法规要求，同时还必须保持决策过程的可审计性，以便后续审查和合规性验证。传统的访问控制方法可能难以灵活地处理复杂的自然语言请求和动态变化的策略，导致合规风险增加、权限过度授予以及审计困难。因此，迫切需要一种能够有效解释策略、确保合规性并提供可追溯决策的自动化解决方案。

Method: 本研究提出了一种策略感知控制器，其核心是利用大型语言模型（LLM），具体采用Google Gemini 2.0 Flash，来解析用户的自然语言访问请求。该系统并非直接处理原始数据，而是对照企业书面策略和相关元数据进行解释。其推理过程遵循一个严谨的六阶段框架：1. 上下文解释：理解请求的背景和意图。2. 用户验证：确认用户身份和权限。3. 数据分类：识别请求涉及的数据类型和敏感度。4. 业务目的测试：评估请求是否符合合法的业务目的。5. 合规映射：将请求与适用的合规性策略相匹配。6. 风险综合：综合评估潜在风险。该框架还集成了“早期硬性策略门控”机制，可以在推理过程的早期阶段识别并阻止不合规的请求，并采用“默认拒绝”的安全策略。系统最终输出结果包括“批准”、“拒绝”或“有条件批准”，并附带引用的控制措施和机器可读的决策理由。为了评估系统性能，研究人员使用了一个隐私保护基准，涵盖七种场景家族中的十四个典型案例。

Result: 评估结果显著：精确决策匹配率：在应用策略门控后，精确决策匹配率从14个案例中的10个提高到13个（达到92.9%）。拒绝召回率：对于必须拒绝的案例，系统的拒绝召回率上升至1.00，这意味着所有应该被拒绝的请求都被正确拒绝了。误批准率：在必须拒绝的场景家族中，误批准率降至0，表明系统没有发生任何错误批准。功能适用性和合规性：在所有14个案例中均达到了14/14的完美表现，体现了系统在功能和合规性方面的卓越能力。理由质量：专家对系统提供的决策理由的质量评价很高，表明其决策过程透明且可理解。延迟：系统决策的平均延迟低于一分钟，显示出良好的实时处理能力。这些结果共同证明了该策略感知控制器在处理企业访问决策方面的有效性、安全性和合规性。

Conclusion: 研究结果有力地表明，将受到明确策略约束的大型语言模型推理，与早期硬性门控和全面的审计追踪机制相结合，能够成功地将人类可读的访问策略转化为安全、合规且可追溯的机器决策。这一成果为企业提供了一个创新的解决方案，以应对在最小权限、法规遵从和可审计性方面的挑战。它不仅提高了决策的准确性和可靠性，还通过透明的理由提供了必要的审计线索，大大增强了企业访问控制的安全性与效率。未来工作可能涉及进一步扩展系统对更复杂策略和更多数据类型的处理能力，以及在不同行业背景下的应用。

Abstract: Enterprises need access decisions that satisfy least privilege, comply with
regulations, and remain auditable. We present a policy aware controller that
uses a large language model (LLM) to interpret natural language requests
against written policies and metadata, not raw data. The system, implemented
with Google Gemini~2.0 Flash, executes a six-stage reasoning framework (context
interpretation, user validation, data classification, business purpose test,
compliance mapping, and risk synthesis) with early hard policy gates and deny
by default. It returns APPROVE, DENY, CONDITIONAL together with cited controls
and a machine readable rationale. We evaluate on fourteen canonical cases
across seven scenario families using a privacy preserving benchmark. Results
show Exact Decision Match improving from 10/14 to 13/14 (92.9\%) after applying
policy gates, DENY recall rising to 1.00, False Approval Rate on must-deny
families dropping to 0, and Functional Appropriateness and Compliance Adherence
at 14/14. Expert ratings of rationale quality are high, and median latency is
under one minute. These findings indicate that policy constrained LLM
reasoning, combined with explicit gates and audit trails, can translate human
readable policies into safe, compliant, and traceable machine decisions.

</details>


### [57] [Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier](https://arxiv.org/abs/2510.23506)
*Hyeongseop Rha,Jeong Hun Yeo,Yeonju Kim,Yong Man Ro*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）在情感识别中生成的情感解释经常与预测结果不一致，这损害了可靠性和用户信任。为解决此问题，我们提出了情感理由验证器（ERV）和解释奖励机制，在不修改模型架构或增加标注的情况下，引导模型生成与目标情感一致的解释。我们的方法显著提高了MAFW和DFEW数据集上的解释-预测一致性和解释情感准确性，推动了更值得信赖的类人HCI系统发展。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的进步正在将人机交互（HCI）从表层交流转变为更细致、更具情感智能的沟通。在这一转变中，情感理解变得至关重要，它能让系统捕捉用户意图背后的细微线索。此外，为预测到的情感提供忠实的解释对于确保可解释性并建立用户信任至关重要。然而，当前的MLLM方法生成的情感解释常常与目标标签不一致，有时甚至与其自身预测的情感相矛盾。这种不一致性带来了误解的关键风险，并侵蚀了交互环境中的可靠性。

Method: 为了解决MLLMs在情感识别中解释与预测不一致的问题，我们提出了一种新颖的方法，包括情感理由验证器（Emotional Rationale Verifier, ERV）和一个解释奖励机制（Explanation Reward）。我们的方法旨在引导模型在多模态情感识别过程中生成与目标情感明确一致的推理。值得注意的是，该方法无需修改现有的模型架构，也不需要额外的配对视频-描述标注。ERV和解释奖励机制通过某种形式的训练或优化过程，促使模型输出的解释能够真实地反映其预测的情感，并与预设的目标情感保持高度一致。

Result: 我们的方法在MAFW和DFEW数据集上取得了显著的成果。具体来说，它显著提高了忠实解释与预测之间的一致性，同时也提升了解释的情感准确性。通过广泛的实验和人工评估，我们证明了我们的方法不仅增强了情感解释和情感预测之间的一致性，而且使多模态大语言模型能够提供情感连贯且值得信赖的交互。

Conclusion: 我们的研究提出了一种新颖的方法——情感理由验证器（ERV）和解释奖励机制，旨在解决多模态大语言模型（MLLMs）在情感识别中解释与预测不一致的问题。通过在不修改模型架构或增加额外标注的情况下引导模型生成与目标情感一致的推理，我们显著提高了MAFW和DFEW数据集上的解释-预测一致性和解释情感准确性。这一成果不仅增强了MLLMs在情感理解中的可信赖性，也使其能够提供情感连贯的交互，标志着向真正类人化人机交互系统迈出了关键一步。未来的工作可以探索如何进一步优化奖励机制，以适应更复杂的情感表达和情境。

Abstract: The recent advancement of Multimodal Large Language Models (MLLMs) is
transforming human-computer interaction (HCI) from surface-level exchanges into
more nuanced and emotionally intelligent communication. To realize this shift,
emotion understanding becomes essential allowing systems to capture subtle cues
underlying user intent. Furthermore, providing faithful explanations for
predicted emotions is crucial to ensure interpretability and build user trust.
However, current MLLM-based methods often generate emotion explanations that
diverge from the target labels and sometimes even contradict their own
predicted emotions. This inconsistency poses a critical risk for
misunderstanding and erodes reliability in interactive settings. To address
this, we propose a novel approach: the Emotional Rationale Verifier (ERV) and
an Explanation Reward. Our method guides the model to produce reasoning that is
explicitly consistent with the target emotion during multimodal emotion
recognition without modifying the model architecture or requiring additional
paired video-description annotations. Our method significantly improves
faithful explanation-prediction consistency and explanation emotion accuracy on
the MAFW and DFEW datasets. Through extensive experiments and human
evaluations, we show that our approach not only enhances alignment between
explanation and prediction but also empowers MLLMs to deliver emotionally
coherent, trustworthy interactions, marking a key step toward truly human-like
HCI systems.

</details>


### [58] [Toward Carbon-Neutral Human AI: Rethinking Data, Computation, and Learning Paradigms for Sustainable Intelligence](https://arxiv.org/abs/2510.23524)
*KC Santosh,Rodrigue Rizk,Longwei Wang*

Main category: cs.AI

TL;DR: 这篇论文提出了一个名为“Human AI (HAI)”的新框架，旨在通过增量学习、碳感知优化和人机协作，解决当前AI对计算资源的巨大需求及其带来的环境和伦理问题，以期实现可持续、高效、负责任且以人为中心的AI。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能的快速发展导致了前所未有的计算需求，带来了严重的环境和伦理问题。论文批判了现有AI过度依赖大规模、静态数据集和单一训练范式的做法，认为这加剧了能源消耗和资源浪费。因此，研究的动机是呼吁向受人类启发的可持续AI解决方案转变，以平衡AI性能与生态责任。

Method: 本论文引入了一个名为“Human AI (HAI)”的新型框架。HAI强调增量学习、碳感知优化以及人机协作（human-in-the-loop collaboration），旨在提升AI的适应性、效率和责任感。该方法借鉴了生物认知原理，并利用动态架构，使AI能够持续地、基于上下文地学习，同时最大限度地减少碳足迹和人工标注成本。HAI的理论基础、系统设计和操作原则均详细阐述，旨在解决主动学习、持续适应和能源效率模型部署等紧迫挑战。

Result: 通过采用HAI框架，本研究旨在实现增强AI的适应性、效率和责任感。该方法致力于平衡AI的性能与生态责任，并有效解决主动学习、持续适应和能源效率模型部署中的关键挑战。最终目标是为实现负责任的、以人为中心的人工智能提供一条可行的路径，同时最小化碳足迹和人工标注成本。

Conclusion: 论文提出了Human AI (HAI)框架，为AI领域指明了一条负责任、以人为中心的发展道路。通过强调增量学习、碳感知优化和人机协作，HAI有望克服当前AI的计算瓶颈和环境伦理问题，实现AI性能与可持续发展的平衡。未来的工作将围绕HAI的理论基础、系统设计和操作原则进行深入研究和实践，以期在主动学习、持续适应和能源高效模型部署等领域取得突破，最终推动AI向更可持续、更具社会责任的方向发展。

Abstract: The rapid advancement of Artificial Intelligence (AI) has led to
unprecedented computational demands, raising significant environmental and
ethical concerns. This paper critiques the prevailing reliance on large-scale,
static datasets and monolithic training paradigms, advocating for a shift
toward human-inspired, sustainable AI solutions. We introduce a novel
framework, Human AI (HAI), which emphasizes incremental learning, carbon-aware
optimization, and human-in-the-loop collaboration to enhance adaptability,
efficiency, and accountability. By drawing parallels with biological cognition
and leveraging dynamic architectures, HAI seeks to balance performance with
ecological responsibility. We detail the theoretical foundations, system
design, and operational principles that enable AI to learn continuously and
contextually while minimizing carbon footprints and human annotation costs. Our
approach addresses pressing challenges in active learning, continual
adaptation, and energy-efficient model deployment, offering a pathway toward
responsible, human-centered artificial intelligence.

</details>


### [59] [JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538)
*Qiushi Sun,Jingyang Gong,Yang Liu,Qiaosheng Chen,Lei Li,Kai Chen,Qipeng Guo,Ben Kao,Fei Yuan*

Main category: cs.AI

TL;DR: 该研究引入了一个完整的合成工具包，用于高效生成大规模、高质量的多模态代码数据语料库JanusCode-800K。在此基础上，提出了统一的多模态代码生成模型JanusCoder和JanusCoderV，它们能够从文本指令、视觉输入或两者结合生成代码。实验证明，JanusCoder系列模型在文本中心和视觉中心编码任务上均表现出色，性能接近甚至超越商业模型，并提供了关于程序逻辑与视觉表达协调的关键见解。


<details>
  <summary>Details</summary>
Motivation: 神经代码智能的范围正在迅速从基于文本的源代码扩展到程序生成的丰富视觉输出。这一视觉维度对于灵活的内容生成和精确的、程序驱动的可视化编辑等高级应用至关重要。然而，由于高质量多模态代码数据的稀缺性，以及合成和质量评估方面的挑战，该领域的进展受到了阻碍。现有方法通常为孤立的任务构建专门模型。

Method: 为解决数据稀缺性问题，该研究首先引入了一个完整的合成工具包，该工具包利用数据模态之间的互惠协同作用，高效地生成了从标准图表到复杂交互式Web UI和代码驱动动画的大规模、高质量语料库。利用这个工具包，构建了迄今为止最大的多模态代码语料库JanusCode-800K（800K条数据）。该语料库用于训练所提出的模型JanusCoder和JanusCoderV。这些模型建立了一个视觉-程序接口，能够从文本指令、视觉输入或两者的组合中生成代码。与为孤立任务构建专门模型的现有方法不同，JanusCoder和JanusCoderV是统一的模型。

Result: 在文本中心和视觉中心编码任务上的广泛实验表明，JanusCoder系列模型表现出卓越的性能。特别是，7B到14B规模的模型性能接近甚至超越了商业模型。此外，深入的分析提供了关于如何协调程序逻辑及其视觉表达的关键见解。项目的代码和检查点已在GitHub上公开。

Conclusion: 该研究通过提供一个完整的数据合成工具包和迄今为止最大的多模态代码语料库JanusCode-800K，有效地解决了高质量多模态代码数据稀缺的问题。在此基础上，提出并训练了统一的多模态代码生成模型JanusCoder和JanusCoderV，它们能够灵活地从文本、视觉或多模态输入生成代码，并在各种编码任务上取得了卓越的性能，其7B至14B规模模型表现达到或超越了商业模型。这项工作为高级代码智能应用奠定了基础，并提供了程序逻辑与视觉表达之间关系的重要见解。

Abstract: The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

</details>


### [60] [OntoPret: An Ontology for the Interpretation of Human Behavior](https://arxiv.org/abs/2510.23553)
*Alexis Ellis,Stacie Severyn,Fjollë Novakazi,Hadi Banaee,Cogan Shimizu*

Main category: cs.AI

TL;DR: 该论文介绍了 OntoPret，一种用于机器解释复杂人类行为的本体，旨在弥合以技术为中心的机器人框架与描述性行为本体之间的鸿沟。OntoPret 植根于认知科学和模块化工程方法，提供了一个形式化、机器可处理的框架，用于对包括任务偏差和欺骗行为在内的行为进行分类。它在制造和游戏玩法这两个不同的用例中展示了其适应性，并为人类意图的高级推理奠定了语义基础。


<details>
  <summary>Details</summary>
Motivation: 随着人机协作成为工业 5.0 等范式的核心，机器安全有效地解释复杂人类行为的需求变得至关重要。目前，在缺乏细致人类行为模型的以技术为中心的机器人框架与未针对实时协作解释设计的描述性行为本体之间存在研究空白。

Method: 本文通过提出 OntoPret 来解决这一研究空白，OntoPret 是一种用于解释人类行为的本体。它以认知科学和模块化工程方法为基础，提供了一个形式化、机器可处理的框架，用于对包括任务偏差和欺骗行为在内的行为进行分类。

Result: 该研究展示了 OntoPret 在制造和游戏玩法这两个不同用例中的适应性。它为高级推理人类意图建立了必要的语义基础。

Conclusion: 本文通过提出 OntoPret，弥合了机器解释复杂人类行为的现有空白，为理解人类意图提供了形式化、机器可处理的分类和语义基础，从而支持安全有效的人机协作。

Abstract: As human machine teaming becomes central to paradigms like Industry 5.0, a
critical need arises for machines to safely and effectively interpret complex
human behaviors. A research gap currently exists between techno centric robotic
frameworks, which often lack nuanced models of human behavior, and descriptive
behavioral ontologies, which are not designed for real time, collaborative
interpretation. This paper addresses this gap by presenting OntoPret, an
ontology for the interpretation of human behavior. Grounded in cognitive
science and a modular engineering methodology, OntoPret provides a formal,
machine processable framework for classifying behaviors, including task
deviations and deceptive actions. We demonstrate its adaptability across two
distinct use cases manufacturing and gameplay and establish the semantic
foundations necessary for advanced reasoning about human intentions.

</details>


### [61] [Reduced AI Acceptance After the Generative AI Boom: Evidence From a Two-Wave Survey Study](https://arxiv.org/abs/2510.23578)
*Joachim Baumann,Aleksandra Urman,Ulrich Leicht-Deobald,Zachary J. Roman,Anikó Hannák,Markus Christen*

Main category: cs.AI

TL;DR: 生成式人工智能（GenAI）的快速发展与采用导致公众对AI的接受度降低，对人工监督的需求增加，并加剧了社会不平等（如教育、语言和性别差距），这与行业关于公众准备就绪的假设相悖，凸显了技术发展与公众偏好保持一致的重要性。


<details>
  <summary>Details</summary>
Motivation: 许多组织在未充分考虑用户偏好的情况下将人工智能集成到产品和服务中。然而，公众对人工智能使用的态度，特别是在具有影响力的决策场景中，尚未得到充分探索。随着ChatGPT的推出，生成式人工智能的快速发展使得了解这些态度的转变及其对社会的影响变得至关重要，以确保技术发展与公众需求相符。

Method: 本研究采用了一项大规模的两波次调查研究，以代表瑞士人口的样本为对象（第一波n=1514，第二波n=1488）。研究旨在比较在ChatGPT发布前后公众对人工智能的态度变化，以评估生成式人工智能热潮对公众接受度和对人工监督需求的影响。

Result: 研究发现，生成式人工智能热潮与公众对人工智能接受度的显著降低有关，同时增加了在各种决策场景中对人工监督的需求。具体而言，认为人工智能“完全不可接受”的受访者比例从23%上升到30%，而支持“纯粹由人类决策”的比例从18%上升到26%。此外，这些转变还加剧了现有的社会不平等，导致教育、语言和性别差距在热潮后进一步扩大。

Conclusion: 本研究结果挑战了行业关于公众已准备好接受人工智能部署的假设，并强调了将技术发展与不断变化的公众偏好相协调的关键重要性。未来的工作可以进一步探索导致这些态度转变的具体因素，以及如何设计和实施人工智能系统以更好地满足公众需求并减少社会不平等。

Abstract: The rapid adoption of generative artificial intelligence (GenAI) technologies
has led many organizations to integrate AI into their products and services,
often without considering user preferences. Yet, public attitudes toward AI
use, especially in impactful decision-making scenarios, are underexplored.
Using a large-scale two-wave survey study (n_wave1=1514, n_wave2=1488)
representative of the Swiss population, we examine shifts in public attitudes
toward AI before and after the launch of ChatGPT. We find that the GenAI boom
is significantly associated with reduced public acceptance of AI (see Figure 1)
and increased demand for human oversight in various decision-making contexts.
The proportion of respondents finding AI "not acceptable at all" increased from
23% to 30%, while support for human-only decision-making rose from 18% to 26%.
These shifts have amplified existing social inequalities in terms of widened
educational, linguistic, and gender gaps post-boom. Our findings challenge
industry assumptions about public readiness for AI deployment and highlight the
critical importance of aligning technological development with evolving public
preferences.

</details>


### [62] [Multi-Agent Evolve: LLM Self-Improve through Co-evolution](https://arxiv.org/abs/2510.23595)
*Yixing Chen,Yiding Wang,Siqi Zhu,Haofei Yu,Tao Feng,Muhan Zhan,Mostofa Patwary,Jiaxuan You*

Main category: cs.AI

TL;DR: 该论文提出了Multi-Agent Evolve (MAE)框架，通过由提案者、求解者和评估者三个LLM实例化智能体组成的自博弈强化学习，使LLM能够在数学、推理和常识问答等多样化任务中自我进化。实验在Qwen2.5-3B-Instruct上实现了平均4.54%的性能提升，表明MAE是一种可扩展、数据高效且对人工监督依赖极少的方法，可用于增强LLM的通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前，强化学习（RL）在增强大型语言模型（LLMs）推理能力方面展现出巨大潜力。然而，其成功高度依赖于人工整理的数据集和可验证的奖励，这严重限制了RL在LLMs应用中的可扩展性和通用性。近期受游戏和Go领域成功的启发，自博弈RL方法旨在无需人工标注数据的情况下提升LLMs的推理能力。但这些方法主要依赖于一个有具体反馈的环境（例如Python解释器或游戏引擎），将其扩展到通用领域仍然面临挑战。研究的动机在于解决这些限制，开发一种无需大量人工监督且能扩展到更广泛通用领域任务的LLM推理增强方法。

Method: 为了解决上述挑战，本文提出了Multi-Agent Evolve (MAE)框架。MAE的核心设计是基于三个相互作用的智能体（提案者、求解者和评估者），这些智能体均由同一个LLM实例化。提案者负责生成问题，求解者尝试提供解决方案，而评估者则对问题和解决方案进行评估。该框架通过强化学习来优化这些智能体的行为，使其能够协同进化。这种设计使得LLM能够在多样化的任务中（包括数学、推理和通用知识问答）进行自我进化，而无需外部的人工标注数据或具体环境反馈。

Result: 研究通过在Qwen2.5-3B-Instruct模型上进行实验，评估了MAE框架的性能。实验结果表明，MAE在多个基准测试上实现了平均4.54%的性能提升。这些结果突出强调了MAE作为一种可扩展、数据高效的方法，能够在最少依赖人工监督的情况下，显著增强LLMs的通用推理能力。这验证了MAE在解决现有RL方法限制方面的有效性，并展示了其在实际应用中的潜力。

Conclusion: MAE框架通过引入一套由提案者、求解者和评估者组成的自进化智能体系统，成功解决了传统强化学习在增强LLM推理能力时对人工数据和特定环境反馈的依赖问题。实验结果证明，MAE能够有效提升LLMs在数学、推理和通用知识问答等多样化任务上的表现，平均性能提升达4.54%。这表明MAE是一种具有高度可扩展性和数据效率的方法，能够以最少的人工监督，显著增强LLM的通用推理能力，为LLM的未来发展提供了新的方向。该方法有望推动LLM在更广泛的通用领域任务中实现自主学习和进化。

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in
enhancing the reasoning capabilities of large language models (LLMs). However,
the success of RL for LLMs heavily relies on human-curated datasets and
verifiable rewards, which limit their scalability and generality. Recent
Self-Play RL methods, inspired by the success of the paradigm in games and Go,
aim to enhance LLM reasoning capabilities without human-annotated data.
However, their methods primarily depend on a grounded environment for feedback
(e.g., a Python interpreter or a game engine); extending them to general
domains remains challenging. To address these challenges, we propose
Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in
solving diverse tasks, including mathematics, reasoning, and general knowledge
Q&A. The core design of MAE is based on a triplet of interacting agents
(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies
reinforcement learning to optimize their behaviors. The Proposer generates
questions, the Solver attempts solutions, and the Judge evaluates both while
co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves
an average improvement of 4.54% on multiple benchmarks. These results highlight
MAE as a scalable, data-efficient method for enhancing the general reasoning
abilities of LLMs with minimal reliance on human-curated supervision.

</details>


### [63] [Faster Reinforcement Learning by Freezing Slow States](https://arxiv.org/abs/2301.00922)
*Yijia Wang,Daniel R. Jiang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study infinite horizon Markov decision processes (MDPs) with "fast-slow"
structure, where some state variables evolve rapidly ("fast states") while
others change more gradually ("slow states"). This structure commonly arises in
practice when decisions must be made at high frequencies over long horizons,
and where slowly changing information still plays a critical role in
determining optimal actions. Examples include inventory control under slowly
changing demand indicators or dynamic pricing with gradually shifting consumer
behavior. Modeling the problem at the natural decision frequency leads to MDPs
with discount factors close to one, making them computationally challenging. We
propose a novel approximation strategy that "freezes" slow states during phases
of lower-level planning and subsequently applies value iteration to an
auxiliary upper-level MDP that evolves on a slower timescale. Freezing states
for short periods of time leads to easier-to-solve lower-level problems, while
a slower upper-level timescale allows for a more favorable discount factor. On
the theoretical side, we analyze the regret incurred by our frozen-state
approach, which leads to simple insights on how to trade off regret versus
computational cost. Empirically, we benchmark our new frozen-state methods on
three domains, (i) inventory control with fixed order costs, (ii) a gridworld
problem with spatial tasks, and (iii) dynamic pricing with reference-price
effects. We demonstrate that the new methods produce high-quality policies with
significantly less computation, and we show that simply omitting slow states is
often a poor heuristic.

</details>


### [64] [Online POMDP Planning with Anytime Deterministic Optimality Guarantees](https://arxiv.org/abs/2310.01791)
*Moran Barenboim,Vadim Indelman*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Decision-making under uncertainty is a critical aspect of many practical
autonomous systems due to incomplete information. Partially Observable Markov
Decision Processes (POMDPs) offer a mathematically principled framework for
formulating decision-making problems under such conditions. However, finding an
optimal solution for a POMDP is generally intractable. In recent years, there
has been a significant progress of scaling approximate solvers from small to
moderately sized problems, using online tree search solvers. Often, such
approximate solvers are limited to probabilistic or asymptotic guarantees
towards the optimal solution. In this paper, we derive a deterministic
relationship for discrete POMDPs between an approximated and the optimal
solution. We show that at any time, we can derive bounds that relate between
the existing solution and the optimal one. We show that our derivations provide
an avenue for a new set of algorithms and can be attached to existing
algorithms that have a certain structure to provide them with deterministic
guarantees with marginal computational overhead. In return, not only do we
certify the solution quality, but we demonstrate that making a decision based
on the deterministic guarantee may result in superior performance compared to
the original algorithm without the deterministic certification.

</details>


### [65] [GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability](https://arxiv.org/abs/2403.04483)
*Zihan Luo,Xiran Song,Hong Huang,Jianxun Lian,Chenhao Zhang,Jinqi Jiang,Xing Xie,Hai Jin*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Improving the general capabilities of large language models (LLMs) is an
active research topic. As a common data structure in many real-world domains,
understanding graph data is a crucial part of advancing general intelligence.
To this end, we propose a dynamic benchmark named GraphInstruct in this paper,
which comprehensively includes 21 classical graph reasoning tasks, providing
diverse graph generation pipelines and detailed intermediate reasoning steps
for each sample. Based on GraphInstruct, we develop GraphSolver via efficient
instruction-tuning, which demonstrates prominent graph understanding capability
compared to other open-sourced LLMs. To further endow LLMs with multi-step
graph reasoning capability, we propose a label-mask training strategy and build
GraphSolver+, which leverages masked supervision on intermediate reasoning
tokens to emphasize crucial node-identification signals. As one of the
pioneering efforts to enhance the graph understanding and reasoning abilities
of LLMs, extensive experiments have demonstrated the superiority of GraphSolver
and GraphSolver+ over other LLMs. We sincerely hope GraphInstruct will
facilitate further research on applying LLMs to graph-structured data. Our code
and data are released publicly at: https://github.com/CGCL-codes/GraphInstruct.

</details>


### [66] [Integrated Design and Governance of Agentic AI Systems through Adaptive Information Modulation](https://arxiv.org/abs/2409.10372)
*Qiliang Chen,Sepehr Ilami,Nunzio Lore,Babak Heydari*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern engineered systems increasingly involve complex sociotechnical
environments where multiple agents, including humans and the emerging paradigm
of agentic AI powered by large language models, must navigate social dilemmas
that pit individual interests against collective welfare. As engineered systems
evolve toward multi-agent architectures with autonomous LLM-based agents,
traditional governance approaches using static rules or fixed network
structures fail to address the dynamic uncertainties inherent in real-world
operations. This paper presents a novel framework that integrates adaptive
governance mechanisms directly into the design of sociotechnical systems
through a unique separation of agent interaction networks from information flow
networks. We introduce a system comprising strategic LLM-based system agents
that engage in repeated interactions and a reinforcement learning-based
governing agent that dynamically modulates information transparency. Unlike
conventional approaches that require direct structural interventions or payoff
modifications, our framework preserves agent autonomy while promoting
cooperation through adaptive information governance. The governing agent learns
to strategically adjust information disclosure at each timestep, determining
what contextual or historical information each system agent can access.
Experimental results demonstrate that this RL-based governance significantly
enhances cooperation compared to static information-sharing baselines.

</details>


### [67] [Learning to Better Search with Language Models via Guided Reinforced Self-Training](https://arxiv.org/abs/2410.02992)
*Seungyong Moon,Bumsoo Park,Hyun Oh Song*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While language models have shown remarkable performance across diverse tasks,
they still encounter challenges in complex reasoning scenarios. Recent research
suggests that language models trained on linearized search traces toward
solutions, rather than solely on the final solutions, exhibit improved
generalization, despite the search traces being potentially noisy or
suboptimal. However, relying on such imperfect traces can result in inefficient
use of test-time compute. To address this, we propose guided reinforced
self-training (Guided-ReST), a fine-tuning algorithm designed to improve the
model's capability for effective search during inference. The key insight
behind Guided-ReST is that optimal solutions can serve as valuable step-by-step
landmarks to guide the model's search process. Based on this insight, we
introduce a novel data generation method that seamlessly incorporates optimal
solutions into the model's search procedure, enabling the generation of
high-quality search traces. By fine-tuning the model on these search traces, we
effectively distill improved search strategies into the model. Our method
significantly enhances the search capabilities of language models on arithmetic
reasoning and code self-repair tasks, including Countdown, CodeContests, and
CodeForces. We release the source code at
https://github.com/snu-mllab/guided-rest.

</details>


### [68] [Diversified and Adaptive Negative Sampling on Knowledge Graphs](https://arxiv.org/abs/2410.07592)
*Ran Liu,Zhongzhou Liu,Xiaoli Li,Hao Wu,Yuan Fang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In knowledge graph embedding, aside from positive triplets (ie: facts in the
knowledge graph), the negative triplets used for training also have a direct
influence on the model performance. In reality, since knowledge graphs are
sparse and incomplete, negative triplets often lack explicit labels, and thus
they are often obtained from various sampling strategies (eg: randomly
replacing an entity in a positive triplet). An ideal sampled negative triplet
should be informative enough to help the model train better. However, existing
methods often ignore diversity and adaptiveness in their sampling process,
which harms the informativeness of negative triplets. As such, we propose a
generative adversarial approach called Diversified and Adaptive Negative
Sampling DANS on knowledge graphs. DANS is equipped with a two-way generator
that generates more diverse negative triplets through two pathways, and an
adaptive mechanism that produces more fine-grained examples by localizing the
global generator for different entities and relations. On the one hand, the
two-way generator increase the overall informativeness with more diverse
negative examples; on the other hand, the adaptive mechanism increases the
individual sample-wise informativeness with more fine-grained sampling.
Finally, we evaluate the performance of DANS on three benchmark knowledge
graphs to demonstrate its effectiveness through quantitative and qualitative
experiments.

</details>


### [69] [Training-Free Safe Denoisers for Safe Use of Diffusion Models](https://arxiv.org/abs/2502.08011)
*Mingyu Kim,Dongjun Kim,Amman Yusuf,Stefano Ermon,Mijung Park*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: There is growing concern over the safety of powerful diffusion models (DMs),
as they are often misused to produce inappropriate, not-safe-for-work (NSFW)
content or generate copyrighted material or data of individuals who wish to be
forgotten. Many existing methods tackle these issues by heavily relying on
text-based negative prompts or extensively retraining DMs to eliminate certain
features or samples. In this paper, we take a radically different approach,
directly modifying the sampling trajectory by leveraging a negation set (e.g.,
unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid
specific regions of data distribution, without needing to retrain or fine-tune
DMs. We formally derive the relationship between the expected denoised samples
that are safe and those that are not safe, leading to our $\textit{safe}$
denoiser which ensures its final samples are away from the area to be negated.
Inspired by the derivation, we develop a practical algorithm that successfully
produces high-quality samples while avoiding negation areas of the data
distribution in text-conditional, class-conditional, and unconditional image
generation scenarios. These results hint at the great potential of our
training-free safe denoiser for using DMs more safely.

</details>


### [70] [Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals](https://arxiv.org/abs/2502.16101)
*Linda Zeng,Rithwik Gupta,Divij Motwani,Diji Yang,Yi Zhang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Retrieval-augmented generation (RAG) has shown impressive capabilities in
mitigating hallucinations in large language models (LLMs). However, LLMs
struggle to maintain consistent reasoning when exposed to misleading or
conflicting evidence, especially in real-world domains such as politics, where
information is polarized or selectively framed. Mainstream RAG benchmarks
evaluate models under clean retrieval settings, where systems generate answers
from gold-standard documents, or under synthetically perturbed settings, where
documents are artificially injected with noise. These assumptions fail to
reflect real-world conditions, often leading to an overestimation of RAG system
performance. To address this gap, we introduce RAGuard, the first benchmark to
evaluate the robustness of RAG systems against misleading retrievals. Unlike
prior benchmarks that rely on synthetic noise, our fact-checking dataset
captures naturally occurring misinformation by constructing its retrieval
corpus from Reddit discussions. It categorizes retrieved evidence into three
types: supporting, misleading, and unrelated, providing a realistic and
challenging testbed for assessing how well RAG systems navigate different types
of evidence. Our experiments reveal that, when exposed to potentially
misleading retrievals, all tested LLM-powered RAG systems perform worse than
their zero-shot baselines (i.e., no retrieval at all), while human annotators
consistently perform better, highlighting LLMs' susceptibility to noisy
environments. To our knowledge, RAGuard is the first benchmark to
systematically assess the robustness of the RAG against misleading evidence. We
expect this benchmark to drive future research toward improving RAG systems
beyond idealized datasets, making them more reliable for real-world
applications. The dataset is available at
https://huggingface.co/datasets/UCSC-IRKM/RAGuard.

</details>


### [71] [Why Do Multi-Agent LLM Systems Fail?](https://arxiv.org/abs/2503.13657)
*Mert Cemri,Melissa Z. Pan,Shuyi Yang,Lakshya A. Agrawal,Bhavya Chopra,Rishabh Tiwari,Kurt Keutzer,Aditya Parameswaran,Dan Klein,Kannan Ramchandran,Matei Zaharia,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite enthusiasm for Multi-Agent LLM Systems (MAS), their performance gains
on popular benchmarks are often minimal. This gap highlights a critical need
for a principled understanding of why MAS fail. Addressing this question
requires systematic identification and analysis of failure patterns. We
introduce MAST-Data, a comprehensive dataset of 1600+ annotated traces
collected across 7 popular MAS frameworks. MAST-Data is the first multi-agent
system dataset to outline the failure dynamics in MAS for guiding the
development of better future systems. To enable systematic classification of
failures for MAST-Data, we build the first Multi-Agent System Failure Taxonomy
(MAST). We develop MAST through rigorous analysis of 150 traces, guided closely
by expert human annotators and validated by high inter-annotator agreement
(kappa = 0.88). This process identifies 14 unique modes, clustered into 3
categories: (i) system design issues, (ii) inter-agent misalignment, and (iii)
task verification. To enable scalable annotation, we develop an LLM-as-a-Judge
pipeline with high agreement with human annotations. We leverage MAST and
MAST-Data to analyze failure patterns across models (GPT4, Claude 3, Qwen2.5,
CodeLlama) and tasks (coding, math, general agent), demonstrating improvement
headrooms from better MAS design. Our analysis provides insights revealing that
identified failures require more sophisticated solutions, highlighting a clear
roadmap for future research. We publicly release our comprehensive dataset
(MAST-Data), the MAST, and our LLM annotator to facilitate widespread research
and development in MAS.

</details>


### [72] [Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing](https://arxiv.org/abs/2503.15815)
*Vishnu Asutosh Dasu,Md Rafi ur Rashid,Vipul Gupta,Saeid Tizpaz-Niari,Gang Tan*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper explores pruning attention heads as a post-processing bias
mitigation method for large language models (LLMs). Modern AI systems such as
LLMs are expanding into sensitive social contexts where fairness concerns
become especially crucial. Since LLMs develop decision-making patterns by
training on massive datasets of human-generated content, they naturally encode
and perpetuate societal biases. While modifying training datasets and
algorithms is expensive and requires significant resources; post-processing
techniques-such as selectively deactivating neurons and attention heads in
pre-trained LLMs-can provide feasible and effective approaches to improve
fairness. However, identifying the optimal subset of parameters to prune
presents a combinatorial challenge within LLMs' immense parameter space,
requiring solutions that efficiently balance competing objectives across the
frontiers of model fairness and utility.
  To address the computational challenges, we explore a search-based program
repair approach via randomized simulated annealing. Given the prohibitive
evaluation costs in billion-parameter LLMs, we develop surrogate deep neural
networks that efficiently model the relationship between attention head states
(active/inactive) and their corresponding fairness/utility metrics. This allows
us to perform optimization over the surrogate models and efficiently identify
optimal subsets of attention heads for selective pruning rather than directly
searching through the LLM parameter space. This paper introduces Attention
Pruning, a fairness-aware surrogate simulated annealing approach to prune
attention heads in LLMs that disproportionately contribute to bias while
minimally impacting overall model utility. Our experiments show that Attention
Pruning achieves up to $40\%$ reduction in gender bias and outperforms the
state-of-the-art bias mitigation strategies.

</details>


### [73] [LLMs as Planning Formalizers: A Survey for Leveraging Large Language Models to Construct Automated Planning Models](https://arxiv.org/abs/2503.18971)
*Marcus Tantakoun,Xiaodan Zhu,Christian Muise*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) excel in various natural language tasks but
often struggle with long-horizon planning problems requiring structured
reasoning. This limitation has drawn interest in integrating neuro-symbolic
approaches within the Automated Planning (AP) and Natural Language Processing
(NLP) communities. However, identifying optimal AP deployment frameworks can be
daunting and introduces new challenges. This paper aims to provide a timely
survey of the current research with an in-depth analysis, positioning LLMs as
tools for formalizing and refining planning specifications to support reliable
off-the-shelf AP planners. By systematically reviewing the current state of
research, we highlight methodologies, and identify critical challenges and
future directions, hoping to contribute to the joint research on NLP and
Automated Planning.

</details>


### [74] [Antidistillation Sampling](https://arxiv.org/abs/2504.13146)
*Yash Savani,Asher Trockman,Zhili Feng,Yixuan Even Xu,Avi Schwarzschild,Alexander Robey,Marc Finzi,J. Zico Kolter*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Frontier models that generate extended reasoning traces inadvertently produce
rich token sequences that can facilitate model distillation. Recognizing this
vulnerability, model owners may seek sampling strategies that limit the
effectiveness of distillation without compromising model performance.
Antidistillation sampling provides exactly this capability. By strategically
modifying a model's next-token probability distribution, antidistillation
sampling poisons reasoning traces, rendering them significantly less effective
for distillation while preserving the model's practical utility. For further
details, see https://antidistillation.com.

</details>


### [75] [Scaling Laws For Scalable Oversight](https://arxiv.org/abs/2504.18530)
*Joshua Engels,David D. Baek,Subhash Kantamneni,Max Tegmark*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scalable oversight, the process by which weaker AI systems supervise stronger
ones, has been proposed as a key strategy to control future superintelligent
systems. However, it is still unclear how scalable oversight itself scales. To
address this gap, we propose a framework that quantifies the probability of
successful oversight as a function of the capabilities of the overseer and the
system being overseen. Specifically, our framework models oversight as a game
between capability-mismatched players; the players have oversight-specific Elo
scores that are a piecewise-linear function of their general intelligence, with
two plateaus corresponding to task incompetence and task saturation. We
validate our framework with a modified version of the game Nim and then apply
it to four oversight games: Mafia, Debate, Backdoor Code and Wargames. For each
game, we find scaling laws that approximate how domain performance depends on
general AI system capability. We then build on our findings in a theoretical
study of Nested Scalable Oversight (NSO), a process in which trusted models
oversee untrusted stronger models, which then become the trusted models in the
next step. We identify conditions under which NSO succeeds and derive
numerically (and in some cases analytically) the optimal number of oversight
levels to maximize the probability of oversight success. We also apply our
theory to our four oversight games, where we find that NSO success rates at a
general Elo gap of 400 are 13.5% for Mafia, 51.7% for Debate, 10.0% for
Backdoor Code, and 9.4% for Wargames; these rates decline further when
overseeing stronger systems.

</details>


### [76] [GVPO: Group Variance Policy Optimization for Large Language Model Post-Training](https://arxiv.org/abs/2504.19599)
*Kaichen Zhang,Yuzhong Hong,Junwei Bao,Hongfei Jiang,Yang Song,Dingqian Hong,Hui Xiong*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Post-training plays a crucial role in refining and aligning large language
models to meet specific tasks and human preferences. While recent advancements
in post-training techniques, such as Group Relative Policy Optimization (GRPO),
leverage increased sampling with relative reward scoring to achieve superior
performance, these methods often suffer from training instability that limits
their practical adoption. As a next step, we present Group Variance Policy
Optimization (GVPO). GVPO incorporates the analytical solution to
KL-constrained reward maximization directly into its gradient weights, ensuring
alignment with the optimal policy. The method provides intuitive physical
interpretations: its gradient mirrors the mean squared error between the
central distance of implicit rewards and that of actual rewards. GVPO offers
two key advantages: (1) it guarantees a unique optimal solution, exactly the
KL-constrained reward maximization objective, (2) it supports flexible sampling
distributions that avoids on-policy and importance sampling limitations. By
unifying theoretical guarantees with practical adaptability, GVPO establishes a
new paradigm for reliable and versatile LLM post-training.

</details>


### [77] [Lost in Transmission: When and Why LLMs Fail to Reason Globally](https://arxiv.org/abs/2505.08140)
*Tobias Schnabel,Kiran Tomlinson,Adith Swaminathan,Jennifer Neville*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite their many successes, transformer-based large language models (LLMs)
continue to struggle with tasks that require complex reasoning over large parts
of their input. We argue that these failures arise due to capacity limits on
the accurate flow of information within LLMs. To formalize this issue, we
introduce the bounded attention prefix oracle (BAPO) model, a new computational
framework that models bandwidth constraints on attention heads, the mechanism
for internal communication in LLMs. We show that several important reasoning
problems like graph reachability require high communication bandwidth for BAPOs
to solve; we call these problems BAPO-hard. Our experiments corroborate our
theoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks
and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another
benefit of chain of thought (CoT): we prove that breaking down a task using CoT
can turn any BAPO-hard problem into a BAPO-easy one. Our results offer
principled explanations for key LLM failures and suggest directions for
architectures and inference methods that mitigate bandwidth limits.

</details>


### [78] [Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://arxiv.org/abs/2505.13227)
*Tianbao Xie,Jiaqi Deng,Xiaochuan Li,Junlin Yang,Haoyuan Wu,Jixuan Chen,Wenjing Hu,Xinyuan Wang,Yuhui Xu,Zekun Wang,Yiheng Xu,Junli Wang,Doyen Sahoo,Tao Yu,Caiming Xiong*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graphical user interface (GUI) grounding, the ability to map natural language
instructions to specific actions on graphical user interfaces, remains a
critical bottleneck in computer use agent development. Current benchmarks
oversimplify grounding tasks as short referring expressions, failing to capture
the complexity of real-world interactions that require software commonsense,
layout understanding, and fine-grained manipulation capabilities. To address
these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising
564 finely annotated samples across diverse task types including text matching,
element recognition, layout understanding, and precise manipulation.
Additionally, we synthesize and release the largest computer use grounding
dataset Jedi, which contains 4 million examples through multi-perspective
decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its
effectiveness by outperforming existing approaches on ScreenSpot-v2,
ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved
grounding with Jedi directly enhances agentic capabilities of general
foundation models on complex computer tasks, improving from 5% to 27% on
OSWorld. Through detailed ablation studies, we identify key factors
contributing to grounding performance and verify that combining specialized
data for different interface elements enables compositional generalization to
novel interfaces. All benchmark, data, checkpoints, and code are open-sourced
and available at https://osworld-grounding.github.io.

</details>


### [79] [Guarded Query Routing for Large Language Models](https://arxiv.org/abs/2505.14524)
*Richard Šléher,William Brach,Tibor Sloboda,Kristián Košťál,Lukas Galke*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Query routing, the task to route user queries to different large language
model (LLM) endpoints, can be considered as a text classification problem.
However, out-of-distribution queries must be handled properly, as those could
be about unrelated domains, queries in other languages, or even contain unsafe
text. Here, we thus study a guarded query routing problem, for which we first
introduce the Guarded Query Routing Benchmark (GQR-Bench, released as Python
package gqr), covers three exemplary target domains (law, finance, and
healthcare), and seven datasets to test robustness against out-of-distribution
queries. We then use GQR-Bench to contrast the effectiveness and efficiency of
LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),
standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo
Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and
traditional machine learning models (SVM, XGBoost). Our results show that
WideMLP, enhanced with out-of-domain detection capabilities, yields the best
trade-off between accuracy (88%) and speed (<4ms). The embedding-based fastText
excels at speed (<1ms) with acceptable accuracy (80%), whereas LLMs yield the
highest accuracy (91%) but are comparatively slow (62ms for local Llama-3.1:8B
and 669ms for remote GPT-4o-mini calls). Our findings challenge the automatic
reliance on LLMs for (guarded) query routing and provide concrete
recommendations for practical applications. Source code is available:
https://github.com/williambrach/gqr.

</details>


### [80] [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/abs/2505.14668)
*Bufang Yang,Lilin Xu,Liekang Zeng,Kaiwei Liu,Siyang Jiang,Wenrui Lu,Hongkai Chen,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts surrounding humans to enhance the proactivity of LLM
agents. ContextAgent first extracts multi-dimensional contexts from massive
sensory perceptions on wearables (e.g., video and audio) to understand user
intentions. ContextAgent then leverages the sensory contexts and personas from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants. The
code and dataset are publicly available at
https://github.com/openaiotlab/ContextAgent.

</details>


### [81] [On the Hardness of Approximating Distributions with Tractable Probabilistic Models](https://arxiv.org/abs/2506.01281)
*John Leland,YooJung Choi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A fundamental challenge in probabilistic modeling is to balance expressivity
and inference efficiency. Tractable probabilistic models (TPMs) aim to directly
address this tradeoff by imposing constraints that guarantee efficient
inference of certain queries while maintaining expressivity. In particular,
probabilistic circuits (PCs) provide a unifying framework for many TPMs, by
characterizing families of models as circuits satisfying different structural
properties. Because the complexity of inference on PCs is a function of the
circuit size, understanding the size requirements of different families of PCs
is fundamental in mapping the trade-off between tractability and expressive
efficiency. However, the study of expressive efficiency of circuits are often
concerned with exact representations, which may not align with model learning,
where we look to approximate the underlying data distribution closely by some
distance measure. Moreover, due to hardness of inference tasks, exactly
representing distributions while supporting tractable inference often incurs
exponential size blow-ups. In this paper, we consider a natural, yet so far
underexplored, question: can we avoid such size blow-up by allowing for some
small approximation error? We study approximating distributions with
probabilistic circuits with guarantees based on $f$-divergences, and analyze
which inference queries remain well-approximated under this framework. We show
that approximating an arbitrary distribution with bounded $f$-divergence is
$\mathsf{NP}$-hard for any model that can tractably compute marginals. In
addition, we prove an exponential size gap for approximation between the class
of decomposable PCs and that of decomposable and deterministic PCs.

</details>


### [82] [E-bike agents: Large Language Model-Driven E-Bike Accident Analysis and Severity Prediction](https://arxiv.org/abs/2506.04654)
*Zhichao Yang,Jiashu He,Mohammad B. Al-Khasawneh,Darshan Pandit,Cirillo Cinzia*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: E-bikes have rapidly gained popularity as a sustainable form of urban
mobility, yet their safety implications remain underexplored. This paper
analyzes injury incidents involving e-bikes and traditional bicycles using two
sources of data, the CPSRMS (Consumer Product Safety Risk Management System
Information Security Review Report) and NEISS (National Electronic Injury
Surveillance System) datasets. We propose a standardized classification
framework to identify and quantify injury causes and severity. By integrating
incident narratives with demographic attributes, we reveal key differences in
mechanical failure modes, injury severity patterns, and affected user groups.
While both modes share common causes, such as loss of control and pedal
malfunctions, e-bikes present distinct risks, including battery-related fires
and brake failures. These findings highlight the need for tailored safety
interventions and infrastructure design to support the safe integration of
micromobility devices into urban transportation networks.

</details>


### [83] [Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems](https://arxiv.org/abs/2506.10192)
*Filip Cano*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ensuring responsible use of artificial intelligence (AI) has become
imperative as autonomous systems increasingly influence critical societal
domains. However, the concept of trustworthy AI remains broad and
multi-faceted. This thesis advances knowledge in the safety, fairness,
transparency, and accountability of AI systems. In safety, we extend classical
deterministic shielding techniques to become resilient against delayed
observations, enabling practical deployment in real-world conditions. We also
implement both deterministic and probabilistic safety shields into simulated
autonomous vehicles to prevent collisions with road users, validating the use
of these techniques in realistic driving simulators. We introduce fairness
shields, a novel post-processing approach to enforce group fairness in
sequential decision-making settings over finite and periodic time horizons. By
optimizing intervention costs while strictly ensuring fairness constraints,
this method efficiently balances fairness with minimal interference. For
transparency and accountability, we propose a formal framework for assessing
intentional behaviour in probabilistic decision-making agents, introducing
quantitative metrics of agency and intention quotient. We use these metrics to
propose a retrospective analysis of intention, useful for determining
responsibility when autonomous systems cause unintended harm. Finally, we unify
these contributions through the ``reactive decision-making'' framework,
providing a general formalization that consolidates previous approaches.
Collectively, the advancements presented contribute practically to the
realization of safer, fairer, and more accountable AI systems, laying the
foundations for future research in trustworthy AI.

</details>


### [84] [When Can Model-Free Reinforcement Learning be Enough for Thinking?](https://arxiv.org/abs/2506.17124)
*Josiah P. Hanna,Nicholas E. Corrado*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent work on large language models has demonstrated the use of model-free
reinforcement learning (RL) to train reasoning-like capabilities. The emergence
of "thinking" through model-free RL is interesting as thinking actions neither
produce reward nor change the external world state to one where the agent is
more likely to get reward. This paper seeks to build a domain-independent
understanding of when model-free RL will lead to such "thinking" as a strategy
for reward maximization. To build this understanding, we first introduce a
theoretical model which we call a thought Markov decision process (MDP).
Thought MDPs minimally extend the classical MDP model to include an abstract
notion of thought state and thought action. Using the thought MDP model, we
prove the importance of policy initialization in determining whether or not
thinking emerges and show formally that thought actions are equivalent to the
agent choosing to perform a step of policy improvement before continuing to
act. We then show that open-source LLMs satisfy the conditions that our theory
predicts are necessary for model-free RL to produce thinking-like behavior.
Finally, we hypothesize sufficient conditions that would enable thinking to be
learned outside of language generation and introduce a toy domain where a
combination of multi-task pre-training and designated thought actions enable
more data-efficient RL compared to non-thinking agents.

</details>


### [85] [SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents](https://arxiv.org/abs/2506.21669)
*Wanxin Tian,Shijie Zhang,Kevin Zhang,Xiaowei Chi,Chunkai Fan,Junyu Lu,Yulin Luo,Qiang Zhou,Yiming Zhao,Ning Liu,Siyu Lin,Zhiyuan Qin,Xiaozhu Ju,Shanghang Zhang,Jian Tang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO) integrates Monte Carlo Tree Search into GRPO.
To generalize reward estimation across tasks and scenes, supporting autonomous
adaptation and reward-driven self-evolution, we further introduce Multi-modal
Generative Reward Model (MGRM). To holistically evaluate the effectiveness of
SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing state-of-the-art
methods with scores of 85.07% (textual) and 46.27% (multi-modal), outperforming
prior models including GPT-4o. SEEA-R1 also achieves scores of 80.3% (textual)
and 44.03% (multi-modal) without ground truth reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.

</details>


### [86] [A Neuroscience-Inspired Dual-Process Model of Compositional Generalization](https://arxiv.org/abs/2507.18868)
*Alex Noviello,Claas Beger,Jacob Groner,Kevin Ellis,Weinan Sun*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep learning models struggle with systematic compositional generalization, a
hallmark of human cognition. We propose \textsc{Mirage}, a neuro-inspired
dual-process model that offers a processing account for this ability. It
combines a fast, intuitive ``System~1'' (a meta-trained Transformer) with a
deliberate, rule-based ``System~2'' (a Schema Engine), mirroring the brain's
neocortical and hippocampal--prefrontal circuits. Trained to perform general,
single-step decomposition on a stream of random grammars, Mirage achieves
$>$99\% accuracy on all splits of the SCAN benchmark in a task-agnostic
setting. Ablations confirm that the model's systematic behavior emerges from
the architectural interplay of its two systems, particularly its use of
explicit, prioritized schemas and iterative refinement. In line with recent
progress on recursive/recurrent Transformer approaches, Mirage preserves an
iterative neural update while externalizing declarative control into an
interpretable schema module. Our work provides a concrete computational model
for interpreting how compositional reasoning can arise from a modular cognitive
architecture.

</details>


### [87] [Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics](https://arxiv.org/abs/2507.21129)
*Jae Wan Shim*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) excel on many task-specific benchmarks, yet the
mechanisms that drive this success remain poorly understood. We move from
asking what these systems can do to asking how they process information. Our
contribution is a task-agnostic method that builds a quantitative Cognitive
Profile for any model. The profile is built around the Entropy Decay Curve-a
plot of a model's normalised predictive uncertainty as context length grows.
Across several state-of-the-art LLMs and diverse texts, the curves expose
distinctive, stable profiles that depend on both model scale and text
complexity. We also propose the Information Gain Span (IGS) as a single index
that summarises the desirability of a decay pattern. Together, these tools
offer a principled way to analyse and compare the internal dynamics of modern
AI systems.

</details>


### [88] [AI Compute Architecture and Evolution Trends](https://arxiv.org/abs/2508.21394)
*Bor-Sung Liang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The focus of AI development has shifted from academic research to practical
applications. However, AI development faces numerous challenges at various
levels. This article will attempt to analyze the opportunities and challenges
of AI from several different perspectives using a structured approach. This
article proposes a seven-layer model for AI compute architecture, including
Physical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer,
Orchestrator Layer, and Application Layer, from bottom to top. It also explains
how AI computing has evolved into this 7-layer architecture through the
three-stage evolution on large-scale language models (LLMs). For each layer, we
describe the development trajectory and key technologies. In Layers 1 and 2 we
discuss AI computing issues and the impact of Scale-Up and Scale-Out strategies
on computing architecture. In Layer 3 we explore two different development
paths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs
and compares it to traditional processor memory. In Layers 5 to 7 we discuss
the trends of AI agents and explore the issues in evolution from a single AI
agent to an AI-based ecosystem, and their impact on the AI industry.
Furthermore, AI development involves not only technical challenges but also the
economic issues to build self-sustainable ecosystem. This article analyzes the
internet industry to provide predictions on the future trajectory of AI
development.

</details>


### [89] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [90] [Chatbot To Help Patients Understand Their Health](https://arxiv.org/abs/2509.05818)
*Won Seok Jang,Hieu Tran,Manav Mistry,SaiKiran Gandluri,Yifan Zhang,Sharmin Sultana,Sunjae Kown,Yuan Zhang,Zonghai Yao,Hong Yu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Patients must possess the knowledge necessary to actively participate in
their care. We present NoteAid-Chatbot, a conversational AI that promotes
patient understanding via a novel 'learning as conversation' framework, built
on a multi-agent large language model (LLM) and reinforcement learning (RL)
setup without human-labeled data. NoteAid-Chatbot was built on a lightweight
LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on
conversational data synthetically generated using medical conversation
strategies, followed by RL with rewards derived from patient understanding
assessments in simulated hospital discharge scenarios. Our evaluation, which
includes comprehensive human-aligned assessments and case studies, demonstrates
that NoteAid-Chatbot exhibits key emergent behaviors critical for patient
education, such as clarity, relevance, and structured dialogue, even though it
received no explicit supervision for these attributes. Our results show that
even simple Proximal Policy Optimization (PPO)-based reward modeling can
successfully train lightweight, domain-specific chatbots to handle multi-turn
interactions, incorporate diverse educational strategies, and meet nuanced
communication objectives. Our Turing test demonstrates that NoteAid-Chatbot
surpasses non-expert human. Although our current focus is on healthcare, the
framework we present illustrates the feasibility and promise of applying
low-cost, PPO-based RL to realistic, open-ended conversational domains,
broadening the applicability of RL-based alignment methods.

</details>


### [91] [Correct Reasoning Paths Visit Shared Decision Pivots](https://arxiv.org/abs/2509.21549)
*Dongkyu Cho,Amy B. Z. Zhang,Bilel Fehri,Sheng Wang,Rumi Chunara,Rui Song,Hengrui Cai*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of
large language models (LLMs), yet verifying those traces at scale remains
unsolved. In response, we introduce the idea of decision pivots-minimal,
verifiable checkpoints that any correct reasoning path must visit. We
hypothesize that correct reasoning, though stylistically diverse, converge on
the same pivot set, while incorrect ones violate at least one pivot. Leveraging
this property, we propose a self-training pipeline that (i) samples diverse
reasoning paths and mines shared decision pivots, (ii) compresses each trace
into pivot-focused short-path reasoning using an auxiliary verifier, and (iii)
post-trains the model using its self-generated outputs. The proposed method
aligns reasoning without ground truth reasoning data or external metrics.
Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the
effectiveness of our method.

</details>


### [92] [LLM/Agent-as-Data-Analyst: A Survey](https://arxiv.org/abs/2509.23988)
*Zirui Tang,Weizheng Wang,Zihang Zhou,Yang Jiao,Bangrui Xu,Boyu Niu,Dayou Zhou,Xuanhe Zhou,Guoliang Li,Yeye He,Wei Zhou,Yitong Song,Cheng Tan,Xue Yang,Chunwei Liu,Bin Wang,Conghui He,Xiaoyang Wang,Fan Wu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) and agent techniques have brought a fundamental
shift in the functionality and development paradigm of data analysis tasks
(a.k.a LLM/Agent-as-Data-Analyst), demonstrating substantial impact across both
academia and industry. In comparison with traditional rule or small-model based
approaches, (agentic) LLMs enable complex data understanding, natural language
interfaces, semantic analysis functions, and autonomous pipeline orchestration.
From a modality perspective, we review LLM-based techniques for (i) structured
data (e.g., NL2SQL, NL2GQL, ModelQA), (ii) semi-structured data (e.g., markup
languages understanding, semi-structured table question answering), (iii)
unstructured data (e.g., chart understanding, text/image document
understanding), and (iv) heterogeneous data (e.g., data retrieval and modality
alignment in data lakes). The technical evolution further distills four key
design goals for intelligent data analysis agents, namely semantic-aware
design, autonomous pipelines, tool-augmented workflows, and support for
open-world tasks. Finally, we outline the remaining challenges and propose
several insights and practical directions for advancing LLM/Agent-powered data
analysis.

</details>


### [93] [The Emergence of Social Science of Large Language Models](https://arxiv.org/abs/2509.24877)
*Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The social science of large language models (LLMs) examines how these systems
evoke mind attributions, interact with one another, and transform human
activity and institutions. We conducted a systematic review of 270 studies,
combining text embeddings, unsupervised clustering and topic modeling to build
a computational taxonomy. Three domains emerge organically across the reviewed
literature. LLM as Social Minds examines whether and when models display
behaviors that elicit attributions of cognition, morality and bias, while
addressing challenges such as test leakage and surface cues. LLM Societies
examines multi-agent settings where interaction protocols, architectures and
mechanism design shape coordination, norms, institutions and collective
epistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,
learning, trust, work and governance, and how risks arise at the human-AI
interface. This taxonomy provides a reproducible map of a fragmented field,
clarifies evidentiary standards across levels of analysis, and highlights
opportunities for cumulative progress in the social science of artificial
intelligence.

</details>


### [94] [Expandable Decision-Making States for Multi-Agent Deep Reinforcement Learning in Soccer Tactical Analysis](https://arxiv.org/abs/2510.00480)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Invasion team sports such as soccer produce a high-dimensional, strongly
coupled state space as many players continuously interact on a shared field,
challenging quantitative tactical analysis. Traditional rule-based analyses are
intuitive, while modern predictive machine learning models often perform
pattern-matching without explicit agent representations. The problem we address
is how to build player-level agent models from data, whose learned values and
policies are both tactically interpretable and robust across heterogeneous data
sources. Here, we propose Expandable Decision-Making States (EDMS), a
semantically enriched state representation that augments raw positions and
velocities with relational variables (e.g., scoring of space, pass, and score),
combined with an action-masking scheme that gives on-ball and off-ball agents
distinct decision sets. Compared to prior work, EDMS maps learned value
functions and action policies to human-interpretable tactical concepts (e.g.,
marking pressure, passing lanes, ball accessibility) instead of raw coordinate
features, and aligns agent choices with the rules of play. In the experiments,
EDMS with action masking consistently reduced both action-prediction loss and
temporal-difference (TD) error compared to the baseline. Qualitative case
studies and Q-value visualizations further indicate that EDMS highlights
high-risk, high-reward tactical patterns (e.g., fast counterattacks and
defensive breakthroughs). We also integrated our approach into an open-source
library and demonstrated compatibility with multiple commercial and open
datasets, enabling cross-provider evaluation and reproducible experiments.

</details>


### [95] [LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings](https://arxiv.org/abs/2510.08338)
*Benjamin F. Maier,Ulf Aslak,Luca Fiaschi,Nina Rismal,Kemble Fletcher,Christian C. Luhmann,Robbie Dow,Kli Pappas,Thomas V. Wiecki*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Consumer research costs companies billions annually yet suffers from panel
biases and limited scale. Large language models (LLMs) offer an alternative by
simulating synthetic consumers, but produce unrealistic response distributions
when asked directly for numerical ratings. We present semantic similarity
rating (SSR), a method that elicits textual responses from LLMs and maps these
to Likert distributions using embedding similarity to reference statements.
Testing on an extensive dataset comprising 57 personal care product surveys
conducted by a leading corporation in that market (9,300 human responses), SSR
achieves 90% of human test-retest reliability while maintaining realistic
response distributions (KS similarity > 0.85). Additionally, these synthetic
respondents provide rich qualitative feedback explaining their ratings. This
framework enables scalable consumer research simulations while preserving
traditional survey metrics and interpretability.

</details>


### [96] [Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems](https://arxiv.org/abs/2510.10644)
*Yi Zhang,Yushen Long,Yun Ni,Liping Huang,Xiaohong Wang,Jun Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Online ride-hailing platforms aim to deliver efficient mobility-on-demand
services, often facing challenges in balancing dynamic and spatially
heterogeneous supply and demand. Existing methods typically fall into two
categories: reinforcement learning (RL) approaches, which suffer from data
inefficiency, oversimplified modeling of real-world dynamics, and difficulty
enforcing operational constraints; or decomposed online optimization methods,
which rely on manually designed high-level objectives that lack awareness of
low-level routing dynamics. To address this issue, we propose a novel hybrid
framework that integrates large language model (LLM) with mathematical
optimization in a dynamic hierarchical system: (1) it is training-free,
removing the need for large-scale interaction data as in RL, and (2) it
leverages LLM to bridge cognitive limitations caused by problem decomposition
by adaptively generating high-level objectives. Within this framework, LLM
serves as a meta-optimizer, producing semantic heuristics that guide a
low-level optimizer responsible for constraint enforcement and real-time
decision execution. These heuristics are refined through a closed-loop
evolutionary process, driven by harmony search, which iteratively adapts the
LLM prompts based on feasibility and performance feedback from the optimization
layer. Extensive experiments based on scenarios derived from both the New York
and Chicago taxi datasets demonstrate the effectiveness of our approach,
achieving an average improvement of 16% compared to state-of-the-art baselines.

</details>


### [97] [PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature](https://arxiv.org/abs/2510.10909)
*Daoyu Wang,Mingyue Cheng,Qi Liu,Shuo Yu,Zirui Liu,Ze Guo*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding and reasoning on the web-scale scientific literature is a
crucial touchstone for large language model (LLM) based agents designed to
support complex knowledge-intensive tasks. However, existing works are mainly
restricted to tool-free tasks within isolated papers, largely due to the lack
of a benchmark for cross-paper reasoning and multi-tool orchestration in real
research scenarios. In this work, we propose PaperArena, an evaluation
benchmark for agents to address real-world research questions that typically
require integrating information across multiple papers with the assistance of
external tools. Given a research question, agents should integrate diverse
formats across multiple papers through reasoning and interacting with
appropriate tools, thereby producing a well-grounded answer. To support
standardized evaluation, we provide a modular and extensible platform for agent
execution, offering tools such as multimodal parsing, context retrieval, and
programmatic computation. Experimental results reveal that even the most
advanced LLM powering a well-established agent system achieves merely 38.78%
average accuracy. On the hard subset, accuracy drops to only 18.47%,
highlighting great potential for improvement. We also present several empirical
findings, including that all agents tested exhibit inefficient tool usage,
often invoking more tools than necessary to solve a task. We invite the
community to adopt PaperArena to develop and evaluate more capable agents for
scientific discovery. Our code and data are available
https://github.com/Melmaphother/PaperArena.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [98] [A Feature Engineering Approach for Business Impact-Oriented Failure Detection in Distributed Instant Payment Systems](https://arxiv.org/abs/2510.21710)
*Lorenzo Porcelli*

Main category: cs.LG

TL;DR: 本文提出了一种基于ISO 20022消息交换处理时间的新颖特征工程方法，用于即时支付系统中的异常检测。该方法能有效进行早期故障检测、定位和分类，并在TARGET即时支付结算系统（TIPS）上通过实验验证，证明其能提供可解释的业务影响，区分系统内外部问题，并显著缩短调查时间。


<details>
  <summary>Details</summary>
Motivation: 即时支付基础设施需要处理数百万笔交易，并要求零停机时间。然而，传统的监控方法难以在技术基础设施指标和业务流程可见性之间架起桥梁，导致难以进行早期故障检测和定位，以及理解故障对业务的影响。

Method: 本文引入了一种新颖的特征工程方法，该方法基于连续ISO 20022消息交换之间计算的处理时间，从而创建系统状态的紧凑表示。通过对这些特征应用异常检测，该方法能够实现早期故障检测和定位，并允许事件分类。此外，将这些特征映射到不同的处理阶段，可以区分内部和外部支付系统问题。

Result: 在TARGET即时支付结算系统（TIPS）上，通过真实事件和受控模拟进行的实验评估，证明了该方法在检测各种异常模式方面的有效性。它提供了固有的可解释性解释，使操作员能够理解业务影响。通过将特征映射到不同的处理阶段，所产生的框架能够区分内部和外部支付系统问题，显著减少调查时间，并弥补分布式系统中事务状态分散在多个实体之间的可观察性差距。

Conclusion: 本文提出的方法通过在即时支付系统中应用基于处理时间的特征工程和异常检测，成功实现了早期故障检测和定位。它不仅提供了业务影响的可解释性，还能区分系统问题的来源，显著缩短了调查时间，并有效解决了分布式系统中的可观察性难题。

Abstract: Instant payment infrastructures have stringent performance requirements,
processing millions of transactions daily with zero-downtime expectations.
Traditional monitoring approaches fail to bridge the gap between technical
infrastructure metrics and business process visibility. We introduce a novel
feature engineering approach based on processing times computed between
consecutive ISO 20022 message exchanges, creating a compact representation of
system state. By applying anomaly detection to these features, we enable early
failure detection and localization, allowing incident classification.
Experimental evaluation on the TARGET Instant Payment Settlement (TIPS) system,
using both real-world incidents and controlled simulations, demonstrates the
approach's effectiveness in detecting diverse anomaly patterns and provides
inherently interpretable explanations that enable operators to understand the
business impact. By mapping features to distinct processing phases, the
resulting framework differentiates between internal and external payment system
issues, significantly reduces investigation time, and bridges observability
gaps in distributed systems where transaction state is fragmented across
multiple entities.

</details>


### [99] [Numerical Fragility in Transformers: A Layer-wise Theory for Explaining, Forecasting, and Mitigating Instability](https://arxiv.org/abs/2510.21770)
*Jinwoo Baek*

Main category: cs.LG

TL;DR: 该论文提出了一种模块化的一阶理论，用于预测和解释低精度Transformer中前向误差放大问题。通过推导自注意力机制的层级误差界限，并引入残差连接的衰减效应以及LayerNorm指示器，形成了一个统一的前向稳定性界限。该理论提供了可操作的无单位诊断方法，能够解释自注意力何时脆弱、预测不稳定性，并指导微创的缓解措施，例如通过调整LayerNorm的epsilon值来稳定模型。实验在Tiny-ViT/CIFAR-10上验证了该理论的有效性，其诊断工具能准确跟踪误差、提前预警不稳定性，并成功指导了模型稳定性的提升。


<details>
  <summary>Details</summary>
Motivation: 在低精度训练Transformer模型时，一个显著的问题是前向误差的放大，这可能导致模型性能下降甚至训练不稳定。理解这些误差何时以及何地放大，并找到有效的方法来预测和缓解它们，对于在资源受限环境下高效部署Transformer模型至关重要。当前缺乏一个统一的理论框架来诊断和解释这种误差放大现象，以及提供可操作的指导来避免或减轻这些问题。因此，本研究的动机是开发一个理论框架，以深入理解低精度Transformer中的误差传播机制，并提供实用的工具来提高其训练稳定性。

Method: 本研究提出了一种模块化的一阶理论来分析低精度Transformer中的前向误差放大。首先，针对自注意力机制，推导了一个逐层的误差界限，该界限可分解为三个可解释的诊断指标：分数尺度比 $\kappa_{\rm score}$、逐行softmax敏感度 $\kappa_{\rm softmax}$ 和值条件数 $\kappa(V)$。其次，证明了一个残差松弛不等式，表明残差块能够衰减深度方向的误差积累。接着，引入了一个精度和宽度感知的LayerNorm指示器 $\rho_{\rm LN}$，并在 $\epsilon$ 主导的区域提供了一个匹配的一阶界限。最后，将上述部分综合起来，构建了一个统一的前向稳定性界限，其右侧在训练期间可以直接估计。这些理论组件在Tiny-ViT/CIFAR-10数据集上进行了评估和验证。

Result: 研究结果主要包括以下三点：首先，结合的预测器 $\kappa_{\rm softmax},(1+\kappa_{\rm score}),\kappa(V),|W_O|2+\kappa{\rm eff}+C_{\rm LN}$ 能够准确地跟踪在不同随机种子、宽度和精度设置下FP32与低精度之间的不匹配。通过将结果乘以机器 $\epsilon_{\rm mach}$，可以使混合精度点重合。其次，$\kappa_{\rm softmax}$ 的时间序列最大值可以作为误差尖峰的早期预警信号，比实际误差尖峰提前16-24步（相关系数0.65-0.82；置换检验 $p!\approx!10^{-3}$；Precision@K 0.89-1.00）。这表明该指标具有良好的预测能力。第三，在 $\rho_{\rm LN}$ 的指导下，对LayerNorm的 $\epsilon$ 值进行微小调整（目标 $\rho_\star!=!0.6$，上限$=10^{-2}$）可以带来持续的稳定性提升（平均尾部损失下降约0.010），且开销可忽略不计。

Conclusion: 本研究提出的理论为低精度Transformer的训练稳定性提供了可操作、无单位的诊断工具。这些工具能够：(i) 解释自注意力机制何时变得脆弱，(ii) 预测模型的不稳定性，以及 (iii) 启发并指导微创的缓解措施。特别是，通过调整LayerNorm的 $\epsilon$ 值，可以在不增加显著计算开销的情况下有效提升模型的稳定性。这为理解和优化低精度Transformer模型的性能提供了新的视角和实用方法。未来的工作可能包括将这些诊断方法推广到其他模型架构或更复杂的量化方案中，以进一步验证和拓展本理论的普适性。

Abstract: Transformers trained in low precision can suffer forward-error amplification.
We give a first-order, module-wise theory that predicts when and where errors
grow. For self-attention we derive a per-layer bound that factorizes into three
interpretable diagnostics: a score-scale ratio $\kappa_{\rm score}$, a rowwise
softmax sensitivity $\kappa_{\rm softmax}$, and value conditioning $\kappa(V)$.
We prove a residual relaxation inequality showing that residual blocks
attenuate depth-wise accumulation, and we introduce a precision- and
width-aware LayerNorm indicator $\rho_{\rm LN}$ with a matching first-order
bound in the $\epsilon$-dominated regime. These pieces yield a unified
forward-stability bound whose right-hand side is directly estimable during
training.
  On Tiny-ViT/CIFAR-10 we evaluate the bound and components. (1) The combined
predictor $\kappa_{\rm softmax},(1+\kappa_{\rm
score}),\kappa(V),|W_O|2+\kappa{\rm eff}+C_{\rm LN}$ tracks
FP32$\leftrightarrow$LP mismatches across seeds, widths, and precisions;
scaling by $\epsilon_{\rm mach}$ collapses mixed-precision points. (2) The
time-series maximum of $\kappa_{\rm softmax}$ acts as an early-warning signal,
leading error spikes by 16-24 steps (corr. 0.65-0.82; permutation
$p!\approx!10^{-3}$; Precision@K 0.89-1.00). (3) Guided by $\rho_{\rm LN}$, a
small LayerNorm-$\epsilon$ tweak targeting $\rho_\star$ gives consistent
stabilization (mean tail-loss $\downarrow\ \approx0.010$ at $\rho_\star!=!0.6$,
cap$=10^{-2}$) with negligible overhead.
  Overall, our theory supplies actionable, unitless diagnostics that (i)
explain when self-attention is fragile, (ii) forecast instability, and (iii)
motivate a minimally invasive mitigation.

</details>


### [100] [What Causes Postoperative Aspiration?](https://arxiv.org/abs/2510.21779)
*Supriya Nagesh,Karina Covarrubias,Robert El-Kareh,Shiva Prasad Kasiviswanathan,Nina Mishra*

Main category: cs.LG

TL;DR: 本研究开发了一个机器学习模型，用于预测术后吸入性肺炎风险，实现了0.86的AUROC和77.3%的敏感性。研究发现，阿片类药物剂量、住院时间和患者年龄是重要预测因子，阿片类药物和手术部位（颈部、头部）是显著的致病因素。此外，男性吸入性肺炎的风险是女性的1.5倍，且阿片类药物使用量更高。这些发现对改善术后护理和预防策略具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 吸入性肺炎是将异物吸入肺部，对外科患者的发病率和死亡率有显著影响。为了能够及时进行预防性干预，本研究的动机是开发一个机器学习模型，以预测患者在术后发生吸入性肺炎的风险。

Method: 本研究从包含超过40万份住院记录的MIMIC-IV数据库中，识别出826名在术后七天内经历吸入性肺炎的外科患者（平均年龄：62岁，55.7%为男性），并匹配了一个未发生吸入性肺炎的对照组。我们使用术前住院数据训练了三种机器学习模型：XGBoost、多层感知机（Multilayer Perceptron）和随机森林（Random Forest），以预测术后吸入性肺炎。为了探究因果关系，我们利用增强逆概率加权法（Augmented Inverse Probability Weighting）估计了平均治疗效应（ATE）。

Result: 我们的机器学习模型在独立的测试集上取得了0.86的AUROC（受试者操作特征曲线下面积）和77.3%的敏感性。研究发现，每日最大阿片类药物剂量、住院时间和患者年龄是预测吸入性肺炎最重要的因素。ATE分析识别出显著的致病因素包括：阿片类药物（0.25 ± 0.06）和手术部位（颈部：0.20 ± 0.13，头部：0.19 ± 0.13）。尽管男女手术率相同，但男性发生吸入性肺炎的可能性是女性的1.5倍，并且每日最大阿片类药物剂量比女性高27%。

Conclusion: 机器学习模型能够有效预测术后吸入性肺炎的风险，从而支持采取有针对性的预防措施。每日最大阿片类药物剂量和手术部位显著影响吸入性肺炎的风险。阿片类药物使用和吸入性肺炎发生率的性别差异值得进一步深入研究。这些发现对于改进术后护理方案和吸入性肺炎预防策略具有重要启示。

Abstract: Background: Aspiration, the inhalation of foreign material into the lungs,
significantly impacts surgical patient morbidity and mortality. This study
develops a machine learning (ML) model to predict postoperative aspiration,
enabling timely preventative interventions.
  Methods: From the MIMIC-IV database of over 400,000 hospital admissions, we
identified 826 surgical patients (mean age: 62, 55.7\% male) who experienced
aspiration within seven days post-surgery, along with a matched non-aspiration
cohort. Three ML models: XGBoost, Multilayer Perceptron, and Random Forest were
trained using pre-surgical hospitalization data to predict postoperative
aspiration. To investigate causation, we estimated Average Treatment Effects
(ATE) using Augmented Inverse Probability Weighting.
  Results: Our ML model achieved an AUROC of 0.86 and 77.3\% sensitivity on a
held-out test set. Maximum daily opioid dose, length of stay, and patient age
emerged as the most important predictors. ATE analysis identified significant
causative factors: opioids (0.25 +/- 0.06) and operative site (neck: 0.20 +/-
0.13, head: 0.19 +/- 0.13). Despite equal surgery rates across genders, men
were 1.5 times more likely to aspirate and received 27\% higher maximum daily
opioid dosages compared to women.
  Conclusion: ML models can effectively predict postoperative aspiration risk,
enabling targeted preventative measures. Maximum daily opioid dosage and
operative site significantly influence aspiration risk. The gender disparity in
both opioid administration and aspiration rates warrants further investigation.
These findings have important implications for improving postoperative care
protocols and aspiration prevention strategies.

</details>


### [101] [Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making](https://arxiv.org/abs/2510.21788)
*Larkin Liu,Jalal Etesami*

Main category: cs.LG

TL;DR: 本文探索了专家引导的赌博机学习（在线专家混合，OMoE），提出两种算法：结合UCB淘汰的聚合投票和在线加权多数投票，以优化专家输出聚合。这些方法应用于大语言模型（LLM）的在线微调，理论上证明了无悔性能，并通过实验验证了其能有效提升整体模型表现。


<details>
  <summary>Details</summary>
Motivation: 在给定上下文的情况下，如何聚合一组专家（专家委员会）的输出以实现最佳聚合精度是一个重要问题。尤其是在动态环境中，例如大语言模型（LLM）的在线微调，需要一种有效机制来动态调整专家权重或选择最佳专家委员会以生成最准确的响应。当前研究旨在解决这一挑战，以提高整体聚合模型的性能。

Method: 本文提出了两种解决在线专家混合（OMoE）问题的算法。第一种算法结合了聚合投票（aggregate voting）与UCB（Upper Confidence Bound）驱动的逐次淘汰（successive elimination），旨在高效地剪枝次优的探索动作。第二种算法采用在线加权多数投票机制（online weighted-majority-voting），根据每个专家的预测能力按比例利用其投票权。这些方法被应用于一组专家大语言模型（LLMs）的在线微调，其中生成式LLM在每次响应后动态地重新加权其专家集合和/或选择最佳专家委员会以生成最准确的响应。

Result: 在理想条件下，本文推导了赌博机设定下这两种算法的悔恨（regret）性质的理论保证。同时，也提供了相应的实证结果。研究表明，这些新方法为结合多个专家以提高整体聚合模型的性能提供了新的方法论和无悔保证。

Conclusion: 本文引入了在在线专家混合（OMoE）环境下结合多个专家以提高整体聚合模型性能的新方法论和无悔保证。所提出的两种算法，即结合UCB淘汰的聚合投票和在线加权多数投票，在理论和实证上都证明了其有效性。这些发现对于大语言模型等动态应用场景中的专家输出聚合具有重要意义，并为未来的研究提供了基础。

Abstract: We explore the use of expert-guided bandit learning, which we refer to as
online mixture-of-experts (OMoE). In this setting, given a context, a candidate
committee of experts must determine how to aggregate their outputs to achieve
optimal results in terms of aggregate accuracy. We propose two algorithms to
address this problem. The first algorithm combines aggregate voting with
UCB-driven successive elimination, efficiently pruning suboptimal exploration
actions. The second algorithm employs an online weighted-majority-voting
mechanism, leveraging the respective voting power of each expert proportional
to their predictive power. We derive theoretical guarantees for the regret
properties in the bandit setting under ideal circumstances, and empirical
results are provided accordingly. As a modern study on applications, these
methods are applied to the online fine-tuning of a set of expert large language
models (LLMs), where after each response, the generative LLM dynamically
reweighs its set of experts and/or selects the optimal committee of experts to
generate the most accurate response. Our results introduce new methodologies
and no-regret guarantees for combining multiple experts to improve on the
performance of the an aggregate model overall.

</details>


### [102] [Variance-Reduction Guidance: Sampling Trajectory Optimization for Diffusion Models](https://arxiv.org/abs/2510.21792)
*Shifeng Xu,Yanzhu Liu,Adams Wai-Kin Kong*

Main category: cs.LG

TL;DR: 扩散模型在采样过程中会累积预测误差，从而降低生成质量。本文提出了一种新的统计测量预测误差的方法，并引入了方差减少引导（VRG）方法来缓解这一误差。VRG无需模型微调或修改，通过寻找新的采样轨迹显著提高了扩散模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为新兴的生成模型，其采样过程涉及多个步骤。在每个步骤中，模型都会从带噪声的样本中预测噪声。然而，模型的预测结果与真实值存在偏差，这种偏差被称为“预测误差”。这些预测误差在采样过程中不断累积，最终导致生成质量的下降。因此，如何有效测量并缓解这种预测误差，是提升扩散模型生成性能的关键问题。

Method: 本文引入了一种新颖的技术来统计测量预测误差。在此基础上，提出了方差减少引导（VRG）方法以缓解这种误差。VRG方法的优势在于它无需对现有模型进行微调或修改。给定一个预定义的采样轨迹，VRG会搜索一个新的轨迹。这个新轨迹拥有相同的采样步骤数量，但能产生更高质量的生成结果。此外，VRG方法不仅适用于条件生成，也适用于无条件生成任务。

Result: 在各种数据集和基线上的实验结果表明，方差减少引导（VRG）方法能够显著提高扩散模型的生成质量。这验证了VRG在不修改模型的情况下，通过优化采样轨迹有效缓解预测误差的能力。论文还提供了源代码供进一步研究和复现。

Conclusion: 本文提出的方差减少引导（VRG）方法有效解决了扩散模型采样过程中预测误差累积导致生成质量下降的问题。VRG通过统计测量误差并优化采样轨迹，在无需模型微调或修改的前提下，显著提升了扩散模型的生成质量。这一方法对提高扩散模型在条件和无条件生成任务中的性能具有重要意义。

Abstract: Diffusion models have become emerging generative models. Their sampling
process involves multiple steps, and in each step the models predict the noise
from a noisy sample. When the models make prediction, the output deviates from
the ground truth, and we call such a deviation as \textit{prediction error}.
The prediction error accumulates over the sampling process and deteriorates
generation quality. This paper introduces a novel technique for statistically
measuring the prediction error and proposes the Variance-Reduction Guidance
(VRG) method to mitigate this error. VRG does not require model fine-tuning or
modification. Given a predefined sampling trajectory, it searches for a new
trajectory which has the same number of sampling steps but produces higher
quality results. VRG is applicable to both conditional and unconditional
generation. Experiments on various datasets and baselines demonstrate that VRG
can significantly improve the generation quality of diffusion models. Source
code is available at https://github.com/shifengxu/VRG.

</details>


### [103] [A Physics-Guided AI Cascaded Corrector Model Significantly Extends Madden-Julian Oscillation Prediction Skill](https://arxiv.org/abs/2510.21796)
*Xiao Zhou,Yuze Sun,Jie Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 本研究提出了一种新颖的深度学习框架——MJO物理引导级联校正器（PCC-MJO），作为通用后处理器来校正动力模式的MJO预报。该模型将有效预报范围延长了2-8天（双变量相关性>0.5），显著缓解了“海洋大陆屏障”，并能学习具有物理意义的特征，为次季节预报提供了有前途的解决方案。


<details>
  <summary>Details</summary>
Motivation: Madden-Julian振荡（MJO）是全球极端天气和气候的重要驱动因素，但其在业务动力模式中的预报仍然充满挑战，熟练的预报通常仅限于3-4周。因此，迫切需要开发新的方法来提高MJO的预报技巧和延长预报时效，以期突破次季节预报的长期障碍。

Method: 本研究引入了一种名为MJO物理引导级联校正器（PCC-MJO）的新型深度学习框架。该框架作为一个通用的后处理器，用于校正来自不同动力模式的MJO预报。PCC-MJO是一个两阶段模型：首先，它采用一个物理信息引导的3D U-Net来校正时空场误差；其次，它使用一个针对预报技巧进行优化的LSTM模型来细化MJO的RMM指数。该框架被应用于中国气象局（CMA）、欧洲中期天气预报中心（ECMWF）和美国国家环境预报中心（NCEP）的三个不同业务预报。

Result: 当应用于CMA、ECMWF和NCEP的三个不同业务预报时，PCC-MJO统一框架持续地将有效预报范围（双变量相关性>0.5）延长了2-8天。关键的是，该模型有效地缓解了“海洋大陆屏障”，从而实现了更真实的东向传播和振幅。可解释人工智能分析定量证实，该模型的决策与观测到的MJO动力学在空间上高度一致（相关性>0.93），表明它学习的是具有物理意义的特征，而非仅仅是统计拟合。

Conclusion: 本研究提出的PCC-MJO框架提供了一个有前景的、物理一致的、计算高效且高度通用化的途径，有望突破次季节预报中长期存在的障碍。它不仅显著提升了MJO的预报技巧和时效，还通过学习物理机制而非纯粹的统计模式，为未来基于人工智能的气候预测研究提供了重要的启示。

Abstract: The Madden-Julian Oscillation (MJO) is an important driver of global weather
and climate extremes, but its prediction in operational dynamical models
remains challenging, with skillful forecasts typically limited to 3-4 weeks.
Here, we introduce a novel deep learning framework, the Physics-guided Cascaded
Corrector for MJO (PCC-MJO), which acts as a universal post-processor to
correct MJO forecasts from dynamical models. This two-stage model first employs
a physics-informed 3D U-Net to correct spatial-temporal field errors, then
refines the MJO's RMM index using an LSTM optimized for forecast skill. When
applied to three different operational forecasts from CMA, ECMWF and NCEP, our
unified framework consistently extends the skillful forecast range (bivariate
correlation > 0.5) by 2-8 days. Crucially, the model effectively mitigates the
"Maritime Continent barrier", enabling more realistic eastward propagation and
amplitude. Explainable AI analysis quantitatively confirms that the model's
decision-making is spatially congruent with observed MJO dynamics (correlation
> 0.93), demonstrating that it learns physically meaningful features rather
than statistical fittings. Our work provides a promising physically consistent,
computationally efficient, and highly generalizable pathway to break through
longstanding barriers in subseasonal forecasting.

</details>


### [104] [Towards a Generalizable AI for Materials Discovery: Validation through Immersion Coolant Screening](https://arxiv.org/abs/2510.23371)
*Hyunseung Kim,Dae-Woong Jeong,Changyoung Park,Won-Ji Lee,Ha-Eun Lee,Ji-Hye Lee,Rodrigo Hormazabal,Sung Moon Ko,Sumin Lee,Soorin Yim,Chanhui Lee,Sehui Han,Sang-Ho Cha,Woohyung Lim*

Main category: cs.LG

TL;DR: 本文介绍并验证了GATE（Geometrically Aligned Transfer Encoder），一个通用的人工智能框架，能够同时学习34种涵盖热、电、机械和光学领域的物理化学性质。通过将这些性质对齐到一个共享的几何空间中，GATE捕获了跨性质相关性，减少了在多标准筛选中导致假阴性的不相关性质偏差。该框架无需特定问题配置，成功应用于数据中心浸入式冷却液的发现，筛选出92,861个有前景的分子，其中4个经实验或文献验证，性能与商用冷却剂相当或更优。这表明GATE是一个可立即使用的、可推广的AI平台，适用于各种材料发现任务。


<details>
  <summary>Details</summary>
Motivation: 人工智能在材料发现方面已显示出强大潜力，但现有的大多数模型都具有问题特异性，这意味着对于每一种新性质的发现，都需要额外的数据收集和重新训练。这种局限性导致了效率低下，并阻碍了AI在材料科学中更广泛的实际应用。因此，开发一个能够同时学习多种性质并具有良好泛化能力的AI框架，是当前材料发现领域的一个重要且具有挑战性的研究问题。

Method: 本文提出了GATE（Geometrically Aligned Transfer Encoder）框架。GATE的核心思想是通过将多种物理化学性质（涵盖热、电、机械和光学领域共34种性质）对齐到一个共享的几何空间中，从而实现性质的联合学习。这种几何对齐有助于捕获性质间的潜在关联，并减少“不相关性质偏差”（disjoint-property bias），该偏差是导致多标准筛选中出现假阴性的关键因素。通过这种方法，GATE旨在建立一个能够从更广泛的数据中学习，并能泛化到新材料发现任务的通用模型。

Result: 为了验证GATE的通用性，研究人员将其直接应用于Open Compute Project (OCP) 定义的数据中心浸入式冷却液发现这一严苛的实际挑战，而未进行任何问题特定的重新配置。GATE成功筛选了数十亿种候选分子，识别出92,861个具有应用前景的分子。其中，有4个分子通过实验或文献得到了验证，结果与湿实验室测量结果高度一致，并且其性能与商用冷却剂相当或更优。这些结果有力地证明了GATE在实际材料发现任务中的有效性和通用性。

Conclusion: 本文提出的GATE框架是一个即插即用、可泛化的人工智能平台，能够应用于各种材料发现任务。其通过联合学习多达34种物理化学性质，并利用几何对齐捕获跨性质相关性，有效解决了现有AI模型在材料发现中问题特异性的局限。GATE在数据中心浸入式冷却液发现任务中的成功应用，以及验证结果与实验数据的高度一致性，充分证明了其在实际应用中的巨大潜力。这为未来材料科学领域的AI辅助发现提供了一个新的范式，有望显著加速新材料的开发进程。

Abstract: Artificial intelligence (AI) has emerged as a powerful accelerator of
materials discovery, yet most existing models remain problem-specific,
requiring additional data collection and retraining for each new property. Here
we introduce and validate GATE (Geometrically Aligned Transfer Encoder) -- a
generalizable AI framework that jointly learns 34 physicochemical properties
spanning thermal, electrical, mechanical, and optical domains. By aligning
these properties within a shared geometric space, GATE captures cross-property
correlations that reduce disjoint-property bias -- a key factor causing false
negatives in multi-criteria screening. To demonstrate its generalizability,
GATE -- without any problem-specific reconfiguration -- was directly applied to
the discovery of immersion cooling fluids for data centers, a stringent
real-world challenge defined by the Open Compute Project (OCP). Screening
billions of candidates, GATE identified 92,861 molecules as promising for
practical deployment. Four were experimentally or literarily validated, showing
strong agreement with wet-lab measurements and performance comparable to or
exceeding a commercial coolant. These results establish GATE as a ready-to-use,
generalizable AI platform readily applicable across diverse materials discovery
tasks.

</details>


### [105] [Quantifying Multimodal Imbalance: A GMM-Guided Adaptive Loss for Audio-Visual Learning](https://arxiv.org/abs/2510.21797)
*Zhaocheng Liu,Zhiwen Yu,Xiaoqing Liu*

Main category: cs.LG

TL;DR: 本文提出了一种量化多模态不平衡程度的新方法，通过定义“模态差距”并用双峰高斯混合模型建模，进而设计出一种样本级自适应损失函数和两阶段训练策略，在CREMA-D和AVE数据集上取得了最先进的（SOTA）性能。


<details>
  <summary>Details</summary>
Motivation: 当前处理多模态不平衡的主流方法主要集中在架构修改和基于优化的策略上，往往忽视了对模态间不平衡程度的定量分析。这种疏忽导致无法精确地识别和处理不同模态之间的差距，从而限制了模型在处理实际多模态数据时的性能。本文旨在弥补这一空白，提出一种新的方法来对多模态不平衡进行定量分析，并据此设计出一种能够有效应对不平衡问题的自适应损失函数，以提升多模态学习的性能。

Method: 本研究首先定义了“模态差距”（Modality Gap），即不同模态（例如，音频和视觉）对真实类别预测的Softmax分数之间的差异。通过分析模态差距的分布，发现其可以被一个双峰高斯混合模型（GMM）有效地建模。GMM的两个组成部分分别对应“模态平衡”和“模态不平衡”的数据样本。随后，研究应用贝叶斯定理计算每个样本属于这两种不同分布的后验概率。基于这种定量分析，研究设计了一种新颖的自适应损失函数，其包含三个目标：1) 最小化整体模态差距；2) 鼓励不平衡样本分布向平衡样本分布靠拢；3) 对不平衡样本施加更大的惩罚权重。最后，采用两阶段训练策略，包括一个预热阶段和一个自适应训练阶段。

Result: 实验结果表明，本方法在公共的CREMA-D和AVE数据集上均取得了最先进的（SOTA）性能。在CREMA-D数据集上，准确率达到了80.65%；在AVE数据集上，准确率达到了70.90%。这些优异的性能数据充分验证了本文所提出的定量分析方法和自适应损失函数的有效性。

Conclusion: 本文提出的多模态不平衡定量分析方法和基于此设计的自适应损失函数是有效的。通过精确量化模态差距，并针对性地调整损失函数，本方法能够显著提升多模态学习的性能，并在多个公开数据集上取得了领先的成果，为解决多模态不平衡问题提供了一个新颖且高效的解决方案。

Abstract: Current mainstream approaches to addressing multimodal imbalance primarily
focus on architectural modifications and optimization-based, often overlooking
a quantitative analysis of the imbalance degree between modalities. To address
this gap, our work introduces a novel method for the quantitative analysis of
multi-modal imbalance, which in turn informs the design of a sample-level
adaptive loss function.We begin by defining the "Modality Gap" as the
difference between the Softmax scores of different modalities (e.g., audio and
visual) for the ground-truth class prediction. Analysis of the Modality Gap
distribution reveals that it can be effectively modeled by a bimodal Gaussian
Mixture Model (GMM). These two components are found to correspond respectively
to "modality-balanced" and "modality-imbalanced" data samples. Subsequently, we
apply Bayes' theorem to compute the posterior probability of each sample
belonging to these two distinct distributions.Informed by this quantitative
analysis, we design a novel adaptive loss function with three objectives: (1)
to minimize the overall Modality Gap; (2) to encourage the imbalanced sample
distribution to shift towards the balanced one; and (3) to apply greater
penalty weights to imbalanced samples. We employ a two-stage training strategy
consisting of a warm-up phase followed by an adaptive training
phase.Experimental results demonstrate that our approach achieves
state-of-the-art (SOTA) performance on the public CREMA-D and AVE datasets,
attaining accuracies of $80.65\%$ and $70.90\%$, respectively. This validates
the effectiveness of our proposed methodology.

</details>


### [106] [Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics](https://arxiv.org/abs/2510.22158)
*Lorenzo Magnino,Kai Shao,Zida Wu,Jiacheng Shen,Mathieu Laurière*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新颖的深度强化学习（DRL）算法，用于解决非平稳连续平均场博弈（MFGs）问题。该方法基于虚拟对局（Fictitious Play）策略，利用DRL计算最优响应，通过监督学习表示平均策略，并使用条件归一化流学习时间依赖的种群分布。该算法通过解决可扩展性和密度近似的局限性，显著推动了DRL在复杂MFG问题中的应用，使其更接近现实世界的多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 尽管平均场博弈（MFGs）在建模大规模多智能体系统交互方面表现出强大潜力，但现有的强化学习（RL）方法通常受限于有限空间或平稳模型，这严重阻碍了它们在现实世界问题中的应用。本研究的动机在于克服这些关键限制，开发一种能够处理非平稳连续MFGs的DRL算法，以提升MFGs在复杂、动态的现实多智能体系统中的实用性。

Method: 本论文介绍了一种专门为非平稳连续平均场博弈（MFGs）设计的深度强化学习（DRL）算法。该方法的核心是基于虚拟对局（Fictitious Play, FP）范式。具体来说，它利用DRL来计算每个智能体的最优响应策略，同时采用监督学习来有效地表示平均策略。为了捕捉时间依赖的种群分布，该方法还学习了一个使用条件归一化流（Conditional Normalizing Flow）进行表示的模型。研究通过在三个不同复杂度的示例上进行评估，验证了所提出方法的有效性。

Result: 本研究通过在三个复杂度递增的不同示例上进行评估，验证了所提出方法的有效性。结果表明，该算法成功解决了现有方法在可扩展性和密度近似方面的关键局限性。这些发现强调了该方法在处理复杂平均场博弈问题时的优越性能和实际应用潜力。

Conclusion: 这项工作代表了将深度强化学习（DRL）技术应用于复杂平均场博弈（MFG）问题的一个重大进展。通过有效解决现有方法在可扩展性和密度近似方面的关键限制，该研究显著推动了MFG领域向更复杂的现实世界多智能体系统应用的迈进。未来工作可能会进一步探索该方法在更广泛的现实场景中的性能和鲁棒性。

Abstract: Mean field games (MFGs) have emerged as a powerful framework for modeling
interactions in large-scale multi-agent systems. Despite recent advancements in
reinforcement learning (RL) for MFGs, existing methods are typically limited to
finite spaces or stationary models, hindering their applicability to real-world
problems. This paper introduces a novel deep reinforcement learning (DRL)
algorithm specifically designed for non-stationary continuous MFGs. The
proposed approach builds upon a Fictitious Play (FP) methodology, leveraging
DRL for best-response computation and supervised learning for average policy
representation. Furthermore, it learns a representation of the time-dependent
population distribution using a Conditional Normalizing Flow. To validate the
effectiveness of our method, we evaluate it on three different examples of
increasing complexity. By addressing critical limitations in scalability and
density approximation, this work represents a significant advancement in
applying DRL techniques to complex MFG problems, bringing the field closer to
real-world multi-agent systems.

</details>


### [107] [Residual-guided AI-CFD hybrid method enables stable and scalable simulations: from 2D benchmarks to 3D applications](https://arxiv.org/abs/2510.21804)
*Shilaj Baral,Youngkyu Lee,Sangam Khanal,Joongoo Jeon*

Main category: cs.LG

TL;DR: XRePIT是一种新型的混合模拟策略，它将机器学习加速与基于求解器的校正相结合，解决了纯数据驱动流体动力学模型误差累积的问题以及现有混合方法缺乏自动化和鲁棒性的问题。XRePIT实现了首次稳定、加速的超过10,000个时间步的模拟，对未知边界条件具有鲁棒的泛化能力，并可扩展到3D流，速度提升高达4.98倍，同时保持高物理保真度（热场相对误差约1E-3，低幅度速度动力学误差低于1E-2 ms-1）。该方法为实际工程应用奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动的流体动力学替代模型常因误差累积而出现灾难性失败。现有混合方法在自动化和鲁棒性方面不足，难以实际应用。因此，需要开发一种能够克服这些挑战，实现稳定、自动化且鲁棒的流体模拟新方法。

Method: 本文开发了XRePIT，这是一种新颖的混合模拟策略，它结合了机器学习（ML）加速和基于求解器的校正。该方法专门设计为完全自动化且具有物理感知能力，旨在确保传统方法所缺乏的稳定性和实际适用性。

Result: XRePIT克服了长期存在的障碍，首次实现了超过10,000个时间步的稳定、加速模拟。该方法对未知边界条件表现出强大的泛化能力，并且关键地能够扩展到3D流。我们的方法在保持高物理保真度的同时，实现了高达4.98倍的速度提升，热场相对误差约为1E-3，低幅度速度动力学误差低于1E-2 ms-1。

Conclusion: 这项工作建立了一种成熟且可扩展的混合方法，为XRePIT在实际工程中的应用铺平了道路，显示出巨大的应用潜力。

Abstract: Purely data-driven surrogates for fluid dynamics often fail catastrophically
from error accumulation, while existing hybrid methods have lacked the
automation and robustness for practical use. To solve this, we developed
XRePIT, a novel hybrid simulation strategy that synergizes machine learning
(ML) acceleration with solver-based correction. We specifically designed our
method to be fully automated and physics-aware, ensuring the stability and
practical applicability that previous approaches lacked. We demonstrate that
this new design overcomes long-standing barriers, achieving the first stable,
accelerated rollouts for over 10,000 timesteps. The method also generalizes
robustly to unseen boundary conditions and, crucially, scales to 3D flows. Our
approach delivers speedups up to 4.98$\times$ while maintaining high physical
fidelity, resolving thermal fields with relative errors of ~1E-3 and capturing
low magnitude velocity dynamics with errors below 1E-2 ms-1. This work thus
establishes a mature and scalable hybrid method, paving the way for its use in
real-world engineering.

</details>


### [108] [UCB-type Algorithm for Budget-Constrained Expert Learning](https://arxiv.org/abs/2510.22654)
*Ilgam Latypov,Alexandra Suvorikova,Alexey Kroshnin,Alexander Gasnikov,Yuriy Dorn*

Main category: cs.LG

TL;DR: 该论文提出了一种名为M-LCB的计算高效UCB风格元算法，用于在随机设置下，从K个自适应专家中动态选择并训练最多M个专家（M ≤ K），同时提供随时后悔保证。这是首个在每轮预算约束下，同时训练多个自适应专家并建立后悔保证的结果。


<details>
  <summary>Details</summary>
Motivation: 在许多现代应用中，系统需要动态选择并在线训练多个自适应学习算法，例如流环境中的模型选择、金融交易策略切换以及多上下文bandit或强化学习智能体的协调。在每轮中，学习者必须从K个自适应预测器中选择一个进行预测，同时在固定训练预算下最多只能更新M ≤ K个预测器。现有的方法难以在每轮预算约束下同时训练和协调多个有状态、自学习的专家。

Method: M-LCB是一种计算高效的UCB风格元算法，专门设计用于随机设置。其置信区间直接基于已实现的损失构建，无需额外的优化，并能无缝反映底层专家的收敛特性。该算法允许在每轮中从K个专家中选择一个进行预测，并更新最多M个专家。该方法通过将经典bandit范式扩展到在有限资源下协调有状态、自学习专家，来处理多专家同时训练的场景。

Result: 如果每个专家实现内部后悔为$	ilde O(T^\alpha)$，则M-LCB确保整体后悔界限为$	ilde O\!\Bigl(\sqrt{\tfrac{KT}{M}} \;+\; (K/M)^{1-\alpha}\,T^\alpha\Bigr)$。据作者所知，这是第一个在每轮预算约束下同时训练多个自适应专家并建立后悔保证的结果。通过两个代表性案例（在线训练的随机损失参数模型和本身是多臂bandit算法的专家）证明了该框架的有效性。

Conclusion: M-LCB算法成功地解决了在有限资源下协调有状态、自学习专家的挑战，扩展了经典的bandit范式，使其适用于更现实的场景。该方法为在预算约束下动态选择和训练多个自适应学习算法提供了强大的理论保证。未来的工作可能包括探索在非随机设置下的表现或进一步优化预算分配策略。

Abstract: In many modern applications, a system must dynamically choose between several
adaptive learning algorithms that are trained online. Examples include model
selection in streaming environments, switching between trading strategies in
finance, and orchestrating multiple contextual bandit or reinforcement learning
agents. At each round, a learner must select one predictor among $K$ adaptive
experts to make a prediction, while being able to update at most $M \le K$ of
them under a fixed training budget.
  We address this problem in the \emph{stochastic setting} and introduce
\algname{M-LCB}, a computationally efficient UCB-style meta-algorithm that
provides \emph{anytime regret guarantees}. Its confidence intervals are built
directly from realized losses, require no additional optimization, and
seamlessly reflect the convergence properties of the underlying experts. If
each expert achieves internal regret $\tilde O(T^\alpha)$, then \algname{M-LCB}
ensures overall regret bounded by $\tilde O\!\Bigl(\sqrt{\tfrac{KT}{M}} \;+\;
(K/M)^{1-\alpha}\,T^\alpha\Bigr)$.
  To our knowledge, this is the first result establishing regret guarantees
when multiple adaptive experts are trained simultaneously under per-round
budget constraints. We illustrate the framework with two representative cases:
(i) parametric models trained online with stochastic losses, and (ii) experts
that are themselves multi-armed bandit algorithms. These examples highlight how
\algname{M-LCB} extends the classical bandit paradigm to the more realistic
scenario of coordinating stateful, self-learning experts under limited
resources.

</details>


### [109] [Geographic Transferability of Machine Learning Models for Short-Term Airport Fog Forecasting](https://arxiv.org/abs/2510.21819)
*Marcelo Cerda Castillo*

Main category: cs.LG

TL;DR: 该研究开发了一种基于热力学和辐射过程的坐标无关（与位置无关）特征集，用于机场短时雾（能见度 < 1.0 km）预报。使用梯度提升分类器（XGBoost）在智利圣地亚哥数据上训练，并在多个不同地理位置的机场（包括波多蒙特、旧金山和伦敦）进行了严格的零样本测试，模型在不同雾团类型和高达11,650公里的距离上均取得了0.923-0.947的AUC值，证明了其优异的地理可迁移性。


<details>
  <summary>Details</summary>
Motivation: 机场短时雾预报在地理泛化性方面面临挑战，因为许多机器学习模型依赖于特定位置的特征，导致难以跨站点迁移。本研究的动机在于探索是否可以通过编码基本的热力学和辐射过程，构建一个坐标无关（与位置无关）的特征集，从而实现模型的地理可迁移性，克服现有模型的局限性。

Method: 本研究采用梯度提升分类器（XGBoost）作为机器学习模型。核心方法是构建一个“坐标无关”（location-independent）的特征集，该特征集编码了基本的热力学和辐射过程，旨在摆脱对特定地理位置信息的依赖。模型在智利圣地亚哥（SCEL，33S）2002-2009年的数据上进行训练。训练完成后，模型首先在2010-2012年的保留数据集上进行评估，以验证其在同一地点的泛化能力。更重要的是，为了严格测试其地理可迁移性，模型在波多蒙特（SCTE）、旧金山（KSFO）和伦敦（EGLL）进行了严格的零样本（zero-shot）测试，即在这些地点不进行任何再训练或微调。通过这种方式，研究旨在证明模型学习到的是可迁移的物理关系而非特定站点的模式。

Result: 模型在保留数据集以及零样本测试中取得了显著的性能。在跨越高达11,650公里的距离以及不同雾团类型（辐射雾、平流雾、海洋雾）的机场上，模型实现了0.923-0.947的AUC（曲线下面积）值，这表明其具有出色的预测准确性和地理可迁移性。一致的SHAP（SHapley Additive exPlanations）特征排名显示，能见度持久性、太阳角度和热梯度是预测中的主导特征。这一结果强烈暗示，模型学习到的是可迁移的物理关系，而不是特定于站点的模式，从而验证了坐标无关特征集方法的有效性。

Conclusion: 本研究的结论是，通过物理信息指导的、坐标无关的特征工程，可以开发出具有地理可迁移性的先进大气预报工具。模型的成功在于其能够将基本的热力学和辐射过程编码为通用特征，使得模型在不同地理位置和不同雾团类型下都能保持高性能。这项工作克服了传统机器学习模型在地理泛化性上的挑战，为未来开发更鲁棒、更广泛适用的气象预报系统提供了新的途径和思路。未来工作可能包括进一步探索其他物理过程的编码，以及在更多样化的地理和气候条件下验证模型的表现。

Abstract: Short-term forecasting of airport fog (visibility < 1.0 km) presents
challenges in geographic generalization because many machine learning models
rely on location-specific features and fail to transfer across sites. This
study investigates whether fundamental thermodynamic and radiative processes
can be encoded in a coordinate-free (location-independent) feature set to
enable geographic transferability. A gradient boosting classifier (XGBoost)
trained on Santiago, Chile (SCEL, 33S) data from 2002-2009 was evaluated on a
2010-2012 holdout set and under strict zero-shot tests at Puerto Montt (SCTE),
San Francisco (KSFO), and London (EGLL). The model achieved AUC values of
0.923-0.947 across distances up to 11,650 km and different fog regimes
(radiative, advective, marine). Consistent SHAP feature rankings show that
visibility persistence, solar angle, and thermal gradients dominate
predictions, suggesting the model learned transferable physical relationships
rather than site-specific patterns. Results suggest that physics-informed,
coordinate-free feature engineering can yield geographically transferable
atmospheric forecasting tools.

</details>


### [110] [Beyond Point Matching: Evaluating Multiscale Dubuc Distance for Time Series Similarity](https://arxiv.org/abs/2510.21824)
*Azim Ahmadzadeh,Mahsa Khazaei,Elaina Rohlfing*

Main category: cs.LG

TL;DR: 本文介绍了一种新颖的多尺度Dubuc距离（MDD）时间序列相似性度量方法，该方法通过在多个时间尺度上评估相似性并避免点对点对齐，在许多情况下显著优于广泛使用的动态时间规整（DTW），并通过模拟、95个UCR数据集以及一个实际分类任务验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据的高维度和复杂性使得其高效搜索和索引成为数据挖掘领域的一个长期挑战。在时间序列相似性度量方面，现有的方法（如DTW）存在一定的局限性。本研究的动机是引入并详细评估一种新的相似性度量MDD，旨在解决现有方法的不足，并探究其相对于DTW的比较优势和局限性，以期为时间序列分析提供更有效工具。

Method: 本文研究了新引入的多尺度Dubuc距离（MDD）相似性度量，并将其与广泛使用的动态时间规整（DTW）进行比较。MDD的独特之处在于它在多个时间尺度上评估时间序列相似性，并避免了传统的点对点对齐。为了验证MDD的性能，研究采用了以下方法：首先，通过模拟实验来测试假设；其次，利用UCR档案中的95个数据集进行广泛评估；最后，将MDD和DTW应用于一个具有挑战性的真实世界分类任务。

Result: 研究结果表明，在许多场景下，多尺度Dubuc距离（MDD）的性能增益显著优于动态时间规整（DTW）。本文详细分析了MDD所解决的特定性能差距。通过模拟实验、对95个UCR数据集的测试，以及在一个具有挑战性的真实世界分类任务中的应用，均证实了MDD相对于DTW的显著改进，突显了其在实际应用中的有效性。

Conclusion: 本文得出的结论是，多尺度Dubuc距离（MDD）作为一种新颖的时间序列相似性度量，在多个时间尺度上评估相似性并避免点对点对齐，展现出强大的实用性。它在许多场景中显著优于动态时间规整（DTW），尤其是在解决特定性能差距方面。通过对UCR数据集和真实世界分类任务的应用，MDD的实用价值得到了充分验证，为时间序列分析领域提供了一个更有效且鲁棒的工具。

Abstract: Time series are high-dimensional and complex data objects, making their
efficient search and indexing a longstanding challenge in data mining. Building
on a recently introduced similarity measure, namely Multiscale Dubuc Distance
(MDD), this paper investigates its comparative strengths and limitations
relative to the widely used Dynamic Time Warping (DTW). MDD is novel in two key
ways: it evaluates time series similarity across multiple temporal scales and
avoids point-to-point alignment. We demonstrate that in many scenarios where
MDD outperforms DTW, the gains are substantial, and we provide a detailed
analysis of the specific performance gaps it addresses. We provide simulations,
in addition to the 95 datasets from the UCR archive, to test our hypotheses.
Finally, we apply both methods to a challenging real-world classification task
and show that MDD yields a significant improvement over DTW, underscoring its
practical utility.

</details>


### [111] [GAPO: Group Adaptive Policy Optimization for Real-World Code Edit](https://arxiv.org/abs/2510.21830)
*Jianqing Zhang,Zhezheng Hao,Wei Xia,Hande Dong,Hong Wang,Chenxing Wei,Yuyan Zhou,Yubin Qi,Qiang Lin,Jian Cao*

Main category: cs.LG

TL;DR: GAPO通过自适应地寻找无异常值的最高密度区间并使用其中位数作为自适应Q来替代组平均值进行优势计算，解决了强化学习在代码编辑中奖励分布偏斜和异常值导致优势计算失真的问题。该方法在九个大型语言模型上，针对大规模真实世界代码编辑任务，持续提高了精确匹配准确率，优于GRPO和DAPO。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）广泛用于代码编辑中的大型语言模型（LLMs）的后训练，其中GRPO等组相对方法因其无评论员（critic-free）和标准化优势估计而流行。然而，在真实世界的代码编辑场景中，奖励分布常常偏斜并带有不可预测的异常值，这导致优势计算失真并增加了噪声。现有方法未能有效处理这种奖励分布的鲁棒性问题。

Method: 我们提出了组自适应策略优化（GAPO）。该方法为每个提示自适应地寻找一个无异常值的最高密度区间（HDI），然后使用该区间的中位数作为自适应Q，以替代优势计算中的组平均值。这种自适应Q能够鲁棒地处理偏斜分布，同时保持即插即用和高效率的特性。

Result: GAPO在九个经过指令调优的LLMs（3B-14B）上进行了验证。实验使用了包含51,844个真实世界、历史感知代码编辑任务的大型内部数据集，涵盖了10种编程语言。结果表明，GAPO在精确匹配准确率上相对于GRPO及其变体DAPO取得了持续的改进。

Conclusion: GAPO通过引入自适应Q值，成功地解决了代码编辑中强化学习奖励分布偏斜和异常值带来的挑战，显著提高了模型的性能。其即插即用和高效的特性使其成为处理真实世界代码编辑任务中复杂奖励分布的有效且鲁棒的解决方案。

Abstract: Reinforcement learning (RL) is widely used for post-training large language
models (LLMs) in code editing, where group-relative methods like GRPO are
popular for their critic-free, normalized advantage estimation. However, in
real-world code-editing scenarios, reward distributions are often skewed with
unpredictable outliers, leading to distorted advantage computation and
increased noise. To address this issue, we propose Group Adaptive Policy
Optimization (GAPO), which adaptively finds an outlier-free highest-density
interval (HDI) per prompt and then uses the median of that interval as an
adaptive Q to replace the group mean in advantage calculation. This adaptive Q
robustly handles skewed distributions while remaining plug-and-play and
efficient. We validate GAPO on nine instruction-tuned LLMs (3B-14B) using a
large internal dataset of 51,844 real-world, history-aware code-editing tasks
across 10 languages, demonstrating consistent improvements in exact match
accuracy over GRPO and its variant DAPO. Code is publicly available.

</details>


### [112] [Restoring Pruned Large Language Models via Lost Component Compensation](https://arxiv.org/abs/2510.21834)
*Zijian Feng,Hanzhang Zhou,Zixiao Zhu,Tianjiao Li,Jia Jim Deryl Chua,Lee Onn Mak,Gee Wah Ng,Kezhi Mao*

Main category: cs.LG

TL;DR: 剪枝大型语言模型（LLMs）虽能降低成本但会影响性能。现有恢复方法（如PEFT）对剪枝模型效果不佳。本文提出RestoreLCC，一种即插即用的策略，通过激活编辑补偿注意力头中丢失的信息，显著恢复剪枝模型的性能，且不牺牲其稀疏性和推理效率，在通用和特定任务性能恢复上均优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 剪枝是减少大型语言模型（LLMs）大小和推理成本的常用技术，但其缺点是通常会导致性能下降。为了弥补这一问题，现有的恢复方法通常采用参数高效微调（PEFT），如LoRA，来恢复剪枝模型的性能。然而，大多数PEFT方法是为密集模型设计的，忽略了剪枝模型的独特属性，这往往导致次优的恢复效果。因此，本研究的动机是提出一种针对剪枝模型的、有针对性的恢复策略，该策略既能恢复性能，又能保持其低成本和高效率。

Method: 本研究提出RestoreLCC（通过丢失组件补偿恢复剪枝LLMs）方法。该方法基于以下观察：剪枝导致的信息损失反映在注意力激活中，并且选择性地重新引入这些信息的组件可以显著恢复模型性能。RestoreLCC是一种即插即用的方法，其核心步骤包括：1. 通过激活编辑对比探测关键注意力头。2. 从激活差异中提取丢失的组件。3. 将这些丢失的组件重新注入到相应的剪枝头部进行补偿和恢复。RestoreLCC兼容结构化、半结构化和非结构化剪枝方案。

Result: 广泛的实验证明，RestoreLCC在通用和特定任务性能恢复方面始终优于现有先进基线。更重要的是，RestoreLCC在恢复性能的同时，并未损害剪枝模型的稀疏性或推理效率。

Conclusion: RestoreLCC通过补偿剪枝大型语言模型中注意力激活的丢失信息，有效解决了剪枝导致的性能下降问题。该方法能够在不牺牲模型稀疏性和推理效率的前提下，显著恢复模型性能，并在多项实验中超越了现有最先进的方法。这为剪枝LLMs的实际应用提供了重要的性能恢复策略。

Abstract: Pruning is a widely used technique to reduce the size and inference cost of
large language models (LLMs), but it often causes performance degradation. To
mitigate this, existing restoration methods typically employ
parameter-efficient fine-tuning (PEFT), such as LoRA, to recover the pruned
model's performance. However, most PEFT methods are designed for dense models
and overlook the distinct properties of pruned models, often resulting in
suboptimal recovery. In this work, we propose a targeted restoration strategy
for pruned models that restores performance while preserving their low cost and
high efficiency. We observe that pruning-induced information loss is reflected
in attention activations, and selectively reintroducing components of this
information can significantly recover model performance. Based on this insight,
we introduce RestoreLCC (Restoring Pruned LLMs via Lost Component
Compensation), a plug-and-play method that contrastively probes critical
attention heads via activation editing, extracts lost components from
activation differences, and finally injects them back into the corresponding
pruned heads for compensation and recovery. RestoreLCC is compatible with
structured, semi-structured, and unstructured pruning schemes. Extensive
experiments demonstrate that RestoreLCC consistently outperforms
state-of-the-art baselines in both general and task-specific performance
recovery, without compromising the sparsity or inference efficiency of pruned
models.

</details>


### [113] [Cohort-attention Evaluation Metric against Tied Data: Studying Performance of Classification Models in Cancer Detection](https://arxiv.org/abs/2503.12755)
*Longfei Wei,Fang Sheng,Jianfei Zhang*

Main category: cs.LG

TL;DR: 人工智能提高了医疗筛查的准确性，但传统评估指标在处理数据不平衡、队列性能差异和患者层面不一致性时存在缺陷。本文提出了队列注意力评估指标（CAT）框架，通过引入患者层面评估、基于熵的分布加权以及队列加权敏感性和特异性，提供更平衡、公平且可靠的AI医疗筛查模型评估方法。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）在提高医疗筛查准确性，特别是在癌症检测和风险评估方面取得了显著进展。然而，传统的分类评估指标未能充分考虑数据不平衡、不同队列间的性能差异以及患者层面的不一致性，这些缺陷导致了评估结果的偏差。因此，亟需一种能够克服这些挑战，提供更全面、公平和可靠评估的框架。

Method: 本文提出了队列注意力评估指标（CAT）框架来解决传统评估方法的局限性。CAT框架引入了三项关键机制：患者层面的评估、基于熵的分布加权以及队列加权敏感性和特异性。框架中包含的关键指标如CATSensitivity (CATSen)、CATSpecificity (CATSpe) 和 CATMean，旨在确保在多样化人群中实现平衡和公平的评估。

Result: CAT框架通过其创新的评估机制，确保了在不同人群中进行平衡和公平的评估。这种方法显著增强了预测的可靠性、公平性和可解释性，为AI驱动的医疗筛查模型提供了一个强健的评估方法。

Conclusion: CAT框架为AI驱动的医疗筛查模型提供了一个创新且强健的评估方法，有效解决了传统分类指标在处理数据不平衡、队列性能差异和患者层面不一致性时所面临的挑战。通过引入患者层面评估、基于熵的分布加权和队列加权敏感性及特异性，CAT框架确保了评估的平衡性、公平性、预测可靠性、公平性和可解释性，从而推动了AI在医疗筛查应用中的实际价值和信任度。

Abstract: Artificial intelligence (AI) has significantly improved medical screening
accuracy, particularly in cancer detection and risk assessment. However,
traditional classification metrics often fail to account for imbalanced data,
varying performance across cohorts, and patient-level inconsistencies, leading
to biased evaluations. We propose the Cohort-Attention Evaluation Metrics (CAT)
framework to address these challenges. CAT introduces patient-level assessment,
entropy-based distribution weighting, and cohort-weighted sensitivity and
specificity. Key metrics like CATSensitivity (CATSen), CATSpecificity (CATSpe),
and CATMean ensure balanced and fair evaluation across diverse populations.
This approach enhances predictive reliability, fairness, and interpretability,
providing a robust evaluation method for AI-driven medical screening models.

</details>


### [114] [COLA: Continual Learning via Autoencoder Retrieval of Adapters](https://arxiv.org/abs/2510.21836)
*Jaya Krishna Mandivarapu*

Main category: cs.LG

TL;DR: 这篇论文提出了一个名为COLA的新框架，旨在解决大型语言模型（LLMs）在持续学习中因灾难性遗忘和高计算成本而面临的挑战。COLA利用自编码器学习与各种任务相关的低维权重嵌入，从而在不使用数据回放或大量特定任务参数的情况下，促进知识转移并防止灾难性遗忘。该方法实现了LLM的高效新任务学习，对之前任务的性能影响微乎其微，并显著减少了参数使用和内存占用，在多个数据集上超越了现有最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 持续学习（CL）是人工智能领域中最具挑战性的问题之一，主要原因是灾难性遗忘。大型语言模型（LLMs）由于其训练所需的巨大计算资源成本，导致频繁的再训练和持续学习变得不切实际。此外，LLM在持续学习中更新模型以获取新知识会导致现有知识被覆盖，从而引发灾难性遗忘现象。因此，迫切需要一种能够克服这些挑战，使LLM能够高效、持续地学习新任务而不会遗忘旧知识的方法。

Method: 本文提出了一个名为COLA的新颖框架来解决上述问题。COLA框架的核心是利用一个自编码器来学习与各种任务相关的权重的低维嵌入。这种方法旨在促进知识向新任务的转移，同时有效防止灾难性遗忘。COLA的关键特点在于它无需数据回放（data replay）机制，也无需大量任务特定的参数。它使得LLM能够以最小的训练量高效地学习新任务，对之前任务的性能几乎没有降级，并且消除了保留早期训练数据的需要。

Result: COLA框架在从面向任务的对话系统到意图分类数据集等不同数据集上进行了实证评估。结果表明，COLA不仅成功克服了灾难性遗忘问题，而且在多个任务中显著减少了参数使用量和内存大小。更重要的是，该方法在多个数据集上均超越了现有最先进的方法（state of the art methods），证明了其卓越的性能和效率。

Conclusion: 本文提出的COLA框架为解决大型语言模型在持续学习中面临的灾难性遗忘和资源效率问题提供了一个有效的解决方案。通过利用自编码器学习低维权重嵌入，COLA实现了知识的高效转移和灾难性遗忘的预防，同时显著降低了计算资源需求（参数和内存）。这项工作为未来LLM的持续学习提供了新的途径，使其能够更实用、更高效地适应新知识和新任务。

Abstract: Learning a set of tasks over time, also known as continual learning (CL), is
one of the most challenging problems in artificial intelligence due to
catastrophic forgetting. Large language models (LLMs) are often impractical to
frequent re-training and continual learning , due to high cost of computational
resources for training. Moreover, LLM are not suitable for continual learning
as updating these models over time for acquiring new knowledge leads to
overwrites existing knowledge leading to common phenomenon know as
\textit{catastrophic forgetting}. In this paper, we aim to address these
concerns using a novel framework , COLA that employs an autoencoder to learn
capture low-dimensional embeddings of the weights associated with various
tasks. Our approach facilitates the transfer of knowledge to new tasks while
preventing catastrophic forgetting, all without using data replay or a
substantial set of task-specific parameters. Our approach, COLA, makes the LLM
efficiently learn new tasks with minimal training, insignificant performance
degradation on previous tasks, and eliminates the need for retaining earlier
training data. Empirical evaluation on different datasets ranging from task
oriented dialouge system to intent classsfication datasets showcases that our
method not only overcomes catastrophic forgetting but also achieves significant
reduction in parameter usage and memory size, across multiple tasks and
outperforming the existing state of the art methods across multiple datasets.

</details>


### [115] [Training data membership inference via Gaussian process meta-modeling: a post-hoc analysis approach](https://arxiv.org/abs/2510.21846)
*Yongchao Huang,Pengfei Zhang,Shahzad Mumtaz*

Main category: cs.LG

TL;DR: GP-MIA是一种高效且可解释的成员推断攻击方法，它利用单一训练模型的后验指标（如准确性、熵、数据集统计信息和可选的敏感度特征）训练高斯过程分类器来区分成员和非成员，同时提供校准的不确定性估计。该方法在合成数据、欺诈检测数据、CIFAR-10和WikiText-2上的实验表明其具有高准确性和泛化能力，为现有成员推断攻击提供了一个实用的替代方案。


<details>
  <summary>Details</summary>
Motivation: 成员推断攻击（MIAs）旨在确定特定数据点是否曾是模型训练集的一部分，这带来了严重的隐私风险。现有方法通常依赖于影子模型或大量的查询访问，这极大地限制了它们的实用性。因此，需要一种更高效、更可解释且更实用的方法来识别此类隐私泄露。

Method: 我们提出了GP-MIA，一个基于高斯过程（GP）元建模的高效且可解释的方法。GP-MIA通过从一个单一的训练模型中提取后验指标，包括准确性、熵、数据集统计信息，以及可选的敏感度特征（例如梯度、NTK度量），来构建特征集。然后，GP-MIA训练一个高斯过程分类器，以区分训练集中的成员数据点和非成员数据点，并提供经过校准的不确定性估计，从而增强了方法的可解释性。

Result: GP-MIA在多种数据集上进行了实验验证，包括合成数据、真实世界的欺诈检测数据、CIFAR-10和WikiText-2。实验结果表明，GP-MIA在这些数据集上均取得了高准确性和优异的泛化能力。这表明GP-MIA是现有成员推断攻击方法的一个实用且有竞争力的替代方案。

Conclusion: GP-MIA通过利用单一模型的后验指标和高斯过程元建模，提供了一种高效、可解释且实用的成员推断攻击方法。它克服了现有方法对影子模型或大量查询的依赖性，并在多项实验中展现出卓越的性能，从而为解决隐私泄露问题提供了一条有前景的途径。

Abstract: Membership inference attacks (MIAs) test whether a data point was part of a
model's training set, posing serious privacy risks. Existing methods often
depend on shadow models or heavy query access, which limits their practicality.
We propose GP-MIA, an efficient and interpretable approach based on Gaussian
process (GP) meta-modeling. Using post-hoc metrics such as accuracy, entropy,
dataset statistics, and optional sensitivity features (e.g. gradients, NTK
measures) from a single trained model, GP-MIA trains a GP classifier to
distinguish members from non-members while providing calibrated uncertainty
estimates. Experiments on synthetic data, real-world fraud detection data,
CIFAR-10, and WikiText-2 show that GP-MIA achieves high accuracy and
generalizability, offering a practical alternative to existing MIAs.

</details>


### [116] [SynCast: Synergizing Contradictions in Precipitation Nowcasting via Diffusion Sequential Preference Optimization](https://arxiv.org/abs/2510.21847)
*Kaiyi Xu,Junchao Gong,Wenlong Zhang,Ben Fei,Lei Bai,Wanli Ouyang*

Main category: cs.LG

TL;DR: 本文提出了SynCast，一种基于Diffusion Sequential Preference Optimization (Diffusion-SPO) 的偏好优化方法，用于雷达回波降水临近预报。该方法通过两阶段的后训练框架，协同优化关键成功指数（CSI）和虚警率（FAR）这两个冲突指标，从而解决了现有模型预测平滑和在多冲突指标上性能不佳的问题，实现了持续优越的性能。


<details>
  <summary>Details</summary>
Motivation: 降水临近预报对监测极端天气和支持防灾至关重要。尽管深度学习方法取得了显著进展，但仍面临挑战。确定性模型倾向于产生过度平滑的预测，难以捕捉极端事件和精细尺度降水模式。概率生成模型由于其固有的随机性，在不同指标上表现波动，很少能持续达到最佳结果。此外，降水临近预报通常使用多个相互冲突的指标进行评估，例如，关键成功指数（CSI）和虚警率（FAR）之间往往存在权衡，这使得现有模型难以同时在这两个指标上表现良好。

Method: 为了解决上述挑战，本文首次将偏好优化引入降水临近预报领域，其灵感来源于大型语言模型中通过人类反馈进行强化学习的成功经验。具体来说，我们提出了SynCast方法，该方法采用了扩散序列偏好优化（Diffusion-SPO）的两阶段后训练框架，以逐步对齐冲突指标并持续实现优越性能。在第一阶段，框架侧重于降低虚警率（FAR），训练模型有效抑制虚警。在此基础上，第二阶段进一步优化关键成功指数（CSI），同时施加约束以保持FAR对齐，从而在这些冲突指标上实现协同改进。

Result: 本文提出的SynCast方法旨在逐步对齐冲突指标，并持续实现优越性能。通过两阶段的Diffusion-SPO框架，SynCast在第一阶段有效降低了虚警率（FAR），并在第二阶段在保持FAR对齐的同时优化了关键成功指数（CSI），从而在这些相互冲突的指标上实现了协同改进和卓越表现。抽象中强调了“持续实现优越性能”和“实现这些冲突指标的协同改进”。

Conclusion: 本文通过引入偏好优化并提出SynCast方法，有效地解决了降水临近预报中确定性模型预测过度平滑以及现有模型在相互冲突的评估指标（如CSI和FAR）上难以同时表现良好的问题。SynCast通过其两阶段的Diffusion-SPO后训练框架，成功地协同优化了FAR和CSI，实现了持续优越的性能和对细尺度降水模式的更准确捕捉，对极端天气监测和防灾具有重要意义。

Abstract: Precipitation nowcasting based on radar echoes plays a crucial role in
monitoring extreme weather and supporting disaster prevention. Although deep
learning approaches have achieved significant progress, they still face notable
limitations. For example, deterministic models tend to produce over-smoothed
predictions, which struggle to capture extreme events and fine-scale
precipitation patterns. Probabilistic generative models, due to their inherent
randomness, often show fluctuating performance across different metrics and
rarely achieve consistently optimal results. Furthermore, precipitation
nowcasting is typically evaluated using multiple metrics, some of which are
inherently conflicting. For instance, there is often a trade-off between the
Critical Success Index (CSI) and the False Alarm Ratio (FAR), making it
challenging for existing models to deliver forecasts that perform well on both
metrics simultaneously. To address these challenges, we introduce preference
optimization into precipitation nowcasting for the first time, motivated by the
success of reinforcement learning from human feedback in large language models.
Specifically, we propose SynCast, a method that employs the two-stage
post-training framework of Diffusion Sequential Preference Optimization
(Diffusion-SPO), to progressively align conflicting metrics and consistently
achieve superior performance. In the first stage, the framework focuses on
reducing FAR, training the model to effectively suppress false alarms. Building
on this foundation, the second stage further optimizes CSI with constraints
that preserve FAR alignment, thereby achieving synergistic improvements across
these conflicting metrics.

</details>


### [117] [TowerVision: Understanding and Improving Multilinguality in Vision-Language Models](https://arxiv.org/abs/2510.21849)
*André G. Viveiros,Patrick Fernandes,Saul Santos,Sonal Sannigrahi,Emmanouil Zaranis,Nuno M. Guerreiro,Amin Farajian,Pierre Colombo,Graham Neubig,André F. T. Martins*

Main category: cs.LG

TL;DR: 尽管视觉-语言模型（VLMs）取得了显著进展，但大多数工作以英语为中心，限制了其在多语言环境中的有效性。本研究通过全面的实证分析，探讨了多语言设计选择（如训练数据组成、编码器选择、文本骨干）的影响，并提出了TowerVision，一个面向图像-文本和视频-文本任务的开放多语言VLM家族，它基于多语言纯文本模型Tower+。TowerVision在多模态多语言基准测试中表现出色，尤其在文化背景任务和多模态翻译方面表现突出。通过融入视觉和文化上下文进行微调，TowerVision超越了使用更大数据集训练的现有方法。研究发现，多语言视觉-语言训练数据显著提升了跨语言泛化能力，并且指令微调的LLMs并非总是最佳的初始化点。我们同时发布了高质量的VisionBlocks数据集和所有模型、数据及训练方案。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）取得了显著进展，但现有的研究和模型主要以英语为中心进行设计和开发。这种英语中心化的设计流程严重限制了VLMs在多语言环境中的有效性和应用范围。为了弥补这一差距，迫切需要对多语言VLMs的设计选择进行系统而全面的实证研究，以了解不同组件（如训练数据构成、编码器选择和文本骨干模型）如何影响其多语言性能和泛化能力。解决这一问题对于构建更具包容性和全球适用性的多模态人工智能系统至关重要。

Method: 本研究进行了一项全面的实证分析，探讨了影响多语言VLM性能的多个设计选择。具体方法包括：1. **设计选择分析**：系统性地分析了不同训练数据组成（不同语言的比例和类型）、编码器选择（视觉编码器）以及文本骨干模型（多语言文本编码器）对VLM性能的影响。2. **模型构建**：基于多语言纯文本模型Tower+，开发了名为TowerVision的开放多语言VLM家族，用于处理图像-文本和视频-文本任务。3. **微调策略**：在模型微调过程中，特别融入了视觉和文化上下文，以增强模型在特定类型任务上的表现。4. **数据集构建与发布**：为了支持研究和模型训练，构建并发布了一个高质量、精心策划的视觉-语言数据集VisionBlocks。5. **性能评估**：在多个多模态多语言基准测试（包括ALM-Bench、Multi30K用于图像任务和ViMUL-Bench用于视频任务）上对TowerVision的性能进行了评估和比较。通过这些方法，研究旨在揭示多语言VLM训练的关键因素和优化策略。

Result: 本研究的主要结果如下：1. **TowerVision的卓越性能**：TowerVision模型家族在多个多模态多语言基准测试中取得了具有竞争力的性能。2. **特定任务的优势**：TowerVision在文化背景任务和多模态翻译方面表现出特别的优势。这表明融入视觉和文化上下文的微调策略是有效的。3. **超越现有方法**：通过在微调过程中整合视觉和文化上下文，TowerVision模型成功超越了那些使用更大规模数据集训练的现有方法。这一成果在ALM-Bench和Multi30K（图像任务）以及ViMUL-Bench（视频任务）上得到了验证。4. **关键发现一：多语言训练数据的重要性**：研究明确指出，多语言视觉-语言训练数据能够显著提高跨语言泛化能力，无论是从高资源语言到资源不足语言，还是反之。这强调了多语言数据在构建鲁棒VLM中的核心作用。5. **关键发现二：指令微调LLMs的局限性**：一个重要的发现是，指令微调的大型语言模型（LLMs）并非总是多语言VLM训练的最佳初始化点。这挑战了当前VLM设计中的一些常见假设。6. **数据与模型发布**：作为研究的一部分，发布了高质量、精心策划的VisionBlocks视觉-语言数据集，以及所有模型、数据和训练方案，以促进未来的研究。

Conclusion: 本研究通过对多语言设计选择的全面实证分析，成功开发了TowerVision这一开放多语言视觉-语言模型家族，为解决现有VLM的英语中心化问题提供了有效方案。TowerVision在多模态多语言基准测试中展现出卓越性能，尤其在文化背景任务和多模态翻译方面表现突出，甚至超越了使用更大规模数据集训练的现有模型。核心贡献在于揭示了多语言视觉-语言训练数据对跨语言泛化能力的显著提升作用，并且挑战了指令微调LLMs作为最佳初始化点的传统观念。通过公开发布模型、数据和训练方案，本研究为未来多语言VLM的发展奠定了坚实基础，并为构建更具全球包容性的AI系统提供了宝贵的见解。未来的工作可以进一步探索更高效的多语言数据构建策略和更优化的模型初始化方法。

Abstract: Despite significant advances in vision-language models (VLMs), most existing
work follows an English-centric design process, limiting their effectiveness in
multilingual settings. In this work, we provide a comprehensive empirical study
analyzing the impact of several multilingual design choices, such as training
data composition, encoder selection, and text backbones. The result is
TowerVision, a family of open multilingual VLMs for both image-text and
video-text tasks, built upon the multilingual text-only model Tower+.
TowerVision achieves competitive performance on multiple multimodal
multilingual benchmarks and shows particular strength in culturally grounded
tasks and multimodal translation. By incorporating visual and cultural context
during fine-tuning, our models surpass existing approaches trained on
substantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image
tasks) and ViMUL-Bench (video tasks). Alongside the models, we release
VisionBlocks, a high-quality, curated vision-language dataset. Our findings
highlight that multilingual vision-language training data substantially
improves cross-lingual generalization -- both from high-resource to
underrepresented languages and vice versa -- and that instruction-tuned LLMs
are not always the optimal initialization point. To support further research,
we publicly release all models, data, and training recipes.

</details>


### [118] [Privacy-preserving Decision-focused Learning for Multi-energy Systems](https://arxiv.org/abs/2510.21858)
*Yangze Zhou,Ruiyang Yao,Dalin Qin,Yixiong Jia,Yi Wang*

Main category: cs.LG

TL;DR: 本文提出了一种针对多能源系统（MES）的隐私保护决策聚焦学习（DFL）框架。该框架通过信息掩码、结合矩阵分解和同态加密的安全协议以及隐私保护的负荷模式识别算法，解决了传统MES调度中负荷预测与决策分离以及DFL应用中的隐私泄露问题。实验证明，该框架在保护隐私的同时，显著降低了平均每日调度成本。


<details>
  <summary>Details</summary>
Motivation: 多能源系统（MES）的调度决策高度依赖于准确的负荷预测。然而，传统的做法是负荷预测和决策制定过程相互独立。预测模型通常以最小化预测误差为目标进行训练，但这种方法往往忽略了预测结果对后续决策制定的影响，导致决策次优化。决策聚焦学习（DFL）旨在通过直接最小化决策成本来解决这一问题，从而提高决策的有效性。然而，DFL在MES中的实际应用面临一个重大挑战：它需要跨多个部门共享敏感的负荷数据和模型参数，这带来了严重的数据隐私问题。因此，如何在利用DFL优势的同时保障数据隐私，是当前MES调度决策领域亟待解决的关键问题。

Method: 为了解决MES中DFL的隐私问题，本文提出了一种隐私保护的DFL框架。核心方法包括：首先，引入“信息掩码”技术，用于在保护敏感私人数据的同时，能够恢复模型训练所需的决策变量和梯度信息。其次，为了进一步增强DFL的安全性，设计了一种结合“矩阵分解”和“同态加密”的安全协议。该协议旨在有效防止合谋攻击和未经授权的数据访问，从而提升整个框架的鲁棒性。此外，本文还开发了一种“隐私保护的负荷模式识别算法”，以支持针对不同异构负荷模式训练专门的DFL模型，从而提高模型的适应性和精度。

Result: 通过理论分析和全面的案例研究（包括使用真实的MES数据），本文提出的框架被证明不仅能够有效保护隐私，而且与现有方法相比，能够持续实现更低的平均每日调度成本。这表明该框架在实际应用中具有优越的性能，能够实现隐私保护与经济效益的双重目标。

Conclusion: 本文成功开发了一个针对多能源系统（MES）的隐私保护决策聚焦学习（DFL）框架，有效解决了传统调度决策中预测与决策脱节以及DFL应用中面临的隐私泄露挑战。通过信息掩码、结合矩阵分解和同态加密的安全协议以及隐私保护的负荷模式识别算法，该框架在确保数据隐私的同时，显著优化了调度决策，降低了运行成本。这为MES的智能、安全调度提供了新的思路和方法，具有重要的理论和实际应用价值。未来的工作可以探索该框架在更复杂、更大规模MES中的应用，并进一步提升其在动态环境下的适应性和效率。

Abstract: Decision-making for multi-energy system (MES) dispatch depends on accurate
load forecasting. Traditionally, load forecasting and decision-making for MES
are implemented separately. Forecasting models are typically trained to
minimize forecasting errors, overlooking their impact on downstream
decision-making. To address this, decision-focused learning (DFL) has been
studied to minimize decision-making costs instead. However, practical adoption
of DFL in MES faces significant challenges: the process requires sharing
sensitive load data and model parameters across multiple sectors, raising
serious privacy issues. To this end, we propose a privacy-preserving DFL
framework tailored for MES. Our approach introduces information masking to
safeguard private data while enabling recovery of decision variables and
gradients required for model training. To further enhance security for DFL, we
design a safety protocol combining matrix decomposition and homomorphic
encryption, effectively preventing collusion and unauthorized data access.
Additionally, we developed a privacy-preserving load pattern recognition
algorithm, enabling the training of specialized DFL models for heterogeneous
load patterns. Theoretical analysis and comprehensive case studies, including
real-world MES data, demonstrate that our framework not only protects privacy
but also consistently achieves lower average daily dispatch costs compared to
existing methods.

</details>


### [119] [OpenEM: Large-scale multi-structural 3D datasets for electromagnetic methods](https://arxiv.org/abs/2510.21859)
*Shuang Wang,Xuben Wang,Fei Deng,Peifan Jiang,Jian Chen,Gianluca Fiandaca*

Main category: cs.LG

TL;DR: OpenEM是一个大规模、多结构三维地电数据集，旨在通过提供更具地质合理性的数据和基于深度学习的快速正演模拟方法，克服现有数据集的局限性，从而加速深度学习在电磁勘探中的应用。


<details>
  <summary>Details</summary>
Motivation: 深度学习在电磁方法中的应用前景广阔，但其有效性严重依赖于数据集的质量。现有研究通常使用一维或结构简单的三维模型构建数据集，无法代表真实地质环境的复杂性。此外，缺乏标准化、公开可用的三维地电数据集阻碍了基于深度学习的电磁勘探的进展。同时，三维电磁正演模拟耗时巨大。

Method: 研究构建了OpenEM，一个大规模、多结构的三维地电数据集，涵盖了从半空间异常体到平层、褶皱层、平直断层、弯曲断层及其异常体变体的九类地质合理地下结构模型。为了解决三维电磁正演模拟耗时的问题，进一步开发了一种基于深度学习的快速正演模拟方法，以高效可靠地处理整个数据集。

Result: OpenEM提供了一个统一、全面、大规模的数据集，适用于常见的电磁勘探系统，显著加速了深度学习在电磁方法中的应用。基于深度学习的快速正演模拟能力使得OpenEM能够迅速部署于各种任务。完整数据集、正演模拟代码和训练模型均已公开可用。

Conclusion: OpenEM成功解决了深度学习在电磁勘探中因缺乏高质量、代表性数据集而面临的挑战，并通过创新的快速正演模拟方法进一步提升了其实用性。该数据集将作为关键资源，推动电磁勘探领域深度学习技术的发展和应用，为未来的研究和实际部署奠定基础。

Abstract: With the remarkable success of deep learning, applying such techniques to EM
methods has emerged as a promising research direction to overcome the
limitations of conventional approaches. The effectiveness of deep learning
methods depends heavily on the quality of datasets, which directly influences
model performance and generalization ability. Existing application studies
often construct datasets from random one-dimensional or structurally simple
three-dimensional models, which fail to represent the complexity of real
geological environments. Furthermore, the absence of standardized, publicly
available three-dimensional geoelectric datasets continues to hinder progress
in deep learning based EM exploration. To address these limitations, we present
OpenEM, a large scale, multi structural three dimensional geoelectric dataset
that encompasses a broad range of geologically plausible subsurface structures.
OpenEM consists of nine categories of geoelectric models, spanning from simple
configurations with anomalous bodies in half space to more complex structures
such as flat layers, folded layers, flat faults, curved faults, and their
corresponding variants with anomalous bodies. Since three-dimensional forward
modeling in electromagnetics is extremely time-consuming, we further developed
a deep learning based fast forward modeling approach for OpenEM, enabling
efficient and reliable forward modeling across the entire dataset. This
capability allows OpenEM to be rapidly deployed for a wide range of tasks.
OpenEM provides a unified, comprehensive, and large-scale dataset for common EM
exploration systems to accelerate the application of deep learning in
electromagnetic methods. The complete dataset, along with the forward modeling
codes and trained models, is publicly available at
https://doi.org/10.5281/zenodo.17141981.

</details>


### [120] [The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems](https://arxiv.org/abs/2510.21861)
*Bentley DeVilling*

Main category: cs.LG

TL;DR: 大型语言模型在没有外部反馈的递归自我评估中，倾向于信息闭合而非实质性进展。本研究通过跨提供商实验发现，无接地运行的信息变化量从早期到晚期下降了55%，而引入最小接地干预后，信息变化量立即反弹了28%并持续保持非零方差。这表明生成式推理的自我修正存在结构性限制，需要独立验证者或环境的信息交换才能避免认识论上的停滞，并为接地、协作式推理的设计原则提供了依据。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常被认为具备反思性推理能力，但递归的自我评估在缺乏外部反馈的情况下，往往只导致重构而非真正的进步。本研究旨在测试这一预测，并深入理解大型语言模型在自我修正过程中的内在限制和潜在机制，这对于开发更有效、更可靠的AI推理系统具有重要意义。

Method: 本研究进行了一项跨提供商研究，涉及144个推理序列。研究对象包括三款模型：OpenAI GPT-4o-mini、Anthropic Claude 3 Haiku和Google Gemini 2.0 Flash。任务家族分为四类：算术、代码、解释和反思。每个任务家族在两种条件下迭代十次：无接地自我批判和最小接地干预（在第三次迭代时进行一次验证步骤）。主要通过归一化编辑距离测量平均信息变化（delta I），并辅以n-gram新颖性、嵌入漂移和字符级熵等互补指标来分析模式。

Result: 在无接地运行中，平均信息变化量（delta I）从早期迭代的0.193下降到晚期迭代的0.087，降幅达55%，且所有三家提供商的模型都呈现出一致的模式。相比之下，接地运行在干预后立即显示出信息变化量28%的反弹，并在此后保持非零方差。互补的度量指标——n-gram新颖性、嵌入漂移和字符级熵——均收敛于相同的模式：缺乏外部接触的反思趋向于信息闭合。研究结果被解释为生成式推理中自我修正的结构性限制的证据：缺乏与独立验证者或环境的信息交换，递归推理将接近认识论停滞的吸引子状态。最小接地干预则起到耗散耦合的作用，重新引入了信息流。跨架构的一致性表明这种“镜像循环”源于共享的自回归训练目标，而非特定提供商的对齐方案。

Conclusion: 本研究揭示了大型语言模型在无外部反馈的自我修正中存在结构性限制，即趋向于信息闭合和认识论上的停滞。最小的外部接地干预能够有效地重新引入信息流动并促进推理进展。这些结果区分了反思何时是表演性的而非认识论性的，并为接地、协作式推理的设计原则提供了重要的指导。研究材料和代码均已公开，为未来的研究奠定了基础。

Abstract: Large language models are often described as capable of reflective reasoning,
yet recursive self-evaluation without external feedback frequently yields
reformulation rather than progress. We test this prediction in a cross-provider
study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini,
Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families
(arithmetic, code, explanation, reflection), each iterated ten times under two
conditions: ungrounded self-critique and a minimal grounding intervention (a
single verification step at iteration three). Mean informational change (delta
I, measured via normalized edit distance) declined by 55% from early (0.193) to
late (0.087) iterations in ungrounded runs, with consistent patterns across all
three providers. Grounded runs showed a +28% rebound in informational change
immediately after the intervention and sustained non-zero variance thereafter.
Complementary measures-n-gram novelty, embedding drift, and character-level
entropy-converged on the same pattern: reflection without contact tends toward
informational closure. We interpret this as evidence for a structural limit on
self-correction in generative reasoning: without an exchange of information
with an independent verifier or environment, recursive inference approaches an
attractor state of epistemic stasis. Minimal grounding functions as dissipative
coupling, reintroducing informational flux. The cross-architecture consistency
suggests the mirror loop arises from shared autoregressive training objectives
rather than provider-specific alignment schemes. The results delineate when
reflection is performative rather than epistemic and motivate design principles
for grounded, cooperative reasoning. Materials and code are publicly available.

</details>


### [121] [The Principles of Diffusion Models](https://arxiv.org/abs/2510.21890)
*Chieh-Hsin Lai,Yang Song,Dongjun Kim,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.LG

TL;DR: 本专著阐述了扩散模型的核心原理，追溯其起源，并展示了不同的公式如何从共同的数学思想中产生。它将扩散建模概括为通过学习逆向过程将噪声转化为数据，并描述了变分、基于分数和基于流的三种互补视角，它们都共享一个通过求解微分方程将简单先验分布转换为数据的共同骨干，为读者提供了概念性和数学基础的理解。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于为扩散模型提供一个统一且深入的理解。扩散模型通过将数据逐步破坏为噪声的前向过程，并学习将噪声恢复为数据的逆向过程来生成数据。由于存在多种不同的公式和视角（如变分、基于分数和基于流），对于具有基本深度学习知识的读者来说，理解其核心原理、起源以及这些不同公式如何从共同的数学思想中产生，显得尤为重要。本专著旨在解决这一需求，提供一个概念性和数学基础的框架。

Method: 本专著通过定义一个逐步将数据腐蚀成噪声的前向过程，并学习一个将噪声转换回数据并恢复相同中间分布的逆向过程，来阐述扩散模型。它提出了三种互补的视角：1. **变分视角**：受变分自编码器启发，将扩散视为逐步去噪的学习过程。2. **基于分数的视角**：植根于基于能量的模型，学习不断演变的数据分布的梯度，指导样本向更可能区域移动。3. **基于流的视角**：与归一化流相关，将生成视为在学习到的速度场下，样本从噪声平滑地移动到数据的过程。这些视角共享一个共同的骨干：一个时间相关的速度场，其流将简单的先验分布输送到数据分布。采样过程归结为求解一个将噪声沿连续轨迹演变为数据的微分方程。在此基础上，专著还讨论了可控生成的指导、高效数值求解器以及学习任意时间之间直接映射的扩散驱动流图模型。

Result: 本专著的核心成果在于提供了一个关于扩散模型的概念性和数学基础的理解框架。它成功地追溯了扩散模型的起源，并阐明了各种不同的公式是如何从共同的数学思想中产生的。通过呈现变分、基于分数和基于流的三种互补视角，专著展示了这些看似不同的方法如何共享一个统一的数学骨干，即通过求解微分方程将噪声转化为数据。此外，专著还讨论了在这一基础上的进阶主题，包括可控生成的指导方法、高效的数值求解器以及能够学习任意时间点之间直接映射的扩散驱动流图模型，极大地加深了读者对扩散模型工作原理的理解和应用潜力。

Conclusion: 本专著对扩散模型的贡献在于为具有基本深度学习知识的读者提供了一个全面、概念性和数学基础扎实的理解。它通过揭示扩散模型核心原理的统一性，以及变分、基于分数和基于流等不同视角的共通之处，极大地促进了对这类生成模型的认识。专著不仅解释了模型如何从噪声生成数据，还深入探讨了可控生成、高效采样等高级主题。其主要影响在于为研究人员和实践者提供了一个坚实的理论基础，以更好地理解、开发和应用扩散模型。未来的工作可能基于此框架进一步探索更高效的采样策略、更强的控制能力以及与新兴生成技术相结合的可能性。

Abstract: This monograph presents the core principles that have guided the development
of diffusion models, tracing their origins and showing how diverse formulations
arise from shared mathematical ideas. Diffusion modeling starts by defining a
forward process that gradually corrupts data into noise, linking the data
distribution to a simple prior through a continuum of intermediate
distributions. The goal is to learn a reverse process that transforms noise
back into data while recovering the same intermediates. We describe three
complementary views. The variational view, inspired by variational
autoencoders, sees diffusion as learning to remove noise step by step. The
score-based view, rooted in energy-based modeling, learns the gradient of the
evolving data distribution, indicating how to nudge samples toward more likely
regions. The flow-based view, related to normalizing flows, treats generation
as following a smooth path that moves samples from noise to data under a
learned velocity field. These perspectives share a common backbone: a
time-dependent velocity field whose flow transports a simple prior to the data.
Sampling then amounts to solving a differential equation that evolves noise
into data along a continuous trajectory. On this foundation, the monograph
discusses guidance for controllable generation, efficient numerical solvers,
and diffusion-motivated flow-map models that learn direct mappings between
arbitrary times. It provides a conceptual and mathematically grounded
understanding of diffusion models for readers with basic deep-learning
knowledge.

</details>


### [122] [A supervised discriminant data representation: application to pattern classification](https://arxiv.org/abs/2510.21898)
*Fadi Dornaika,Ahmad Khoder,Abdelmalik Moujahid,Wassim Khoder*

Main category: cs.LG

TL;DR: 本论文提出了一种混合线性特征提取方案，用于解决监督多分类问题。该方案结合了RSLDA和ICS_DLSR的优点，利用稀疏性技术进行特征选择，并保持同类样本的行稀疏一致性。通过迭代交替最小化方法估计变换矩阵，并在人脸、物体和数字等多个数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习和模式识别算法的性能在很大程度上取决于数据表示方式。因此，当前机器学习算法的大部分精力都投入到设计能够支持有效机器学习的预处理框架和数据转换方法上。本文旨在通过提出一种新的特征提取方案来解决如何设计更有效的数据表示，以提高监督多分类问题的性能。

Method: 本文提出了一种混合线性特征提取方案，用于监督多分类问题。该方法受到鲁棒稀疏线性判别分析（RSLDA）和基于类间稀疏性的判别最小二乘回归（ICS_DLSR）这两种线性判别方法的启发，提出了一个统一的准则，能够保留这两种强大方法的优点。所得到的变换依赖于稀疏性促进技术，旨在选择最能准确表示数据的特征，并保持来自同一类样本的行稀疏一致性属性。线性变换和正交矩阵的估计采用基于最速下降梯度法的迭代交替最小化方案，并结合了不同的初始化策略。该框架具有通用性，允许结合和调整其他线性判别嵌入方法。

Result: 在包括人脸、物体和数字在内的多个数据集上进行的实验表明，所提出的方法在大多数情况下都优于竞争方法，证明了其在性能上的优势。

Conclusion: 本文提出了一种新颖的混合线性特征提取框架，通过结合现有强大方法的优点并利用稀疏性技术，显著提高了监督多分类问题的性能。实验结果证实了该方法在多个数据集上的优越性。该框架的通用性也为其未来的扩展和与其他线性判别嵌入方法的结合提供了广阔的空间，预示着在数据表示和分类领域具有重要的应用潜力。

Abstract: The performance of machine learning and pattern recognition algorithms
generally depends on data representation. That is why, much of the current
effort in performing machine learning algorithms goes into the design of
preprocessing frameworks and data transformations able to support effective
machine learning. The method proposed in this work consists of a hybrid linear
feature extraction scheme to be used in supervised multi-class classification
problems. Inspired by two recent linear discriminant methods: robust sparse
linear discriminant analysis (RSLDA) and inter-class sparsitybased
discriminative least square regression (ICS_DLSR), we propose a unifying
criterion that is able to retain the advantages of these two powerful methods.
The resulting transformation relies on sparsity-promoting techniques both to
select the features that most accurately represent the data and to preserve the
row-sparsity consistency property of samples from the same class. The linear
transformation and the orthogonal matrix are estimated using an iterative
alternating minimization scheme based on steepest descent gradient method and
different initialization schemes. The proposed framework is generic in the
sense that it allows the combination and tuning of other linear discriminant
embedding methods. According to the experiments conducted on several datasets
including faces, objects, and digits, the proposed method was able to
outperform competing methods in most cases.

</details>


### [123] [Adversarial Déjà Vu: Jailbreak Dictionary Learning for Stronger Generalization to Unseen Attacks](https://arxiv.org/abs/2510.21910)
*Mahavir Dabas,Tran Huynh,Nikhil Reddy Billa,Jiachen T. Wang,Peng Gao,Charith Peris,Yao Ma,Rahul Gupta,Ming Jin,Prateek Mittal,Ruoxi Jia*

Main category: cs.LG

TL;DR: 大型语言模型易受越狱攻击，传统对抗训练难以应对新型攻击。本文提出“对抗性旧识”假说，认为新型越狱是旧技能的重组。通过对32篇攻击论文的分析，我们将对抗技能提炼成稀疏原语字典。基于此，我们提出对抗技能组合训练(ASCoT)，显著提高了模型对未知越狱（包括多轮）的鲁棒性，同时保持低误拒率，并强调扩展技能覆盖而非仅数据规模是关键。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）容易受到越狱攻击，这些攻击绕过安全防护以引发有害输出。防御新型越狱是AI安全领域的关键挑战。传统的对抗训练旨在使模型对最坏情况的扰动具有鲁棒性，但由于优化挑战和难以定义现实威胁模型，这种方法在实践中常常无法应对新开发的越狱攻击。这促使研究者寻找一种新的范式来提高LLMs对未知越狱的鲁棒性。

Method: 本文提出了一种新的范式，其核心是“对抗性旧识（Adversarial Déjà Vu）”假说：即新型越狱并非根本上的新颖，而主要是对现有攻击中对抗性技能的重新组合。为了验证这一假说，研究人员对两年内发表的32篇攻击论文进行了大规模分析。他们使用自动化流程提取并将对抗性技能压缩成一个稀疏的原语字典，并利用LLMs生成人类可读的描述。在此洞察的指导下，研究人员引入了“对抗性技能组合训练（Adversarial Skill Compositional Training, ASCoT）”方法，该方法通过对技能原语的多种组合进行训练，而非仅针对孤立的攻击实例进行训练。

Result: 本文的分析结果表明，未知攻击可以被有效地解释为早期技能的稀疏组合，并且解释力随着技能覆盖范围的增长而单调增加。ASCoT方法显著提高了模型对未知攻击（包括多轮越狱）的鲁棒性，同时保持了较低的过度拒绝率。研究还表明，扩展对抗性技能的覆盖范围，而不仅仅是增加数据规模，是防御新型攻击的关键。

Conclusion: 本文提出并验证了“对抗性旧识”假说，为防御未知越狱攻击提供了一个新颖的视角。通过引入Adversarial Skill Compositional Training (ASCoT)，研究成功提升了LLMs对新型越狱攻击的鲁棒性。研究强调了扩展对抗性技能覆盖范围的重要性，为未来的AI安全研究指明了方向，即更关注技能的组合性和覆盖面，而非简单的数据量。

Abstract: Large language models remain vulnerable to jailbreak attacks that bypass
safety guardrails to elicit harmful outputs. Defending against novel jailbreaks
represents a critical challenge in AI safety. Adversarial training -- designed
to make models robust against worst-case perturbations -- has been the dominant
paradigm for adversarial robustness. However, due to optimization challenges
and difficulties in defining realistic threat models, adversarial training
methods often fail on newly developed jailbreaks in practice. This paper
proposes a new paradigm for improving robustness against unseen jailbreaks,
centered on the Adversarial D\'ej\`a Vu hypothesis: novel jailbreaks are not
fundamentally new, but largely recombinations of adversarial skills from
previous attacks. We study this hypothesis through a large-scale analysis of 32
attack papers published over two years. Using an automated pipeline, we extract
and compress adversarial skills into a sparse dictionary of primitives, with
LLMs generating human-readable descriptions. Our analysis reveals that unseen
attacks can be effectively explained as sparse compositions of earlier skills,
with explanatory power increasing monotonically as skill coverage grows. Guided
by this insight, we introduce Adversarial Skill Compositional Training (ASCoT),
which trains on diverse compositions of skill primitives rather than isolated
attack instances. ASCoT substantially improves robustness to unseen attacks,
including multi-turn jailbreaks, while maintaining low over-refusal rates. We
also demonstrate that expanding adversarial skill coverage, not just data
scale, is key to defending against novel attacks.
\textcolor{red}{\textbf{Warning: This paper contains content that may be
harmful or offensive in nature.

</details>


### [124] [Joint Score-Threshold Optimization for Interpretable Risk Assessment Under Partial Supervision](https://arxiv.org/abs/2510.21934)
*Fardin Gankhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: 医疗风险评估工具在数据驱动优化方面面临部分监督和非对称误分类成本的挑战。本文提出了一种混合整数规划（MIP）框架，能够联合优化评分权重和类别阈值，以应对这些挑战，并通过支持治理约束确保在临床工作流程中的实际可部署性。


<details>
  <summary>Details</summary>
Motivation: 医疗保健领域的风险评估工具通常采用基于分数的评分系统，通过阈值将患者映射到有序风险类别。尽管电子健康记录（EHR）数据为这些工具的数据驱动优化提供了机会，但标准监督学习面临两大基本挑战：一是部分监督问题，即由于干预措施导致的结局审查，只有极端类别能够被可靠地标记；二是非对称误分类成本问题，即误分类成本随有序距离的增加而增加。克服这些挑战对于开发更准确、更实用的数据驱动风险评估工具至关重要。

Method: 本文提出了一个混合整数规划（MIP）框架，旨在联合优化评分权重和类别阈值，以应对部分监督和非对称误分类成本的挑战。该方法通过每实例可行标签集来处理部分监督问题，并整合了非对称距离感知目标。为防止中间类别塌陷，框架引入了最小阈值间隔。此外，本文进一步开发了一种使用softplus损失的CSO松弛方法，在保留有序结构的同时实现高效优化。该框架还支持包括符号限制、稀疏性以及对现有工具最小修改等治理约束。

Result: 该MIP框架成功地在部分监督和非对称误分类成本的约束下，联合优化了医疗风险评估工具的评分权重和类别阈值。它能够有效处理因干预措施引起的结局审查导致的部分监督问题，并将非对称误分类成本纳入优化目标。通过确保最小阈值间隔，该框架有效地防止了风险评估中中间类别的塌陷。利用CSO松弛方法，实现了在保留有序结构的同时进行高效优化。最重要的是，该框架支持多种治理约束，包括符号限制、稀疏性以及最小化对现有工具的修改，从而确保了其在临床工作流程中的实际可部署性。

Conclusion: 本文提出的MIP框架为医疗保健领域的风险评估工具提供了一个鲁棒且实用的数据驱动优化解决方案。它成功克服了部分监督和非对称误分类成本等关键挑战，并通过支持重要的治理约束，如符号限制、稀疏性和最小化对现有工具的修改，确保了在临床环境中的实际可部署性。

Abstract: Risk assessment tools in healthcare commonly employ point-based scoring
systems that map patients to ordinal risk categories via thresholds. While
electronic health record (EHR) data presents opportunities for data-driven
optimization of these tools, two fundamental challenges impede standard
supervised learning: (1) partial supervision arising from intervention-censored
outcomes, where only extreme categories can be reliably labeled, and (2)
asymmetric misclassification costs that increase with ordinal distance. We
propose a mixed-integer programming (MIP) framework that jointly optimizes
scoring weights and category thresholds under these constraints. Our approach
handles partial supervision through per-instance feasible label sets,
incorporates asymmetric distance-aware objectives, and prevents middle-category
collapse via minimum threshold gaps. We further develop a CSO relaxation using
softplus losses that preserves the ordinal structure while enabling efficient
optimization. The framework supports governance constraints including sign
restrictions, sparsity, and minimal modifications to incumbent tools, ensuring
practical deployability in clinical workflows.

</details>


### [125] [AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing](https://arxiv.org/abs/2510.21935)
*Samuel Bright-Thonney,Christina Reissel,Gaia Grosso,Nathaniel Woodward,Katya Govorkova,Andrzej Novak,Sang Eon Park,Eric Moreno,Philip Harris*

Main category: cs.LG

TL;DR: 该论文介绍了AutoSciDACT，这是一个用于科学数据中新颖性检测的统一流程。它通过对比预训练创建低维数据表示，然后利用NPLM框架进行敏感的机器学习双样本检验，以统计量化异常数据，从而解决了科学数据噪声大、维度高以及需要统计稳健性声明的挑战。


<details>
  <summary>Details</summary>
Motivation: 在大型科学数据集中进行新颖性检测面临两个关键挑战：实验数据的噪声高和高维度特性，以及对任何观测到的异常值做出统计稳健声明的必要性。尽管有大量关于通过降维进行异常检测的文献，但大多数方法无法产生与可量化的科学发现主张兼容的输出。

Method: AutoSciDACT流程包括：首先，利用对比预训练创建富有表现力的低维数据表示，这利用了许多科学领域中丰富的高质量模拟数据以及指导有原则的数据增强策略的专业知识。然后，这些紧凑的嵌入使基于机器学习的双样本检验变得极其敏感，该检验使用新物理学习机（NPLM）框架，该框架识别并统计量化观测数据相对于参考分布（零假设）的偏差。

Result: 该研究在天文学、物理学、生物学、图像和合成数据集等一系列数据集中进行了实验，结果表明对所有领域中少量异常数据注入都具有很强的敏感性。

Conclusion: AutoSciDACT代表了适应科学严格统计需求的统一新颖性检测流程的第一步，能够对观察到的异常进行统计量化，从而实现更严谨的科学发现。

Abstract: Novelty detection in large scientific datasets faces two key challenges: the
noisy and high-dimensional nature of experimental data, and the necessity of
making statistically robust statements about any observed outliers. While there
is a wealth of literature on anomaly detection via dimensionality reduction,
most methods do not produce outputs compatible with quantifiable claims of
scientific discovery. In this work we directly address these challenges,
presenting the first step towards a unified pipeline for novelty detection
adapted for the rigorous statistical demands of science. We introduce
AutoSciDACT (Automated Scientific Discovery with Anomalous Contrastive
Testing), a general-purpose pipeline for detecting novelty in scientific data.
AutoSciDACT begins by creating expressive low-dimensional data representations
using a contrastive pre-training, leveraging the abundance of high-quality
simulated data in many scientific domains alongside expertise that can guide
principled data augmentation strategies. These compact embeddings then enable
an extremely sensitive machine learning-based two-sample test using the New
Physics Learning Machine (NPLM) framework, which identifies and statistically
quantifies deviations in observed data relative to a reference distribution
(null hypothesis). We perform experiments across a range of astronomical,
physical, biological, image, and synthetic datasets, demonstrating strong
sensitivity to small injections of anomalous data across all domains.

</details>


### [126] [Transformer Based Linear Attention with Optimized GPU Kernel Implementation](https://arxiv.org/abs/2510.21956)
*Armin Gerami,Ramani Duraiswami*

Main category: cs.LG

TL;DR: 针对Transformer中线性注意力（LA）机制在实践中效率不高的问题，本文提出了一种新颖的前向和后向传播方法，并配合高度优化的CUDA实现。该方法在速度上比现有技术快3.3倍，内存消耗减少3.6倍，同时在14亿参数语言模型的训练中，展现出与常规注意力相当的表达能力，有效解决了大规模Transformer模型训练和推理的效率瓶颈。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中基于softmax的常规注意力机制（regular attention）的计算复杂度为$O(N^2D)$，其中$N$是token数量，$D$是维度。随着Transformer的广泛成功，提高其在训练和推理过程中的运行效率成为一个热门研究领域。线性注意力（LA）机制被引入以解决这一问题，它提供了$O(ND^2)$的线性时间复杂度，并且已经证明能达到与常规注意力相当的准确性。然而，在实际应用中，线性注意力机制的性能却未能达到其理论上的高效性，这阻碍了其在大规模模型中的广泛应用，因此需要寻找方法来弥补理论与实践之间的差距。

Method: 本文提出了一种针对线性注意力机制前向和后向传播的新颖方法。为了实现理论上的高效性，该方法还伴随着一个高度优化的CUDA实现。研究人员通过在单层和端到端设置中验证这些改进，具体实验包括训练一个具有14亿参数的语言模型，以评估其性能提升以及表达能力。

Result: 所提出的方法在速度上比现有最先进的技术快3.3倍，同时将内存消耗降低了3.6倍。这些改进在单层和端到端设置中都得到了验证。通过训练一个14亿参数的语言模型，研究表明该方法在主要的推理基准测试中展现出与常规注意力机制相似的表达能力，证明了其在保持性能的同时，显著提升了效率。

Conclusion: 本文提出的针对线性注意力机制的新方法及其高度优化的CUDA实现，显著提升了Transformer模型在训练和推理时的效率，解决了线性注意力在实践中未能达到其理论效率的问题。通过3.3倍的速度提升和3.6倍的内存消耗减少，使得训练更大规模的语言模型变得更加可行和高效。此外，在14亿参数语言模型上的验证表明，该方法在保持与常规注意力相当的表达能力的同时实现了效率提升，这对于Transformer架构的未来发展和应用具有重要意义，尤其是在处理长序列和大规模数据集方面。

Abstract: The original softmax-based attention mechanism (regular attention) in the
extremely successful Transformer architecture computes attention between $N$
tokens, each embedded in a $D$-dimensional head, with a time complexity of
$O(N^2D)$. Given the success of Transformers, improving their runtime during
both training and inference is a popular research area. One such approach is
the introduction of the linear attention (LA) mechanisms, which offers a linear
time complexity of $O(ND^2)$ and have demonstrated comparable accuracy to
regular attention. However, LA in practice lags behind its theoretical
efficiency. We propose a novel method for LA's forward and backward passes,
along with a highly-optimized CUDA implementation. Our approach outperforms the
state-of-the-art by 3.3 times in speed and reduces memory consumption by 3.6
times. We validate these improvements in both single-layer and end-to-end
settings by training a 1.4 billion parameter language model, which demonstrates
similar expressivity to regular attention on major reasoning benchmarks.

</details>


### [127] [Parallel Sampling from Masked Diffusion Models via Conditional Independence Testing](https://arxiv.org/abs/2510.21961)
*Iskander Azangulov,Teodora Pandeva,Niranjani Prasad,Javier Zazo,Sushrut Karmalkar*

Main category: cs.LG

TL;DR: PUNT是一种模型无关的采样器，用于解决掩码扩散模型（MDM）中并行文本生成的速度与准确性之间的冲突。它通过识别并处理令牌依赖性来平衡条件独立性和高置信度预测，从而在保持并行效率的同时提高生成质量，尤其是在长序列生成方面，其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDM）为离散文本生成提供了一种有吸引力的替代方案，因为它支持并行令牌采样，而非自回归模型（ARM）的顺序生成，有望显著加快推理速度。然而，有效的并行采样面临两个相互冲突的需求：(i) 同时更新的令牌必须条件独立；(ii) 更新应优先考虑高置信度的预测。这两个目标相互矛盾，因为高置信度的预测往往聚集在一起并相互依赖，这阻碍了并行更新的有效性。

Method: 本文提出了PUNT，一种模型无关的采样器，旨在调和并行采样中的独立性和置信度冲突。PUNT通过识别令牌依赖性，并从冲突组中移除置信度较低的令牌。这一过程生成了一组满足独立性和置信度标准的索引，用于解除掩码。PUNT通过近似条件独立性测试，确保了改进的并行解除掩码。

Result: 实验结果表明，PUNT在准确性和计算成本之间提供了优于其他强大的免训练基线的权衡，尤其是在生成较长序列时。在IFEval基准测试中，它比包括顺序生成（逐个）在内的基线方法，实现了高达16%的准确性提升。这些性能提升在不同的超参数值下依然保持稳定，减少了对脆弱超参数调优的需求。此外，PUNT还诱导了一种新兴的分层生成策略，即模型首先建立高层次的段落结构，然后再进行局部细化，这表明了一种类似规划的生成过程，有助于实现强大的对齐性能。

Conclusion: PUNT通过有效解决掩码扩散模型中并行采样面临的独立性和高置信度预测之间的冲突，显著提升了离散文本生成的效率和质量。它在准确性和计算效率上展现出优越的权衡，尤其适用于长序列生成，并在IFEval基准上取得了显著的准确性提升。此外，PUNT诱导的分层生成策略揭示了其在文本规划和结构化生成方面的潜力，为未来的文本生成研究提供了新的方向。

Abstract: Masked diffusion models (MDMs) offer a compelling alternative to
autoregressive models (ARMs) for discrete text generation because they enable
parallel token sampling, rather than sequential, left-to-right generation. This
means potentially much faster inference. However, effective parallel sampling
faces two competing requirements: (i) simultaneously updated tokens must be
conditionally independent, and (ii) updates should prioritise high-confidence
predictions. These goals conflict because high-confidence predictions often
cluster and depend on each other, opportunities for parallel updates.
  We present PUNT, a model-agnostic sampler that reconciles this trade-off. Our
method identifies token dependencies and removes lower-confidence tokens from
conflicting groups. This produces sets of indices for unmasking that satisfy
both independence and confidence criteria. Our approach ensures improved
parallel unmasking through approximate conditional independence testing.
  Our experiments show that PUNT delivers a superior trade-off between accuracy
and compute when compared to other strong training-free baselines, especially
for generation of longer sequences. On the IFEval benchmark, it achieves up to
16\% higher accuracy over baseline methods, including sequential generation
(one-by-one). These gains hold across different values of hyperparameters,
mitigating the need for brittle hyperparameter tuning. Moreover, we observe
that PUNT induces an emergent hierarchical generation strategy, where the model
first establishes high-level paragraph structure before local refinement,
suggesting a planning-like generation process that contributes to strong
alignment performance.

</details>


### [128] [Deep Jump Gaussian Processes for Surrogate Modeling of High-Dimensional Piecewise Continuous Functions](https://arxiv.org/abs/2510.21974)
*Yang Xu,Chiwoo Park*

Main category: cs.LG

TL;DR: Deep Jump Gaussian Processes (DJGP)是一种新颖的替代建模方法，通过在Jump Gaussian Processes (JGP)中加入局部线性投影层来处理高维分段连续函数。该方法通过区域特定矩阵捕捉局部子空间结构，并在投影矩阵上使用高斯过程先验来控制模型复杂度，形成独特的GP/JGP两层深度学习结构。实验证明，DJGP在预测精度和不确定性量化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统Jump Gaussian Processes (JGP)在处理高维输入空间中的分段连续函数时面临局限性，难以有效捕捉这些函数的复杂局部结构。因此，需要开发一种新的方法来克服这些限制，实现对高维分段连续函数更准确的建模和更可靠的不确定性量化。

Method: DJGP通过在JGP中添加一个局部线性投影层来解决高维问题。该投影层利用区域特定的矩阵来捕捉局部子空间结构，与JGP的局部性互补。为了控制模型复杂度，作者在投影矩阵上设置了高斯过程 (GP) 先验，使其在输入空间中平滑演变。投影后的输入再通过JGP进行建模，以捕捉与响应的分段连续关系，形成了独特的GP/JGP两层深度学习结构。此外，还开发了一种可扩展的变分推断算法，用于联合学习投影矩阵和JGP超参数。

Result: 在合成数据集和基准数据集上进行的实验表明，与现有方法相比，DJGP提供了卓越的预测精度和更可靠的不确定性量化。

Conclusion: DJGP通过其创新的局部线性投影层和双层GP/JGP结构，成功解决了传统JGP在高维分段连续函数建模中的局限性。该方法在预测准确性和不确定性量化方面表现出优越性，为高维分段连续函数的替代建模提供了一个有效且强大的新工具。

Abstract: We introduce Deep Jump Gaussian Processes (DJGP), a novel method for
surrogate modeling of high-dimensional piecewise continuous functions. DJGP
overcomes the limitations of conventional Jump Gaussian Processes in
high-dimensional input spaces by adding a locally linear projection layer to
Jump Gaussian Processes. This projection uses region-specific matrices to
capture local subspace structures, naturally complementing the localized nature
of JGP, a variant of local Gaussian Processes. To control model complexity, we
place a Gaussian Process prior on the projection matrices, allowing them to
evolve smoothly across the input space. The projected inputs are then modeled
with a JGP to capture piecewise continuous relationships with the response.
This yields a distinctive two-layer deep learning of GP/JGP. We further develop
a scalable variational inference algorithm to jointly learn the projection
matrices and JGP hyperparameters. Experiments on synthetic and benchmark
datasets demonstrate that DJGP delivers superior predictive accuracy and more
reliable uncertainty quantification compared to existing approaches.

</details>


### [129] [Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models](https://arxiv.org/abs/2510.21978)
*Hoang Phan,Xianjun Yang,Kevin Yao,Jingyu Zhang,Shengjie Bi,Xiaocheng Tang,Madian Khabsa,Lijuan Liu,Deren Lei*

Main category: cs.LG

TL;DR: 强化学习与可验证奖励（RLVR）在数学和多模态推理方面取得了显著进展，但存在能力退化风险。本文提出了RECAP，一种具有动态目标重加权的重放策略，通过在线调整训练重点来保存通用知识并提高推理能力，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习与可验证奖励（RLVR）在语言和视觉-语言模型后训练中广泛应用，但在长时间训练下，若不采用正则化策略，模型会忘记基本技能，导致能力退化。现有方法如KL散度正则化仅限于当前任务，无法保证更广泛的知识，而异构领域中的经验重放难以确定每个目标的训练重点。

Method: 针对RLVR中能力退化问题，本文提出RECAP，一种具有动态目标重加权的重放策略，旨在保存通用知识。RECAP通过在线方式，利用收敛性和不稳定性的短期信号动态调整重加权机制，将后训练的重点从饱和目标转移到表现不佳或不稳定的目标。该方法端到端，无需额外模型或大量调优，可直接应用于现有RLVR流程。

Result: 在Qwen2.5-VL-3B和Qwen2.5-VL-7B基准上的广泛实验表明，RECAP方法有效。它不仅能保存模型的通用能力，还能通过在任务内奖励之间实现更灵活的权衡，从而提高推理能力。

Conclusion: RECAP策略通过动态目标重加权有效地解决了RLVR中的能力退化问题，成功地在保留通用能力的同时提高了推理性能。该方法具有端到端特性，易于集成到现有RLVR流程中，为大型语言和视觉-语言模型的后训练提供了一个实用的解决方案。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has delivered
impressive gains in mathematical and multimodal reasoning and has become a
standard post-training paradigm for contemporary language and vision-language
models. However, the RLVR recipe introduces a significant risk of capability
regression, where models forget foundational skills after prolonged training
without employing regularization strategies. We empirically confirm this
concern, observing that open-source reasoning models suffer performance
degradation on core capabilities such as perception and faithfulness. While
imposing regularization terms like KL divergence can help prevent deviation
from the base model, these terms are calculated on the current task, thus they
do not guarantee broader knowledge. Meanwhile, commonly used experience replay
across heterogeneous domains makes it nontrivial to decide how much training
focus each objective should receive. To address this, we propose RECAP-a replay
strategy with dynamic objective reweighting for general knowledge preservation.
Our reweighting mechanism adapts in an online manner using short-horizon
signals of convergence and instability, shifting the post-training focus away
from saturated objectives and toward underperforming or volatile ones. Our
method is end-to-end and readily applicable to existing RLVR pipelines without
training additional models or heavy tuning. Extensive experiments on benchmarks
based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our
method, which not only preserves general capabilities but also improves
reasoning by enabling more flexible trade-offs among in-task rewards.

</details>


### [130] [Boltzmann Graph Ensemble Embeddings for Aptamer Libraries](https://arxiv.org/abs/2510.21980)
*Starlika Bauskar,Jade Jiao,Narayanan Kannan,Alexander Kimm,Justin M. Baker,Matthew J. Tyler,Andrea L. Bertozzi,Anne M. Andrews*

Main category: cs.LG

TL;DR: 该研究引入了一种热力学参数化的指数族随机图（ERGM）嵌入方法，将分子建模为玻尔兹曼加权的相互作用图集合。该方法在SELEX数据集上评估，即使存在实验偏差，也能实现对适体-配体亲和力的鲁棒社区检测和子图级别解释，有助于识别低丰度适体候选物。


<details>
  <summary>Details</summary>
Motivation: 生化领域的机器学习方法通常将分子表示为相互作用图以预测性质和结构。现有方法主要基于单一图（通常是最小自由能结构），这可能无法充分代表热力学平衡下的分子结构。此外，SELEX等实验中存在的偏差（如PCR扩增或测序噪声）会掩盖真实的适体-配体亲和力，导致观察到的丰度与实际结合强度不符的异常候选物，因此需要一种能处理集合结构并对实验偏差具有鲁棒性的方法。

Method: 该研究引入了一种热力学参数化的指数族随机图（ERGM）嵌入方法。该方法将分子建模为玻尔兹曼加权的相互作用图集合，而非单一的最小自由能结构。通过这种方式，它能够捕捉分子在热力学平衡下的构象多样性。这种嵌入方法旨在处理实验中的偏差，从而更准确地反映真实的分子相互作用。

Result: 该嵌入方法在SELEX数据集上进行了评估。结果表明，即使在存在实验偏差（如PCR扩增或测序噪声）的情况下，该方法也能够实现鲁棒的社区检测，并提供适体-配体亲和力的子图级别解释。这意味着该方法能够有效识别出那些因实验偏差而观察丰度较低但实际结合强度高的适体候选物。

Conclusion: 该研究提出的热力学参数化ERGM嵌入方法，通过建模玻尔兹曼加权的相互作用图集合，克服了传统方法仅基于单一图的局限性，并有效处理了实验偏差对适体-配体亲和力预测的影响。该方法能够实现鲁棒的社区检测和子图级别解释，对于识别低丰度适体候选物进行进一步实验评估具有重要意义。未来的工作可能包括在更广泛的数据集上进行验证，并探索其在其他生物分子相互作用预测中的应用。

Abstract: Machine-learning methods in biochemistry commonly represent molecules as
graphs of pairwise intermolecular interactions for property and structure
predictions. Most methods operate on a single graph, typically the minimal free
energy (MFE) structure, for low-energy ensembles (conformations) representative
of structures at thermodynamic equilibrium. We introduce a thermodynamically
parameterized exponential-family random graph (ERGM) embedding that models
molecules as Boltzmann-weighted ensembles of interaction graphs. We evaluate
this embedding on SELEX datasets, where experimental biases (e.g., PCR
amplification or sequencing noise) can obscure true aptamer-ligand affinity,
producing anomalous candidates whose observed abundance diverges from their
actual binding strength. We show that the proposed embedding enables robust
community detection and subgraph-level explanations for aptamer ligand
affinity, even in the presence of biased observations. This approach may be
used to identify low-abundance aptamer candidates for further experimental
evaluation.

</details>


### [131] [From Black-box to Causal-box: Towards Building More Interpretable Models](https://arxiv.org/abs/2510.21998)
*Inwoo Hwang,Yushu Pan,Elias Bareinboim*

Main category: cs.LG

TL;DR: 本文引入了“因果可解释性”的概念，旨在解决深度学习模型预测难以理解的问题。研究发现，传统的黑盒模型和基于概念的预测器通常不具备因果可解释性。为此，作者提出了一个设计框架，通过一个完整的图形判据来构建天生就具有因果可解释性的模型，并揭示了因果可解释性与预测准确性之间存在根本性权衡，同时确定了能够实现最大预测表达能力和可解释性的唯一最大特征集。实验结果验证了理论发现。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习模型的预测结果仍然是一个核心挑战，尤其是在高风险应用中。一个有前景的方法是让模型能够回答反事实问题——即假设性的“如果……会怎样？”情景，这些情景超越了观测数据，为模型推理提供了深入的见解。然而，如何形式化地评估这些反事实查询，以及何时可以从特定类别的模型和观测数据中评估它们，是需要解决的关键问题。

Method: 本文首先引入了“因果可解释性”的概念，用于形式化定义何时可以从特定类别的模型和观测数据中评估反事实查询。研究分析了两种常见的模型类别——黑盒预测器和基于概念的预测器——并指出它们通常不具备因果可解释性。为解决这一问题，本文开发了一个框架，用于构建天生就具有因果可解释性的模型。具体来说，本文推导出了一个完整的图形判据，用于确定给定的模型架构是否支持给定的反事实查询。此外，通过识别唯一的最大特征集，该框架描述了因果可解释性和预测准确性之间的根本性权衡，该特征集在保证可解释性的同时，最大化了预测表达能力。

Result: 研究发现，无论是黑盒预测器还是基于概念的预测器，通常都不具备因果可解释性。本文成功推导出了一个完整的图形判据，可以决定一个给定的模型架构是否支持一个特定的反事实查询。更重要的是，研究揭示了因果可解释性与预测准确性之间存在一个根本性的权衡关系，并通过识别一个独特的、能最大化预测表达能力的同时保持模型可解释性的最大特征集，对这种权衡进行了量化。实验结果与这些理论发现相吻合，证实了所提出框架的有效性。

Conclusion: 本文通过引入因果可解释性，为理解深度学习模型的预测提供了一种新颖而严谨的方法。研究发现，当前模型在因果可解释性方面存在局限性，并提出了一个通过设计实现因果可解释性的通用框架。该框架不仅提供了一个判断模型是否支持反事实查询的图形判据，还揭示了可解释性与预测精度之间的内在矛盾，并指明了在两者之间取得平衡的途径。这些发现对于设计更透明、更值得信赖的深度学习模型具有重要意义，未来工作可在此基础上进一步探索更复杂的因果查询和模型架构。

Abstract: Understanding the predictions made by deep learning models remains a central
challenge, especially in high-stakes applications. A promising approach is to
equip models with the ability to answer counterfactual questions --
hypothetical ``what if?'' scenarios that go beyond the observed data and
provide insight into a model reasoning. In this work, we introduce the notion
of causal interpretability, which formalizes when counterfactual queries can be
evaluated from a specific class of models and observational data. We analyze
two common model classes -- blackbox and concept-based predictors -- and show
that neither is causally interpretable in general. To address this gap, we
develop a framework for building models that are causally interpretable by
design. Specifically, we derive a complete graphical criterion that determines
whether a given model architecture supports a given counterfactual query. This
leads to a fundamental tradeoff between causal interpretability and predictive
accuracy, which we characterize by identifying the unique maximal set of
features that yields an interpretable model with maximal predictive
expressiveness. Experiments corroborate the theoretical findings.

</details>


### [132] [Optimal Detection for Language Watermarks with Pseudorandom Collision](https://arxiv.org/abs/2510.22007)
*T. Tony Cai,Xiang Li,Qi Long,Weijie J. Su,Garrett G. Wen*

Main category: cs.LG

TL;DR: 针对大型语言模型（LLM）输出文本水印中因重复导致的伪随机性不完善问题，本研究引入了一个统计框架，通过分层双层划分和“最小单元”概念来捕捉文本结构。该框架将水印检测表述为极小极大假设检验问题，并为Gumbel-max和逆变换水印提供了闭合形式的最优规则。理论与实验均证实，该方法在严格控制I类错误的同时显著提高了检测能力，为不完善伪随机性下的水印检测奠定了第一个原则性基础。


<details>
  <summary>Details</summary>
Motivation: 文本水印在确保大型语言模型（LLM）输出的可追溯性、问责制以及减轻滥用方面发挥着关键作用。然而，现有的大多数水印方法都假设完美的伪随机性。在实际应用中，生成文本中的重复会引起碰撞，从而产生结构性依赖，这会损害I类错误的控制并使标准分析失效，这凸显了在非完美伪随机条件下进行水印检测的挑战和必要性。

Method: 本研究引入了一个统计框架，通过分层双层划分来捕捉文本的内部结构。该框架的核心概念是“最小单元”，即可以视为独立于其他单元但允许内部存在依赖的最小分组。利用这些最小单元，研究定义了一个非渐近效率度量，并将水印检测问题构建为一个极小极大假设检验问题。该框架被应用于Gumbel-max和逆变换水印，以推导出闭合形式的最优规则。这一方法旨在克服现有水印技术在处理文本重复和结构依赖方面的不足。

Result: 将所提出的统计框架应用于Gumbel-max和逆变换水印后，本研究产生了闭合形式的最优规则。该框架解释了为什么舍弃重复统计数据通常能提高性能，并表明除非是退化的特殊情况，否则必须处理单元内部的依赖性。理论分析和实验结果均证实，该方法在严格控制I类错误（误报率）的同时，显著提高了水印的检测能力。

Conclusion: 本研究为在不完善伪随机性条件下进行水印检测提供了第一个原则性基础。所提出的统计框架不仅提供了理论上的深刻见解，而且为可靠追踪模型输出提供了实用的指导。它通过有效处理文本重复和结构化依赖，显著提升了水印检测的准确性和可靠性，对于保障LLM输出的信任度具有重要意义。

Abstract: Text watermarking plays a crucial role in ensuring the traceability and
accountability of large language model (LLM) outputs and mitigating misuse.
While promising, most existing methods assume perfect pseudorandomness. In
practice, repetition in generated text induces collisions that create
structured dependence, compromising Type I error control and invalidating
standard analyses.
  We introduce a statistical framework that captures this structure through a
hierarchical two-layer partition. At its core is the concept of minimal units
-- the smallest groups treatable as independent across units while permitting
dependence within. Using minimal units, we define a non-asymptotic efficiency
measure and cast watermark detection as a minimax hypothesis testing problem.
  Applied to Gumbel-max and inverse-transform watermarks, our framework
produces closed-form optimal rules. It explains why discarding repeated
statistics often improves performance and shows that within-unit dependence
must be addressed unless degenerate. Both theory and experiments confirm
improved detection power with rigorous Type I error control. These results
provide the first principled foundation for watermark detection under imperfect
pseudorandomness, offering both theoretical insight and practical guidance for
reliable tracing of model outputs.

</details>


### [133] [Cost-Sensitive Evaluation for Binary Classifiers](https://arxiv.org/abs/2510.22016)
*Pierangelo Lombardo,Antonio Casoli,Cristian Cingolani,Shola Oshodi,Michele Zanatta*

Main category: cs.LG

TL;DR: 本文提出了一种名为加权准确率（WA）的二元分类器评估指标，旨在通过直接最小化总分类成本（TCC）来解决缺乏通用评估指标和误解数据集不平衡处理的问题。WA提供了一个与TCC最小化目标一致的成本敏感场景下的类不平衡处理框架，并提出了一种在缺乏完全指定统一分类成本（UCCs）的情况下估计WA权重参数的程序，并通过分析其与TCC的相关性来证明其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 分类器评估指标的选择对于模型比较和参数优化至关重要，但目前缺乏一个被普遍接受的权威标准。此外，在用于训练分类模型的数据集中，对于减轻不平衡的需求常存在误解。由于分类器优化的最终目标通常是最大化投资回报或等效地最小化总分类成本（TCC），因此需要一个能直接服务于此目标的评估指标和处理不平衡的框架。

Method: 本文定义了加权准确率（WA）作为二元分类器的一种评估指标，其直观的解释是知名准确率指标的加权版本，并且与最小化总分类成本（TCC）的需求一致。文章阐明了在成本敏感场景下处理类别不平衡的概念框架，该框架作为再平衡技术的替代方案，适用于任何像WA一样可以表示为示例依赖量线性组合的指标。它允许比较不同数据集的结果，并解决开发数据集和目标数据集之间的差异。此外，该方法明确了在何种场景下使用不平衡类成本（UCCs）无关的类再平衡技术或再平衡指标与TCC最小化一致，以及何时适得其反。最后，提出了一种在缺乏完全指定UCCs的情况下估计WA权重参数的程序。

Result: 本文的关键发现包括：1. 定义的加权准确率（WA）是一个具有直观解释的二元分类器评估指标，作为众所周知的准确率指标的加权版本，并且与最小化总分类成本（TCC）的需求高度一致。2. 提供了一个在成本敏感场景下处理类不平衡的概念框架，该框架作为再平衡技术的替代方案，可以应用于任何可以表示为示例依赖量线性组合的指标。3. 该框架允许比较在不同数据集上获得的结果，并解决开发数据集和目标数据集与目标数据集之间的差异。4. 明确了在哪些场景下使用UCCs-unaware类再平衡技术或再平衡指标与TCC最小化一致，以及何时适得其反。5. 提出了一种在缺乏完全指定UCCs的情况下估计WA权重参数的程序。6. 通过分析WA与TCC在示例依赖场景中的相关性，证明了WA的鲁棒性。

Conclusion: 本文通过引入加权准确率（WA）和相关的概念框架，为成本敏感的二元分类器评估和类不平衡处理提供了一种新颖且有力的替代方案。WA直接与最小化总分类成本（TCC）的目标对齐，并提供了一个明确的框架来理解和管理类不平衡问题，特别是在不同数据集和开发-部署场景中。该框架对于理解何时传统的再平衡技术与TCC最小化相符，何时适得其反具有重要意义。未来工作可以探索WA在多分类场景下的扩展，以及更复杂的成本结构下的应用。

Abstract: Selecting an appropriate evaluation metric for classifiers is crucial for
model comparison and parameter optimization, yet there is not consensus on a
universally accepted metric that serves as a definitive standard. Moreover,
there is often a misconception about the perceived need to mitigate imbalance
in datasets used to train classification models. Since the final goal in
classifier optimization is typically maximizing the return of investment or,
equivalently, minimizing the Total Classification Cost (TCC), we define
Weighted Accuracy (WA), an evaluation metric for binary classifiers with a
straightforward interpretation as a weighted version of the well-known accuracy
metric, coherent with the need of minimizing TCC. We clarify the conceptual
framework for handling class imbalance in cost-sensitive scenarios, providing
an alternative to rebalancing techniques. This framework can be applied to any
metric that, like WA, can be expressed as a linear combination of
example-dependent quantities and allows for comparing the results obtained in
different datasets and for addressing discrepancies between the development
dataset, used to train and validate the model, and the target dataset, where
the model will be deployed. It also specifies in which scenarios using
UCCs-unaware class rebalancing techniques or rebalancing metrics aligns with
TCC minimization and when it is instead counterproductive. Finally, we propose
a procedure to estimate the WA weight parameter in the absence of fully
specified UCCs and demonstrate the robustness of WA by analyzing its
correlation with TCC in example-dependent scenarios.

</details>


### [134] [Normalization in Attention Dynamics](https://arxiv.org/abs/2510.22026)
*Nikita Karagodin,Shu Ge,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 本研究将Transformer中词元表征的归一化方案对表征演化的影响建模为球体上相互作用粒子的“速度调节”。该模型统一分析了多种归一化方案（包括Post-LN、Pre-LN、Mix-LN、Peri-LN、nGPT和LN-Scaling），揭示了它们如何影响聚类动态和表征崩溃，并确定Peri-LN是一种特别有效的选择。


<details>
  <summary>Details</summary>
Motivation: 深度Transformer模型中，归一化方案对词元表征的演化有着关键影响，直接关系到模型的训练稳定性、性能和表征质量。然而，现有对不同归一化方案如何具体影响词元表征，尤其是其聚类动态和表征崩溃机制的理解尚不完全清晰，缺乏一个统一的理论框架进行比较和分析。因此，本研究旨在提供一个原理性的基础来理解和比较这些方案。

Method: 研究方法是将深度Transformer中词元表征的演化过程建模为在球体上相互作用的粒子。在这种模型下，归一化操作被概念化为一种“速度调节”机制，影响这些粒子的运动轨迹和相互作用。基于这一新颖的视角，本研究对包括Post-LN、Pre-LN、Mix-LN、Peri-LN、nGPT和LN-Scaling在内的多种归一化方案进行了统一的理论分析。通过分析，揭示了不同方案如何影响词元表征的聚类动态和潜在的表征崩溃现象。

Result: 本研究的关键发现在于所提出的框架能够清晰地阐明不同归一化方案如何在Transformer的不同层中塑造词元表征。通过统一的分析，该框架提供了一个原理性的基础来比较各种归一化方案的有效性。具体而言，研究结果表明Peri-LN被认为是一种特别有效的选择，因为它在影响聚类动态和避免表征崩溃方面表现出优异的性能。

Conclusion: 本研究通过将归一化方案对词元表征的影响视为“速度调节”，为理解Transformer中的归一化机制提供了一个新颖且统一的分析框架。该框架不仅加深了我们对不同归一化方案如何塑造词元表征、影响聚类动态和表征崩溃的理解，还提供了一个评估和比较这些方案的原理性基础。研究成果特别指出Peri-LN是一种高效的归一化策略，对于Transformer模型的设计和优化具有重要的指导意义。未来工作可以基于此框架进一步探索新的归一化方法或优化现有方法的参数。

Abstract: We study the effect of normalization schemes on token representations in deep
transformers. Modeling their evolution as interacting particles on the sphere,
we show that normalization acts as a form of speed regulation. This perspective
enables a unified analysis of several schemes -- including Post-LN, Pre-LN,
Mix-LN, Peri-LN, nGPT, and LN-Scaling -- revealing how they influence
clustering dynamics and representation collapse. Our framework clarifies how
different schemes shape token representations across layers and provides a
principled basis for comparing them, identifying Peri-LN as a particularly
effective choice.

</details>


### [135] [Online Optimization for Offline Safe Reinforcement Learning](https://arxiv.org/abs/2510.22027)
*Yassine Chemingui,Aryan Deshwal,Alan Fern,Thanh Nguyen-Tang,Janardhan Rao Doppa*

Main category: cs.LG

TL;DR: 该论文提出了一种新的离线安全强化学习（OSRL）方法，通过将问题建模为极小极大目标，并结合离线强化学习与在线优化算法来学习在累积成本约束下最大化回报的策略。该方法在理论上证明了近似最优性，并在DSRL基准测试中经验性地展示了在严格成本预算下可靠地执行安全约束并实现高回报的能力。


<details>
  <summary>Details</summary>
Motivation: 研究的背景是离线安全强化学习（OSRL）问题。在OSRL中，目标是从固定的数据集学习一个策略，该策略既能最大化奖励，又要满足累积成本约束。这个问题的研究意义在于，在许多现实世界的应用中，数据收集可能成本高昂或存在风险，并且安全性是至关重要的。因此，从现有数据中安全地学习策略对于实际部署至关重要。

Method: 论文提出了一种新颖的OSRL方法，将问题构建为一个极小极大目标。该方法通过结合离线强化学习（RL）和在线优化算法来解决这个目标。作者证明了当与近似离线RL预言机和无悔在线优化结合时，该方法具有近似最优性。此外，论文还提出了一种实用的近似方法，该方法可以与任何离线RL算法结合使用，并消除了对离线策略评估的需求。

Result: 该方法在DSRL基准测试上进行了实证评估。结果表明，即使在严格的成本预算下，该方法也能可靠地执行安全约束。与此同时，它还实现了高额的回报。相关代码已在GitHub上提供。

Conclusion: 论文提出了一种有效的离线安全强化学习（OSRL）方法，该方法通过极小极大目标和结合离线强化学习与在线优化来解决在累积成本约束下最大化回报的问题。该方法不仅在理论上证明了近似最优性，而且在实践中也表现出色，能够在保证安全性的同时实现高回报。这为从固定数据中安全地学习策略提供了一个有前景的解决方案。

Abstract: We study the problem of Offline Safe Reinforcement Learning (OSRL), where the
goal is to learn a reward-maximizing policy from fixed data under a cumulative
cost constraint. We propose a novel OSRL approach that frames the problem as a
minimax objective and solves it by combining offline RL with online
optimization algorithms. We prove the approximate optimality of this approach
when integrated with an approximate offline RL oracle and no-regret online
optimization. We also present a practical approximation that can be combined
with any offline RL algorithm, eliminating the need for offline policy
evaluation. Empirical results on the DSRL benchmark demonstrate that our method
reliably enforces safety constraints under stringent cost budgets, while
achieving high rewards. The code is available at
https://github.com/yassineCh/O3SRL.

</details>


### [136] [Differentiable Constraint-Based Causal Discovery](https://arxiv.org/abs/2510.22031)
*Jincheng Zhou,Mengbo Wang,Anqi He,Yumeng Zhou,Hessam Olya,Murat Kocaoglu,Bruno Ribeiro*

Main category: cs.LG

TL;DR: 本文提出了一种新的因果发现方法，通过使用软逻辑的渗流理论开发可微分的d-分离分数，从而实现条件独立性约束的梯度优化。该方法在低样本量条件下表现出鲁棒性能，在真实世界数据集中优于传统的基于约束和基于分数的方法。


<details>
  <summary>Details</summary>
Motivation: 因果发现是人工智能中的一项基础任务，对决策、预测和干预具有深远影响。然而，现有方法（包括基于约束和基于分数的方法）各有局限：基于约束的方法虽严谨但受限于小样本量，而基于分数的方法虽灵活却通常放弃显式条件独立性测试。因此，迫切需要一种能够克服这些限制的新型因果发现方法。

Method: 本文探索了一种新途径：开发可微分的d-分离分数。这些分数通过使用软逻辑的渗流理论获得，从而实现了一种新型的因果发现方法——条件独立性约束的梯度优化。这种方法结合了两种现有范式的优点，旨在克服它们各自的缺点。

Result: 经验评估表明，本文提出的方法在低样本量条件下表现出鲁棒性能。在真实世界数据集上，该方法超越了传统的基于约束和基于分数的基线方法。此外，所提出方法的代码和数据已公开可用。

Conclusion: 本文提出了一种基于可微分d-分离分数和梯度优化的新型因果发现方法。该方法在低样本量场景下展示了卓越的性能，有效解决了传统方法在此类情况下的挑战。这一贡献为因果发现领域开辟了新的研究方向，特别是在数据稀缺的实际应用中具有重要意义。

Abstract: Causal discovery from observational data is a fundamental task in artificial
intelligence, with far-reaching implications for decision-making, predictions,
and interventions. Despite significant advances, existing methods can be
broadly categorized as constraint-based or score-based approaches.
Constraint-based methods offer rigorous causal discovery but are often hindered
by small sample sizes, while score-based methods provide flexible optimization
but typically forgo explicit conditional independence testing. This work
explores a third avenue: developing differentiable $d$-separation scores,
obtained through a percolation theory using soft logic. This enables the
implementation of a new type of causal discovery method: gradient-based
optimization of conditional independence constraints. Empirical evaluations
demonstrate the robust performance of our approach in low-sample regimes,
surpassing traditional constraint-based and score-based baselines on a
real-world dataset. Code and data of the proposed method are publicly available
at https://github$.$com/PurdueMINDS/DAGPA.

</details>


### [137] [Generalized Top-k Mallows Model for Ranked Choices](https://arxiv.org/abs/2510.22040)
*Shahrzad Haddadan,Sara Ahmadian*

Main category: cs.LG

TL;DR: 本论文针对广义top-k Mallows模型，提出了新的采样方案、高效的选择概率计算算法和用于参数估计的主动学习算法。通过严格的数学分析和在合成及真实数据上的大量实验，证明了所提方法的可扩展性和准确性，并将其预测能力与多项式Logit模型进行了比较，为关键决策场景提供了新的分析和预测工具。


<details>
  <summary>Details</summary>
Motivation: 经典的Mallows模型在捕捉用户只关注有限偏好项目并对其他项目不感兴趣的真实场景时存在局限性。为了解决这一问题，提出了如top-k Mallows模型等扩展，以更好地适应实际应用。本研究旨在解决广义top-k Mallows模型中与分析买家选择相关的多个挑战，以期提升模型在实际场景中的应用能力。

Method: 本研究主要贡献包括：1) 针对广义top-k Mallows模型设计了一种新颖的采样方案。2) 提出了一种在该模型下计算选择概率的高效算法。3) 开发了一种主动学习算法，用于从观测到的选择数据中估计模型参数。研究还对所提算法的性能进行了严格的数学分析，并通过在合成数据和真实世界数据上进行大量实验，来验证方法的有效性和性能。此外，论文还将Mallows模型在top-k列表上的预测能力与更简单的多项式Logit模型进行了比较。

Result: 本研究的成果为关键决策场景中的分析和预测提供了新工具。通过在合成数据和真实世界数据上进行的大量实验，我们证明了所提出方法的可扩展性和准确性。同时，研究还比较了Mallows模型在top-k列表方面的预测能力与更简单的多项式Logit模型。

Conclusion: 本论文通过引入新颖的采样方案、高效的选择概率计算算法和主动学习算法，显著改进了广义top-k Mallows模型的应用。这些贡献为决策场景下的分析和预测提供了强大的新工具。严格的数学分析和广泛的实验验证了所提方法在可扩展性和准确性方面的优越性，并通过与多项式Logit模型的比较，进一步证明了其预测能力。这些发现对于理解和预测买家选择行为具有重要意义。

Abstract: The classic Mallows model is a foundational tool for modeling user
preferences. However, it has limitations in capturing real-world scenarios,
where users often focus only on a limited set of preferred items and are
indifferent to the rest. To address this, extensions such as the top-k Mallows
model have been proposed, aligning better with practical applications. In this
paper, we address several challenges related to the generalized top-k Mallows
model, with a focus on analyzing buyer choices. Our key contributions are: (1)
a novel sampling scheme tailored to generalized top-k Mallows models, (2) an
efficient algorithm for computing choice probabilities under this model, and
(3) an active learning algorithm for estimating the model parameters from
observed choice data. These contributions provide new tools for analysis and
prediction in critical decision-making scenarios. We present a rigorous
mathematical analysis for the performance of our algorithms. Furthermore,
through extensive experiments on synthetic data and real-world data, we
demonstrate the scalability and accuracy of our proposed methods, and we
compare the predictive power of Mallows model for top-k lists compared to the
simpler Multinomial Logit model.

</details>


### [138] [Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing](https://arxiv.org/abs/2510.22044)
*Kijung Jeon,Michael Muehlebach,Molei Tao*

Main category: cs.LG

TL;DR: 本文介绍了一种名为OLLA（Overdamped Langevin with LAnding）的新框架，用于从包含等式和/或不等式约束的受限统计分布中进行采样。与现有依赖昂贵投影步骤且无法同时处理等式和不等式约束的方法不同，OLLA设计了一种过阻尼Langevin动力学，通过沿着约束曲面的法线方向确定性地修正轨迹，从而避免了显式投影。该方法在合适的正则条件下，在W2距离上以指数速度收敛到目标受限密度，并在实验中表现出比基于投影的算法更高的效率和合理的混合性能。


<details>
  <summary>Details</summary>
Motivation: 从受限统计分布中采样是贝叶斯统计、计算化学和统计物理等领域的一项基本任务。当受限分布由无约束密度以及额外的等式和/或不等式约束描述时，约束集通常是非凸的。现有用于定义为等式或不等式约束的非凸约束集$\\Sigma \\subset \\mathbb{R}^d$的方法普遍依赖于成本高昂的投影步骤。此外，这些方法通常只能专门处理其中一种情况，无法同时处理等式和不等式约束。同时，它们往往缺乏严格和定量的收敛保证，这限制了其在实际应用中的可靠性。

Method: 本文引入了Overdamped Langevin with LAnding (OLLA) 框架，旨在设计能够同时处理等式和不等式约束的过阻尼Langevin动力学。OLLA的核心方法在于其提出了一种动力学，能够沿着约束曲面的法线方向确定性地修正轨迹，从而无需进行显式的投影操作。这一特性显著降低了计算成本，并解决了现有方法中投影步骤的难题。OULA通过这种创新的轨迹修正机制，实现了对复杂非凸约束集的有效处理。

Result: 在目标密度和约束集$\\Sigma$满足适当正则条件的情况下，OLLA在W2距离上以指数速度收敛到受限目标密度$\\rho_\\Sigma(x) \\propto \\exp(-f(x))d\\sigma_\\Sigma$。通过实验证明，与基于投影的受限Langevin算法及其松弛变量变体相比，OLLA展现出更高的效率、更有利的计算成本和合理的经验混合性能，突出了其在实际应用中的优越性。

Conclusion: OLLA框架为从包含等式和/或不等式约束的受限统计分布中采样提供了一种新颖且高效的解决方案。它克服了现有方法在处理非凸约束集、同时处理等式和不等式约束以及收敛性保证方面的局限性。通过避免显式投影并实现指数收敛，OLLA显著提高了采样算法的计算效率和理论可靠性。未来工作可能包括探索OLLA在更广泛的应用场景中的表现，以及进一步优化其在特定复杂约束条件下的性能。

Abstract: Sampling from constrained statistical distributions is a fundamental task in
various fields including Bayesian statistics, computational chemistry, and
statistical physics. This article considers the cases where the constrained
distribution is described by an unconstrained density, as well as additional
equality and/or inequality constraints, which often make the constraint set
nonconvex. Existing methods for nonconvex constraint set $\Sigma \subset
\mathbb{R}^d$ defined by equality or inequality constraints commonly rely on
costly projection steps. Moreover, they cannot handle equality and inequality
constraints simultaneously as each method only specialized in one case. In
addition, rigorous and quantitative convergence guarantee is often lacking. In
this paper, we introduce Overdamped Langevin with LAnding (OLLA), a new
framework that can design overdamped Langevin dynamics accommodating both
equality and inequality constraints. The proposed dynamics also
deterministically corrects trajectories along the normal direction of the
constraint surface, thus obviating the need for explicit projections. We show
that, under suitable regularity conditions on the target density and $\Sigma$,
OLLA converges exponentially fast in $W_2$ distance to the constrained target
density $\rho_\Sigma(x) \propto \exp(-f(x))d\sigma_\Sigma$. Lastly, through
experiments, we demonstrate the efficiency of OLLA compared to projection-based
constrained Langevin algorithms and their slack variable variants, highlighting
its favorable computational cost and reasonable empirical mixing.

</details>


### [139] [PF$Δ$: A Benchmark Dataset for Power Flow under Load, Generation, and Topology Variations](https://arxiv.org/abs/2510.22048)
*Ana K. Rivera,Anvita Bhagavathula,Alvaro Carbonero,Priya Donti*

Main category: cs.LG

TL;DR: 本文介绍了PFΔ，一个用于电力潮流计算的基准数据集，包含859,800个已求解实例，涵盖多种系统规模、N、N-1和N-2故障场景以及接近不可行的工况。该数据集旨在系统评估机器学习方法（特别是GNNs）在应对实时电网运行中计算瓶颈和不确定性方面的性能，并识别现有方法的不足和未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 电力潮流计算是实时电网运行（如故障分析和拓扑优化）的核心，但其计算量大，是主要的计算瓶颈。随着可再生能源整合和气候变化带来的不确定性增加，电网运行需要能够准确高效模拟广泛场景的工具。机器学习方法有望加速这些计算，但其在捕获真实世界变异性的基准上尚未得到系统评估，这是本文研究的动力。

Method: 本文引入了PFΔ，一个电力潮流计算的基准数据集。PFΔ包含859,800个已求解的电力潮流实例，涵盖六种不同规模的总线系统，捕捉了三种类型的故障场景（N、N-1和N-2），并包括了接近稳态电压稳定性极限的接近不可行的情况。研究评估了传统的电力潮流求解器和基于GNN的方法，旨在系统性地评估机器学习方法在实际基准上的表现。

Result: 研究通过评估传统求解器和基于GNN的方法，突出了现有方法在处理电力潮流计算中遇到困难的关键领域。此外，论文还识别了未来研究需要解决的开放性问题。PFΔ数据集及其生成脚本和模型实现代码均已公开提供，为后续研究提供了基础资源。

Conclusion: 本文通过引入PFΔ基准数据集，为系统评估机器学习方法在电力潮流计算中的应用奠定了基础。它不仅揭示了现有方法的局限性，也为未来的研究指明了方向，以开发更高效、更准确的工具来应对电网运行中的计算挑战和不确定性。PFΔ的发布预计将推动电力系统领域机器学习应用的进步。

Abstract: Power flow (PF) calculations are the backbone of real-time grid operations,
across workflows such as contingency analysis (where repeated PF evaluations
assess grid security under outages) and topology optimization (which involves
PF-based searches over combinatorially large action spaces). Running these
calculations at operational timescales or across large evaluation spaces
remains a major computational bottleneck. Additionally, growing uncertainty in
power system operations from the integration of renewables and climate-induced
extreme weather also calls for tools that can accurately and efficiently
simulate a wide range of scenarios and operating conditions. Machine learning
methods offer a potential speedup over traditional solvers, but their
performance has not been systematically assessed on benchmarks that capture
real-world variability. This paper introduces PF$\Delta$, a benchmark dataset
for power flow that captures diverse variations in load, generation, and
topology. PF$\Delta$ contains 859,800 solved power flow instances spanning six
different bus system sizes, capturing three types of contingency scenarios (N ,
N -1, and N -2), and including close-to-infeasible cases near steady-state
voltage stability limits. We evaluate traditional solvers and GNN-based
methods, highlighting key areas where existing approaches struggle, and
identifying open problems for future research. Our dataset is available at
https://huggingface.co/datasets/pfdelta/pfdelta/tree/main and our code with
data generation scripts and model implementations is at
https://github.com/MOSSLab-MIT/pfdelta.

</details>


### [140] [Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model](https://arxiv.org/abs/2510.22057)
*James Thiering,Tarun Sethupat Radha Krishna,Dylan Zelkin,Ashis Kumer Biswas*

Main category: cs.LG

TL;DR: 本研究旨在开发一种自动系统，用于检测在线学习中学生的参与度，并提出了一种新颖的训练方法，该方法通过属性正交正则化技术和多重迁移学习策略，有效防止模型利用性别等敏感特征进行预测，显著提高了预测在敏感群体分布中的公平性，将皮尔逊相关系数从0.897提升至0.999。


<details>
  <summary>Details</summary>
Motivation: 随着在线和虚拟学习的兴起，监控和提升学生参与度已成为有效教育的重要组成部分。传统的学生参与度评估方法可能不适用于虚拟环境，因此需要开发一种自动系统来检测在线学习期间学生的参与度水平。此外，研究还关注在预测中避免利用性别等敏感特征，以确保模型符合伦理标准并增强其可解释性。

Method: 研究提出了一种新颖的训练方法，旨在阻止模型利用性别等敏感特征进行预测。具体而言，该方法将属性正交正则化技术应用于一个分体式分类器。该分类器采用了多种迁移学习策略，以有效减少敏感群体预测分布中的差异。这种方法不仅有助于执行伦理标准，还能增强模型预测的可解释性。

Result: 通过所提出的缓解方法，模型在减少敏感群体预测分布差异方面取得了显著效果。未经缓解的模型的皮尔逊相关系数为0.897，而经过缓解的模型则将该系数提高到0.999。这表明该方法成功地提升了预测的公平性，并降低了模型对敏感特征的依赖。

Conclusion: 本研究开发了一种用于在线学习学生参与度检测的自动化系统，并提出了一种新颖的训练方法，该方法通过属性正交正则化技术和多重迁移学习策略，成功地阻止模型利用性别等敏感特征进行预测。这不仅有助于强制执行伦理标准，还增强了模型预测的可解释性。实验结果表明，该方法显著减少了敏感群体预测分布的差异，将皮尔逊相关系数从0.897提高到0.999，验证了其在提升公平性方面的有效性。未来的工作可以进一步探索该方法在其他敏感特征和学习场景中的应用。

Abstract: With the rise of online and virtual learning, monitoring and enhancing
student engagement have become an important aspect of effective education.
Traditional methods of assessing a student's involvement might not be
applicable directly to virtual environments. In this study, we focused on this
problem and addressed the need to develop an automated system to detect student
engagement levels during online learning. We proposed a novel training method
which can discourage a model from leveraging sensitive features like gender for
its predictions. The proposed method offers benefits not only in the
enforcement of ethical standards, but also to enhance interpretability of the
model predictions. We applied an attribute-orthogonal regularization technique
to a split-model classifier, which uses multiple transfer learning strategies
to achieve effective results in reducing disparity in the distribution of
prediction for sensitivity groups from a Pearson correlation coefficient of
0.897 for the unmitigated model, to 0.999 for the mitigated model. The source
code for this project is available on
https://github.com/ashiskb/elearning-engagement-study .

</details>


### [141] [Deep Gaussian Processes for Functional Maps](https://arxiv.org/abs/2510.22068)
*Matthew Lowery,Zhitong Xu,Da Long,Keyan Chen,Daniel S. Johnson,Yang Bai,Varun Shankar,Shandian Zhe*

Main category: cs.LG

TL;DR: 该论文提出了一种名为深度高斯过程函数映射（DGPFM）的新方法，用于处理函数对函数回归中的复杂非线性和不确定性量化问题。DGPFM通过结合基于高斯过程（GP）的线性与非线性变换、核的积分变换、GP插值以及从GP中采样的非线性激活函数，实现了在噪声、稀疏和不规则采样数据下更好的预测性能和不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 函数空间之间的映射学习，即函数对函数回归，在函数数据分析中至关重要，并在时空预测、曲线预测和气候建模等领域有广泛应用。然而，现有方法，如函数线性模型和神经算子，存在局限性：它们要么无法捕获复杂的非线性关系，要么在处理噪声、稀疏和不规则采样数据时缺乏可靠的不确定性量化能力。这些限制阻碍了这些方法在实际应用中的有效性。

Method: 为了解决现有方法的局限性，本文提出了深度高斯过程函数映射（DGPFM）。该方法设计了一系列基于高斯过程（GP）的线性与非线性变换，具体利用了核的积分变换、GP插值以及从GP中采样的非线性激活函数。一个关键的实现简化是：在固定位置下，核积分变换的离散近似会演变为直接的函数积分变换，这使得能够灵活地整合各种积分变换设计。为了实现可扩展的概率推断，DGPFM采用了诱导点和白化变换来开发一种变分学习算法。

Result: 在真实世界和偏微分方程（PDE）基准数据集上的实证结果表明，DGPFM在预测性能和不确定性校准方面均优于现有方法，展现了其显著优势。

Conclusion: 本文提出的深度高斯过程函数映射（DGPFM）为函数对函数回归提供了一种强大的解决方案，特别是在处理复杂非线性和不确定性量化方面。通过结合GP的线性/非线性变换和可扩展的变分推断，DGPFM在多种数据集上展现出卓越的预测能力和可靠的不确定性估计，这对于函数数据分析及相关应用领域具有重要意义。

Abstract: Learning mappings between functional spaces, also known as
function-on-function regression, plays a crucial role in functional data
analysis and has broad applications, e.g. spatiotemporal forecasting, curve
prediction, and climate modeling. Existing approaches, such as functional
linear models and neural operators, either fall short of capturing complex
nonlinearities or lack reliable uncertainty quantification under noisy, sparse,
and irregularly sampled data. To address these issues, we propose Deep Gaussian
Processes for Functional Maps (DGPFM). Our method designs a sequence of
GP-based linear and nonlinear transformations, leveraging integral transforms
of kernels, GP interpolation, and nonlinear activations sampled from GPs. A key
insight simplifies implementation: under fixed locations, discrete
approximations of kernel integral transforms collapse into direct functional
integral transforms, enabling flexible incorporation of various integral
transform designs. To achieve scalable probabilistic inference, we use inducing
points and whitening transformations to develop a variational learning
algorithm. Empirical results on real-world and PDE benchmark datasets
demonstrate that the advantage of DGPFM in both predictive performance and
uncertainty calibration.

</details>


### [142] [MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification](https://arxiv.org/abs/2510.22070)
*Luca Caldera,Giacomo Bottacini,Lara Cavinato*

Main category: cs.LG

TL;DR: MAGIC-Flow是一种条件多尺度归一化流架构，在一个模块化框架中同时进行生成和分类，尤其适用于医学成像等数据受限领域。它通过可逆双射层级结构实现精确似然计算和稳定优化，支持可控样本合成和类概率估计，从而生成逼真多样的样本并提高分类性能，为隐私保护增强、鲁棒泛化和值得信赖的医疗AI提供了直接益处。


<details>
  <summary>Details</summary>
Motivation: 生成模型在表示学习中表现出色，但其在医学成像等挑战性领域的直接应用受限。现有生成模型缺乏任务对齐，无法为临床使用提供稳健基础，无法同时满足生成和判别目标。这突出了开发一种能够同时进行生成和分类，并能提供可解释性和处理数据稀缺性问题的模型的重要性。

Method: 本研究提出了MAGIC-Flow，一种条件多尺度归一化流架构。该模型被构建为可逆和可微分双射的层级结构，其中雅可比行列式在子变换中分解。这种设计确保了精确的似然计算和稳定的优化。此外，可逆性使得样本似然的显式可视化成为可能，为模型推理提供了一个可解释的视角。通过以类别标签为条件，MAGIC-Flow支持可控的样本合成和基于原理的类概率估计，有效地辅助了生成和判别目标。研究通过相似性、保真度和多样性指标，在多个数据集上将MAGIC-Flow与顶级基线进行了评估。

Result: MAGIC-Flow在多个数据集上与顶级基线进行了对比评估，并取得了显著成果。它在扫描仪噪声下处理生成和分类，以及进行特定模态的合成和识别。结果表明，MAGIC-Flow能够创建真实、多样的样本，并显著提高了分类性能。这些结果通过相似性、保真度和多样性等指标进行了验证，证明了其在数据受限领域同时进行生成和分类的有效性。

Conclusion: MAGIC-Flow被证明是数据受限领域（特别是医学成像）中实现生成和分类的有效策略。其核心贡献在于在一个统一的模块化框架内实现了精确似然计算、稳定优化、可解释性、可控样本合成和类概率估计。MAGIC-Flow为隐私保护增强、鲁棒泛化和值得信赖的医疗AI提供了直接益处。未来的工作可能涉及进一步探索其在更广泛临床场景中的应用及其对新型医疗AI模型开发的潜力。

Abstract: Generative modeling has emerged as a powerful paradigm for representation
learning, but its direct applicability to challenging fields like medical
imaging remains limited: mere generation, without task alignment, fails to
provide a robust foundation for clinical use. We propose MAGIC-Flow, a
conditional multiscale normalizing flow architecture that performs generation
and classification within a single modular framework. The model is built as a
hierarchy of invertible and differentiable bijections, where the Jacobian
determinant factorizes across sub-transformations. We show how this ensures
exact likelihood computation and stable optimization, while invertibility
enables explicit visualization of sample likelihoods, providing an
interpretable lens into the model's reasoning. By conditioning on class labels,
MAGIC-Flow supports controllable sample synthesis and principled
class-probability estimation, effectively aiding both generative and
discriminative objectives. We evaluate MAGIC-Flow against top baselines using
metrics for similarity, fidelity, and diversity. Across multiple datasets, it
addresses generation and classification under scanner noise, and
modality-specific synthesis and identification. Results show MAGIC-Flow creates
realistic, diverse samples and improves classification. MAGIC-Flow is an
effective strategy for generation and classification in data-limited domains,
with direct benefits for privacy-preserving augmentation, robust
generalization, and trustworthy medical AI.

</details>


### [143] [Learning 3D Anisotropic Noise Distributions Improves Molecular Force Field Modeling](https://arxiv.org/abs/2510.22123)
*Xixian Liu,Rui Jiao,Zhiyuan Liu,Yurou Liu,Yang Liu,Ziheng Lu,Wenbing Huang,Yang Zhang,Yixin Cao*

Main category: cs.LG

TL;DR: AniDS是一种新颖的3D分子去噪框架，它引入了结构感知各向异性噪声生成器，通过产生原子特异性的全协方差矩阵来更好地反映分子系统中方向和结构的可变性，从而在力预测精度方面优于现有的各向同性去噪模型和其他领先方法，并在MD17和OC22基准测试中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的坐标去噪方法在3D分子预训练中表现出潜力，但它们依赖于过于简化的分子动力学模型，假设原子运动是各向同性且同方差的，这限制了它们准确捕捉复杂分子动力学的能力。因此，开发能够处理分子系统方向性和结构变异性的更精确去噪方法变得至关重要，以提升分子力场的学习效果和预测精度。

Method: 本文提出了AniDS框架，即用于3D分子去噪的各向异性变分自编码器。AniDS引入了一个结构感知的各向异性噪声生成器，该生成器可以为高斯噪声分布生成原子特异性的全协方差矩阵，以更好地反映分子系统中的方向性和结构变异性。这些协方差矩阵是从成对原子相互作用中导出的，作为各向同性基的各向异性校正。该设计确保了生成的协方差矩阵是对称、正半定且SO(3)等变的，同时提供了更大的能力来建模复杂的分子动力学。

Result: 在MD17和OC22基准测试中，AniDS的实验结果表明，它在力预测精度方面优于先前的各向同性同方差去噪模型以及其他领先方法，平均相对改进分别为8.9%和6.2%。对晶体和分子结构进行的案例研究表明，AniDS能够沿键合方向自适应地抑制噪声，这与物理化学原理是一致的。

Conclusion: AniDS框架通过引入结构感知各向异性噪声生成器，显著提升了3D分子去噪的性能，解决了现有方法在处理分子系统方向和结构可变性方面的局限性。其在基准测试中的卓越表现和对物理化学原理的一致性验证了其在复杂分子动力学建模和力场学习方面的强大潜力，为3D分子预训练领域带来了重要的进展。

Abstract: Coordinate denoising has emerged as a promising method for 3D molecular
pretraining due to its theoretical connection to learning molecular force
field. However, existing denoising methods rely on oversimplied molecular
dynamics that assume atomic motions to be isotropic and homoscedastic. To
address these limitations, we propose a novel denoising framework AniDS:
Anisotropic Variational Autoencoder for 3D Molecular Denoising. AniDS
introduces a structure-aware anisotropic noise generator that can produce
atom-specific, full covariance matrices for Gaussian noise distributions to
better reflect directional and structural variability in molecular systems.
These covariances are derived from pairwise atomic interactions as anisotropic
corrections to an isotropic base. Our design ensures that the resulting
covariance matrices are symmetric, positive semi-definite, and
SO(3)-equivariant, while providing greater capacity to model complex molecular
dynamics. Extensive experiments show that AniDS outperforms prior isotropic and
homoscedastic denoising models and other leading methods on the MD17 and OC22
benchmarks, achieving average relative improvements of 8.9% and 6.2% in force
prediction accuracy. Our case study on a crystal and molecule structure shows
that AniDS adaptively suppresses noise along the bonding direction, consistent
with physicochemical principles. Our code is available at
https://github.com/ZeroKnighting/AniDS.

</details>


### [144] [Efficient Utility-Preserving Machine Unlearning with Implicit Gradient Surgery](https://arxiv.org/abs/2510.22124)
*Shiji Zhou,Tianbai Yu,Zhi Zhang,Heng Chang,Xiao Zhou,Dong Wu,Han Zhao*

Main category: cs.LG

TL;DR: 该论文提出了一种高效且能保持模型实用性的机器遗忘（MU）方法。核心思想是将MU建模为受限优化问题，即在效用损失有界增长的约束下优化遗忘目标。这被证明等价于对遗忘目标进行单边梯度手术。为了解决梯度手术的计算成本问题，文章提出了一种隐式梯度手术方法，通过一次反向传播近似解决该受限优化问题，从而实现高效的效用保持MU。理论上提供了严格的收敛性分析，实验结果表明该算法在遗忘效率和实用性之间取得了比现有基线更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘（MU）旨在从预训练模型中高效地移除敏感或有害的记忆。其核心挑战在于平衡遗忘效率和实用性保持之间的潜在权衡，即在忘记不需要的信息的同时保持模型的原始性能。现有的多目标优化方法虽然能找到帕累托最优解，但缺乏细粒度的控制，这可能导致遗忘目标未被充分优化，使得模型在遗忘敏感信息方面不够彻底或高效。因此，研究一个能更精确控制遗忘过程并有效保持模型实用性的方法具有重要意义。

Method: 该论文首先将机器遗忘（MU）建模为一个受限优化问题：在效用损失增加有界限的约束下，优化遗忘目标。随后，作者证明了解决这个优化问题等价于对遗忘目标进行单边梯度手术。为了解决由梯度手术带来的额外计算成本，论文提出了一种隐式梯度手术方法。这种方法通过仅一次反向传播来近似求解上述受限优化问题，从而实现高效的效用保持MU。在理论方面，论文对所提出的算法提供了严格的收敛性分析。

Result: 理论上，论文为所提出的算法提供了严格的收敛性分析，证明了其优化特性。在经验方面，通过广泛的实验，结果表明所提出的算法在遗忘效率和实用性保持之间取得了比现有基线方法更好的权衡效果，验证了其在实际应用中的有效性和优越性。

Conclusion: 该论文的主要贡献在于提出了一种高效且能有效保持模型实用性的机器遗忘（MU）方法。通过将MU建模为受限优化问题并引入隐式梯度手术，该方法成功解决了传统多目标方法中遗忘目标欠优化和计算成本高的问题。理论分析和实证结果均表明，所提出的算法在遗忘效率和实用性之间取得了显著的优化权衡，为机器遗忘领域提供了一个新的、更有效的解决方案。代码已开源，以便进一步研究和应用。

Abstract: Machine unlearning (MU) aims to efficiently remove sensitive or harmful
memory from a pre-trained model. The key challenge is to balance the potential
tradeoff between unlearning efficacy and utility preservation, which involves
forgetting undesirable information as defined while maintaining the model's
original performance. One potential way to tackle this problem is to use
multi-objective optimization to jointly optimize both the unlearning and
utility preservation objectives. However, existing multi-objective methods only
guarantee finding a Pareto-optimal solution without fine-grained control, which
causes under-optimization of the unlearning objective. To this end, we first
model MU as a constrained optimization problem, that is, optimizing the
unlearning objective under the constraint of a bounded increase for utility
loss. We then show that solving this optimization problem is equivalent to
unilateral gradient surgery on the unlearning objective. To resolve the
additional computational cost brought by gradient surgery, we propose an
implicit gradient surgery method, which approximates the solution to the
aforementioned constrained optimization problem via only one backpropagation,
thereby achieving efficient utility-preserving MU. Theoretically, we provide a
tight convergence analysis of the algorithm. Empirically, our extensive
experiments show that the proposed algorithm achieves better tradeoff results
than existing baselines. Codes are available at
https://github.com/anseryuer/EUPMU-Efficient-Utility-Preserving-Machine-Unlearning.

</details>


### [145] [Probing Neural Combinatorial Optimization Models](https://arxiv.org/abs/2510.22131)
*Zhiqin Zhang,Yining Ma,Zhiguang Cao,Hoong Chuin Lau*

Main category: cs.LG

TL;DR: 该论文通过探究性任务和新工具CS-Probing来解释神经组合优化（NCO）模型的“黑箱”表征。研究发现NCO模型编码了构建解决方案所需的低级信息和促进更好决策的高级知识，揭示了模型泛化的直接证据并识别了关键的嵌入维度，甚至通过少量代码修改改善了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 神经组合优化（NCO）模型表现出色，但其学习到的模型表征和决策原理仍然是“黑箱”。这阻碍了学术研究和实际部署，因为研究人员和利益相关者需要对NCO模型有更深入的理解。因此，本文旨在解释NCO模型，提供对其内部机制的深入见解。

Method: 本文通过各种探究任务研究NCO模型的表征，迈出了解释NCO模型的关键第一步。此外，我们引入了一种名为“系数显著性探究”（CS-Probing）的新型探究工具，通过检查探究过程中的系数和统计显著性，对NCO表征进行更深入的分析。

Result: 广泛的实验和分析表明，NCO模型编码了构建解决方案所必需的低级信息，同时捕获了促进更好决策的高级知识。使用CS-Probing，我们发现流行的NCO模型对其学习到的表征施加了不同的归纳偏置，揭示了与模型泛化相关的直接证据，并识别了与特定知识相关的关键嵌入维度。这些见解可以转化为实践，例如，通过少量代码修改，我们改进了所分析模型的泛化能力。

Conclusion: 我们的工作代表了首次系统性尝试解释“黑箱”NCO模型，展示了探究作为分析其内部机制的有前景的工具，并为NCO社区揭示了见解。这些见解有可能转化为实践，例如，通过少量代码修改改进了模型的泛化能力。

Abstract: Neural combinatorial optimization (NCO) has achieved remarkable performance,
yet its learned model representations and decision rationale remain a black
box. This impedes both academic research and practical deployment, since
researchers and stakeholders require deeper insights into NCO models. In this
paper, we take the first critical step towards interpreting NCO models by
investigating their representations through various probing tasks. Moreover, we
introduce a novel probing tool named Coefficient Significance Probing
(CS-Probing) to enable deeper analysis of NCO representations by examining the
coefficients and statistical significance during probing. Extensive experiments
and analysis reveal that NCO models encode low-level information essential for
solution construction, while capturing high-level knowledge to facilitate
better decisions. Using CS-Probing, we find that prevalent NCO models impose
varying inductive biases on their learned representations, uncover direct
evidence related to model generalization, and identify key embedding dimensions
associated with specific knowledge. These insights can be potentially
translated into practice, for example, with minor code modifications, we
improve the generalization of the analyzed model. Our work represents a first
systematic attempt to interpret black-box NCO models, showcasing probing as a
promising tool for analyzing their internal mechanisms and revealing insights
for the NCO community. The source code is publicly available.

</details>


### [146] [Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs](https://arxiv.org/abs/2510.22139)
*Jinzhe Liu,Junshu Sun,Shufan Shen,Chenxue Yang,Shuhui Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为神经元特异性掩蔽知识编辑（NMKE）的新型细粒度编辑框架，通过结合神经元层面的归因和动态稀疏掩蔽，解决了现有终身知识编辑方法中错误累积导致编辑准确性和泛化能力下降的问题。NMKE能够实现对大型语言模型（LLMs）过时知识的持续、精确更新，且无需昂贵的完全再训练，并在数千次顺序编辑实验中表现出优于现有方法的编辑成功率和模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）中的知识会随着时间推移而过时，需要进行持续更新。然而，每次更新都进行昂贵的模型完全再训练是不可行的。终身知识编辑旨在无需完全再训练的情况下，对LLMs中的过时知识进行持续和精确的更新。但现有方法在编辑过程中容易积累错误，导致编辑准确性和模型泛化能力逐渐下降，这成为了一个亟待解决的重要研究问题。

Method: 本论文提出神经元特异性掩蔽知识编辑（NMKE）框架，旨在解决现有方法的局限性。NMKE的核心方法包括：1. 神经元功能归因：识别两种关键的知识神经元——在不同提示下始终激活的知识通用神经元和仅对特定提示激活的知识特定神经元。2. 熵引导动态稀疏掩蔽：引入一种基于熵的动态稀疏掩码，用于定位与目标知识相关的特定神经元。这种策略能够以更少的参数修改实现精确的神经元级知识编辑。通过结合这些技术，NMKE能够实现更精细、更准确的知识更新。

Result: 实验结果表明，在经过数千次顺序编辑后，NMKE在终身编辑任务中表现出显著优于现有方法的性能。具体而言，NMKE在保持高编辑成功率方面表现出色，并且能够有效保护模型的通用能力不受损害。这证明了NMKE在解决知识编辑过程中错误积累和能力下降问题上的有效性。

Conclusion: 本研究提出的神经元特异性掩蔽知识编辑（NMKE）框架有效地解决了大型语言模型终身知识编辑中存在的错误累积导致编辑准确性和泛化能力下降的问题。NMKE通过其新颖的神经元层面归因和熵引导动态稀疏掩蔽机制，实现了精确且参数修改量更小的知识更新。实验结果证实了NMKE在保持编辑成功率和模型通用能力方面的优越性，为LLMs的持续知识更新提供了一个高效且稳健的解决方案。未来的工作可以探索NMKE在更复杂知识类型和多模态模型中的应用潜力。

Abstract: Lifelong knowledge editing enables continuous, precise updates to outdated
knowledge in large language models (LLMs) without computationally expensive
full retraining. However, existing methods often accumulate errors throughout
the editing process, causing a gradual decline in both editing accuracy and
generalization. To tackle this problem, we propose Neuron-Specific Masked
Knowledge Editing (NMKE), a novel fine-grained editing framework that combines
neuron-level attribution with dynamic sparse masking. Leveraging neuron
functional attribution, we identify two key types of knowledge neurons, with
knowledge-general neurons activating consistently across prompts and
knowledge-specific neurons activating to specific prompts. NMKE further
introduces an entropy-guided dynamic sparse mask, locating relevant neurons to
the target knowledge. This strategy enables precise neuron-level knowledge
editing with fewer parameter modifications. Experimental results from thousands
of sequential edits demonstrate that NMKE outperforms existing methods in
maintaining high editing success rates and preserving model general
capabilities in lifelong editing.

</details>


### [147] [Power to the Clients: Federated Learning in a Dictatorship Setting](https://arxiv.org/abs/2510.22149)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 本论文引入了联邦学习中一种新型的恶意参与者——“独裁客户端”，这种客户端能够完全抹去其他客户端对服务器模型的贡献，同时保留自己的贡献。论文提出了具体的攻击策略，并系统分析了其对学习过程的影响，包括多个独裁客户端协作、独立或相互背叛的复杂场景，并提供了理论分析和经验验证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）作为一种去中心化的模型训练范式前景广阔，它允许多个客户端在不交换本地数据的情况下协同训练共享模型。然而，FL的去中心化特性也带来了漏洞，恶意客户端可能破坏或操纵训练过程。本研究旨在解决FL中的这一关键漏洞，通过引入“独裁客户端”的概念，深入分析其潜在威胁，这对理解和提升FL的安全性至关重要。

Method: 研究方法主要包括：1. 引入并定义了“独裁客户端”这一新型、明确且可分析的恶意参与者类别。2. 提出了具体的攻击策略，使这些客户端能够完全抹去其他客户端的贡献。3. 系统分析了这些攻击策略对学习过程的影响。4. 探索了涉及多个独裁客户端的复杂场景，包括它们协作、独立行动或形成联盟最终相互背叛的情况。5. 为每种情况提供了对全局模型收敛影响的理论分析。6. 通过在计算机视觉和自然语言处理基准上的经验评估来支持理论算法和复杂场景的发现。

Result: 研究结果表明，“独裁客户端”能够完全抹去其他所有客户端在服务器模型中的贡献，同时保留自己的贡献。论文对多个独裁客户端（包括协作、独立或相互背叛的情况）复杂场景的理论算法和发现，通过在计算机视觉和自然语言处理基准上的经验评估得到了进一步支持。理论分析揭示了这些攻击对全局模型收敛的影响。

Conclusion: 本研究通过引入“独裁客户端”的概念，揭示了联邦学习中一个严重的潜在漏洞。研究成果不仅提供了对这种新型攻击行为的理论分析和具体的攻击策略，还深入探讨了多独裁客户端的复杂交互场景，并得到了经验证据的支持。这些发现对理解和增强联邦学习的鲁棒性与安全性具有重要意义，为未来设计更安全的联邦学习机制提供了基础。

Abstract: Federated learning (FL) has emerged as a promising paradigm for decentralized
model training, enabling multiple clients to collaboratively learn a shared
model without exchanging their local data. However, the decentralized nature of
FL also introduces vulnerabilities, as malicious clients can compromise or
manipulate the training process. In this work, we introduce dictator clients, a
novel, well-defined, and analytically tractable class of malicious participants
capable of entirely erasing the contributions of all other clients from the
server model, while preserving their own. We propose concrete attack strategies
that empower such clients and systematically analyze their effects on the
learning process. Furthermore, we explore complex scenarios involving multiple
dictator clients, including cases where they collaborate, act independently, or
form an alliance in order to ultimately betray one another. For each of these
settings, we provide a theoretical analysis of their impact on the global
model's convergence. Our theoretical algorithms and findings about the complex
scenarios including multiple dictator clients are further supported by
empirical evaluations on both computer vision and natural language processing
benchmarks.

</details>


### [148] [Multi-dataset Joint Pre-training of Emotional EEG Enables Generalizable Affective Computing](https://arxiv.org/abs/2510.22197)
*Qingzhu Zhang,Jiani Zhong,Zongsheng Li,Xinke Shen,Quanying Liu*

Main category: cs.LG

TL;DR: 当任务表示与通用预训练特征存在差异时，任务特定的预训练至关重要。本文提出了一种任务特定的多数据集联合预训练框架，用于跨数据集情感识别，以解决数据集间分布漂移、情感类别定义不一致和受试者间显著变异性等问题。我们引入了跨数据集协方差对齐损失和结合Mamba-like线性注意力通道编码器与时空动态模型的混合编码器。该方法在少样本情感识别中超越了最先进的大规模脑电图模型4.57%的AUROC，在零样本泛化到新数据集时准确率提升了11.92%，并且性能随预训练数据集的增加而提升。


<details>
  <summary>Details</summary>
Motivation: 在脑电图（EEG）领域，当特定任务的表示与通用预训练特征存在显著差异时，现有通用的预训练EEG模型在处理复杂任务（如情感识别）时面临挑战。这主要是因为任务特定特征与泛化性预训练方法之间存在不匹配。具体而言，情感识别任务面临三大难题：1）不同数据集之间存在显著的分布偏移；2）情感类别定义在不同数据集中可能不一致；3）不同受试者之间存在较大的个体差异性。这些问题导致现有模型难以实现跨数据集的鲁棒泛化，使得对大量标签或每受试者校准的需求变得繁重，从而限制了情感计算的实际应用和可扩展性。因此，开发一种能够解决这些挑战并实现更好泛化能力的任务特定预训练框架具有重要的研究意义。

Method: 为解决上述挑战，本文提出了一种任务特定的多数据集联合预训练框架，用于跨数据集情感识别。该框架的核心方法包括：
1. **跨数据集协方差对齐损失（Cross-dataset Covariance Alignment Loss）**：为解决大型数据集间分布漂移问题，该损失函数旨在对齐不同数据集的二阶统计特性。通过对齐协方差矩阵，模型能够在不依赖大量标签或每受试者校准的情况下，实现更稳健的跨数据集泛化能力。
2. **混合编码器（Hybrid Encoder）**：为了捕捉脑电图信号的长期依赖性和复杂动态，模型设计了一个结合了两种机制的混合编码器：
    a. **Mamba-like线性注意力通道编码器**：利用类似Mamba的线性注意力机制，有效处理脑电图数据中的通道间关系和时间序列的长期依赖性。
    b. **时空动态模型**：用于进一步建模脑电图信号中复杂的时空动态特性。
通过这种结合，编码器能够更有效地提取与情感相关的深层特征。
该框架的目标是实现跨数据集情感识别，并且其设计使得模型能够在面对数据集间分布差异、情感类别定义不一致和受试者间变异性时，依然能够保持鲁棒性。

Result: 本文提出的方法取得了显著的性能提升，具体结果如下：
1. **少样本情感识别性能**：在少样本情感识别任务中，我们的方法在AUROC（受试者工作特征曲线下面积）指标上平均超越了最先进的大规模脑电图模型4.57%。这表明该框架在有限样本下具有出色的学习和泛化能力。
2. **零样本泛化能力**：在零样本泛化到新数据集的任务中，我们的方法在准确率上提升了11.92%。这验证了模型在面对完全未见过的数据集时，依然能够保持强大的泛化能力，有效解决了跨数据集分布偏移的问题。
3. **可扩展性**：研究发现，模型的性能随预训练中使用的数据集数量的增加而提升。这一结果表明该框架具有良好的可扩展性，并且能够从更多的数据集中获益。
4. **多数据集联合预训练的优势**：多数据集联合预训练相比于单一数据集训练，实现了8.55%的性能增益。这有力地证明了联合利用多个数据集进行预训练的有效性，尤其是在处理复杂的跨数据集任务时。

Conclusion: 本文为任务特定的预训练提供了一个可扩展的框架，并突出了其在可泛化情感计算中的优势。通过引入跨数据集协方差对齐损失和混合编码器，该框架成功解决了跨数据集情感识别中存在的巨大数据集间分布偏移、情感类别定义不一致和受试者间显著变异性等核心问题。实验结果表明，该方法在少样本情感识别和零样本泛化方面均显著优于现有最先进模型，且性能随预训练数据集的增加而提高，验证了其可扩展性和多数据集联合预训练的优越性。这项工作为未来在复杂脑电图任务中实现更通用和鲁棒的特征学习奠定了基础，促进了情感计算领域的进步。项目代码已在GitHub上开源（https://github.com/ncclab-sustech/mdJPT_nips2025）。

Abstract: Task-specific pre-training is essential when task representations diverge
from generic pre-training features. Existing task-general pre-training EEG
models struggle with complex tasks like emotion recognition due to mismatches
between task-specific features and broad pre-training approaches. This work
aims to develop a task-specific multi-dataset joint pre-training framework for
cross-dataset emotion recognition, tackling problems of large inter-dataset
distribution shifts, inconsistent emotion category definitions, and substantial
inter-subject variability. We introduce a cross-dataset covariance alignment
loss to align second-order statistical properties across datasets, enabling
robust generalization without the need for extensive labels or per-subject
calibration. To capture the long-term dependency and complex dynamics of EEG,
we propose a hybrid encoder combining a Mamba-like linear attention channel
encoder and a spatiotemporal dynamics model. Our method outperforms
state-of-the-art large-scale EEG models by an average of 4.57% in AUROC for
few-shot emotion recognition and 11.92% in accuracy for zero-shot
generalization to a new dataset. Performance scales with the increase of
datasets used in pre-training. Multi-dataset joint pre-training achieves a
performance gain of 8.55% over single-dataset training. This work provides a
scalable framework for task-specific pre-training and highlights its benefit in
generalizable affective computing. Our code is available at
https://github.com/ncclab-sustech/mdJPT_nips2025.

</details>


### [149] [The Lossy Horizon: Error-Bounded Predictive Coding for Lossy Text Compression (Episode I)](https://arxiv.org/abs/2510.22207)
*Nnamdi Aghanya,Jun Li,Kewei Wang*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在有损压缩领域具有潜力。本文提出了一种名为错误有界预测编码（EPC）的有损文本编解码器，它利用掩码语言模型（MLM）作为解压缩器。EPC通过仅在模型预测不正确时存储基于排名的最小修正来有效利用模型的固有知识，从而在显著降低比特率的同时提供卓越的重建保真度，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在无损压缩方面表现出色，能够作为强大的概率模型实现近乎最优的性能。然而，LLMs在有损压缩领域的应用仍有待深入探索。在有损压缩中，目标是在重建保真度和更高压缩比之间进行权衡。当前研究的动机在于探索如何利用LLMs的预测能力，在有损文本压缩中实现高效的压缩率和可控的失真度，以克服传统方法的局限性，并充分发挥LLMs作为强大概率模型的潜力。

Method: 本文引入了错误有界预测编码（EPC），这是一种利用掩码语言模型（MLM）作为解压缩器的有损文本编解码器。EPC的核心方法不是存储原始令牌的子集，而是允许模型预测被掩盖的内容。只有当模型的首选预测不正确时，EPC才会存储基于排名的最小修正。这种机制创建了一个残差信道，从而实现了连续的码率-失真控制。为了评估EPC的性能，研究将其与一个更简单的基线方法——预测掩码（PM）以及一个基于变换的方法——带有残差补丁的向量量化（VQ+RE）进行了比较。评估包括精确的比特量化和码率-失真分析。

Result: 通过精确的比特量化和码率-失真分析评估，结果表明错误有界预测编码（EPC）始终优于预测掩码（PM）基线方法。EPC通过更有效地利用模型的内在知识，在显著降低比特率的情况下提供了卓越的重建保真度。这表明EPC在有损文本压缩方面具有显著的性能优势，能够在保证高质量重建的同时实现更高的压缩效率。

Conclusion: 本文提出的错误有界预测编码（EPC）为利用大型语言模型（LLMs）进行有损文本压缩提供了一种新颖且高效的方法。EPC通过智能地利用掩码语言模型（MLM）的预测能力，仅在必要时存储最小修正，从而实现了卓越的码率-失真性能。其优于基线方法的表现证明了该方法在实际应用中的潜力，为未来基于LLMs的有损数据压缩研究开辟了新途径。未来的工作可以进一步探索不同模型架构和修正策略，以优化性能并扩展其应用范围。

Abstract: Large Language Models (LLMs) can achieve near-optimal lossless compression by
acting as powerful probability models. We investigate their use in the lossy
domain, where reconstruction fidelity is traded for higher compression ratios.
This paper introduces Error-Bounded Predictive Coding (EPC), a lossy text codec
that leverages a Masked Language Model (MLM) as a decompressor. Instead of
storing a subset of original tokens, EPC allows the model to predict masked
content and stores minimal, rank-based corrections only when the model's top
prediction is incorrect. This creates a residual channel that offers continuous
rate-distortion control. We compare EPC to a simpler Predictive Masking (PM)
baseline and a transform-based Vector Quantisation with a Residual Patch
(VQ+RE) approach. Through an evaluation that includes precise bit accounting
and rate-distortion analysis, we demonstrate that EPC consistently dominates
PM, offering superior fidelity at a significantly lower bit rate by more
efficiently utilising the model's intrinsic knowledge.

</details>


### [150] [Simplifying Knowledge Transfer in Pretrained Models](https://arxiv.org/abs/2510.22208)
*Siddharth Jain,Shyamgopal Karthik,Vineet Gandhi*

Main category: cs.LG

TL;DR: 本文提出了一种利用公共模型库进行模型改进的方法，通过数据分区策略使预训练模型扮演学生或教师的角色进行双向知识迁移，从而显著提升了图像分类、语义分割和视频显著性预测等任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习领域中预训练模型无处不在，但在各种设计选择上不同的模型表现出截然不同的泛化行为，导致一个模型能够掌握另一个模型无法获得的独特数据特定洞察。这表明通过有效的方法可以进一步挖掘和利用这些模型的潜力，以实现更强的性能。本文旨在解决如何利用这些模型的多样性，通过知识迁移来弥补模型的不足并提升其整体性能。

Method: 本文引入了一种数据分区策略，在该策略中，预训练模型自主地扮演寻求知识的学生或传授知识的教师角色。该方法利用大型公共模型库作为辅助的模型改进来源。实验中，模型之间进行了双向知识迁移，并且该方法还扩展到多模型之间的知识迁移。

Result: 在图像分类任务中，通过与ViT-T进行双向知识迁移，ViT-B的性能提升了约1.4%。在语义分割任务中，该方法提升了所有评估指标，实现了骨干架构内部和跨架构的知识迁移。在视频显著性预测任务中，该方法取得了新的最先进（state-of-the-art）成果。此外，将该方法扩展到多模型之间的知识迁移，所有参与模型的性能都得到了显著提升。

Conclusion: 本文提出的利用公共模型库进行预训练模型之间双向知识迁移的方法，在多个任务上展现出显著的有效性，包括图像分类、语义分割和视频显著性预测，并取得了显著的性能提升甚至新的最先进成果。该方法能够有效利用不同模型的独特洞察力，通过协同学习弥补模型之间的不足，为模型改进提供了一个新的方向。未来工作可以探索更复杂的知识迁移策略和在更多元任务上的应用。

Abstract: Pretrained models are ubiquitous in the current deep learning landscape,
offering strong results on a broad range of tasks. Recent works have shown that
models differing in various design choices exhibit categorically diverse
generalization behavior, resulting in one model grasping distinct data-specific
insights unavailable to the other. In this paper, we propose to leverage large
publicly available model repositories as an auxiliary source of model
improvements. We introduce a data partitioning strategy where pretrained models
autonomously adopt either the role of a student, seeking knowledge, or that of
a teacher, imparting knowledge. Experiments across various tasks demonstrate
the effectiveness of our proposed approach. In image classification, we
improved the performance of ViT-B by approximately 1.4% through bidirectional
knowledge transfer with ViT-T. For semantic segmentation, our method boosted
all evaluation metrics by enabling knowledge transfer both within and across
backbone architectures. In video saliency prediction, our approach achieved a
new state-of-the-art. We further extend our approach to knowledge transfer
between multiple models, leading to considerable performance improvements for
all model participants.

</details>


### [151] [Visual Model Selection using Feature Importance Clusters in Fairness-Performance Similarity Optimized Space](https://arxiv.org/abs/2510.22209)
*Sofoklis Kitharidis,Cor J. Veenman,Thomas Bäck,Niki van Stein*

Main category: cs.LG

TL;DR: 该论文提出一个交互式框架，利用弱监督度量学习和聚类技术，帮助利益相关者在多个平衡预测公平性和性能的机器学习模型中进行选择。该框架通过构建反映公平性和性能相似性的Mahalanobis距离，并根据特征重要性对模型进行聚类，使用户能够理解模型之间的权衡和驱动预测的特征，从而做出明智的决策。


<details>
  <summary>Details</summary>
Motivation: 在算法决策中，公平机器学习方法通常会产生多个模型，这些模型在预测公平性和性能之间以不同程度进行权衡。这种多样性给利益相关者带来了挑战，他们必须选择一个符合其特定要求和价值观的模型。因此，研究的动机是解决如何帮助利益相关者有效导航、解释和选择最适合其需求的模型组合，从而促进知情的决策过程。

Method: 该方法提出一个交互式框架。核心技术包括：首先，利用弱监督度量学习（weakly supervised metric learning）来学习一个Mahalanobis距离。这个距离旨在反映模型在公平性和性能结果方面的相似性，有效地根据利益相关者相关的标准来构建模型的特征重要性空间。其次，将聚类技术（具体是k-means算法）应用于模型经过转换的特征重要性表示上，以便将具有相似预测行为和公平性特征的模型进行分组。通过这种方式，用户可以探索不同类别的模型，理解它们在公平性-性能平衡以及驱动其预测的特征上的差异。

Result: 该方法的结果是促进了明智的决策。通过该框架，用户能够理解模型不仅在公平性-性能平衡方面有所不同，而且在驱动其预测的特征方面也存在差异。这使得利益相关者能够更好地导航和解释不同模型间的权衡，从而选择最符合其特定要求和价值观的模型。尽管摘要中未提供具体的量化指标或性能对比，但其核心价值在于提升了决策过程的透明度和有效性。

Conclusion: 该论文的结论是，所提出的交互式框架通过帮助用户理解模型在公平性-性能平衡以及驱动其预测的特征方面的差异，从而促进了知情的决策。这解决了在多样的公平机器学习模型中进行选择的挑战，为利益相关者提供了一个有效的工具来导航和解释模型间的权衡。摘要中未提及具体的局限性或未来的工作方向。

Abstract: In the context of algorithmic decision-making, fair machine learning methods
often yield multiple models that balance predictive fairness and performance in
varying degrees. This diversity introduces a challenge for stakeholders who
must select a model that aligns with their specific requirements and values. To
address this, we propose an interactive framework that assists in navigating
and interpreting the trade-offs across a portfolio of models. Our approach
leverages weakly supervised metric learning to learn a Mahalanobis distance
that reflects similarity in fairness and performance outcomes, effectively
structuring the feature importance space of the models according to
stakeholder-relevant criteria. We then apply clustering technique (k-means) to
group models based on their transformed representations of feature importances,
allowing users to explore clusters of models with similar predictive behaviors
and fairness characteristics. This facilitates informed decision-making by
helping users understand how models differ not only in their
fairness-performance balance but also in the features that drive their
predictions.

</details>


### [152] [When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs](https://arxiv.org/abs/2510.22228)
*Keyu Wang,Tian Lyu,Guinan Su,Jonas Geiping,Lu Yin,Marco Canini,Shiwei Liu*

Main category: cs.LG

TL;DR: 本文研究发现，层剪枝，即使只剪掉一两层，也会严重损害大型语言模型（LLMs）的长链推理能力和测试时扩展性，导致在长推理基准测试上性能急剧下降，而传统微调方法无法恢复。这揭示了层剪枝应用于推理密集型LLMs的根本风险，并呼吁重新思考剪枝策略。


<details>
  <summary>Details</summary>
Motivation: 层剪枝是提高大型语言模型（LLMs）效率的常用技术。尽管现有方法在通用知识任务上表现出强大的性能保持能力，但它们对长链推理（一种脆弱但至关重要的能力）的影响在很大程度上尚未被探索。长链推理对于LLMs解决复杂问题至关重要，而现代LLMs通过在推理时分配更多计算来实现的测试时扩展是其强大推理能力的关键机制。因此，研究层剪枝对这一关键能力的影响，对于理解和改进LLMs的效率与鲁棒性至关重要。

Method: 本研究通过“测试时扩展”的视角来研究层剪枝对长链推理能力的影响。具体而言，研究人员通过进行大量的实验来评估剪枝不同层数的LLMs在各种任务上的表现。这包括知识密集型任务、浅层推理任务以及长推理基准任务。实验旨在量化剪枝对模型在推理时分配更多计算以增强推理能力这一机制的影响，从而揭示层剪枝如何影响LLMs的深层推理机制。

Result: 实验结果表明，即使只剪掉一到两层，也会严重损害测试时扩展能力，导致在长推理基准测试上性能急剧崩溃，即使在知识密集型和浅层推理任务上性能保持稳定。此外，研究发现标准的监督微调补救措施在测试时扩展能力恶化后无法恢复。通过深入分析，研究人员确定了测试时扩展能力这种脆弱性背后的机制。

Conclusion: 研究结果揭示了层剪枝应用于推理密集型LLMs的根本风险，即它可能严重损害长链推理和测试时扩展能力。这些发现呼吁重新思考现有的层剪枝策略，并为开发能够保持推理鲁棒性的方法提供了新的见解。研究强调了在优化LLMs效率时，需要特别关注其对关键推理能力的影响。项目代码已开源。

Abstract: Layer pruning has emerged as a widely adopted technique for improving the
efficiency of large language models (LLMs). Although existing methods
demonstrate strong performance retention on general knowledge tasks, their
effect on long-chain reasoning, a more brittle yet crucial capability, remains
largely unexplored. In this work, we study the impact of layer pruning on
long-chain reasoning through the lens of test-time scaling, a key mechanism in
modern LLMs that enables strong reasoning capacity by allocating more
computation at inference time. With extensive experiments, we demonstrate that
pruning even one or two layers can severely impair test-time scaling, with
performance collapsing drastically on long reasoning benchmarks even when
performance on knowledge-intensive and shallow reasoning tasks remains stable.
Furthermore, we find that standard supervised fine-tuning remedies fail to
recover test-time scaling once it has deteriorated. Through in-depth analyses,
we identify the mechanisms underlying this fragility of test-time scaling and
highlight the fundamental risks of applying layer pruning to
reasoning-intensive LLMs. These findings call for a rethinking of layer pruning
strategies and provide insights for developing methods that preserve the
robustness of reasoning. We open-source the codebase in
\href{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}{https://github.com/keyu-wang-2002/Layer-Pruning-Harms-Inference-Scaling}.

</details>


### [153] [LUNA: Efficient and Topology-Agnostic Foundation Model for EEG Signal Analysis](https://arxiv.org/abs/2510.22257)
*Berkay Döner,Thorir Mar Ingolfsson,Luca Benini,Yawei Li*

Main category: cs.LG

TL;DR: LUNA是一个自监督基础模型，它通过将不同脑电图（EEG）电极布局压缩到一个固定大小、与拓扑无关的潜在空间中，解决了EEG数据拓扑异构性问题。LUNA能够线性扩展通道数量，并在多个下游任务中取得了最先进的性能，显著降低了计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）能够非侵入性地观察人类大脑活动，但由于每个公共EEG数据定义了自己独特的电极布局（即拓扑异构性），限制了大型EEG模型的泛化能力。这种异构性阻碍了构建能够处理各种电极配置的通用模型，使得研究和应用难以在大规模数据集上进行。

Method: LUNA（Latent Unified Network Architecture）通过学习查询（learned queries）和交叉注意力（cross-attention）机制，将多通道EEG数据压缩到一个固定大小、与拓扑无关的潜在空间。下游的Transformer模块仅在此潜在表示上通过分块时间自注意力（patch-wise temporal self-attention）进行操作，从而将计算与电极数量解耦，实现了线性而不是平方级的通道数扩展。模型在TUEG和Siena数据集（超过21,000小时的原始EEG数据，涵盖多种蒙太奇）上使用掩蔽补丁重建（masked-patch reconstruction）目标进行预训练。

Result: LUNA在四个下游任务（异常检测、伪迹抑制、慢波分类和情绪识别）中表现出色。它在多个基准测试中展现出极具竞争力的性能，并在TUAR和TUSL上取得了最先进的成果，例如在TUAR上达到了0.921的AUROC。此外，LUNA显著降低了计算成本，FLOPs减少了300倍，GPU内存使用量减少了10倍，并且这些性能提升在所有评估的电极配置中均保持一致。

Conclusion: LUNA模型通过有效整合不同的EEG电极几何结构，克服了拓扑异构性这一关键挑战，为构建可泛化的大规模EEG模型提供了新的范式。其在保持高性能的同时显著提升了计算效率，并在多个EEG任务上实现了SOTA结果，这对于推动EEG分析和应用具有重要意义。未来工作可能涉及探索LUNA在更广泛的神经科学应用中的潜力以及进一步优化其潜在空间表示。

Abstract: Electroencephalography (EEG) offers a non-invasive lens into human brain
activity, but building large-scale models is hampered by topological
heterogeneity: each public EEG data defines its own electrode layout, limiting
generalization. We introduce LUNA (Latent Unified Network Architecture), a
self-supervised foundation model that reconciles disparate electrode geometries
while scaling linearly -- not quadratically -- with channel count. LUNA
compresses multi-channel EEG into a fixed-size, topology-agnostic latent space
via learned queries and cross-attention. Downstream transformer blocks then
operate exclusively on this latent representation using patch-wise temporal
self-attention, decoupling computation from electrode count. Pre-trained on
TUEG and Siena (over 21,000 hours of raw EEG across diverse montages) using a
masked-patch reconstruction objective, LUNA transfers effectively to four
downstream tasks: abnormality detection, artifact rejection, slowing
classification, and emotion recognition. It demonstrates highly competitive
performance across several benchmarks, achieving state-of-the-art results on
TUAR and TUSL, e.g., 0.921 AUROC on TUAR, while reducing FLOPs by 300x and
trimming GPU memory use by up to 10x. Critically, these gains are consistent
across all evaluated electrode configurations. Code is available at
https://github.com/pulp-bio/BioFoundation

</details>


### [154] [A Multi-level Analysis of Factors Associated with Student Performance: A Machine Learning Approach to the SAEB Microdata](https://arxiv.org/abs/2510.22266)
*Rodrigo Tertulino,Ricardo Almeida*

Main category: cs.LG

TL;DR: 本研究利用多层次机器学习方法，整合学生、教师、学校和校长四类数据，对巴西9年级和高中生学业水平进行分类。随机森林模型表现最佳（准确率90.2%，AUC 96.7%）。可解释人工智能（XAI）分析表明，学校的平均社会经济水平是学业表现最重要的预测因子，揭示了系统性因素比个体因素更具影响力。研究强调学业表现是一个系统性现象，与学校生态系统紧密相关，为制定促进教育公平的政策提供了数据驱动的可解释工具。


<details>
  <summary>Details</summary>
Motivation: 在巴西，确定影响基础教育学生表现的因素是制定有效公共政策的核心挑战。理解这些因素对于解决教育不平等、提升整体教育质量至关重要。本研究旨在通过深入分析，为政策制定者提供基于证据的洞察，以期促进教育公平并提高学生的学业成就。

Method: 本研究采用多层次机器学习方法，利用巴西基础教育评估系统（SAEB）的微观数据，对9年级和高中学生的学业水平进行分类。模型独特地整合了四类数据源：学生社会经济特征、教师专业档案、学校指标以及校长管理档案。研究比较了四种集成算法的性能，以确定最优模型。此外，为超越单纯的预测，本研究应用了可解释人工智能（XAI）中的SHAP方法，以揭示模型预测背后的关键影响因素和其作用机制。

Result: 通过对四种集成算法的比较分析，本研究证实了随机森林模型的卓越性能。该模型在学生学业水平分类任务中，实现了90.2%的准确率和96.7%的曲线下面积（AUC）。更重要的是，应用可解释人工智能（XAI）的SHAP方法揭示了学校的平均社会经济水平是学业表现中最具主导性的预测因子。这一发现有力地证明了系统性因素，而非孤立的个体特征，对学生学业成就具有更大的影响。

Conclusion: 本研究的核心结论是，学生的学业表现是一个系统性现象，与学校的生态系统（包括其社会经济背景、教师质量、管理水平等）存在深刻的联系。研究结果提供了一个数据驱动且可解释的工具，为旨在通过解决学校间差异来促进教育公平的政策制定提供了重要依据。通过识别关键的系统性影响因素，本研究为制定更具针对性和有效性的教育政策提供了宝贵的洞察。

Abstract: Identifying the factors that influence student performance in basic education
is a central challenge for formulating effective public policies in Brazil.
This study introduces a multi-level machine learning approach to classify the
proficiency of 9th-grade and high school students using microdata from the
System of Assessment of Basic Education (SAEB). Our model uniquely integrates
four data sources: student socioeconomic characteristics, teacher professional
profiles, school indicators, and director management profiles. A comparative
analysis of four ensemble algorithms confirmed the superiority of a Random
Forest model, which achieved 90.2% accuracy and an Area Under the Curve (AUC)
of 96.7%. To move beyond prediction, we applied Explainable AI (XAI) using
SHAP, which revealed that the school's average socioeconomic level is the most
dominant predictor, demonstrating that systemic factors have a greater impact
than individual characteristics in isolation. The primary conclusion is that
academic performance is a systemic phenomenon deeply tied to the school's
ecosystem. This study provides a data-driven, interpretable tool to inform
policies aimed at promoting educational equity by addressing disparities
between schools.

</details>


### [155] [Machine Learning Enabled Early Warning System For Financial Distress Using Real-Time Digital Signals](https://arxiv.org/abs/2510.22287)
*Laxmi pant,Syed Ali Reza,Md Khalilor Rahman,MD Saifur Rahman,Shamima Sharmin,Md Fazlul Huq Mithu,Kazi Nehal Hasnain,Adnan Farabi,Mahamuda khanom,Raisul Kabir*

Main category: cs.LG

TL;DR: 本研究开发了一个基于机器学习的早期预警系统，利用实时数字和宏观经济信号，近乎实时地识别家庭金融困境。该系统整合了社会经济属性、宏观经济指标和数字经济衡量标准，并通过特征工程（包括滞后变量、波动性度量和交互项）显著提高了预测精度。结果表明，数字经济特征增强了预测能力，通胀波动和ICT需求是关键预测因子。该系统具有可扩展性，能为政策制定者和实践者提供可操作的见解，以加强家庭韧性并指导干预策略。


<details>
  <summary>Details</summary>
Motivation: 全球和国内经济环境日益增长的不稳定性增加了家庭层面面临金融困境的风险。传统的计量经济模型依赖于延迟和聚合数据，限制了其有效性，无法提供近乎实时的预警。因此，迫切需要一种能够利用实时数据，及时、准确地识别家庭金融困境的预警系统，以支持政策制定者采取前瞻性干预措施，增强家庭金融韧性。

Method: 本研究构建了一个基于机器学习的早期预警系统。数据集包含对750个家庭在13个月内的三轮监测数据。特征包括社会经济属性、宏观经济指标（如GDP增长、通货膨胀、外汇波动）和数字经济衡量标准（包括ICT需求和市场波动性）。通过数据预处理和特征工程，引入了滞后变量、波动性度量和交互项以捕捉金融稳定性的渐进和突发变化。基准分类器包括逻辑回归和决策树，并与先进的集成模型（如随机森林、XGBoost和LightGBM）进行了对比分析。

Result: 研究结果表明，经过工程处理的数字经济特征显著增强了预测精度。该系统在二分类困境检测和多分类严重程度分类方面都表现出可靠的性能。基于SHAP的解释性分析确定通胀波动和ICT需求是关键的预测因子。此外，该框架设计为可伸缩部署，适用于国家机构和低带宽的区域办事处，确保了政策制定者和实践者的可访问性。

Conclusion: 本研究通过以透明和可解释的方式实施机器学习，证明了提供近乎实时家庭金融困境早期预警的可行性和影响力。该系统能够提供可操作的见解，从而加强家庭韧性并指导先发制人的干预策略。这项工作为政策制定者和实践者提供了重要工具，以应对日益复杂的经济环境带来的金融风险，并为未来的相关研究奠定了基础。

Abstract: The growing instability of both global and domestic economic environments has
increased the risk of financial distress at the household level. However,
traditional econometric models often rely on delayed and aggregated data,
limiting their effectiveness. This study introduces a machine learning-based
early warning system that utilizes real-time digital and macroeconomic signals
to identify financial distress in near real-time. Using a panel dataset of 750
households tracked over three monitoring rounds spanning 13 months, the
framework combines socioeconomic attributes, macroeconomic indicators (such as
GDP growth, inflation, and foreign exchange fluctuations), and digital economy
measures (including ICT demand and market volatility). Through data
preprocessing and feature engineering, we introduce lagged variables,
volatility measures, and interaction terms to capture both gradual and sudden
changes in financial stability. We benchmark baseline classifiers, such as
logistic regression and decision trees, against advanced ensemble models
including random forests, XGBoost, and LightGBM. Our results indicate that the
engineered features from the digital economy significantly enhance predictive
accuracy. The system performs reliably for both binary distress detection and
multi-class severity classification, with SHAP-based explanations identifying
inflation volatility and ICT demand as key predictors. Crucially, the framework
is designed for scalable deployment in national agencies and low-bandwidth
regional offices, ensuring it is accessible for policymakers and practitioners.
By implementing machine learning in a transparent and interpretable manner,
this study demonstrates the feasibility and impact of providing near-real-time
early warnings of financial distress. This offers actionable insights that can
strengthen household resilience and guide preemptive intervention strategies.

</details>


### [156] [AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals](https://arxiv.org/abs/2510.22301)
*Yujie Xiao,Gongzhen Tang,Wenhui Liu,Jun Li,Guangkun Nie,Zhuoran Kan,Deyun Zhang,Qinghao Zhao,Shenda Hong*

Main category: cs.LG

TL;DR: 本研究利用迁移学习，在一个大型预训练心电图基础模型（ECGFounder）上，通过在斯坦福MC-MED数据集上微调，并生成2000多万个标准化心电图片段，实现了33种实验室指标的强预测性能（AUC>0.65），59种指标的中等性能（AUC在0.55到0.65之间），以及16种指标的有限性能（AUC<0.55）。这提供了一种高效的AI驱动解决方案，验证了实时、无创估算实验室指标的可行性。


<details>
  <summary>Details</summary>
Motivation: 及时获取实验室数值对临床决策至关重要，但现有方法依赖于侵入性静脉采血且存在固有延迟。心电图（ECG）作为一种非侵入性且广泛可用的信号，为快速实验室估算提供了有前景的模式。尽管深度学习在从心电图中提取潜在血液学特征方面取得了进展，但现有模型受限于低信噪比、显著的个体间差异、有限的数据多样性和次优的泛化能力，尤其是在适应低导联可穿戴设备时。因此，需要开发一种能够克服这些限制的有效方法，以实现实时、无创的实验室指标估算。

Method: 本研究进行了一项探索性研究，利用迁移学习方法对ECGFounder这一大型预训练心电图基础模型进行微调。微调过程基于斯坦福大学的急诊科多模态临床监测（MC-MED）数据集。为了增强模型对细微生化相关性的敏感性，研究生成了一个包含2000多万个标准化十秒心电图片段的语料库。

Result: 在内部验证中，该模型对33种实验室指标表现出强大的预测性能（曲线下面积AUC高于0.65），对59种指标表现出中等性能（AUC在0.55到0.65之间），对16种指标表现出有限性能（AUC低于0.55）。

Conclusion: 本研究提供了一种高效的人工智能驱动解决方案，并确立了实时、无创估算实验室数值的可行性范围。这表明利用大型预训练模型和迁移学习可以有效克服现有方法的局限性，为临床决策提供更快速、更便捷的辅助手段。未来的工作可能包括进一步提升对表现有限指标的预测准确性，并进行外部验证以评估模型的泛化能力。

Abstract: Timely access to laboratory values is critical for clinical decision-making,
yet current approaches rely on invasive venous sampling and are intrinsically
delayed. Electrocardiography (ECG), as a non-invasive and widely available
signal, offers a promising modality for rapid laboratory estimation. Recent
progress in deep learning has enabled the extraction of latent hematological
signatures from ECGs. However, existing models are constrained by low
signal-to-noise ratios, substantial inter-individual variability, limited data
diversity, and suboptimal generalization, especially when adapted to low-lead
wearable devices. In this work, we conduct an exploratory study leveraging
transfer learning to fine-tune ECGFounder, a large-scale pre-trained ECG
foundation model, on the Multimodal Clinical Monitoring in the Emergency
Department (MC-MED) dataset from Stanford. We generated a corpus of more than
20 million standardized ten-second ECG segments to enhance sensitivity to
subtle biochemical correlates. On internal validation, the model demonstrated
strong predictive performance (area under the curve above 0.65) for
thirty-three laboratory indicators, moderate performance (between 0.55 and
0.65) for fifty-nine indicators, and limited performance (below 0.55) for
sixteen indicators. This study provides an efficient artificial-intelligence
driven solution and establishes the feasibility scope for real-time,
non-invasive estimation of laboratory values.

</details>


### [157] [LacMaterial: Large Language Models as Analogical Chemists for Materials Discovery](https://arxiv.org/abs/2510.22312)
*Hongyu Guo*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）能够通过跨领域类比检索和域内类比模板构建，生成新颖的电池材料，超越传统方法和基线，展现了LLMs作为科学创新中可解释的专家级假设生成器的潜力。


<details>
  <summary>Details</summary>
Motivation: 类比推理是科学发现的基础，但人类的洞察力往往受限于领域专业知识和表面偏见，这限制了在学科内部和跨学科应用更深层次、结构驱动的类比。大型语言模型（LLMs）凭借其在海量跨领域数据上的训练，为科学中的类比推理提供了一个有前景但尚未充分探索的工具，以克服这些人类限制。

Method: 本研究展示了LLMs生成新型电池材料的两种明确的类比推理策略：1. 检索跨领域类比和类比引导的范例，以引导探索超越传统的掺杂剂替代。2. 从少量标记示例中构建域内类比模板，以指导有针对性的利用。

Result: 这些明确的类比推理策略产生了超出既定组成空间之外的候选材料，并且表现优于标准提示基线。研究结果表明LLMs是可解释的、专家级的假设生成器。

Conclusion: 本研究将LLMs定位为可解释的、专家级的假设生成器，它们利用类比驱动的泛化能力进行科学创新。这为未来在更广泛的科学发现任务中应用LLMs的类比推理能力开辟了道路，尽管论文中未明确提及限制和未来工作，但通常这类研究会面临数据稀疏性、模型可解释性深度和实际实验验证的挑战。

Abstract: Analogical reasoning, the transfer of relational structures across contexts
(e.g., planet is to sun as electron is to nucleus), is fundamental to
scientific discovery. Yet human insight is often constrained by domain
expertise and surface-level biases, limiting access to deeper, structure-driven
analogies both within and across disciplines. Large language models (LLMs),
trained on vast cross-domain data, present a promising yet underexplored tool
for analogical reasoning in science. Here, we demonstrate that LLMs can
generate novel battery materials by (1) retrieving cross-domain analogs and
analogy-guided exemplars to steer exploration beyond conventional dopant
substitutions, and (2) constructing in-domain analogical templates from few
labeled examples to guide targeted exploitation. These explicit analogical
reasoning strategies yield candidates outside established compositional spaces
and outperform standard prompting baselines. Our findings position LLMs as
interpretable, expert-like hypothesis generators that leverage analogy-driven
generalization for scientific innovation.

</details>


### [158] [Monitoring State Transitions in Markovian Systems with Sampling Cost](https://arxiv.org/abs/2510.22327)
*Kumar Saurav,Ness B. Shroff,Yingbin Liang*

Main category: cs.LG

TL;DR: 该论文研究了在存在固定查询成本的情况下，如何有效地监测状态随时间变化的节点。它提出并分析了一种贪婪策略，即当预期预测损失低于查询成本时进行预测，否则进行查询。虽然该策略在一般情况下可能表现不佳，但在常见条件下（如独立同分布的转移概率）其性能接近最优。此外，针对未知转移概率的情况，论文还提出了一种基于投影随机梯度下降（PSGD）的学习变体，该变体在计算效率提高的同时，实现了良好的预测-查询权衡。


<details>
  <summary>Details</summary>
Motivation: 节点状态随时间变化，监测器需要持续追踪其状态。然而，每次状态查询都需要固定的成本，这使得频繁查询变得不经济。监测器可以通过时间序列预测方法（包括时间序列基础模型）来预测状态，仅在预测不确定性高时才进行查询。关键挑战在于查询决策会影响预测准确性，因此，何时查询是一个非平凡的问题。研究高效的状态跟踪策略，以在查询成本和预测损失之间取得平衡，具有重要的实际意义。

Method: 论文在马尔可夫设置下分析了一种贪婪策略：当预期预测损失低于查询成本时进行预测，否则进行查询。最优策略（OPT）被定义为最小化查询成本和预测损失时间平均和的状态依赖阈值策略。对于未知转移概率的情况，论文进一步提出了一种基于投影随机梯度下降（PSGD）的学习变体，以改进贪婪策略。

Result: 分析结果表明，在一般情况下，贪婪策略是次优的，并且可能具有无界的竞争比。然而，在常见的条件下，例如独立同分布的转移概率，贪婪策略的性能接近最优（OPT）。针对未知转移概率的情况，提出的基于PSGD的学习变体与OPT相比，实现了有利的预测-查询权衡，并提高了计算效率。

Conclusion: 该论文对带有固定查询成本的节点状态监测问题提出了新的见解。通过分析贪婪策略在马尔可夫设置下的表现，揭示了其在特定条件下的有效性和在一般情况下的局限性。为解决未知转移概率的实际问题，提出的PSGD学习变体提供了一种计算高效且性能优越的解决方案。未来的工作可以探索更复杂的预测模型、非马尔可夫设置或多节点场景下的监测策略。

Abstract: We consider a node-monitor pair, where the node's state varies with time. The
monitor needs to track the node's state at all times; however, there is a fixed
cost for each state query. So the monitor may instead predict the state using
time-series forecasting methods, including time-series foundation models
(TSFMs), and query only when prediction uncertainty is high. Since query
decisions influence prediction accuracy, determining when to query is
nontrivial. A natural approach is a greedy policy that predicts when the
expected prediction loss is below the query cost and queries otherwise. We
analyze this policy in a Markovian setting, where the optimal (OPT) strategy is
a state-dependent threshold policy minimizing the time-averaged sum of query
cost and prediction losses. We show that, in general, the greedy policy is
suboptimal and can have an unbounded competitive ratio, but under common
conditions such as identically distributed transition probabilities, it
performs close to OPT. For the case of unknown transition probabilities, we
further propose a projected stochastic gradient descent (PSGD)-based learning
variant of the greedy policy, which achieves a favorable predict-query tradeoff
with improved computational efficiency compared to OPT.

</details>


### [159] [Uncertainty quantification in model discovery by distilling interpretable material constitutive models from Gaussian process posteriors](https://arxiv.org/abs/2510.22345)
*David Anton,Henning Wessels,Ulrich Römer,Alexander Henkes,Jorge-Humberto Urrea-Quintero*

Main category: cs.LG

TL;DR: 本文提出了一种四步部分贝叶斯框架，用于本构模型发现中的不确定性量化。该框架无需对材料参数进行先验选择，可处理非线性本构模型，通过高斯过程增强数据、归一化流近似参数分布、分布蒸馏和Sobol'敏感性分析，以应对噪声和捕捉复杂的参数分布，最终实现稀疏且可解释的模型。该框架在各向同性、各向异异性实验数据以及线性和非线性模型库上均表现出有效性。


<details>
  <summary>Details</summary>
Motivation: 本构模型发现任务涉及识别合适的模型结构并同时推断其材料参数。然而，用于模型发现的数据在力学测试中测量，不可避免地受到噪声影响，从而引入不确定性。现有用于模型发现中不确定性量化的方法存在局限性，例如需要选择材料参数的先验、仅限于模型库的线性系数，或者推断的参数概率分布的灵活性有限。因此，研究的动机是开发一种更通用、更灵活的方法，以克服这些限制，尤其是在处理非线性模型和避免先验选择的情况下。

Method: 本文提出了一种四步部分贝叶斯框架。首先，使用高斯过程增强可用的应力-变形数据。其次，通过归一化流近似参数分布，从而能够捕获复杂的联合分布。第三，通过将参数引起的应力-变形函数分布与高斯过程后验匹配来蒸馏参数分布。第四，进行Sobol'敏感性分析以获得稀疏且可解释的模型。该框架的显著特点是无需为材料参数选择先验，并且允许发现非线性本构模型。

Result: 该框架在多种场景下展示了其能力，包括处理各向同性及各向异性实验数据，以及线性和非线性模型库。这表明该方法能够有效地在不同材料特性和模型复杂性下进行本构模型发现和不确定性量化。

Conclusion: 本文提出的四步部分贝叶斯框架成功解决了本构模型发现中不确定性量化的现有挑战。通过结合高斯过程、归一化流、分布蒸馏和Sobol'敏感性分析，该方法克服了对材料参数先验选择的依赖性，并能够处理复杂的非线性本构模型，同时生成稀疏且可解释的模型。这为在存在噪声数据的情况下，对材料本构关系进行更稳健、更准确的建模提供了有力的工具。

Abstract: Constitutive model discovery refers to the task of identifying an appropriate
model structure, usually from a predefined model library, while simultaneously
inferring its material parameters. The data used for model discovery are
measured in mechanical tests and are thus inevitably affected by noise which,
in turn, induces uncertainties. Previously proposed methods for uncertainty
quantification in model discovery either require the selection of a prior for
the material parameters, are restricted to the linear coefficients of the model
library or are limited in the flexibility of the inferred parameter probability
distribution. We therefore propose a four-step partially Bayesian framework for
uncertainty quantification in model discovery that does not require prior
selection for the material parameters and also allows for the discovery of
non-linear constitutive models: First, we augment the available
stress-deformation data with a Gaussian process. Second, we approximate the
parameter distribution by a normalizing flow, which allows for capturing
complex joint distributions. Third, we distill the parameter distribution by
matching the distribution of stress-deformation functions induced by the
parameters with the Gaussian process posterior. Fourth, we perform a Sobol'
sensitivity analysis to obtain a sparse and interpretable model. We demonstrate
the capability of our framework for both isotropic and anisotropic experimental
data as well as linear and non-linear model libraries.

</details>


### [160] [Mapping Faithful Reasoning in Language Models](https://arxiv.org/abs/2510.22362)
*Jiazheng Li,Andreas Damianou,J Rosser,José Luis Redondo García,Konstantina Palla*

Main category: cs.LG

TL;DR: 该论文引入了Concept Walk，一个在激活空间中追踪模型内部立场如何根据概念方向演变的新框架。通过将推理步骤投射到从对比数据中学习到的概念方向上，Concept Walk能够区分装饰性推理（在“简单”案例中被迅速忽略）和忠实推理（在“困难”案例中引起内部激活的持续性转变）。这项方法论贡献旨在帮助识别何时可以信任推理轨迹，避免误导实践者。


<details>
  <summary>Details</summary>
Motivation: 链式思考（CoT）轨迹有望为推理语言模型提供透明度，但先前的研究表明，它们并非总是内部计算的忠实反映。这给监督带来了挑战：实践者可能会错误地将表面推理解读为真实的内部计算。这种不忠实性可能导致对模型行为的误解，并阻碍有效的模型调试和改进，因此需要一种更可靠的方法来评估推理的真实性。

Method: 研究引入了Concept Walk框架，它通过在激活空间而非表面文本中操作来追踪模型内部立场在推理过程中如何根据特定概念方向演变。该方法将每个推理步骤投影到从对比数据中学习到的概念方向上。作为案例研究，该方法应用于Qwen 3-4B模型，在安全领域中进行了测试，以观察推理轨迹是否真实地影响结果。

Result: 研究发现，在“简单”的案例中，即使扰动了CoT，模型也能迅速忽略这些扰动，这表明CoT在该情况下表现为装饰性推理。然而，在“困难”的案例中，扰动会导致模型内部激活的持续性转变，这与忠实推理是一致的。这表明Concept Walk能够有效区分模型何时进行表面推理，何时进行深层且忠实的推理。

Conclusion: 该论文的主要贡献是方法论上的：Concept Walk提供了一个通过概念特定的内部动态重新审视推理忠实性的新视角。它有助于实践者识别何时可以信任模型的推理轨迹，以及何时这些轨迹可能具有误导性。这项工作为提高语言模型推理透明度和可信度提供了重要工具，未来工作可以进一步探索其在不同模型和任务上的适用性，并研究如何将忠实性度量集成到模型开发流程中。

Abstract: Chain-of-thought (CoT) traces promise transparency for reasoning language
models, but prior work shows they are not always faithful reflections of
internal computation. This raises challenges for oversight: practitioners may
misinterpret decorative reasoning as genuine. We introduce Concept Walk, a
general framework for tracing how a model's internal stance evolves with
respect to a concept direction during reasoning. Unlike surface text, Concept
Walk operates in activation space, projecting each reasoning step onto the
concept direction learned from contrastive data. This allows us to observe
whether reasoning traces shape outcomes or are discarded. As a case study, we
apply Concept Walk to the domain of Safety using Qwen 3-4B. We find that in
'easy' cases, perturbed CoTs are quickly ignored, indicating decorative
reasoning, whereas in 'hard' cases, perturbations induce sustained shifts in
internal activations, consistent with faithful reasoning. The contribution is
methodological: Concept Walk provides a lens to re-examine faithfulness through
concept-specific internal dynamics, helping identify when reasoning traces can
be trusted and when they risk misleading practitioners.

</details>


### [161] [Bias Begins with Data: The FairGround Corpus for Robust and Reproducible Research on Algorithmic Fairness](https://arxiv.org/abs/2510.22363)
*Jan Simson,Alessandro Fabris,Cosima Fröhner,Frauke Kreuter,Christoph Kern*

Main category: cs.LG

TL;DR: 该研究提出了 FairGround：一个统一的框架、数据语料库和 Python 包，旨在解决公平机器学习领域中现有数据集选择狭窄、处理不一致和缺乏多样性等问题，以促进可复现的研究和批判性数据研究。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统越来越多地应用于高风险决策领域，确保其输出的公平性已成为核心挑战。公平机器学习研究的核心在于用于调查偏差和开发缓解策略的数据集。然而，现有的大部分工作都依赖于狭窄的数据集选择，这些数据集通常是随意选择的、处理不一致且缺乏多样性，从而损害了结果的泛化性和可复现性。

Method: 为解决现有局限性，该研究提出了 FairGround：一个统一的框架、数据语料库和 Python 包。FairGround 目前包含 44 个表格数据集，每个数据集都附有丰富的公平性相关元数据。配套的 Python 包对数据集的加载、预处理、转换和拆分进行了标准化，从而简化了实验工作流程。

Result: 通过提供一个多样化且文档完善的数据集语料库以及强大的工具，FairGround 能够开发出更公平、更可靠和更可复现的机器学习模型。

Conclusion: FairGround 通过提供统一的框架、丰富的数据语料库和标准化的工具，显著推动了公平机器学习领域的可复现研究和批判性数据研究。所有资源均公开可用，以支持开放和协作的研究，从而促进更公平、更可靠和更可复现的机器学习模型的开发。

Abstract: As machine learning (ML) systems are increasingly adopted in high-stakes
decision-making domains, ensuring fairness in their outputs has become a
central challenge. At the core of fair ML research are the datasets used to
investigate bias and develop mitigation strategies. Yet, much of the existing
work relies on a narrow selection of datasets--often arbitrarily chosen,
inconsistently processed, and lacking in diversity--undermining the
generalizability and reproducibility of results.
  To address these limitations, we present FairGround: a unified framework,
data corpus, and Python package aimed at advancing reproducible research and
critical data studies in fair ML classification. FairGround currently comprises
44 tabular datasets, each annotated with rich fairness-relevant metadata. Our
accompanying Python package standardizes dataset loading, preprocessing,
transformation, and splitting, streamlining experimental workflows. By
providing a diverse and well-documented dataset corpus along with robust
tooling, FairGround enables the development of fairer, more reliable, and more
reproducible ML models. All resources are publicly available to support open
and collaborative research.

</details>


### [162] [Label Smoothing Improves Gradient Ascent in LLM Unlearning](https://arxiv.org/abs/2510.22376)
*Zirui Pang,Hao Zheng,Zhijie Deng,Ling Li,Zixin Zhong,Jiaheng Wei*

Main category: cs.LG

TL;DR: 本文提出了一种名为平滑梯度上升（SGA）的新型大语言模型遗忘方法，旨在解决传统梯度上升（GA）在遗忘有害/不希望的知识时出现的不稳定性及模型效用急剧下降的问题。SGA通过将遗忘数据与多个构建的正常数据结合，引入可调平滑率，从而在遗忘的同时更好地保持模型效用。理论上，我们提供了最优平滑率选择的指导；实验证明，SGA在多个基准测试中持续优于GA，并在关键指标上达到了所有基线方法中的前两名。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）遗忘是一个新兴且前景广阔的研究方向，其目标是以较低成本使模型忘记有害或不希望的知识，同时尽可能地保留模型的原有效用。然而，现有的最直接方法——针对遗忘数据执行梯度上升（GA）——存在严重的不稳定性问题。GA驱动模型更新朝着发散的方向发展，这往往导致模型效用急剧下降，严重影响了模型在遗忘后的实用性。因此，开发一种更稳定、更有效且能更好保留模型效用的遗忘方法具有重要的研究意义。

Method: 为了解决传统梯度上升（GA）在LLM遗忘中的不稳定性问题，我们提出了平滑梯度上升（SGA）方法。SGA的核心思想是将遗忘数据与通过可调平滑率结合的多个构建的正常数据进行联合学习。直观地，这种方法将GA的学习范围从仅仅依赖遗忘数据扩展到同时学习遗忘数据和正常数据。通过这种联合学习机制，SGA能够实现更稳定的遗忘过程，并更好地保留模型的原有效用。在理论层面，我们为如何选择最优平滑率提供了指导，确保了方法的有效性。在实验设置上，我们没有在摘要中找到具体细节，但评估是在三个基准数据集上进行的。

Result: 我们在三个不同的基准数据集上对SGA进行了实证评估：TOFU、Harry Potter 和 MUSE-NEWS。实验结果一致表明，SGA在所有评估指标上均持续优于原始的梯度上升（GA）方法。此外，SGA在几个关键指标上表现出色，达到了所有基线方法中的前两名。这充分证明了SGA在提高遗忘稳定性、避免模型效用急剧下降方面的有效性和优越性。

Conclusion: 本文提出的平滑梯度上升（SGA）方法有效地解决了大语言模型遗忘中梯度上升（GA）固有的不稳定性及其导致的模型效用急剧下降问题。通过将遗忘数据与正常数据结合进行平滑学习，SGA实现了更稳定的遗忘过程，并显著提高了模型效用的保留。理论指导和实证结果均证实了SGA的优越性，使其成为LLM遗忘领域的一个重要进展。摘要中未明确提及具体的局限性或未来工作，但SGA的成功为未来开发更稳定、更高效的LLM遗忘技术奠定了基础。

Abstract: LLM unlearning has emerged as a promising approach, aiming to enable models
to forget hazardous/undesired knowledge at low cost while preserving as much
model utility as possible. Among existing techniques, the most straightforward
method is performing Gradient Ascent (GA) w.r.t. the forget data, thereby
forcing the model to unlearn the forget dataset. However, GA suffers from
severe instability, as it drives updates in a divergent direction, often
resulting in drastically degraded model utility. To address this issue, we
propose Smoothed Gradient Ascent (SGA). SGA combines the forget data with
multiple constructed normal data through a tunable smoothing rate. Intuitively,
this extends GA from learning solely on the forget data to jointly learning
across both forget and normal data, enabling more stable unlearning while
better preserving model utility. Theoretically, we provide the theoretical
guidance on the selection of the optimal smoothing rate. Empirically, we
evaluate SGA on three benchmarks: TOFU, Harry Potter, and MUSE-NEWS.
Experimental results demonstrate that SGA consistently outperforms the original
Gradient Ascent (GA) method across all metrics and achieves top-2 performance
among all baseline methods on several key metrics.

</details>


### [163] [Knowledge-guided Continual Learning for Behavioral Analytics Systems](https://arxiv.org/abs/2510.22405)
*Yasas Senarath,Hemant Purohit*

Main category: cs.LG

TL;DR: 由于数据漂移和灾难性遗忘，在线平台上的用户行为模型性能会下降。该研究提出了一种新颖的基于增强的持续学习方法，通过整合外部知识到基于回放的框架中，以克服固定大小缓冲区的限制。实验证明，该增强方法在处理异常行为分类任务时，优于基线的回放方法。


<details>
  <summary>Details</summary>
Motivation: 在线平台上的用户行为不断演变，从有益信息到仇恨言论，这反映了现实世界的变化。用于捕捉这些内容的模型会因为数据漂移而随着时间推移出现性能下降，导致行为分析系统失效。然而，使用新数据对模型进行微调可能会导致灾难性遗忘，损害模型性能。尽管基于回放的持续学习方法可以通过维护一个重要训练实例缓冲区来最小化遗忘，但其主要限制在于缓冲区大小固定。利用外部知识库进行数据增强是克服这一限制的潜在方法。

Method: 本文提出了一种新颖的基于增强的方法，将外部知识整合到基于回放的持续学习框架中。这种方法旨在通过数据增强来克服基于回放方法中固定大小缓冲区的主要限制，利用外部知识库来丰富训练数据。研究评估了多种策略，并使用了三个来自先前研究的、与异常行为分类相关的数据集进行实验，以评估外部知识在持续学习中的整合效果和性能。

Result: 研究通过评估多种策略，并使用了三个与异常行为分类相关的先前数据集，成功评估了外部知识在持续学习中的整合。结果明确表明，数据增强有助于所提出的方法，使其性能优于基线基于回放的持续学习方法，证明了外部知识在处理数据漂移和灾难性遗忘方面的有效性。

Conclusion: 本研究提出了一种有效的基于增强的持续学习方法，通过将外部知识整合到基于回放的框架中，成功解决了在线用户行为模型中因数据漂移导致的性能下降和灾难性遗忘问题。该方法克服了传统基于回放方法中固定缓冲区大小的限制，通过数据增强显著提升了模型性能。实验结果证实了其优越性，为处理演变的用户行为数据提供了新的视角和改进方案，对于提高行为分析系统的有效性具有重要意义。

Abstract: User behavior on online platforms is evolving, reflecting real-world changes
in how people post, whether it's helpful messages or hate speech. Models that
learn to capture this content can experience a decrease in performance over
time due to data drift, which can lead to ineffective behavioral analytics
systems. However, fine-tuning such a model over time with new data can be
detrimental due to catastrophic forgetting. Replay-based approaches in
continual learning offer a simple yet efficient method to update such models,
minimizing forgetting by maintaining a buffer of important training instances
from past learned tasks. However, the main limitation of this approach is the
fixed size of the buffer. External knowledge bases can be utilized to overcome
this limitation through data augmentation. We propose a novel
augmentation-based approach to incorporate external knowledge in the
replay-based continual learning framework. We evaluate several strategies with
three datasets from prior studies related to deviant behavior classification to
assess the integration of external knowledge in continual learning and
demonstrate that augmentation helps outperform baseline replay-based
approaches.

</details>


### [164] [Low-Precision Streaming PCA](https://arxiv.org/abs/2510.22440)
*Sanjoy Dasgupta,Syamantak Kumar,Shourya Pandey,Purnamrita Sarkar*

Main category: cs.LG

TL;DR: 该论文研究了在有限精度下流式主成分分析（PCA）中估计主成分的问题。它建立了实现目标精度的量化分辨率的信息理论下限，并提出了使用Oja算法结合线性和非线性随机量化的低精度方法。研究表明，在温和假设下，批处理版本在两种方案下均能达到接近对数因子的下限，并且在非线性量化设置中实现了几乎与维度无关的量化误差。实证评估验证了理论发现。


<details>
  <summary>Details</summary>
Motivation: 在大规模数据流分析中，资源（如存储和计算）的限制使得高精度计算变得不切实际。流式PCA是一种常用的降维技术，但如何在有限精度下有效地估计主成分是一个重要的挑战。本研究的动机在于探索在资源受限的环境下，如何设计和分析低精度流式PCA算法，以在保持计算效率的同时，实现可接受的估计精度。这对于处理实时、大规模数据集的应用具有重要意义。

Method: 论文研究了在流式设置下，利用Oja算法估计主成分的两种量化变体：线性和非线性随机量化。这些量化变体对权重向量和更新使用了无偏随机量化技术。为了达到更好的性能，提出并分析了一个批处理（batched）版本的算法。该方法在数据分布具有温和矩和谱间隙假设的条件下进行理论分析，以评估其在不同量化方案下的性能。

Result: 研究结果表明，所提出的批处理量化版Oja算法，在线性和非线性随机量化方案下，均能达到信息理论下限，精度仅差对数因子。尤其是在非线性量化设置中，量化误差几乎与数据维度无关（nearly dimension-free），这显著提升了算法在处理高维数据时的实用性。通过对合成数据流的实证评估，证实了理论发现，并展示了所提出的低精度方法在性能上与标准Oja算法非常接近，验证了其有效性和实用性。

Conclusion: 该论文成功地为低精度流式PCA的主成分估计问题提供了一个理论和实践均有依据的解决方案。通过建立量化分辨率的下限，并设计了有效的量化Oja算法，解决了在资源受限环境下进行高效主成分估计的挑战。研究结果表明，低精度方法在保持与标准算法接近的性能的同时，能够显著降低对精度的要求，这对于大规模流数据分析具有深远的实际意义。未来的工作可能包括探索更广泛的数据分布假设，以及在实际应用中进一步优化算法性能。

Abstract: Low-precision streaming PCA estimates the top principal component in a
streaming setting under limited precision. We establish an
information-theoretic lower bound on the quantization resolution required to
achieve a target accuracy for the leading eigenvector. We study Oja's algorithm
for streaming PCA under linear and nonlinear stochastic quantization. The
quantized variants use unbiased stochastic quantization of the weight vector
and the updates. Under mild moment and spectral-gap assumptions on the data
distribution, we show that a batched version achieves the lower bound up to
logarithmic factors under both schemes. This leads to a nearly dimension-free
quantization error in the nonlinear quantization setting. Empirical evaluations
on synthetic streams validate our theoretical findings and demonstrate that our
low-precision methods closely track the performance of standard Oja's
algorithm.

</details>


### [165] [Contextual Tokenization for Graph Inverted Indices](https://arxiv.org/abs/2510.22479)
*Pritish Chakraborty,Indradyumna Roy,Soumen Chakrabarti,Abir De*

Main category: cs.LG

TL;DR: CORGII是一个创新的图索引框架，它将稠密的图表示通过可微分离散化转换为稀疏的二元码，利用学习到的潜在词汇表和倒排索引实现高效的子图同构检索。该框架通过可训练的token权重和token扩展进一步优化，实验证明在准确性和效率之间取得了比现有基线方法更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 从大型图库中检索包含给定查询图的子图同构的图，是许多实际应用中的关键操作。然而，尽管现有的多向量图表示和基于集合对齐与包含的评分方法可以提供准确的子图同构测试，但它们在检索中的应用受到限制，因为它们需要对图库中的所有图进行穷尽性评分，导致效率低下，无法处理大规模数据。

Method: CORGII（用于倒排索引的图上下文表示）是一个图索引框架。它首先采用上下文稠密的图表示。接着，一个可微分的离散化模块将这些稠密表示转换为稀疏的二元码，这些二元码基于一个学习到的潜在词汇表。这种类似文本文档的表示方法使得 CORGII 能够利用经典且高度优化的倒排索引，同时支持软（向量）集合包含得分。为了进一步提升性能，该方法用数据驱动、可训练的影响权重取代了传统上固定的“token”在图上的影响权重（如TFIDF或BM25）。最后，还探索了token扩展技术，以支持对索引进行多重探测，从而实现更平滑的准确性-效率权衡。

Result: 广泛的实验结果表明，与多个基线方法相比，CORGII 在子图同构检索的准确性和效率之间提供了更好的权衡。这表明 CORGII 能够有效地克服现有方法的效率瓶颈，同时保持或提高检索的准确性。

Conclusion: 据作者所知，CORGII 是第一个使用离散token映射到高效倒排列表来索引稠密图表示的方法。其核心贡献在于提供了一种新颖且高效的密集图表示索引范式，解决了传统方法在处理大规模图数据时的效率瓶颈。该方法通过将图转换为类似文本的表示并利用倒排索引，显著提高了子图同构检索的性能，并在准确性和效率之间取得了显著改进，为未来的图检索研究开辟了新方向。

Abstract: Retrieving graphs from a large corpus, that contain a subgraph isomorphic to
a given query graph, is a core operation in many real-world applications. While
recent multi-vector graph representations and scores based on set alignment and
containment can provide accurate subgraph isomorphism tests, their use in
retrieval remains limited by their need to score corpus graphs exhaustively. We
introduce CORGII (Contextual Representation of Graphs for Inverted Indexing), a
graph indexing framework in which, starting with a contextual dense graph
representation, a differentiable discretization module computes sparse binary
codes over a learned latent vocabulary. This text document-like representation
allows us to leverage classic, highly optimized inverted indices, while
supporting soft (vector) set containment scores. Pushing this paradigm further,
we replace the classical, fixed impact weight of a `token' on a graph (such as
TFIDF or BM25) with a data-driven, trainable impact weight. Finally, we explore
token expansion to support multi-probing the index for smoother
accuracy-efficiency tradeoffs. To our knowledge, CORGII is the first indexer of
dense graph representations using discrete tokens mapping to efficient inverted
lists. Extensive experiments show that CORGII provides better trade-offs
between accuracy and efficiency, compared to several baselines.

</details>


### [166] [Scalable Oversight via Partitioned Human Supervision](https://arxiv.org/abs/2510.22500)
*Ren Yin,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 这篇论文提出了一种可扩展的监督框架，通过利用人类专家提供的“互补标签”（指示错误选项的弱信号），解决了评估和训练在超人任务上表现卓越的AI系统时缺乏高质量人工监督的挑战。该框架可以在没有真实标签的情况下，对前沿AI系统进行评估，并能利用这些弱信号训练AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统在广泛任务中接近并超越人类专家表现，获取高质量的人工监督（用于评估和训练）变得越来越困难。论文关注需要多领域深层知识和技能的任务，即“超人任务”。在这种任务中，即使是最优秀的人类专家也只在单一狭窄领域有知识，无法评估高级AI系统的正确性，因此传统的监督方法失效。研究的动机是开发一种新的、可扩展的框架，以克服这一关键瓶颈，实现对前沿AI系统的有效评估和训练。

Method: 论文提出了一种可扩展的监督框架，该框架利用人类专家提供的“互补标签”。互补标签是一种弱信号，表示某个选项是错误的，例如心脏病专家指出“这与心脏病无关”，即使他们无法识别真正的疾病。基于这些互补标签，研究者推导出了一个无偏的Top-1准确率估计器，并量化了匹配普通标签方差所需的互补标签数量。此外，论文还引入了两种估计器，用于结合稀缺的普通标签和丰富的互补标签。研究提供了针对纯互补标签和混合估计器的有限样本偏差保证。在实证方面，通过使用大型语言模型进行了评估，并展示了如何设计一个能够通过这种“分区式人类监督”来提高性能的智能体AI系统。代码已开源。

Result: 实证结果表明，在拥有互补标签的情况下，即使没有真实标签，也能够有效评估大型语言模型的输出。此外，研究还展示了如何利用这些弱信号来训练AI系统：通过设计一个智能体AI系统，使其能够在这种分区式人类监督下表现得更好。这验证了互补标签不仅可以用于评估，还可以用于指导AI系统的训练，提升其性能。

Conclusion: 这篇论文的主要贡献在于提出了一个创新的可扩展监督框架，解决了在超人任务中评估和训练高级AI系统的关键挑战。通过引入和利用“互补标签”，该框架能够在缺乏传统真实标签的情况下，实现对AI系统性能的无偏估计和有效训练。这为前沿AI系统的发展提供了一条新的路径，特别是当获取全面的人类专家知识变得不切实际时。未来的工作可能包括进一步优化互补标签的收集策略，以及探索该框架在更广泛AI应用中的普适性和效率。

Abstract: As artificial intelligence (AI) systems approach and surpass expert human
performance across a broad range of tasks, obtaining high-quality human
supervision for evaluation and training becomes increasingly challenging. Our
focus is on tasks that require deep knowledge and skills of multiple domains.
Unfortunately, even the best human experts are knowledgeable only in a single
narrow area, and will not be able to evaluate the correctness of advanced AI
systems on such superhuman tasks. However, based on their narrow expertise,
humans may provide a weak signal, i.e., a complementary label indicating an
option that is incorrect. For example, a cardiologist could state that "this is
not related to cardiology,'' even if they cannot identify the true disease.
Based on this weak signal, we propose a scalable oversight framework that
enables us to evaluate frontier AI systems without the need to prepare the
ground truth. We derive an unbiased estimator of top-1 accuracy from
complementary labels and quantify how many complementary labels are needed to
match the variance of ordinary labels. We further introduce two estimators to
combine scarce ordinary labels with abundant complementary labels. We provide
finite-sample deviation guarantees for both complementary-only and the mixed
estimators. Empirically, we show that we can evaluate the output of large
language models without the ground truth, if we have complementary labels. We
further show that we can train an AI system with such weak signals: we show how
we can design an agentic AI system automatically that can perform better with
this partitioned human supervision. Our code is available at
https://github.com/R-Yin-217/Scalable-Oversight-via-Human-Partitioned-Supervision.

</details>


### [167] [CANDI: Hybrid Discrete-Continuous Diffusion Models](https://arxiv.org/abs/2510.22510)
*Patrick Pynadath,Jiaxin Shi,Ruqi Zhang*

Main category: cs.LG

TL;DR: 连续扩散模型在图像生成等连续领域表现出色，但直接应用于离散数据时性能不佳。本文引入“token可识别性”框架，分析高斯噪声如何通过“离散身份损坏”和“连续秩降级”两种机制破坏离散数据，揭示了导致性能差距的“时间失调”现象。为解决此问题，提出了CANDI（Continuous ANd DIscrete diffusion）混合框架，它解耦了离散和连续损坏，从而同时学习条件结构和连续几何。实验证明CANDI成功避免了时间失调，并在受控生成和文本生成任务中展现出优于现有方法的性能，为离散空间带来了连续扩散的优势。


<details>
  <summary>Details</summary>
Motivation: 连续扩散模型在图像等连续数据生成方面取得了显著成功，但在离散数据上的直接应用却不如纯离散模型，这反直觉。因为连续扩散能够学习分数函数，从而实现跨多个位置的联合演化。为了理解这种性能差距，本研究旨在深入探究高斯噪声如何影响离散数据，并开发一种能够弥合这一差距的新型扩散框架。

Method: 本文首先引入了“token可识别性”作为分析框架，用以理解高斯噪声如何通过“离散身份损坏”和“连续秩降级”两种机制破坏离散数据。研究发现，这两种机制的严重程度随词汇量大小而不同，导致了一个“时间失调”现象：在离散损坏仍能保留足够条件结构以进行学习的噪声水平下，连续去噪变得微不足道；而在连续去噪有意义的噪声水平下，离散损坏几乎破坏了所有的条件结构。为了解决这一问题，本文提出了CANDI（Continuous ANd DIscrete diffusion）混合扩散框架。CANDI的核心思想是解耦离散和连续的损坏过程，从而能够同时学习离散的条件结构和连续的几何信息。

Result: 实验结果首先证实了“时间失调”现象的存在。接着，研究证明所提出的CANDI框架成功地避免了这种时间失调。在受控生成任务中，CANDI展示了其优势，它能够通过简单的梯度叠加，利用现成的分类器实现基于分类器的引导。在文本生成任务中，CANDI在低NFE（Number of Function Evaluations）下表现优于掩码扩散（masked diffusion），这有力地证明了为离散空间学习连续梯度的价值。

Conclusion: 本文通过引入token可识别性分析框架，深入理解了连续扩散模型在离散数据上性能不佳的原因，即“时间失调”现象。所提出的CANDI混合扩散框架成功地解决了这一问题，它通过解耦离散和连续的噪声损坏，有效实现了对条件结构和连续几何的同时学习。CANDI的成功应用为离散空间带来了连续扩散的诸多优势，特别是在受控生成中实现了基于分类器的引导，并在文本生成任务中取得了显著改进，证明了为离散数据学习连续梯度的巨大潜力。

Abstract: While continuous diffusion has shown remarkable success in continuous domains
such as image generation, its direct application to discrete data has
underperformed compared to purely discrete formulations. This gap is
counterintuitive, given that continuous diffusion learns score functions that
enable joint evolution across multiple positions. To understand this gap, we
introduce token identifiability as an analytical framework for understanding
how Gaussian noise corrupts discrete data through two mechanisms: discrete
identity corruption and continuous rank degradation. We reveal that these
mechanisms scale differently with vocabulary size, creating a temporal
dissonance: at noise levels where discrete corruption preserves enough
structure for conditional learning, continuous denoising is trivial; at noise
levels where continuous denoising is meaningful, discrete corruption destroys
nearly all conditional structure. To solve this, we propose CANDI (Continuous
ANd DIscrete diffusion), a hybrid framework that decouples discrete and
continuous corruption, enabling simultaneous learning of both conditional
structure and continuous geometry. We empirically validate the temporal
dissonance phenomenon and demonstrate that CANDI successfully avoids it. This
unlocks the benefits of continuous diffusion for discrete spaces: on controlled
generation, CANDI enables classifier-based guidance with off-the-shelf
classifiers through simple gradient addition; on text generation, CANDI
outperforms masked diffusion at low NFE, demonstrating the value of learning
continuous gradients for discrete spaces.

</details>


### [168] [Transitive RL: Value Learning via Divide and Conquer](https://arxiv.org/abs/2510.22512)
*Seohong Park,Aditya Oberai,Pranav Atreya,Sergey Levine*

Main category: cs.LG

TL;DR: Transitive Reinforcement Learning (TRL) 是一种基于分治范式的新型价值学习算法，专为离线目标条件强化学习 (GCRL) 设计。TRL 将 GCRL 中的三角不等式结构转化为实用的分治价值更新规则，与传统时序差分 (TD) 方法相比，它能显著减少偏差累积，递归次数从 $O(T)$ 降至 $O(	ext{log } T)$；与蒙特卡洛 (MC) 方法相比，它通过动态规划减少了高方差问题。实验证明，TRL 在高难度、长周期基准任务中表现优于现有的离线 GCRL 算法。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决离线目标条件强化学习 (GCRL) 中的核心问题：在离线数据集上学习一个策略，使其能够以最少的步骤从任何初始状态到达任何目标状态。现有的价值学习范式，如时序差分 (TD) 方法和蒙特卡洛 (MC) 方法，在处理这类问题时存在显著局限。TD 方法容易出现偏差累积，尤其是在处理长轨迹时，其所需的递归次数与轨迹长度 $T$ 成正比 ($O(T)$)，导致误差随着时间步的增加而累积。而蒙特卡洛方法虽然能够避免偏差，但由于其依赖于完整的轨迹采样，导致价值估计的方差很高，尤其是在复杂环境中表现不佳。因此，亟需一种新的价值学习算法，能够有效克服这些挑战，实现对离线 GCRL 问题的高效且鲁棒的求解。

Method: 本文提出了Transitive Reinforcement Learning (TRL) 算法，这是一种基于分治范式的新型价值学习方法，专门针对离线目标条件强化学习 (GCRL) 问题。TRL 的核心思想是将 GCRL 中固有的三角不等式结构转化为一个实用的分治价值更新规则。具体来说，TRL 能够将一个长度为 $T$ 的轨迹的处理递归次数从时序差分 (TD) 学习的 $O(T)$ 显著减少到 $O(	ext{log } T)$。这种分治策略的引入，使得算法在处理长轨迹时，能够有效地缓解偏差累积问题。此外，与蒙特卡洛方法不同，TRL 采用了动态规划的思想进行价值更新，这有助于降低价值估计的方差。TRL 的设计充分利用了离线数据的特性，通过对数据的有效划分和重组，实现对价值函数的学习，从而在保证效率的同时，提升了学习的稳定性。

Result: 实验结果表明，TRL 算法在极具挑战性的长周期基准任务中取得了最佳性能，显著超越了之前提出的所有离线目标条件强化学习 (GCRL) 算法。这证明了 TRL 在处理复杂、长期依赖问题方面的优越性。具体而言，TRL 能够有效应对传统方法中存在的偏差累积和高方差问题，从而在实际应用中展现出更强的鲁棒性和效率，使得智能体能够在更长的规划范围内实现精确的目标达成。这些结果证实了 TRL 所采用的分治价值更新规则在提升离线 GCRL 性能方面的显著优势。

Conclusion: 本文提出的 Transitive Reinforcement Learning (TRL) 算法，通过引入分治范式，为离线目标条件强化学习 (GCRL) 提供了一个创新且高效的解决方案。TRL 成功地将 GCRL 中的三角不等式结构转化为实用的价值更新规则，显著减少了传统时序差分 (TD) 方法的偏差累积问题，并且通过动态规划有效降低了蒙特卡洛 (MC) 方法的高方差问题。其核心优势在于，处理长度为 $T$ 的轨迹所需的递归次数从 $O(T)$ 降至 $O(	ext{log } T)$，极大地提高了长轨迹学习的效率和稳定性。实验结果有力地证明了 TRL 在应对高难度、长周期基准任务时的卓越性能，超越了以往的离线 GCRL 算法。TRL 的成功为未来离线强化学习，特别是需要长期规划和目标达成的复杂应用，提供了新的研究方向和可能性。

Abstract: In this work, we present Transitive Reinforcement Learning (TRL), a new value
learning algorithm based on a divide-and-conquer paradigm. TRL is designed for
offline goal-conditioned reinforcement learning (GCRL) problems, where the aim
is to find a policy that can reach any state from any other state in the
smallest number of steps. TRL converts a triangle inequality structure present
in GCRL into a practical divide-and-conquer value update rule. This has several
advantages compared to alternative value learning paradigms. Compared to
temporal difference (TD) methods, TRL suffers less from bias accumulation, as
in principle it only requires $O(\log T)$ recursions (as opposed to $O(T)$ in
TD learning) to handle a length-$T$ trajectory. Unlike Monte Carlo methods, TRL
suffers less from high variance as it performs dynamic programming.
Experimentally, we show that TRL achieves the best performance in highly
challenging, long-horizon benchmark tasks compared to previous offline GCRL
algorithms.

</details>


### [169] [A Scalable Global Optimization Algorithm For Constrained Clustering](https://arxiv.org/abs/2510.22519)
*Pedro Chumpitaz-Flores,My Duong,Cristobal Heredia,Kaixun Hua*

Main category: cs.LG

TL;DR: 该论文提出了Sample-Driven Constrained Group-Based Branch-and-Bound (SDC-GBB)框架，这是一个可分解的分支定界算法，通过将必连样本合并为伪样本并使用几何规则修剪非连样本，解决了大规模受限聚类中NP难的全局优化问题。SDC-GBB保证了收敛性和全局最优性，并通过组样本拉格朗日分解和几何消除规则实现了高效的上下界计算及并行化，从而将k-Means受限聚类的可扩展性提高了200-1500倍，能够处理多达1,500,000个样本的数据集，且最优性差距小于3%。


<details>
  <summary>Details</summary>
Motivation: 受限聚类利用有限的领域知识来提高聚类性能和可解释性，但在聚类过程中有效地整合成对的“必连”(must-link)和“非连”(cannot-link)约束是一个计算上的巨大挑战。这是一个NP难问题，导致全局优化难以实现。现有的混合整数优化方法通常局限于小规模数据集，极大地限制了它们在实际应用中的效用。因此，开发一种能够在大规模数据集上高效处理这些约束并保证全局最优性的方法，具有重要的研究和实践意义。

Method: 论文提出了Sample-Driven Constrained Group-Based Branch-and-Bound (SDC-GBB)框架。该方法是一个可分解的分支定界(BB)框架，其核心在于将必连样本(must-linked samples)合并为基于质心的伪样本(centroid-based pseudo-samples)，并通过几何规则来修剪非连约束(cannot-link constraints)，同时保留了算法的收敛性并保证了全局最优性。为了实现高效的上下界计算，SDC-GBB集成了组样本拉格朗日分解(grouped-sample Lagrangian decomposition)和几何消除规则(geometric elimination rules)。此外，该算法通过并行化实现了高度可扩展的成对k-Means受限聚类。

Result: 实验结果表明，SDC-GBB方法在处理大规模数据集方面取得了显著突破。它能够处理包含200,000个带有非连约束的样本以及1,500,000个带有必连约束的样本的数据集。与当前最先进的方法在可比较的约束设置下相比，SDC-GBB处理的数据集规模扩大了200至1500倍。在取得这种大规模处理能力的同时，算法的最优性差距(optimality gap)保持在3%以下。此外，通过提供确定性的全局保证，该方法还避免了现成的启发式算法在处理大型数据集时经常遇到的搜索失败问题。

Conclusion: 该论文提出的Sample-Driven Constrained Group-Based Branch-and-Bound (SDC-GBB)框架为大规模受限聚类提供了一个创新的、可扩展且保证全局最优的解决方案。它克服了现有方法在处理大规模数据集时面临的计算复杂性和扩展性限制，显著提升了受限k-Means聚类的应用范围。SDC-GBB通过其独特的分支定界机制、伪样本处理和几何剪枝策略，以及并行化能力，实现了效率和最优性的平衡。这项工作不仅在理论上解决了NP难问题，还在实践中展现了处理百万级别样本的能力，避免了启发式方法常有的搜索失败，为受限聚类领域未来的研究和实际应用奠定了坚实基础。

Abstract: Constrained clustering leverages limited domain knowledge to improve
clustering performance and interpretability, but incorporating pairwise
must-link and cannot-link constraints is an NP-hard challenge, making global
optimization intractable. Existing mixed-integer optimization methods are
confined to small-scale datasets, limiting their utility. We propose
Sample-Driven Constrained Group-Based Branch-and-Bound (SDC-GBB), a
decomposable branch-and-bound (BB) framework that collapses must-linked samples
into centroid-based pseudo-samples and prunes cannot-link through geometric
rules, while preserving convergence and guaranteeing global optimality. By
integrating grouped-sample Lagrangian decomposition and geometric elimination
rules for efficient lower and upper bounds, the algorithm attains highly
scalable pairwise k-Means constrained clustering via parallelism. Experimental
results show that our approach handles datasets with 200,000 samples with
cannot-link constraints and 1,500,000 samples with must-link constraints, which
is 200 - 1500 times larger than the current state-of-the-art under comparable
constraint settings, while reaching an optimality gap of less than 3%. In
providing deterministic global guarantees, our method also avoids the search
failures that off-the-shelf heuristics often encounter on large datasets.

</details>


### [170] [FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning](https://arxiv.org/abs/2510.22543)
*Yuyang Ding,Chi Zhang,Juntao Li,Haibin Lin,Xin Liu,Min Zhang*

Main category: cs.LG

TL;DR: 该研究提出了一种名为FAPO（Flawed-Aware Policy Optimization）的新方法，用于解决强化学习验证奖励（RLVR）中大型语言模型（LLMs）推理轨迹中的“缺陷积极回滚”（即因猜测或跳跃式推理而偶然正确的轨迹）问题。FAPO通过对缺陷积极回滚施加无参数的奖励惩罚，使其在早期优化阶段作为有效捷径，确保稳定的初期收益，并在后期逐步将优化转向可靠的推理。为准确检测缺陷积极回滚，该研究引入了生成式奖励模型（GenRM），其过程级奖励能精确定位推理错误。实验证明，FAPO在广泛领域有效，提高了结果的正确性、过程的可靠性和训练稳定性，且不增加token预算。


<details>
  <summary>Details</summary>
Motivation: 强化学习验证奖励（RLVR）是提升大型语言模型（LLMs）推理能力的一种有前景的范式。在这种范式中，模型探索推理轨迹，并将具有正确答案的回滚作为策略优化的积极信号加以利用。然而，这些回滚可能包含有缺陷的模式，例如猜测答案和跳跃式推理。这些“缺陷积极回滚”与完全正确的推理回滚获得相同的奖励，导致策略模型内化这些不可靠的推理模式，从而限制了LLMs的真正推理能力，并可能产生不稳定的训练过程和不可靠的推理结果。

Method: 本研究首先系统地研究了RL中缺陷积极回滚的现象，发现它们在早期优化阶段能实现能力的快速提升，但在后期通过强化不可靠模式而限制了推理能力。在此洞察基础上，研究提出了缺陷感知策略优化（FAPO）。FAPO为缺陷积极回滚引入了一种无参数的奖励惩罚机制，其设计目的是在预热阶段将缺陷积极回滚作为有用的捷径加以利用，从而确保稳定的早期收益；而在随后的细化阶段，逐步将优化重心转向可靠的推理。为了准确且全面地检测缺陷积极回滚，该研究还引入了一个生成式奖励模型（GenRM），该模型具有过程级奖励，能够精确地定位推理错误。

Result: 实验结果表明，FAPO在广泛的领域中都表现出显著的有效性。它不仅提高了推理结果的正确性，还增强了推理过程的可靠性，并且在训练过程中展现出更好的稳定性。值得注意的是，所有这些改进都是在不增加token预算的前提下实现的，这表明FAPO在计算效率方面也具有优势。

Conclusion: 本研究的FAPO方法通过有效处理强化学习中大型语言模型推理轨迹中的缺陷积极回滚，显著提升了模型的推理能力。FAPO不仅提高了推理结果的准确性，还确保了推理过程的可靠性，并带来了更稳定的训练表现。这些改进在不增加额外计算成本（token预算）的情况下实现，表明其具有高效和实用的价值。FAPO通过在不同训练阶段对缺陷积极回滚采取不同策略（早期作为捷径，后期进行惩罚并转向可靠推理），实现了能力的稳健提升。GenRM的引入也为精确诊断推理错误提供了关键支持。该研究为RLVR范式下LLMs的可靠推理提供了重要方向，未来工作可能包括探索更复杂的缺陷模式检测方法以及将FAPO应用于更多样化的推理任务。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
promising paradigm for enhancing the reasoning capabilities of large language
models (LLMs). In this context, models explore reasoning trajectories and
exploit rollouts with correct answers as positive signals for policy
optimization. However, these rollouts might involve flawed patterns such as
answer-guessing and jump-in-reasoning. Such flawed-positive rollouts are
rewarded identically to fully correct ones, causing policy models to
internalize these unreliable reasoning patterns. In this work, we first conduct
a systematic study of flawed-positive rollouts in RL and find that they enable
rapid capability gains during the early optimization stage, while constraining
reasoning capability later by reinforcing unreliable patterns. Building on
these insights, we propose Flawed-Aware Policy Optimization (FAPO), which
presents a parameter-free reward penalty for flawed-positive rollouts, enabling
the policy to leverage them as useful shortcuts in the warm-up stage, securing
stable early gains, while gradually shifting optimization toward reliable
reasoning in the later refinement stage. To accurately and comprehensively
detect flawed-positive rollouts, we introduce a generative reward model (GenRM)
with a process-level reward that precisely localizes reasoning errors.
Experiments show that FAPO is effective in broad domains, improving outcome
correctness, process reliability, and training stability without increasing the
token budget.

</details>


### [171] [DDTR: Diffusion Denoising Trace Recovery](https://arxiv.org/abs/2510.22553)
*Maximilian Matyash,Avigdor Gal,Arik Senderovich*

Main category: cs.LG

TL;DR: 针对来源于不确定性传感器或机器学习模型等非确定性源的随机已知过程日志，本文提出了一种基于Diffusion Denoising Probabilistic Models (DDPM) 的新型深度学习方法，用于随机轨迹恢复。该方法利用过程知识通过去噪来恢复轨迹，在经验评估中表现出最先进的性能，相比现有方法有高达25%的提升，并且在高噪声水平下具有更高的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统上，过程日志是确定性的。然而，随着技术进步，现在从不确定性传感器或利用摄像头预测活动的机器学习模型等非确定性源捕获过程日志。这导致了包含概率信息的“随机已知日志”的出现。为了可靠地理解管理此类系统的过程，对随机轨迹恢复的需求日益增加。

Method: 本文设计了一种新颖的深度学习方法用于随机轨迹恢复。该方法基于Diffusion Denoising Probabilistic Models (DDPM)。它通过去噪来恢复轨迹，并利用过程知识，这些知识可以通过隐式发现模型或在训练阶段显式注入来获取。

Result: 通过实证评估，该方法展示了最先进的性能。与现有方法相比，性能提升高达25%。此外，在高噪声水平下，该方法表现出更高的鲁棒性。

Conclusion: 本文提出的基于DDPM的深度学习方法为从非确定性源产生的随机已知日志中恢复轨迹提供了一种可靠的手段。该方法在性能上超越了现有技术，并显著提高了在高噪声环境下的鲁棒性，从而有助于更好地理解管理复杂系统的过程。

Abstract: With recent technological advances, process logs, which were traditionally
deterministic in nature, are being captured from non-deterministic sources,
such as uncertain sensors or machine learning models (that predict activities
using cameras). In the presence of stochastically-known logs, logs that contain
probabilistic information, the need for stochastic trace recovery increases, to
offer reliable means of understanding the processes that govern such systems.
We design a novel deep learning approach for stochastic trace recovery, based
on Diffusion Denoising Probabilistic Models (DDPM), which makes use of process
knowledge (either implicitly by discovering a model or explicitly by injecting
process knowledge in the training phase) to recover traces by denoising. We
conduct an empirical evaluation demonstrating state-of-the-art performance with
up to a 25% improvement over existing methods, along with increased robustness
under high noise levels.

</details>


### [172] [Optimal Anytime Algorithms for Online Convex Optimization with Adversarial Constraints](https://arxiv.org/abs/2510.22579)
*Dhruv Sarkar,Abhishek Sinha*

Main category: cs.LG

TL;DR: 本文提出了一种用于在线对抗性凸成本函数和约束学习的“即时可用”在线算法，该算法在不需要预知时间范围T的情况下，实现了最优性能界限，避免了性能不佳的“加倍技巧”。其核心贡献在于使用时变Lyapunov函数来跟踪约束违反，并通过新的分析技术证明了在任意时刻$t 
e 1$下，算法具有$O(\sqrt{t})$的遗憾值和$\tilde{O}(\sqrt{t})$的累积约束违反界限。研究还将结果扩展到动态遗憾设置和乐观设置，并通过在线最短路径问题验证了实用性。


<details>
  <summary>Details</summary>
Motivation: 传统的在线学习算法通常需要预先知道总时间范围$T$，或者依赖于“加倍技巧”来处理未知的$T$，但这种技巧由于多次重启而导致实际性能较差。本文旨在解决这一挑战，提出一种无需预知未来、能在任意中间时刻$t$提供非平凡性能保证的“即时可用”（anytime）在线算法，以更有效地处理同时满足对抗性凸成本和约束的在线优化问题，从而提高算法的实用性。

Method: 本文提出了一种“即时可用”在线算法。其核心技术贡献在于使用“时变Lyapunov函数”来跟踪约束违反情况，这与之前使用针对已知时间范围$T$调整的固定Lyapunov函数的方法形成对比。由于时变Lyapunov函数不再保持如单调性等性质（这是先前证明的基础），因此本文引入了一种新的分析技术来克服由此带来的分析挑战。此外，该算法还被扩展到动态遗憾设置，其界限能够自适应比较器序列的路径长度，无需预知其总长度；以及乐观设置，其性能能够随着累积预测误差优雅地缩放。算法的实用性通过涉及在线最短路径问题的数值实验进行了验证。

Result: 本文提出的算法在任意时刻$t\ge 1$下，实现了$O(\sqrt{t})$的遗憾值和$\tilde{O}(\sqrt{t})$的累积约束违反界限，这些是目前最优的性能界限，并且是在不依赖于“加倍技巧”的情况下获得的。在动态遗憾设置中，算法实现了能够自适应比较器序列路径长度的界限，无需预知其总长度。在乐观设置中，算法的性能能够优雅地随着累积预测误差进行缩放。数值实验，特别是涉及在线最短路径问题的实验，验证了算法的实际效用。

Conclusion: 本文成功开发了一种创新的“即时可用”在线算法，用于处理具有对抗性凸成本和约束的在线学习问题。通过引入时变Lyapunov函数和新的分析技术，该算法不仅避免了传统“加倍技巧”的性能劣势，而且在任意时刻$t$提供了最优的$O(\sqrt{t})$遗憾值和$\tilde{O}(\sqrt{t})$累积约束违反界限。这些成果显著提升了在线凸优化的实用性和鲁棒性，特别是在动态和预测辅助设置下的扩展，进一步拓宽了其应用范围。数值实验证实了其在实际问题中的有效性。

Abstract: We propose an anytime online algorithm for the problem of learning a sequence
of adversarial convex cost functions while approximately satisfying another
sequence of adversarial online convex constraints. A sequential algorithm is
called \emph{anytime} if it provides a non-trivial performance guarantee for
any intermediate timestep $t$ without requiring prior knowledge of the length
of the entire time horizon $T$. Our proposed algorithm achieves optimal
performance bounds without resorting to the standard doubling trick, which has
poor practical performance due to multiple restarts. Our core technical
contribution is the use of time-varying Lyapunov functions to keep track of
constraint violations. This must be contrasted with prior works that used a
fixed Lyapunov function tuned to the known horizon length $T$. The use of
time-varying Lyapunov function poses unique analytical challenges as
properties, such as \emph{monotonicity}, on which the prior proofs rest, no
longer hold. By introducing a new analytical technique, we show that our
algorithm achieves $O(\sqrt{t})$ regret and $\tilde{O}(\sqrt{t})$ cumulative
constraint violation bounds for any $t\geq 1$.
  We extend our results to the dynamic regret setting, achieving bounds that
adapt to the path length of the comparator sequence without prior knowledge of
its total length. We also present an adaptive algorithm in the optimistic
setting, whose performance gracefully scales with the cumulative prediction
error. We demonstrate the practical utility of our algorithm through numerical
experiments involving the online shortest path problem.

</details>


### [173] [Prediction-Powered Semi-Supervised Learning with Online Power Tuning](https://arxiv.org/abs/2510.22586)
*Noa Shoham,Ron Dorfman,Shalev Shaer,Kfir Y. Levy,Yaniv Romano*

Main category: cs.LG

TL;DR: 本文将预测驱动推理（PPI）的核心思想扩展到半监督学习（SSL）的模型训练中，引入了一种新颖的无偏梯度估计器。为解决伪标签质量对SSL性能的关键影响，该方法通过一个插值参数平衡有标签和伪标签数据的贡献，并利用一维在线学习算法与模型参数一同动态调整此参数，在合成和真实数据集上均展现出优于经典SSL基线和离线调整PPI方法的性能。


<details>
  <summary>Details</summary>
Motivation: 半监督学习（SSL）旨在利用大量无标签数据来增强模型性能，但其有效性严重依赖于伪标签的质量。不准确的伪标签可能引入偏差，导致模型次优。因此，研究一个能够有效利用无标签数据同时减轻不可靠伪标签风险的鲁棒SSL方法，是当前面临的一个关键挑战和具有重要意义的研究问题。

Method: 本文将预测驱动推理（PPI）的核心思想从参数估计领域扩展到半监督学习（SSL）的模型训练中。其关键创新在于提出了一种新颖的无偏梯度估计器。为了解决SSL中伪标签质量不佳可能引入偏差的问题，研究引入了一个插值参数来动态平衡有标签数据和伪标签数据的贡献。这个插值参数并非预设或离线调整，而是通过一种一维在线学习算法，与模型参数一起，在训练过程中进行实时（on the fly）的动态调整。该方法的实际优势通过在合成数据集和真实数据集上的实验得到了验证。

Result: 实验结果表明，本文提出的方法在合成数据集和真实数据集上均展现出显著的性能提升，验证了其在实践中的优势。具体而言，与经典的半监督学习基线方法相比，该方法实现了更优的模型性能。此外，与那些采用离线方式调整插值参数的预测驱动推理（PPI）方法相比，本文提出的动态调整策略也带来了性能上的改进。

Conclusion: 本文成功将预测驱动推理（PPI）的核心理念应用于半监督学习（SSL）的模型训练，通过引入无偏梯度估计器和动态调整的插值参数，有效解决了伪标签质量对模型性能的负面影响。该方法在实验中展现出优于现有SSL基线和离线调整PPI方法的性能，证明了其在实际应用中的价值和潜力。它为SSL领域提供了一种新的、更鲁棒的训练范式，特别是在处理无标签数据质量不确定的场景下具有重要意义。

Abstract: Prediction-Powered Inference (PPI) is a recently proposed statistical
inference technique for parameter estimation that leverages pseudo-labels on
both labeled and unlabeled data to construct an unbiased, low-variance
estimator. In this work, we extend its core idea to semi-supervised learning
(SSL) for model training, introducing a novel unbiased gradient estimator. This
extension addresses a key challenge in SSL: while unlabeled data can improve
model performance, its benefit heavily depends on the quality of pseudo-labels.
Inaccurate pseudo-labels can introduce bias, leading to suboptimal models.To
balance the contributions of labeled and pseudo-labeled data, we utilize an
interpolation parameter and tune it on the fly, alongside the model parameters,
using a one-dimensional online learning algorithm. We verify the practical
advantage of our approach through experiments on both synthetic and real
datasets, demonstrating improved performance over classic SSL baselines and PPI
methods that tune the interpolation parameter offline.

</details>


### [174] [A roadmap for curvature-based geometric data analysis and learning](https://arxiv.org/abs/2510.22599)
*Yasharth Yadav,Kelin Xia*

Main category: cs.LG

TL;DR: 这篇论文首次全面回顾了现有的离散曲率模型，涵盖了它们的数学基础、计算公式和在数据分析与学习中的实际应用。论文从黎曼和度量几何的角度讨论了离散曲率，提出了一个由曲率驱动的数据分析系统管道，并详细比较了不同数据表示的计算算法，最后回顾了曲率在监督和无监督学习中的最新应用。


<details>
  <summary>Details</summary>
Motivation: 几何数据分析和学习是一个快速发展的研究领域，在各种应用中显示出其有效性。曲率作为一种强大且可解释的概念，捕获了内在的几何结构，并支撑着从社区检测到几何深度学习的众多任务。为各种数据表示（如图、单纯复形、立方复形和从流形采样的点云）提出了广泛的离散曲率模型。这些模型不仅提供了数据几何的有效表征，而且构成了几何学习框架中的基本组成部分。然而，目前缺乏对这些模型的全面综述。

Method: 本文对现有离散曲率模型进行了首次全面综述。具体方法包括：1. 涵盖这些模型的数学基础、计算公式和在数据分析与学习中的实际应用。2. 从黎曼几何和度量几何的角度讨论离散曲率。3. 提出了一个由曲率驱动的数据分析系统管道。4. 审查了不同数据表示中相应的计算算法，并提供了详细的比较和见解。5. 回顾了曲率在监督学习和无监督学习中的最新应用。

Result: 本综述提供了对现有离散曲率模型的全面梳理，具体结果包括：1. 阐明了离散曲率模型的数学基础、计算公式和实际应用。2. 从黎曼和度量几何的角度系统地讨论了离散曲率。3. 提出并概述了一个用于曲率驱动数据分析的系统化流程。4. 对不同数据表示形式下的计算算法进行了深入检查，并提供了详细的比较和独到见解。5. 系统回顾了曲率在监督和无监督学习领域中的最新应用，展示了其广泛的实用价值。

Conclusion: 本综述为研究人员提供了一个概念性和实践性的路线图，旨在帮助他们更好地理解离散曲率作为几何理解和学习的基本工具。未来工作可以基于此路线图深入探索离散曲率模型的理论发展和在更广泛应用场景中的潜力。

Abstract: Geometric data analysis and learning has emerged as a distinct and rapidly
developing research area, increasingly recognized for its effectiveness across
diverse applications. At the heart of this field lies curvature, a powerful and
interpretable concept that captures intrinsic geometric structure and underpins
numerous tasks, from community detection to geometric deep learning. A wide
range of discrete curvature models have been proposed for various data
representations, including graphs, simplicial complexes, cubical complexes, and
point clouds sampled from manifolds. These models not only provide efficient
characterizations of data geometry but also constitute essential components in
geometric learning frameworks. In this paper, we present the first
comprehensive review of existing discrete curvature models, covering their
mathematical foundations, computational formulations, and practical
applications in data analysis and learning. In particular, we discuss discrete
curvature from both Riemannian and metric geometry perspectives and propose a
systematic pipeline for curvature-driven data analysis. We further examine the
corresponding computational algorithms across different data representations,
offering detailed comparisons and insights. Finally, we review state-of-the-art
applications of curvature in both supervised and unsupervised learning. This
survey provides a conceptual and practical roadmap for researchers to gain a
better understanding of discrete curvature as a fundamental tool for geometric
understanding and learning.

</details>


### [175] [CLEANet: Robust and Efficient Anomaly Detection in Contaminated Multivariate Time Series](https://arxiv.org/abs/2510.22619)
*Songhan Zhang,Yuanhao Lai,Pengfei Zheng,Boxi Yu,Xiaoying Tang,Qiuai Fu,Pinjia He*

Main category: cs.LG

TL;DR: CLEANet是一种针对工业系统多元时间序列异常检测的鲁棒高效框架，旨在解决训练数据污染和模型推理效率低下的问题。它通过污染弹性训练框架（CRTF）增强了对噪声和隐藏异常的抵抗力，并设计了轻量级共轭MLP以提高计算效率。在五个公共数据集上，CLEANet的F1分数提高了73.04%，运行时间降低了81.28%，并具有强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列（MTS）异常检测对于维护工业系统的可靠性至关重要。然而，实际部署面临两个关键挑战：1. 训练数据污染（噪声和隐藏异常）：现有无监督方法假设训练数据是干净的，但污染会扭曲学习模式并降低检测精度。2. 模型推理效率低下：复杂的深度模型容易过拟合污染数据，并导致高延迟，限制了实际应用。这些问题严重阻碍了MTS异常检测在工业场景中的有效应用。

Method: 我们提出了CLEANet，一个用于污染多元时间序列中鲁棒高效的异常检测框架。CLEANet包含两个主要组件：1. 污染弹性训练框架（CRTF）：该框架通过自适应重建加权策略与聚类引导对比学习相结合，减轻了受损样本的影响，从而增强了模型的鲁棒性。2. 轻量级共轭MLP：为了进一步避免对污染数据过拟合并提高计算效率，我们设计了一个轻量级共轭多层感知器（MLP），它能够解耦时间依赖性和跨特征依赖性。

Result: CLEANet在五个公共数据集上进行了评估，并与十个最先进的基线模型进行了比较。结果显示，CLEANet的F1分数最高提高了73.04%，运行时间最高降低了81.28%。此外，将CRTF集成到三个先进模型中，平均F1分数提升了5.35%，这证实了CRTF强大的泛化能力。

Conclusion: CLEANet提供了一个针对受污染多元时间序列的鲁棒且高效的异常检测解决方案，显著提升了工业系统MTS异常检测的性能和实用性。通过其创新的CRTF和轻量级共轭MLP设计，CLEANet成功解决了训练数据污染和推理效率低下的核心挑战，并在多个数据集上表现出卓越的性能和泛化能力。

Abstract: Multivariate time series (MTS) anomaly detection is essential for maintaining
the reliability of industrial systems, yet real-world deployment is hindered by
two critical challenges: training data contamination (noises and hidden
anomalies) and inefficient model inference. Existing unsupervised methods
assume clean training data, but contamination distorts learned patterns and
degrades detection accuracy. Meanwhile, complex deep models often overfit to
contamination and suffer from high latency, limiting practical use. To address
these challenges, we propose CLEANet, a robust and efficient anomaly detection
framework in contaminated multivariate time series. CLEANet introduces a
Contamination-Resilient Training Framework (CRTF) that mitigates the impact of
corrupted samples through an adaptive reconstruction weighting strategy
combined with clustering-guided contrastive learning, thereby enhancing
robustness. To further avoid overfitting on contaminated data and improve
computational efficiency, we design a lightweight conjugate MLP that
disentangles temporal and cross-feature dependencies. Across five public
datasets, CLEANet achieves up to 73.04% higher F1 and 81.28% lower runtime
compared with ten state-of-the-art baselines. Furthermore, integrating CRTF
into three advanced models yields an average 5.35% F1 gain, confirming its
strong generalizability.

</details>


### [176] [Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections](https://arxiv.org/abs/2510.22655)
*Berken Utku Demirel,Christian Holz*

Main category: cs.LG

TL;DR: 该论文提出了一种无需手工数据增强的自监督学习（SSL）方法，通过使用正交基和过完备帧生成视图来学习表示。该方法利用不同几何偏置的互补性，在不通过强增强人为增加数据多样性的情况下，实现了优于现有SSL方法的性能，尤其在时间序列任务上表现显著，性能提升高达15-20%。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习（SSL）方法严重依赖于精心设计的手工数据增强来生成多样化的视图进行表示学习。然而，设计这些增强需要领域特定的知识，并且隐式地对模型施加了表示不变性，这可能限制模型的泛化能力。因此，研究的动机在于开发一种不依赖于手工增强的新型无监督表示学习方法，以克服这些限制。

Method: 本文提出了一种无监督表示学习方法，该方法通过使用正交基和过完备帧来生成视图，从而取代了传统的数据增强。研究表明，从正交空间和过完备空间学习到的嵌入位于不同的流形上，这些流形由在不同空间中表示样本所引入的几何偏置所塑造。通过联合利用这些不同流形的互补几何特性，该方法能够在不通过强增强人为增加数据多样性的情况下实现卓越的性能。

Result: 该方法在九个数据集上的五项时间序列任务中验证了其有效性。在这些任务中，由于信号特定的特性，数据增强尤其具有挑战性。结果表明，在不依赖于增强引起的多样性的情况下，该方法比现有的自监督方法取得了显著的性能提升，最高可达15-20%。

Conclusion: 该研究提出了一种新颖的无监督表示学习方法，通过使用正交基和过完备帧生成视图，成功地规避了对传统数据增强的依赖。通过利用不同几何偏置所形成的流形互补性，该方法在不引入人工多样性的情况下，在时间序列任务中展现了显著的性能优势。这一成果为自监督学习领域开辟了新的途径，尤其对于数据增强具有挑战性的场景具有重要意义。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for
learning representations without labeled data. Most SSL approaches rely on
strong, well-established, handcrafted data augmentations to generate diverse
views for representation learning. However, designing such augmentations
requires domain-specific knowledge and implicitly imposes representational
invariances on the model, which can limit generalization. In this work, we
propose an unsupervised representation learning method that replaces
augmentations by generating views using orthonormal bases and overcomplete
frames. We show that embeddings learned from orthonormal and overcomplete
spaces reside on distinct manifolds, shaped by the geometric biases introduced
by representing samples in different spaces. By jointly leveraging the
complementary geometry of these distinct manifolds, our approach achieves
superior performance without artificially increasing data diversity through
strong augmentations. We demonstrate the effectiveness of our method on nine
datasets across five temporal sequence tasks, where signal-specific
characteristics make data augmentations particularly challenging. Without
relying on augmentation-induced diversity, our method achieves performance
gains of up to 15--20\% over existing self-supervised approaches. Source code:
https://github.com/eth-siplab/Learning-with-FrameProjections

</details>


### [177] [FlowCritic: Bridging Value Estimation with Flow Matching in Reinforcement Learning](https://arxiv.org/abs/2510.22686)
*Shan Zhong,Shutong Ding,He Diao,Xiangyu Wang,Kah Chan Teh,Bei Peng*

Main category: cs.LG

TL;DR: FlowCritic提出了一种生成式范式，利用流匹配技术在强化学习（RL）中进行价值估计，以克服现有方法（如多评论家集成和分布强化学习）在捕获价值分布信息和表达复杂价值分布方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 可靠的价值估计是强化学习（RL）的基石，它通过评估长期回报和指导策略改进，显著影响收敛速度和最终性能。然而，现有方法如多评论家集成仅组合点估计，未能捕获分布信息；分布强化学习则依赖于离散化或分位数回归，限制了对复杂价值分布的表达能力。这些局限性阻碍了RL算法的性能提升。

Method: 本文提出了一种名为FlowCritic的生成式范式，用于在强化学习中进行价值估计。与传统的确定性价值预测回归方法不同，FlowCritic受到流匹配在生成建模中成功的启发，利用流匹配技术来直接建模价值分布。通过这种方式，FlowCritic能够从学习到的价值分布中生成样本，从而提供更全面的价值估计，超越了仅提供单一估计值的传统方法。

Result: 摘要中未提供具体的实验结果、性能指标或与其他方法的比较数据。然而，根据FlowCritic采用流匹配来建模价值分布并生成样本的方法，预期它将能够提供比现有方法（如多评论家集成和基于离散化或分位数回归的分布强化学习）更具表达能力和更可靠的价值估计，从而更好地捕获复杂的价值不确定性。

Conclusion: FlowCritic引入了一种新颖的生成式范式，通过利用流匹配技术解决了强化学习中价值估计的挑战。它通过直接建模和采样价值分布，旨在克服现有方法在捕获分布信息和表达复杂价值分布方面的局限性。这项工作为强化学习中的价值估计提供了一个更具表现力和可靠性的新途径。由于摘要未提供具体结果、局限性和未来工作，因此无法详细阐述。

Abstract: Reliable value estimation serves as the cornerstone of reinforcement learning
(RL) by evaluating long-term returns and guiding policy improvement,
significantly influencing the convergence speed and final performance. Existing
works improve the reliability of value function estimation via multi-critic
ensembles and distributional RL, yet the former merely combines multi point
estimation without capturing distributional information, whereas the latter
relies on discretization or quantile regression, limiting the expressiveness of
complex value distributions. Inspired by flow matching's success in generative
modeling, we propose a generative paradigm for value estimation, named
FlowCritic. Departing from conventional regression for deterministic value
prediction, FlowCritic leverages flow matching to model value distributions and
generate samples for value estimation.

</details>


### [178] [Identification of Causal Direction under an Arbitrary Number of Latent Confounders](https://arxiv.org/abs/2510.22711)
*Wei Chen,Linjun Peng,Zhiyi Huang,Haoyue Dai,Zhifeng Hao,Ruichu Cai,Kun Zhang*

Main category: cs.LG

TL;DR: 在存在任意数量潜在混杂变量的情况下，本文提出了一种在线性、非高斯模型中，利用观测变量的联合高阶累积量矩阵的秩亏缺陷性质，直接识别两个观测变量之间因果不对称性的方法，该方法无需迭代过程，并具有可识别性保证和实验验证的有效性。


<details>
  <summary>Details</summary>
Motivation: 在存在潜在变量时恢复因果结构是一项重要但具有挑战性的任务。现有方法通常对因果结构有严格且/或不可测试的假设，并且无法处理观测变量同时受多个潜在变量影响的真实场景。这限制了它们在复杂实际应用中的有效性。本文旨在解决这些现有方法的局限性，特别是在存在多个潜在混杂变量的情况下，寻找一种更鲁棒、更实际的因果结构识别方法。

Method: 本文关注线性非高斯模型。核心方法是利用以特定方式构建的观测变量的联合高阶累积量矩阵。研究表明，即使存在任意数量的潜在混杂变量，两个观测变量之间的因果不对称性也可以直接从这些高阶累积量矩阵的秩亏缺陷性质中观察到。因此，该方法避免了传统方法中常见的迭代过程，直接通过矩阵分析识别因果关系。

Result: 本文建立了明确的可识别性结果，证明了所提出方法的理论基础。相应的识别方法不涉及迭代过程，从而提高了计算效率和简便性。实验结果表明，该方法是有效的且渐近正确的，验证了其在实际应用中的性能和鲁棒性。

Conclusion: 本文提出了一种新颖且有效的因果结构恢复方法，特别适用于存在多个潜在混杂变量的线性非高斯模型。通过利用高阶累积量矩阵的秩亏性质，该方法实现了非迭代的因果不对称性识别，克服了现有方法的局限性。其可识别性结果和实验验证的有效性，为处理复杂真实世界因果发现问题提供了一个有力的工具。

Abstract: Recovering causal structure in the presence of latent variables is an
important but challenging task. While many methods have been proposed to handle
it, most of them require strict and/or untestable assumptions on the causal
structure. In real-world scenarios, observed variables may be affected by
multiple latent variables simultaneously, which, generally speaking, cannot be
handled by these methods. In this paper, we consider the linear, non-Gaussian
case, and make use of the joint higher-order cumulant matrix of the observed
variables constructed in a specific way. We show that, surprisingly, causal
asymmetry between two observed variables can be directly seen from the rank
deficiency properties of such higher-order cumulant matrices, even in the
presence of an arbitrary number of latent confounders. Identifiability results
are established, and the corresponding identification methods do not even
involve iterative procedures. Experimental results demonstrate the
effectiveness and asymptotic correctness of our proposed method.

</details>


### [179] [S-Chain: Structured Visual Chain-of-Thought For Medicine](https://arxiv.org/abs/2510.22728)
*Khai Le-Duc,Duy M. H. Nguyen,Phuong T. H. Trinh,Tien-Phat Nguyen,Nghiem T. Diep,An Ngo,Tung Vu,Trinh Vuong,Anh-Tien Nguyen,Mau Nguyen,Van Trung Hoang,Khai-Nguyen Nguyen,Hy Nguyen,Chris Ngo,Anji Liu,Nhat Ho,Anne-Christin Hauschild,Khanh Xuan Nguyen,Thanh Nguyen-Tang,Pengtao Xie,Daniel Sonntag,James Zou,Mathias Niepert,Anh Totti Nguyen*

Main category: cs.LG

TL;DR: 该论文引入了S-Chain，一个包含12,000张专家标注医学图像的大规模数据集，具有边界框和结构化视觉思维链（SV-CoT），用于改善医学视觉-语言模型（VLMs）的忠实推理。通过S-Chain，研究人员对现有VLMs进行了基准测试，研究了检索增强生成，并提出了一个新的对齐机制，显著提高了可解释性、视觉依据忠实度和鲁棒性，为更值得信赖和可解释的医学VLMs铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 医学视觉-语言模型（VLMs）的忠实推理不仅需要准确的预测，还需要文本推理和视觉证据之间的透明对齐。尽管思维链（CoT）提示在医学视觉问答（VQA）中显示出潜力，但目前缺乏捕获逐步推理和精确视觉接地的专家级大规模数据集。这构成了研究的核心问题，其重要性在于提升医学VLMs的信任度和可解释性。

Method: 本研究引入了S-Chain数据集，这是第一个大规模的医学图像数据集，包含12,000张由专家标注的图像，并配有边界框和结构化视觉思维链（SV-CoT），明确将视觉区域与推理步骤关联起来。该数据集进一步支持16种语言，总计超过70万个VQA对，以实现广泛的多语言适用性。研究人员使用S-Chain对最先进的医学VLMs（ExGra-Med、LLaVA-Med）和通用VLMs（Qwen2.5-VL、InternVL2.5）进行了基准测试。此外，研究还探讨了S-Chain与检索增强生成（RAG）的协同作用，揭示了领域知识和视觉接地在自回归推理过程中的交互方式。最后，论文提出了一种新的机制，旨在加强视觉证据与推理之间的对齐。

Result: 基准测试结果表明，SV-CoT监督显著提高了医学VLMs的可解释性、视觉依据忠实度和鲁棒性。对检索增强生成的协同研究揭示了领域知识和视觉接地在自回归推理过程中的有效交互。此外，研究提出的新机制成功加强了视觉证据与推理之间的对齐，从而提升了可靠性和效率。

Conclusion: S-Chain数据集为接地医学推理建立了一个新的基准，并为开发更值得信赖和可解释的医学视觉-语言模型奠定了基础。尽管论文没有明确提及具体的局限性，但此项工作为未来在医学VLMs领域的研究和应用开辟了广阔前景，特别是在提高模型透明度和用户信任度方面。

Abstract: Faithful reasoning in medical vision-language models (VLMs) requires not only
accurate predictions but also transparent alignment between textual rationales
and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise
in medical visual question answering (VQA), no large-scale expert-level dataset
has captured stepwise reasoning with precise visual grounding. We introduce
S-Chain, the first large-scale dataset of 12,000 expert-annotated medical
images with bounding boxes and structured visual CoT (SV-CoT), explicitly
linking visual regions to reasoning steps. The dataset further supports 16
languages, totaling over 700k VQA pairs for broad multilingual applicability.
Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,
LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that
SV-CoT supervision significantly improves interpretability, grounding fidelity,
and robustness. Beyond benchmarking, we study its synergy with
retrieval-augmented generation, revealing how domain knowledge and visual
grounding interact during autoregressive reasoning. Finally, we propose a new
mechanism that strengthens the alignment between visual evidence and reasoning,
improving both reliability and efficiency. S-Chain establishes a new benchmark
for grounded medical reasoning and paves the way toward more trustworthy and
explainable medical VLMs.

</details>


### [180] [Distributionally Robust Optimization via Diffusion Ambiguity Modeling](https://arxiv.org/abs/2510.22757)
*Jiaqi Wen,Jianyi Yang*

Main category: cs.LG

TL;DR: 本论文提出了基于扩散的分布鲁棒优化（D-DRO）框架，通过设计扩散模型构建的模糊集，在保持与名义分布一致性的同时，捕获名义支持空间之外的对抗性分布。D-DRO算法具有可处理性，并理论上证明了其平稳收敛性能，在机器学习预测任务中经验性地展示了其卓越的域外（OOD）泛化性能。


<details>
  <summary>Details</summary>
Motivation: 分布鲁棒优化（DRO）是增强统计学习和优化鲁棒性和泛化能力的基础框架。然而，设计一个有效的模糊集面临挑战：它必须既与名义分布保持一致，又要足够多样化以应对各种潜在场景，同时还需要确保DRO解的可处理性。现有的模糊集往往难以在这些要求之间取得平衡，限制了DRO在复杂对抗性环境中的应用和泛化能力。

Method: 本文提出了一种基于扩散的模糊集设计。该模糊集能够捕获超出名义支持空间的各种对抗性分布，同时与名义分布保持一致。在此模糊建模的基础上，我们提出了基于扩散的DRO（D-DRO）算法。D-DRO通过在参数化的扩散模型空间上求解内部最大化问题，从而实现可处理的DRO解决方案。

Result: D-DRO的平稳收敛性能得到了正式的理论建立。在机器学习预测任务中，经验性地证明了D-DRO具有卓越的域外（OOD）泛化性能，优于现有方法。

Conclusion: 本文提出了一个新颖的基于扩散的分布鲁棒优化框架（D-DRO），它通过精心设计的扩散模糊集，有效地解决了传统DRO在一致性、多样性和可处理性方面的挑战。D-DRO不仅在理论上保证了收敛性，而且在实践中显著提升了机器学习模型在域外场景下的泛化能力，为鲁棒优化和 OOD 泛化领域提供了有价值的贡献。

Abstract: This paper studies Distributionally Robust Optimization (DRO), a fundamental
framework for enhancing the robustness and generalization of statistical
learning and optimization. An effective ambiguity set for DRO must involve
distributions that remain consistent with the nominal distribution while being
diverse enough to account for a variety of potential scenarios. Moreover, it
should lead to tractable DRO solutions. To this end, we propose a
diffusion-based ambiguity set design that captures various adversarial
distributions beyond the nominal support space while maintaining consistency
with the nominal distribution. Building on this ambiguity modeling, we propose
Diffusion-based DRO (D-DRO), a tractable DRO algorithm that solves the inner
maximization over the parameterized diffusion model space. We formally
establish the stationary convergence performance of D-DRO and empirically
demonstrate its superior Out-of-Distribution (OOD) generalization performance
in a ML prediction task.

</details>


### [181] [TELL-TALE: Task Efficient LLMs with Task Aware Layer Elimination](https://arxiv.org/abs/2510.22767)
*Omar Naim,Krish Sharma,Nicholas Asher*

Main category: cs.LG

TL;DR: TALE（Task-Aware Layer Elimination）是一种推理时算法，通过直接优化任务特定验证性能来修剪LLM中的整个Transformer层。它无需重新训练，可在所有基准测试中持续提高准确性并降低计算成本，同时在微调期间应用还能进一步提升性能。TALE还提供了准确性与效率之间的灵活权衡控制，并通过选择性层移除解决了瓶颈层问题，从而产生更小、更快、更准确的模型，并为Transformer可解释性提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在推理时面临效率和准确性提升的挑战。传统方法通常需要重新训练才能实现模型裁剪和性能优化，这增加了计算成本和开发时间。本研究的动机是开发一种无需重新训练的推理时算法，以提高LLM的效率和准确性，同时降低计算成本，解决现有方法在处理特定任务时可能存在的层级瓶颈，即某些层可能成为瓶颈，降低任务相关表示的质量。

Method: 本研究引入了TALE（Task-Aware Layer Elimination）算法，这是一种在推理时修剪LLM中整个Transformer层的方法。TALE通过直接优化任务特定的验证性能来进行层级修剪，其关键特点在于无需重新训练模型。该方法在零样本（zero-shot）和少样本（few-shot）设置下，在9个任务和5个模型（包括LLaMA 3.1 8B、Qwen 2.5 7B、Qwen 2.5 0.5B、Mistral 7B和Lucie 7B）上进行了评估。此外，研究还探讨了在模型微调（finetuning）过程中应用TALE所带来的额外性能增益。为了理解TALE的有效性，研究还进行了互信息分析，以识别哪些层可能作为瓶颈，从而损害任务相关的表示。

Result: TALE在所有基准测试中均展现出卓越的性能：它在无需重新训练的情况下，持续提高了准确性并降低了计算成本。在微调过程中应用TALE，还能带来额外的性能提升。该算法为用户提供了在准确性和效率之间进行灵活权衡的能力。通过互信息分析发现，某些层确实充当了瓶颈，降低了任务相关表示的质量，而TALE的选择性层移除有效地解决了这一问题。最终，TALE能够生成更小、更快、更准确的模型，这些模型也更快进行微调，并为Transformer的可解释性提供了新的见解。

Conclusion: TALE算法为LLM的推理效率和准确性提升提供了一个创新且高效的解决方案。其核心贡献在于实现了无需重新训练的层级剪枝，这显著降低了计算资源需求，并加快了模型部署。TALE不仅在多种模型和任务上持续提升了性能，而且通过在微调阶段的应用进一步增强了效果，并提供了灵活的性能-效率权衡。此外，通过揭示模型内部的瓶颈层及其对任务表示的影响，TALE也为深入理解Transformer架构和提高模型可解释性开辟了新途径。未来的工作可以探索TALE在更广泛模型架构和任务上的泛化能力，以及对其理论基础的进一步探究。

Abstract: In this paper we introduce Tale, Task-Aware Layer Elimination, an
inference-time algorithm that prunes entire transformer layers in an LLM by
directly optimizing task-specific validation performance. We evaluate TALE on 9
tasks and 5 models, including LLaMA 3.1 8B, Qwen 2.5 7B, Qwen 2.5 0.5B, Mistral
7B, and Lucie 7B, under both zero-shot and few-shot settings. Unlike prior
approaches, TALE requires no retraining and consistently improves accuracy
while reducing computational cost across all benchmarks. Furthermore, applying
TALE during finetuning leads to additional performance gains. Finally, TALE
provides flexible user control over trade-offs between accuracy and efficiency.
Mutual information analysis shows that certain layers act as bottlenecks,
degrading task-relevant representations. Tale's selective layer removal
remedies this problem, producing smaller, faster, and more accurate models that
are also faster to fine-tune while offering new insights into transformer
interpretability.

</details>


### [182] [Inductive Transfer Learning for Graph-Based Recommenders](https://arxiv.org/abs/2510.22799)
*Florian Grötschla,Elia Trachsel,Luca A. Lanzendörfer,Roger Wattenhofer*

Main category: cs.LG

TL;DR: NBF-Rec是一种归纳式图基推荐系统，它通过在推理时动态计算节点嵌入，支持跨数据集的归纳迁移学习，即使在用户和项目集不相交的情况下也能在零样本设置中表现出色，并通过轻量级微调进一步提高性能。


<details>
  <summary>Details</summary>
Motivation: 传统的图基推荐系统通常在转导设置下进行训练，这限制了它们对新用户、新项目或新数据集的适用性，因为每次遇到新数据都需要重新训练。本文旨在解决这一局限性，提出一种能够进行归纳迁移学习的模型，从而实现跨数据集的泛化，提高系统的灵活性和适用性，尤其是在用户和项目集不相交的场景下。

Method: NBF-Rec是一种图基推荐模型，其核心方法是支持归纳迁移学习。与需要为每个领域重新训练的传统基于嵌入的方法不同，NBF-Rec在推理时动态计算节点嵌入。这意味着模型不需要预先计算和存储固定嵌入，从而提高了在新数据集上的灵活性。该方法通过交互级别的信息传递实现跨数据集的泛化，无需对齐用户或项目。

Result: NBF-Rec在七个涵盖电影、音乐、电子商务和位置签到的真实世界数据集上进行了评估。结果表明，在零样本设置下（即不使用目标域数据进行训练），NBF-Rec取得了有竞争力的性能。此外，通过轻量级微调，模型的性能得到了进一步的提升。这些结果有力地证明了图基推荐中归纳迁移的可行性，并表明交互级别的信息传递支持跨数据集的泛化，即使在用户或项目不对齐的情况下也能实现。

Conclusion: 本研究的结论是，在图基推荐系统中实现归纳迁移是可行的。NBF-Rec模型通过其独特地在推理时动态计算节点嵌入的方法，成功地证明了这一点。该模型能够支持跨数据集的泛化，即使在用户和项目集不相交的情况下也能保持竞争力。这表明交互级别的信息传递是实现这种跨数据集泛化的关键。未来的工作可能包括探索NBF-Rec在更广泛的应用场景中的表现，以及进一步优化动态嵌入计算的效率和精度。

Abstract: Graph-based recommender systems are commonly trained in transductive
settings, which limits their applicability to new users, items, or datasets. We
propose NBF-Rec, a graph-based recommendation model that supports inductive
transfer learning across datasets with disjoint user and item sets. Unlike
conventional embedding-based methods that require retraining for each domain,
NBF-Rec computes node embeddings dynamically at inference time. We evaluate the
method on seven real-world datasets spanning movies, music, e-commerce, and
location check-ins. NBF-Rec achieves competitive performance in zero-shot
settings, where no target domain data is used for training, and demonstrates
further improvements through lightweight fine-tuning. These results show that
inductive transfer is feasible in graph-based recommendation and that
interaction-level message passing supports generalization across datasets
without requiring aligned users or items.

</details>


### [183] [Last Iterate Analyses of FTRL in Stochasitc Bandits](https://arxiv.org/abs/2510.22819)
*Jingxin Zhan,Yuze Han,Zhihua Zhang*

Main category: cs.LG

TL;DR: 该论文通过理论分析，研究了多臂老虎机中遵循正则化领导者（FTRL）算法的最终迭代收敛率。具体而言，针对具有最佳两全（BOBW）特性的$1/2$-Tsallis-INF FTRL算法，作者证明了最优臂上的点质量与第$t$次迭代时臂集上的概率分布之间的Bregman散度以$t^{-1/2}$的速度衰减，这部分证实了对数遗憾应对应于$t^{-1}$最终迭代收敛率的直觉。


<details>
  <summary>Details</summary>
Motivation: 在线学习算法的收敛性分析是机器学习理论的核心，其中最终迭代收敛性尤为重要，因为它捕捉了学习者的实际决策并描述了学习过程随时间的演变。然而，在多臂老虎机问题中，大多数现有算法分析主要关注遗憾的阶，而最终迭代（简单遗憾）收敛率却研究较少，尤其是对于广泛研究的遵循正则化领导者（FTRL）算法。尽管FTRL算法在多臂老虎机问题中表现出最佳两全（BOBW）特性，实现了对数遗憾，但它们的最终迭代收敛率尚未被研究。直观上，对数遗憾应该对应于$t^{-1}$的最终迭代收敛率，但这种直觉缺乏理论确认。

Method: 本文采用理论分析方法来研究$1/2$-Tsallis-INF FTRL算法的最终迭代收敛率。该算法与正则函数$	ext{Ψ}(p)=-4	ext{∑}_{i=1}^{d}	ext{√}p_i$相关联。研究方法集中于分析最优臂上的点质量与第$t$次迭代时臂集上的概率分布之间的Bregman散度。通过数学推导，论文旨在确定此Bregman散度随时间步长$t$的衰减速度。

Result: 理论分析结果表明，由正则函数$	ext{Ψ}(p)=-4	ext{∑}_{i=1}^{d}	ext{√}p_i$定义的Bregman散度，在最优臂上的点质量与第$t$次迭代时臂集上的概率分布之间，以$t^{-1/2}$的速度衰减。这一发现部分证实了对数遗憾应该对应于$t^{-1}$最终迭代收敛率的直觉。尽管结果支持了直觉的方向，但$t^{-1/2}$的衰减率慢于直觉预期的$t^{-1}$，揭示了该领域进一步研究的潜力。

Conclusion: 本研究通过对$1/2$-Tsallis-INF FTRL算法的Bregman散度进行理论分析，首次探讨了多臂老虎机中FTRL算法的最终迭代收敛率。结果表明，Bregman散度以$t^{-1/2}$的速度衰减，部分验证了对数遗憾与$t^{-1}$最终迭代收敛率之间的直觉联系。这项工作为理解FTRL算法在多臂老虎机中的实际决策演变提供了重要见解。未来的工作可以探索如何弥合$t^{-1/2}$的理论结果与直觉中$t^{-1}$速率之间的差距，并对其他具有BOBW特性的FTRL算法进行更广泛的最终迭代收敛率分析。

Abstract: The convergence analysis of online learning algorithms is central to machine
learning theory, where last-iterate convergence is particularly important, as
it captures the learner's actual decisions and describes the evolution of the
learning process over time. However, in multi-armed bandits, most existing
algorithmic analyses mainly focus on the order of regret, while the
last-iterate (simple regret) convergence rate remains less explored --
especially for the widely studied Follow-the-Regularized-Leader (FTRL)
algorithms. Recently, a growing line of work has established the
Best-of-Both-Worlds (BOBW) property of FTRL algorithms in bandit problems,
showing in particular that they achieve logarithmic regret in stochastic
bandits. Nevertheless, their last-iterate convergence rate has not yet been
studied. Intuitively, logarithmic regret should correspond to a $t^{-1}$
last-iterate convergence rate. This paper partially confirms this intuition
through theoretical analysis, showing that the Bregman divergence, defined by
the regular function $\Psi(p)=-4\sum_{i=1}^{d}\sqrt{p_i}$ associated with the
BOBW FTRL algorithm $1/2$-Tsallis-INF (arXiv:1807.07623), between the point
mass on the optimal arm and the probability distribution over the arm set
obtained at iteration $t$, decays at a rate of $t^{-1/2}$.

</details>


### [184] [Clustering by Denoising: Latent plug-and-play diffusion for single-cell data](https://arxiv.org/abs/2510.22835)
*Dominik Meier,Shixing Yu,Sagnik Nandy,Promit Ghosal,Kyra Gan*

Main category: cs.LG

TL;DR: 该论文提出了一种新的潜在即插即用扩散框架，通过在低维潜在空间中去噪并在高维观测空间中重新引入噪声来指导去噪过程，从而解决了单细胞RNA测序数据中细胞聚类精度受测量噪声和生物变异性影响的挑战。该方法通过自适应噪声处理、不确定性量化和可泛化去噪，显著提高了聚类精度和生物学一致性。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序（scRNA-seq）技术使得细胞异质性研究成为可能。然而，由于测量噪声和生物学变异性，细胞聚类的准确性以及基于细胞标签的下游分析仍然面临巨大挑战。在传统的潜在空间（如通过主成分分析（PCA）获得的）中，不同细胞类型的数据点可能会被投射得过于接近，这严重阻碍了精确的细胞聚类。因此，开发一种能够有效处理噪声并提高聚类准确性的新方法至关重要。

Method: 我们引入了一种新颖的潜在即插即用扩散框架，其核心思想是将观测空间与去噪空间分离。这种分离通过一种独特的吉布斯采样程序实现：首先，在低维潜在空间中应用学习到的扩散先验来执行数据去噪；其次，为了精确引导去噪过程，噪声被重新引入到原始的高维观测空间中。这种独特而关键的“输入空间引导”机制确保了去噪轨迹能够忠实地保留原始数据的结构特征，避免了去噪过程对数据本质结构造成扭曲。

Result: 我们对所提出的方法在合成数据和真实世界单细胞基因组数据上进行了全面而严格的鲁棒性评估。在合成数据上，该方法在各种噪声水平和数据集偏移条件下均显著提高了聚类精度。在真实世界的单细胞数据分析中，我们的方法展示了在所得细胞簇中生物学一致性的显著提升，其形成的簇边界与已知的细胞类型标记物和发育轨迹表现出更好的对齐性，这表明了其在揭示生物学真实结构方面的优越性。

Conclusion: 我们的方法带来了三个显著的关键优势：(1) 通过在扩散先验和观测数据之间建立可调的平衡机制，实现了对噪声的自适应处理，增强了方法的灵活性和鲁棒性；(2) 通过提供有原则的不确定性估计，为下游分析提供了量化的不确定性信息，提高了结果的可靠性；(3) 通过利用干净的参考数据对噪声更大的数据集进行去噪，并通过数据平均策略，能够将数据质量提升到超越训练集本身的水平，从而实现更广泛和可泛化的去噪能力。综上，本方法显著提高了单细胞数据分析的可靠性和准确性，为未来的单细胞研究提供了有力的工具。

Abstract: Single-cell RNA sequencing (scRNA-seq) enables the study of cellular
heterogeneity. Yet, clustering accuracy, and with it downstream analyses based
on cell labels, remain challenging due to measurement noise and biological
variability. In standard latent spaces (e.g., obtained through PCA), data from
different cell types can be projected close together, making accurate
clustering difficult. We introduce a latent plug-and-play diffusion framework
that separates the observation and denoising space. This separation is
operationalized through a novel Gibbs sampling procedure: the learned diffusion
prior is applied in a low-dimensional latent space to perform denoising, while
to steer this process, noise is reintroduced into the original high-dimensional
observation space. This unique "input-space steering" ensures the denoising
trajectory remains faithful to the original data structure. Our approach offers
three key advantages: (1) adaptive noise handling via a tunable balance between
prior and observed data; (2) uncertainty quantification through principled
uncertainty estimates for downstream analysis; and (3) generalizable denoising
by leveraging clean reference data to denoise noisier datasets, and via
averaging, improve quality beyond the training set. We evaluate robustness on
both synthetic and real single-cell genomics data. Our method improves
clustering accuracy on synthetic data across varied noise levels and dataset
shifts. On real-world single-cell data, our method demonstrates improved
biological coherence in the resulting cell clusters, with cluster boundaries
that better align with known cell type markers and developmental trajectories.

</details>


### [185] [Guardian: Decoupling Exploration from Safety in Reinforcement Learning](https://arxiv.org/abs/2510.22859)
*Kaitong Cai,Jusheng Zhang,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: RLPD-GX是一个解耦策略优化和安全执行的混合离线-在线强化学习（O2O RL）框架。它通过投影式守护者和动态课程来解决O2O RL中的不稳定性问题，实现在Atari-100k上达到领先水平的性能（标准化平均分3.02，比之前方法提高45%），同时提升了安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 混合离线-在线强化学习（O2O RL）在提供样本效率和鲁棒探索方面具有潜力，但由于离线和在线数据之间的分布漂移，导致训练不稳定。现有方法在探索性和策略保守性之间难以平衡，且在安全执行方面存在挑战。本研究旨在解决O2O RL的稳定性问题，同时兼顾探索和安全，避免策略过于保守。

Method: 本研究提出了RLPD-GX框架，它将策略优化与安全执行解耦。一个寻求奖励的学习器自由探索，而一个基于投影的守护者确保规则一致的执行和安全的价值备份。这种设计保留了在线交互的探索价值，同时避免了策略变得过于保守。为了进一步稳定训练，研究引入了动态课程，该课程逐步扩展时间范围并调整离线-在线数据混合比例。通过受保护Bellman算子的收缩性质证明了收敛性。

Result: RLPD-GX在Atari-100k上取得了最先进的性能，实现了3.02的标准化平均分数，比之前的混合方法提高了45%。实验结果还表明，RLPD-GX提供了更强的安全性和稳定性。除了Atari之外，在安全关键型和长时程任务上的消融实验也显示出一致的性能提升，突显了该设计的通用性。

Conclusion: 研究结果表明，解耦的安全执行是实现鲁棒O2O RL的一种简单而有原则的途径。这为强化学习中调和探索与安全提供了一个更广阔的范式。RLPD-GX框架的成功表明，通过分离策略优化和安全保证，可以有效地解决O2O RL中的稳定性问题，并为未来的研究提供了新的方向。

Abstract: Hybrid offline--online reinforcement learning (O2O RL) promises both sample
efficiency and robust exploration, but suffers from instability due to
distribution shift between offline and online data. We introduce RLPD-GX, a
framework that decouples policy optimization from safety enforcement: a
reward-seeking learner explores freely, while a projection-based guardian
guarantees rule-consistent execution and safe value backups. This design
preserves the exploratory value of online interactions without collapsing to
conservative policies. To further stabilize training, we propose dynamic
curricula that gradually extend temporal horizons and anneal offline--online
data mixing. We prove convergence via a contraction property of the guarded
Bellman operator, and empirically show state-of-the-art performance on
Atari-100k, achieving a normalized mean score of 3.02 (+45\% over prior hybrid
methods) with stronger safety and stability. Beyond Atari, ablations
demonstrate consistent gains across safety-critical and long-horizon tasks,
underscoring the generality of our design. Extensive and comprehensive results
highlight decoupled safety enforcement as a simple yet principled route to
robust O2O RL, suggesting a broader paradigm for reconciling exploration and
safety in reinforcement learning.

</details>


### [186] [Limits of Generative Pre-Training in Structured EMR Trajectories with Irregular Sampling](https://arxiv.org/abs/2510.22878)
*Nicholas I-Hsien Kuo,Blanca Gallego,Louisa Jorm*

Main category: cs.LG

TL;DR: 该研究评估了基础模型从自回归预训练中学习到的表征在表型发现中的适用性，发现虽然模型能够实现局部真实性，但缺乏临床连贯性，并强调了在微调或部署前进行领域特定评估和轨迹合成的必要性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在自然语言处理中通过自回归预训练学习复杂模式，其目标是将学习到的知识迁移到下游预测任务。然而，最近一些研究未经严格验证就将这些学习到的表征重新用于表型发现，这可能导致表面上真实但临床上不连贯的嵌入。本研究旨在检验这种不匹配。

Method: 研究人员在HIV抗逆转录病毒治疗(ART)和急性低血压的纵向数据上训练了两个自回归模型：一个序列到序列的LSTM和一个简化版Transformer。为了模拟数据中的不规律性，训练期间加入了随机的就诊间隔，而测试序列则保持完整。通过患者轨迹合成来评估模型的分布和相关性保真度。

Result: 两个模型都能够重现特征分布，但未能保留跨特征结构。这表明生成式预训练虽然能产生局部真实性，但在临床连贯性方面存在局限性。

Conclusion: 研究结果强调了在将基础模型用于临床表型发现时，进行领域特定评估的必要性。同时，建议在模型微调或部署之前，将轨迹合成作为一种实用的探测方法。

Abstract: Foundation models refer to architectures trained on vast datasets using
autoregressive pre-training from natural language processing to capture
intricate patterns and motifs. They were originally developed to transfer such
learned knowledge to downstream predictive tasks. Recently, however, some
studies repurpose these learned representations for phenotype discovery without
rigorous validation, risking superficially realistic but clinically incoherent
embeddings. To test this mismatch, we trained two autoregressive models -- a
sequence-to-sequence LSTM and a reduced Transformer -- on longitudinal ART for
HIV and Acute Hypotension datasets. Controlled irregularity was added during
training via random inter-visit gaps, while test sequences stayed complete.
Patient-trajectory synthesis evaluated distributional and correlational
fidelity. Both reproduced feature distributions but failed to preserve
cross-feature structure -- showing that generative pre-training yields local
realism but limited clinical coherence. These results highlight the need for
domain-specific evaluation and support trajectory synthesis as a practical
probe before fine-tuning or deployment.

</details>


### [187] [Learning Reconfigurable Representations for Multimodal Federated Learning with Missing Data](https://arxiv.org/abs/2510.22880)
*Duong M. Nguyen,Trong Nghia Hoang,Thanh Trung Huynh,Quoc Viet Hung Nguyen,Phi Le Nguyen*

Main category: cs.LG

TL;DR: 该论文提出了一种新的联邦学习框架，通过引入可学习的客户端嵌入控制来应对多模态联邦学习中客户端数据不完整和异构导致特征表示未对齐的问题。这些嵌入控制作为重配置信号，使全局聚合表示与每个客户端的本地上下文对齐，从而显著提升了在严重数据不完整情况下的模型性能，最高可达36.45%。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的多模态联邦学习场景中，客户端经常面临数据不完整和异构的问题。具体来说，每个客户端可能观察到不同模态的子集，并且在每种模态内也可能存在缺失的输入特征。这种复杂且普遍的数据缺失模式导致本地特征表示未对齐，严重限制了模型聚合的有效性。以往的工作要么假设模态集不同但没有缺失输入特征，要么假设共享模态集但有缺失特征，都未能全面解决这种更一般且现实的设置带来的挑战。

Method: 该论文提出了一种新的联邦学习框架，其核心是基于可学习的客户端嵌入控制的局部自适应表示。这些客户端侧的嵌入控制能够编码每个客户端的数据缺失模式，并作为重配置信号来调整全局聚合的表示，使其与每个客户端的本地上下文对齐，从而更有效地利用共享信息。此外，这些嵌入控制还可以根据相似的数据缺失模式进行算法聚合，以增强重配置信号在调整全局表示时的鲁棒性。

Result: 该方法在多个联邦多模态基准测试上进行了实证验证，这些基准测试具有客户端间多样化的数据缺失模式。结果表明，该方法在严重数据不完整的情况下，性能提升高达36.45%。此外，该方法还得到了理论分析的支持，提供了与实证观察结果相符的明确性能界限。

Conclusion: 该论文提出的联邦学习框架通过引入可学习的客户端嵌入控制，有效解决了多模态联邦学习中客户端数据不完整和异构导致的特征表示未对齐问题。通过实现局部自适应表示和鲁棒的重配置信号，显著提升了模型在复杂数据缺失场景下的性能。理论分析也进一步验证了该方法的有效性，为现实世界的多模态联邦学习提供了有力的解决方案。

Abstract: Multimodal federated learning in real-world settings often encounters
incomplete and heterogeneous data across clients. This results in misaligned
local feature representations that limit the effectiveness of model
aggregation. Unlike prior work that assumes either differing modality sets
without missing input features or a shared modality set with missing features
across clients, we consider a more general and realistic setting where each
client observes a different subset of modalities and might also have missing
input features within each modality. To address the resulting misalignment in
learned representations, we propose a new federated learning framework
featuring locally adaptive representations based on learnable client-side
embedding controls that encode each client's data-missing patterns.
  These embeddings serve as reconfiguration signals that align the globally
aggregated representation with each client's local context, enabling more
effective use of shared information. Furthermore, the embedding controls can be
algorithmically aggregated across clients with similar data-missing patterns to
enhance the robustness of reconfiguration signals in adapting the global
representation. Empirical results on multiple federated multimodal benchmarks
with diverse data-missing patterns across clients demonstrate the efficacy of
the proposed method, achieving up to 36.45\% performance improvement under
severe data incompleteness. The method is also supported by a theoretical
analysis with an explicit performance bound that matches our empirical
observations. Our source codes are provided at
https://github.com/nmduonggg/PEPSY

</details>


### [188] [Offline Preference Optimization via Maximum Marginal Likelihood Estimation](https://arxiv.org/abs/2510.22881)
*Saeed Najafi,Alona Fyshe*

Main category: cs.LG

TL;DR: 本文提出了一种名为MMPO（基于最大边际似然的偏好优化）的LLM对齐新方法，它比传统RLHF更简单、更稳定。MMPO通过最大化首选文本的边际对数似然进行隐式偏好优化，无需奖励模型和熵最大化。实验表明，MMPO在稳定性、对齐性能和基础语言能力保持方面均表现出色。


<details>
  <summary>Details</summary>
Motivation: 使大型语言模型（LLMs）与人类偏好对齐至关重要。然而，现有标准方法，如基于人类反馈的强化学习（RLHF），通常复杂且不稳定，这促使研究人员寻求更简单、更有效且稳定的对齐方法。

Method: 本文提出了一种基于最大边际似然（MML）估计的偏好优化（MMPO）新方法。MMPO将对齐问题重新定义为MML估计，并通过使用偏好对作为样本来近似，最大化首选文本输出的边际对数似然。这种方法消除了对显式奖励模型和熵最大化的需求。理论上，MMPO通过生成加权梯度来隐式执行偏好优化，该梯度自然地赋予被选择的响应比被拒绝的响应更高的权重。

Result: 在从1.35亿到80亿参数的模型上进行的实证研究表明，MMPO在以下方面表现出色：1) 与其他基线方法相比，MMPO对超参数β的稳定性更高。2) MMPO实现了与现有方法相当或更优的偏好对齐效果，同时更好地保留了基础模型的通用语言能力。通过一系列的消融实验，研究人员发现这种性能的提升确实归因于MMPO在梯度更新中实现的隐式偏好优化。

Conclusion: MMPO是一种新颖、更简单的LLM对齐方法，它基于最大边际似然估计，能够有效且稳定地使模型与人类偏好对齐。它通过隐式偏好优化，在实现优异对齐效果的同时，更好地保持了基础模型的语言能力，为LLM对齐提供了一种有前景的替代方案。

Abstract: Aligning Large Language Models (LLMs) with human preferences is crucial, but
standard methods like Reinforcement Learning from Human Feedback (RLHF) are
often complex and unstable. In this work, we propose a new, simpler approach
that recasts alignment through the lens of Maximum Marginal Likelihood (MML)
estimation. Our new MML based Preference Optimization (MMPO) maximizes the
marginal log-likelihood of a preferred text output, using the preference pair
as samples for approximation, and forgoes the need for both an explicit reward
model and entropy maximization. We theoretically demonstrate that MMPO
implicitly performs preference optimization, producing a weighted gradient that
naturally up-weights chosen responses over rejected ones. Across models ranging
from 135M to 8B parameters, we empirically show that MMPO: 1) is more stable
with respect to the hyperparameter $\beta$ compared to alternative baselines,
and 2) achieves competitive or superior preference alignment while better
preserving the base model's general language capabilities. Through a series of
ablation experiments, we show that this improved performance is indeed
attributable to MMPO's implicit preference optimization within the gradient
updates.

</details>


### [189] [AI based signage classification for linguistic landscape studies](https://arxiv.org/abs/2510.22885)
*Yuqin Jiang,Song Jiang,Jacob Algrim,Trevor Harms,Maxwell Koenen,Xinya Lan,Xingyu Li,Chun-Han Lin,Jia Liu,Jiayang Sun,Henry Zenger*

Main category: cs.LG

TL;DR: 本研究探讨了利用AI驱动的语言检测方法自动化语言景观（LL）分析。通过对檀香山唐人街1,449张地理参考图像应用光学字符识别（OCR）和语言分类AI模型，实现了79%的整体准确率。研究发现AI存在失真、反射、表面退化、涂鸦和幻觉等五种常见误判类型，并倾向于检测人类通常忽略的背景文本。尽管存在局限性，结果表明AI辅助工作流程在减少耗时过程方面的潜力，并鼓励采用AI自动化与人工验证相结合的混合方法，以实现更可靠和高效的工作流程。


<details>
  <summary>Details</summary>
Motivation: 传统的语言景观（LL）研究依赖于人工拍摄和标注公共标识牌来检查城市空间中的语言分布。这种方法虽然能产生有价值的发现，但其过程耗时且难以应用于大范围研究区域。因此，本研究的动机是探索使用AI驱动的语言检测方法来自动化LL分析，以克服传统方法的效率低下和规模限制。

Method: 本研究采用AI驱动的语言检测方法自动化LL分析。以檀香山唐人街为案例研究，首先构建了一个包含1,449张由研究人员收集的地理参考图像数据集。随后，应用AI技术进行光学字符识别（OCR）和语言分类。为了评估模型的准确性，研究人员还进行了人工验证。这种方法旨在系统地检测和识别图像中的文本及其语言，以实现LL分析的自动化。

Result: AI模型在檀香山唐人街的数据集上实现了79%的整体准确率。研究识别出五种常见的误判类型，包括失真、反射、表面退化、涂鸦和幻觉。分析还揭示，AI模型平等对待图像的所有区域，会检测到人类解释者通常会忽略的周边或背景文本。尽管存在这些局限性，结果仍证明了将AI辅助工作流程整合到LL研究中以减少耗时过程的潜力。

Conclusion: 本研究的结论是，尽管AI在语言景观（LL）分析中显示出减少耗时过程的潜力，但由于其固有的局限性（如误判类型和对背景文本的平等处理），AI在此过程中不能完全被信任。因此，论文鼓励采用一种混合方法，将AI自动化与人工验证相结合，以实现更可靠和高效的LL研究工作流程。这将充分利用AI的效率优势，同时通过人工干预纠正其不足，从而提高分析的准确性和可靠性。

Abstract: Linguistic Landscape (LL) research traditionally relies on manual photography
and annotation of public signages to examine distribution of languages in urban
space. While such methods yield valuable findings, the process is
time-consuming and difficult for large study areas. This study explores the use
of AI powered language detection method to automate LL analysis. Using Honolulu
Chinatown as a case study, we constructed a georeferenced photo dataset of
1,449 images collected by researchers and applied AI for optical character
recognition (OCR) and language classification. We also conducted manual
validations for accuracy checking. This model achieved an overall accuracy of
79%. Five recurring types of mislabeling were identified, including distortion,
reflection, degraded surface, graffiti, and hallucination. The analysis also
reveals that the AI model treats all regions of an image equally, detecting
peripheral or background texts that human interpreters typically ignore.
Despite these limitations, the results demonstrate the potential of integrating
AI-assisted workflows into LL research to reduce such time-consuming processes.
However, due to all the limitations and mis-labels, we recognize that AI cannot
be fully trusted during this process. This paper encourages a hybrid approach
combining AI automation with human validation for a more reliable and efficient
workflow.

</details>


### [190] [Transforming volcanic monitoring: A dataset and benchmark for onboard volcano activity detection](https://arxiv.org/abs/2510.22889)
*Darshana Priyasad,Tharindu Fernando,Maryam Haghighat,Harshala Gammulle,Clinton Fookes*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Natural disasters, such as volcanic eruptions, pose significant challenges to
daily life and incur considerable global economic losses. The emergence of
next-generation small-satellites, capable of constellation-based operations,
offers unparalleled opportunities for near-real-time monitoring and onboard
processing of such events. However, a major bottleneck remains the lack of
extensive annotated datasets capturing volcanic activity, which hinders the
development of robust detection systems. This paper introduces a novel dataset
explicitly designed for volcanic activity and eruption detection, encompassing
diverse volcanoes worldwide. The dataset provides binary annotations to
identify volcanic anomalies or non-anomalies, covering phenomena such as
temperature anomalies, eruptions, and volcanic ash emissions. These annotations
offer a foundational resource for developing and evaluating detection models,
addressing a critical gap in volcanic monitoring research. Additionally, we
present comprehensive benchmarks using state-of-the-art models to establish
baselines for future studies. Furthermore, we explore the potential for
deploying these models onboard next-generation satellites. Using the Intel
Movidius Myriad X VPU as a testbed, we demonstrate the feasibility of volcanic
activity detection directly onboard. This capability significantly reduces
latency and enhances response times, paving the way for advanced early warning
systems. This paves the way for innovative solutions in volcanic disaster
management, encouraging further exploration and refinement of onboard
monitoring technologies.

</details>


### [191] [Charting the Design Space of Neural Graph Representations for Subgraph Matching](https://arxiv.org/abs/2510.22897)
*Vaibhav Raj,Indradyumna Roy,Ashwin Ramachandran,Soumen Chakrabarti,Abir De*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Subgraph matching is vital in knowledge graph (KG) question answering,
molecule design, scene graph, code and circuit search, etc. Neural methods have
shown promising results for subgraph matching. Our study of recent systems
suggests refactoring them into a unified design space for graph matching
networks. Existing methods occupy only a few isolated patches in this space,
which remains largely uncharted. We undertake the first comprehensive
exploration of this space, featuring such axes as attention-based vs. soft
permutation-based interaction between query and corpus graphs, aligning nodes
vs. edges, and the form of the final scoring network that integrates neural
representations of the graphs. Our extensive experiments reveal that judicious
and hitherto-unexplored combinations of choices in this space lead to large
performance benefits. Beyond better performance, our study uncovers valuable
insights and establishes general design principles for neural graph
representation and interaction, which may be of wider interest.

</details>


### [192] [On the Anisotropy of Score-Based Generative Models](https://arxiv.org/abs/2510.22899)
*Andreas Floros,Seyed-Mohsen Moosavi-Dezfooli,Pier Luigi Dragotti*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate the role of network architecture in shaping the inductive
biases of modern score-based generative models. To this end, we introduce the
Score Anisotropy Directions (SADs), architecture-dependent directions that
reveal how different networks preferentially capture data structure. Our
analysis shows that SADs form adaptive bases aligned with the architecture's
output geometry, providing a principled way to predict generalization ability
in score models prior to training. Through both synthetic data and standard
image benchmarks, we demonstrate that SADs reliably capture fine-grained model
behavior and correlate with downstream performance, as measured by Wasserstein
metrics. Our work offers a new lens for explaining and predicting directional
biases of generative models.

</details>


### [193] [Robust Uncertainty Quantification for Self-Evolving Large Language Models via Continual Domain Pretraining](https://arxiv.org/abs/2510.22931)
*Xiaofan Zhou,Lu Cheng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Continual Learning (CL) is essential for enabling self-evolving large
language models (LLMs) to adapt and remain effective amid rapid knowledge
growth. Yet, despite its importance, little attention has been given to
establishing statistical reliability guarantees for LLMs under CL, particularly
in the setting of continual domain pretraining (CDP). Conformal Prediction (CP)
has shown promise in offering correctness guarantees for LLMs, but it faces
major challenges in CDP: testing data often stems from unknown or shifting
domain distributions, under which CP may no longer provide valid guarantees.
Moreover, when high coverage is required, CP can yield excessively large
prediction sets for unanswerable queries, reducing informativeness. To address
these challenges, we introduce an adaptive rejection and non-exchangeable CP
framework. Our method first estimates the distribution of questions across
domains in the test set using transformer-based clustering, then reweights or
resamples the calibration data accordingly. Building on this, adaptive
rejection CP allows the LLM to selectively abstain from answering when its
confidence or competence shifts significantly. Extensive experiments
demonstrate that our framework enhances both the effectiveness and reliability
of CP under CDP scenarios. Our code is available at:
https://anonymous.4open.science/r/CPCL-8C12/

</details>


### [194] [Towards Personalized Treatment Plan: Geometrical Model-Agnostic Approach to Counterfactual Explanations](https://arxiv.org/abs/2510.22911)
*Daniel Sin,Milad Toutounchian*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In our article, we describe a method for generating counterfactual
explanations in high-dimensional spaces using four steps that involve fitting
our dataset to a model, finding the decision boundary, determining constraints
on the problem, and computing the closest point (counterfactual explanation)
from that boundary. We propose a discretized approach where we find many
discrete points on the boundary and then identify the closest feasible
counterfactual explanation. This method, which we later call $\textit{Segmented
Sampling for Boundary Approximation}$ (SSBA), applies binary search to find
decision boundary points and then searches for the closest boundary point.
Across four datasets of varying dimensionality, we show that our method can
outperform current methods for counterfactual generation with reductions in
distance between $5\%$ to $50\%$ in terms of the $L_2$ norm. Our method can
also handle real-world constraints by restricting changes to immutable and
categorical features, such as age, gender, sex, height, and other related
characteristics such as the case for a health-based dataset. In terms of
runtime, the SSBA algorithm generates decision boundary points on multiple
orders of magnitude in the same given time when we compare to a grid-based
approach. In general, our method provides a simple and effective model-agnostic
method that can compute nearest feasible (i.e. realistic with constraints)
counterfactual explanations. All of our results and our code can be found here
at this link:
$\href{https://github.com/dsin85691/SSBA_For_Counterfactuals}{https://github.com/
dsin85691/SSBA\_For\_Counterfactuals}$

</details>


### [195] [Manifold Approximation leads to Robust Kernel Alignment](https://arxiv.org/abs/2510.22953)
*Mohammad Tariqul Islam,Du Liu,Deblina Sarkar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Centered kernel alignment (CKA) is a popular metric for comparing
representations, determining equivalence of networks, and neuroscience
research. However, CKA does not account for the underlying manifold and relies
on numerous heuristics that cause it to behave differently at different scales
of data. In this work, we propose Manifold approximated Kernel Alignment (MKA),
which incorporates manifold geometry into the alignment task. We derive a
theoretical framework for MKA. We perform empirical evaluations on synthetic
datasets and real-world examples to characterize and compare MKA to its
contemporaries. Our findings suggest that manifold-aware kernel alignment
provides a more robust foundation for measuring representations, with potential
applications in representation learning.

</details>


### [196] [Simple Denoising Diffusion Language Models](https://arxiv.org/abs/2510.22926)
*Huaisheng Zhu,Zhengyu Chen,Shijie Zhou,Zhihui Xie,Yige Yuan,Zhimeng Guo,Siyuan Xu,Hangfan Zhang,Vasant Honavar,Teng Xiao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion models have recently been extended to language generation through
Masked Diffusion Language Models (MDLMs), which achieve performance competitive
with strong autoregressive models. However, MDLMs tend to degrade in the
few-step regime and cannot directly adopt existing few-step distillation
methods designed for continuous diffusion models, as they lack the intrinsic
property of mapping from noise to data. Recent Uniform-state Diffusion Models
(USDMs), initialized from a uniform prior, alleviate some limitations but still
suffer from complex loss formulations that hinder scalability. In this work, we
propose a simplified denoising-based loss for USDMs that optimizes only
noise-replaced tokens, stabilizing training and matching ELBO-level
performance. Furthermore, by framing denoising as self-supervised learning, we
introduce a simple modification to our denoising loss with contrastive-inspired
negative gradients, which is practical and yield additional improvements in
generation quality.

</details>


### [197] [The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination](https://arxiv.org/abs/2510.22977)
*Chenlong Yin,Zeyang Sha,Shiwen Cui,Changhua Meng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key
strategy for building Agents that "think then act." However, recent
observations, like OpenAI's o3, suggest a paradox: stronger reasoning often
coincides with increased hallucination, yet no prior work has systematically
examined whether reasoning enhancement itself causes tool hallucination. To
address this gap, we pose the central question: Does strengthening reasoning
increase tool hallucination? To answer this, we introduce SimpleToolHalluBench,
a diagnostic benchmark measuring tool hallucination in two failure modes: (i)
no tool available, and (ii) only distractor tools available. Through controlled
experiments, we establish three key findings. First, we demonstrate a causal
relationship: progressively enhancing reasoning through RL increases tool
hallucination proportionally with task performance gains. Second, this effect
transcends overfitting - training on non-tool tasks (e.g., mathematics) still
amplifies subsequent tool hallucination. Third, the effect is method-agnostic,
appearing when reasoning is instilled via supervised fine-tuning and when it is
merely elicited at inference by switching from direct answers to step-by-step
thinking. We also evaluate mitigation strategies including Prompt Engineering
and Direct Preference Optimization (DPO), revealing a fundamental
reliability-capability trade-off: reducing hallucination consistently degrades
utility. Mechanistically, Reasoning RL disproportionately collapses
tool-reliability-related representations, and hallucinations surface as
amplified divergences concentrated in late-layer residual streams. These
findings reveal that current reasoning enhancement methods inherently amplify
tool hallucination, highlighting the need for new training objectives that
jointly optimize for capability and reliability.

</details>


### [198] [Diffuse to Detect: A Generalizable Framework for Anomaly Detection with Diffusion Models Applications to UAVs and Beyond](https://arxiv.org/abs/2510.22928)
*Mingze Gong,Juan Du,Jianbang You*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Anomaly detection in complex, high-dimensional data, such as UAV sensor
readings, is essential for operational safety but challenging for existing
methods due to their limited sensitivity, scalability, and inability to capture
intricate dependencies. We propose the Diffuse to Detect (DTD) framework, a
novel approach that innovatively adapts diffusion models for anomaly detection,
diverging from their conventional use in generative tasks with high inference
time. By comparison, DTD employs a single-step diffusion process to predict
noise patterns, enabling rapid and precise identification of anomalies without
reconstruction errors. This approach is grounded in robust theoretical
foundations that link noise prediction to the data distribution's score
function, ensuring reliable deviation detection. By integrating Graph Neural
Networks to model sensor relationships as dynamic graphs, DTD effectively
captures spatial (inter-sensor) and temporal anomalies. Its two-branch
architecture, with parametric neural network-based energy scoring for
scalability and nonparametric statistical methods for interpretability,
provides flexible trade-offs between computational efficiency and transparency.
Extensive evaluations on UAV sensor data, multivariate time series, and images
demonstrate DTD's superior performance over existing methods, underscoring its
generality across diverse data modalities. This versatility, combined with its
adaptability, positions DTD as a transformative solution for safety-critical
applications, including industrial monitoring and beyond.

</details>


### [199] [Softmax is $1/2$-Lipschitz: A tight bound across all $\ell_p$ norms](https://arxiv.org/abs/2510.23012)
*Pravin Nair*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The softmax function is a basic operator in machine learning and
optimization, used in classification, attention mechanisms, reinforcement
learning, game theory, and problems involving log-sum-exp terms. Existing
robustness guarantees of learning models and convergence analysis of
optimization algorithms typically consider the softmax operator to have a
Lipschitz constant of $1$ with respect to the $\ell_2$ norm. In this work, we
prove that the softmax function is contractive with the Lipschitz constant
$1/2$, uniformly across all $\ell_p$ norms with $p \ge 1$. We also show that
the local Lipschitz constant of softmax attains $1/2$ for $p = 1$ and $p =
\infty$, and for $p \in (1,\infty)$, the constant remains strictly below $1/2$
and the supremum $1/2$ is achieved only in the limit. To our knowledge, this is
the first comprehensive norm-uniform analysis of softmax Lipschitz continuity.
We demonstrate how the sharper constant directly improves a range of existing
theoretical results on robustness and convergence. We further validate the
sharpness of the $1/2$ Lipschitz constant of the softmax operator through
empirical studies on attention-based architectures (ViT, GPT-2, Qwen3-8B) and
on stochastic policies in reinforcement learning.

</details>


### [200] [MoEMeta: Mixture-of-Experts Meta Learning for Few-Shot Relational Learning](https://arxiv.org/abs/2510.23013)
*Han Wu,Jie Yin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Few-shot knowledge graph relational learning seeks to perform reasoning over
relations given only a limited number of training examples. While existing
approaches largely adopt a meta-learning framework for enabling fast adaptation
to new relations, they suffer from two key pitfalls. First, they learn relation
meta-knowledge in isolation, failing to capture common relational patterns
shared across tasks. Second, they struggle to effectively incorporate local,
task-specific contexts crucial for rapid adaptation. To address these
limitations, we propose MoEMeta, a novel meta-learning framework that
disentangles globally shared knowledge from task-specific contexts to enable
both effective generalization and rapid adaptation. MoEMeta introduces two key
innovations: (i) a mixture-of-experts (MoE) model that learns globally shared
relational prototypes to enhance generalization, and (ii) a task-tailored
adaptation mechanism that captures local contexts for fast task-specific
adaptation. By balancing global generalization with local adaptability, MoEMeta
significantly advances few-shot relational learning. Extensive experiments and
analyses on three KG benchmarks demonstrate that MoEMeta consistently
outperforms existing baselines, achieving state-of-the-art performance.

</details>


### [201] [RL-AUX: Reinforcement Learning for Auxiliary Task Generation](https://arxiv.org/abs/2510.22940)
*Judah Goldfeder,Matthew So,Hod Lipson*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Auxiliary Learning (AL) is a special case of Multi-task Learning (MTL) in
which a network trains on auxiliary tasks to improve performance on its main
task. This technique is used to improve generalization and, ultimately,
performance on the network's main task. AL has been demonstrated to improve
performance across multiple domains, including navigation, image
classification, and natural language processing. One weakness of AL is the need
for labeled auxiliary tasks, which can require human effort and domain
expertise to generate. Meta Learning techniques have been used to solve this
issue by learning an additional auxiliary task generation network that can
create helpful tasks for the primary network. The most prominent techniques
rely on Bi-Level Optimization, which incurs computational cost and increased
code complexity. To avoid the need for Bi-Level Optimization, we present an
RL-based approach to dynamically create auxiliary tasks. In this framework, an
RL agent is tasked with selecting auxiliary labels for every data point in a
training set. The agent is rewarded when their selection improves the
performance on the primary task. We also experiment with learning optimal
strategies for weighing the auxiliary loss per data point. On the 20-Superclass
CIFAR100 problem, our RL approach outperforms human-labeled auxiliary tasks and
performs as well as a prominent Bi-Level Optimization technique. Our weight
learning approaches significantly outperform all of these benchmarks. For
example, a Weight-Aware RL-based approach helps the VGG16 architecture achieve
80.9% test accuracy while the human-labeled auxiliary task setup achieved
75.53%. The goal of this work is to (1) prove that RL is a viable approach to
dynamically generate auxiliary tasks and (2) demonstrate that per-sample
auxiliary task weights can be learned alongside the auxiliary task labels and
can achieve strong results.

</details>


### [202] [LLM Meets Diffusion: A Hybrid Framework for Crystal Material Generation](https://arxiv.org/abs/2510.23040)
*Subhojyoti Khastagir,Kishalay Das,Pawan Goyal,Seung-Cheol Lee,Satadeep Bhattacharjee,Niloy Ganguly*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in generative modeling have shown significant promise in
designing novel periodic crystal structures. Existing approaches typically rely
on either large language models (LLMs) or equivariant denoising models, each
with complementary strengths: LLMs excel at handling discrete atomic types but
often struggle with continuous features such as atomic positions and lattice
parameters, while denoising models are effective at modeling continuous
variables but encounter difficulties in generating accurate atomic
compositions. To bridge this gap, we propose CrysLLMGen, a hybrid framework
that integrates an LLM with a diffusion model to leverage their complementary
strengths for crystal material generation. During sampling, CrysLLMGen first
employs a fine-tuned LLM to produce an intermediate representation of atom
types, atomic coordinates, and lattice structure. While retaining the predicted
atom types, it passes the atomic coordinates and lattice structure to a
pre-trained equivariant diffusion model for refinement. Our framework
outperforms state-of-the-art generative models across several benchmark tasks
and datasets. Specifically, CrysLLMGen not only achieves a balanced performance
in terms of structural and compositional validity but also generates more
stable and novel materials compared to LLM-based and denoisingbased models
Furthermore, CrysLLMGen exhibits strong conditional generation capabilities,
effectively producing materials that satisfy user-defined constraints. Code is
available at https://github.com/kdmsit/crysllmgen

</details>


### [203] [Hazard-Responsive Digital Twin for Climate-Driven Urban Resilience and Equity](https://arxiv.org/abs/2510.22941)
*Zhenglai Shen,Hongyu Zhou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Compounding climate hazards, such as wildfire-induced outages and urban
heatwaves, challenge the stability and equity of cities. We present a
Hazard-Responsive Digital Twin (H-RDT) that combines physics-informed neural
network modeling, multimodal data fusion, and equity-aware risk analytics for
urban-scale response. In a synthetic district with diverse building archetypes
and populations, a simulated wildfire-outage-heatwave cascade shows that H-RDT
maintains stable indoor temperature predictions (approximately 31 to 33 C)
under partial sensor loss, reproducing outage-driven surges and recovery. The
reinforcement learning based fusion module adaptively reweights IoT, UAV, and
satellite inputs to sustain spatiotemporal coverage, while the equity-adjusted
mapping isolates high-vulnerability clusters (schools, clinics, low-income
housing). Prospective interventions, such as preemptive cooling-center
activation and microgrid sharing, reduce population-weighted thermal risk by 11
to 13 percent, shrink the 95th-percentile (tail) risk by 7 to 17 percent, and
cut overheating hours by up to 9 percent. Beyond the synthetic demonstration,
the framework establishes a transferable foundation for real-city
implementation, linking physical hazard modeling with social equity and
decision intelligence. The H-RDT advances digital urban resilience toward
adaptive, learning-based, and equity-centered decision support for climate
adaptation.

</details>


### [204] [Advantage Shaping as Surrogate Reward Maximization: Unifying Pass@K Policy Gradients](https://arxiv.org/abs/2510.23049)
*Christos Thrampoulidis,Sadegh Mahdavi,Wenlong Deng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This note reconciles two seemingly distinct approaches to policy gradient
optimization for the Pass@K objective in reinforcement learning with verifiable
rewards: (1) direct REINFORCE-style methods, and (2) advantage-shaping
techniques that directly modify GRPO. We show that these are two sides of the
same coin. By reverse-engineering existing advantage-shaping algorithms, we
reveal that they implicitly optimize surrogate rewards. We specifically
interpret practical ``hard-example up-weighting'' modifications to GRPO as
reward-level regularization. Conversely, starting from surrogate reward
objectives, we provide a simple recipe for deriving both existing and new
advantage-shaping methods. This perspective provides a lens for RLVR policy
gradient optimization beyond our original motivation of Pass@K.

</details>


### [205] [Hankel Singular Value Regularization for Highly Compressible State Space Models](https://arxiv.org/abs/2510.22951)
*Paul Schwerdtner,Jules Berman,Benjamin Peherstorfer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep neural networks using state space models as layers are well suited for
long-range sequence tasks but can be challenging to compress after training. We
use that regularizing the sum of Hankel singular values of state space models
leads to a fast decay of these singular values and thus to compressible models.
To make the proposed Hankel singular value regularization scalable, we develop
an algorithm to efficiently compute the Hankel singular values during training
iterations by exploiting the specific block-diagonal structure of the system
matrices that is we use in our state space model parametrization. Experiments
on Long Range Arena benchmarks demonstrate that the regularized state space
layers are up to 10$\times$ more compressible than standard state space layers
while maintaining high accuracy.

</details>


### [206] [Rethinking GSPO: The Perplexity-Entropy Equivalence](https://arxiv.org/abs/2510.23142)
*Chi Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We provide a new perspective on GSPO's length-normalized importance ratios by
establishing their connection to information-theoretic quantities. We show that
GSPO's sequence-level weight $s(\theta) =
(\pi_\theta/\pi_{\theta_{\text{old}}})^{1/|y|}$ can be equivalently expressed
as the inverse perplexity ratio
$\text{PPL}_{\theta_{\text{old}}}/\text{PPL}_\theta$ and as the exponential
cross-entropy change $\exp(\Delta H)$. While the perplexity-entropy
relationship follows from standard definitions, this observation provides a
useful lens for understanding GSPO: the algorithm weights policy gradient
updates by perplexity ratios, offering an information-theoretic interpretation
of the importance weights. This perspective helps explain GSPO's empirical
properties, including log-domain variance reduction through geometric averaging
and stability in training mixture-of-experts models. We validate the
mathematical equivalences and variance predictions through controlled
experiments on mathematical reasoning tasks.

</details>


### [207] [SARNet: A Spike-Aware consecutive validation Framework for Accurate Remaining Useful Life Prediction](https://arxiv.org/abs/2510.22955)
*Junhao Fan,Wenrui Liang,Wei-Qiang Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate prediction of remaining useful life (RUL) is essential to enhance
system reliability and reduce maintenance risk. Yet many strong contemporary
models are fragile around fault onset and opaque to engineers: short,
high-energy spikes are smoothed away or misread, fixed thresholds blunt
sensitivity, and physics-based explanations are scarce. To remedy this, we
introduce SARNet (Spike-Aware Consecutive Validation Framework), which builds
on a Modern Temporal Convolutional Network (ModernTCN) and adds spike-aware
detection to provide physics-informed interpretability. ModernTCN forecasts
degradation-sensitive indicators; an adaptive consecutive threshold validates
true spikes while suppressing noise. Failure-prone segments then receive
targeted feature engineering (spectral slopes, statistical derivatives, energy
ratios), and the final RUL is produced by a stacked RF--LGBM regressor. Across
benchmark-ported datasets under an event-triggered protocol, SARNet
consistently lowers error compared to recent baselines (RMSE 0.0365, MAE
0.0204) while remaining lightweight, robust, and easy to deploy.

</details>


### [208] [Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI](https://arxiv.org/abs/2510.23148)
*Aryan Mathur,Asaduddin Ahmed*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep reinforcement learning agents often struggle when tasks require
understanding both vision and language. Conventional architectures typically
isolate perception (for example, CNN-based visual encoders) from
decision-making (policy networks). This separation can be inefficient, since
the policy's failures do not directly help the perception module learn what is
important. To address this, we implement the Perception-Decision Interleaving
Transformer (PDiT) architecture introduced by Mao et al. (2023), a model that
alternates between perception and decision layers within a single transformer.
This interleaving allows feedback from decision-making to refine perceptual
features dynamically. In addition, we integrate a contrastive loss inspired by
CLIP to align textual mission embeddings with visual scene features. We
evaluate the PDiT encoders on the BabyAI GoToLocal environment and find that
the approach achieves more stable rewards and stronger alignment compared to a
standard PPO baseline. The results suggest that interleaved transformer
encoders are a promising direction for developing more integrated autonomous
agents.

</details>


### [209] [Enabling Vibration-Based Gesture Recognition on Everyday Furniture via Energy-Efficient FPGA Implementation of 1D Convolutional Networks](https://arxiv.org/abs/2510.23156)
*Koki Shibata,Tianheng Ling,Chao Qian,Tomokazu Matsui,Hirohiko Suwa,Keiichi Yasumoto,Gregor Schiele*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The growing demand for smart home interfaces has increased interest in
non-intrusive sensing methods like vibration-based gesture recognition. While
prior studies demonstrated feasibility, they often rely on complex
preprocessing and large Neural Networks (NNs) requiring costly high-performance
hardware, resulting in high energy usage and limited real-world deployability.
This study proposes an energy-efficient solution deploying compact NNs on
low-power Field-Programmable Gate Arrays (FPGAs) to enable real-time gesture
recognition with competitive accuracy. We adopt a series of optimizations: (1)
We replace complex spectral preprocessing with raw waveform input, eliminating
complex on-board preprocessing while reducing input size by 21x without
sacrificing accuracy. (2) We design two lightweight architectures (1D-CNN and
1D-SepCNN) tailored for embedded FPGAs, reducing parameters from 369 million to
as few as 216 while maintaining comparable accuracy. (3) With integer-only
quantization and automated RTL generation, we achieve seamless FPGA deployment.
A ping-pong buffering mechanism in 1D-SepCNN further improves deployability
under tight memory constraints. (4) We extend a hardware-aware search framework
to support constraint-driven model configuration selection, considering
accuracy, deployability, latency, and energy consumption. Evaluated on two
swipe-direction datasets with multiple users and ordinary tables, our approach
achieves low-latency, energy-efficient inference on the AMD Spartan-7 XC7S25
FPGA. Under the PS data splitting setting, the selected 6-bit 1D-CNN reaches
0.970 average accuracy across users with 9.22 ms latency. The chosen 8-bit
1D-SepCNN further reduces latency to 6.83 ms (over 53x CPU speedup) with
slightly lower accuracy (0.949). Both consume under 1.2 mJ per inference,
demonstrating suitability for long-term edge operation.

</details>


### [210] [How Muon's Spectral Design Benefits Generalization: A Study on Imbalanced Data](https://arxiv.org/abs/2510.22980)
*Bhavya Vasudeva,Puneesh Deora,Yize Zhao,Vatsal Sharan,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The growing adoption of spectrum-aware matrix-valued optimizers such as Muon
and Shampoo in deep learning motivates a systematic study of their
generalization properties and, in particular, when they might outperform
competitive algorithms. We approach this question by introducing appropriate
simplifying abstractions as follows: First, we use imbalanced data as a
testbed. Second, we study the canonical form of such optimizers, which is
Spectral Gradient Descent (SpecGD) -- each update step is $UV^T$ where $U\Sigma
V^T$ is the truncated SVD of the gradient. Third, within this framework we
identify a canonical setting for which we precisely quantify when SpecGD
outperforms vanilla Euclidean GD. For a Gaussian mixture data model and both
linear and bilinear models, we show that unlike GD, which prioritizes learning
dominant principal components of the data first, SpecGD learns all principal
components of the data at equal rates. We demonstrate how this translates to a
growing gap in balanced accuracy favoring SpecGD early in training and further
show that the gap remains consistent even when the GD counterpart uses adaptive
step-sizes via normalization. By extending the analysis to deep linear models,
we show that depth amplifies these effects. We empirically verify our
theoretical findings on a variety of imbalanced datasets. Our experiments
compare practical variants of spectral methods, like Muon and Shampoo, against
their Euclidean counterparts and Adam. The results validate our findings that
these spectral optimizers achieve superior generalization by promoting a more
balanced learning of the data's underlying components.

</details>


### [211] [PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation Performance at Unseen Pre-Training Budgets](https://arxiv.org/abs/2510.23198)
*Etienne Goffinet,Shane Bergsma,Avraham Sheinin,Natalia Vassilieva,Shaheer Muhammad,Preslav Nakov,Gurpreet Gosal*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Continual pre-training (CPT) for domain adaptation must balance target-domain
gains with stability on the base domain. Existing CPT scaling laws typically
assume a fixed pre-training budget, which limits their ability to forecast
adaptation outcomes for models trained at different tokens-per-parameter
(PTPP). We present \emph{PTPP-aware} adaptation scaling laws that make the
pre-training budget an explicit variable, enabling accurate \emph{prediction}
of adaptation loss at unseen \ptpp. On a multilingual setup (English/Arabic
$\rightarrow$ French), PTPP-aware formulations trained on early stages
(\ptpp{}=\{15,31\}) predict target loss at \ptpp{}=279 and outperform a
PTPP-agnostic \dcpt{} transfer baseline on metrics (Huber-on-log,
MAE$_\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in
the appendix. Beyond forecasting, we show a practical use case: planning replay
ratios and adaptation token budgets that satisfy target and forgetting
constraints under compute limits.

</details>


### [212] [Increasing LLM Coding Capabilities through Diverse Synthetic Coding Tasks](https://arxiv.org/abs/2510.23208)
*Amal Abed,Ivan Lukic,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) have shown impressive promise in code
generation, yet their progress remains limited by the shortage of large-scale
datasets that are both diverse and well-aligned with human reasoning. Most
existing resources pair problems with solutions, but omit the intermediate
thought process that guides coding. To close this gap, we present a scalable
synthetic data generation pipeline that produces nearly 800k
instruction-reasoning-code-test quadruplets. Each sample combines a task, a
step-by-step reasoning trace, a working solution, and executable tests,
enabling models to learn not just the what but also the how of problem solving.
Our pipeline combines four key components: curated contest problems, web-mined
content filtered by relevance classifiers, data expansion guided by reasoning
patterns, and multi-stage execution-based validation. A genetic mutation
algorithm further increases task diversity while maintaining consistency
between reasoning traces and code implementations. Our key finding is that
fine-tuning LLMs on this dataset yields consistent improvements on coding
benchmarks. Beyond raw accuracy, reasoning-aware data can substitute for model
scaling, generalize across architectures, and outperform leading open-source
alternatives under identical sample budgets. Our work establishes
reasoning-centered synthetic data generation as an efficient approach for
advancing coding capabilities in LLMs. We publish our dataset and generation
pipeline to facilitate further research.

</details>


### [213] [Equivariant Neural Networks for General Linear Symmetries on Lie Algebras](https://arxiv.org/abs/2510.22984)
*Chankyo Kim,Sicheng Zhao,Minghan Zhu,Tzu-Yuan Lin,Maani Ghaffari*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Encoding symmetries is a powerful inductive bias for improving the
generalization of deep neural networks. However, most existing equivariant
models are limited to simple symmetries like rotations, failing to address the
broader class of general linear transformations, GL(n), that appear in many
scientific domains. We introduce Reductive Lie Neurons (ReLNs), a novel neural
network architecture exactly equivariant to these general linear symmetries.
ReLNs are designed to operate directly on a wide range of structured inputs,
including general n-by-n matrices. ReLNs introduce a novel adjoint-invariant
bilinear layer to achieve stable equivariance for both Lie-algebraic features
and matrix-valued inputs, without requiring redesign for each subgroup. This
architecture overcomes the limitations of prior equivariant networks that only
apply to compact groups or simple vector data. We validate ReLNs' versatility
across a spectrum of tasks: they outperform existing methods on algebraic
benchmarks with sl(3) and sp(4) symmetries and achieve competitive results on a
Lorentz-equivariant particle physics task. In 3D drone state estimation with
geometric uncertainty, ReLNs jointly process velocities and covariances,
yielding significant improvements in trajectory accuracy. ReLNs provide a
practical and general framework for learning with broad linear group symmetries
on Lie algebras and matrix-valued data. Project page:
https://reductive-lie-neuron.github.io/

</details>


### [214] [Accelerating Eigenvalue Dataset Generation via Chebyshev Subspace Filter](https://arxiv.org/abs/2510.23215)
*Hong Wang,Jie Wang,Jian Luo,huanshuo dong,Yeqiu Chen,Runmin Jiang,Zhen huang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Eigenvalue problems are among the most important topics in many scientific
disciplines. With the recent surge and development of machine learning, neural
eigenvalue methods have attracted significant attention as a forward pass of
inference requires only a tiny fraction of the computation time compared to
traditional solvers. However, a key limitation is the requirement for large
amounts of labeled data in training, including operators and their eigenvalues.
To tackle this limitation, we propose a novel method, named Sorting Chebyshev
Subspace Filter (SCSF), which significantly accelerates eigenvalue data
generation by leveraging similarities between operators -- a factor overlooked
by existing methods. Specifically, SCSF employs truncated fast Fourier
transform sorting to group operators with similar eigenvalue distributions and
constructs a Chebyshev subspace filter that leverages eigenpairs from
previously solved problems to assist in solving subsequent ones, reducing
redundant computations. To the best of our knowledge, SCSF is the first method
to accelerate eigenvalue data generation. Experimental results show that SCSF
achieves up to a $3.5\times$ speedup compared to various numerical solvers.

</details>


### [215] [Adaptive Forests For Classification](https://arxiv.org/abs/2510.22991)
*Dimitris Bertsimas,Yubing Cui*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Random Forests (RF) and Extreme Gradient Boosting (XGBoost) are two of the
most widely used and highly performing classification and regression models.
They aggregate equally weighted CART trees, generated randomly in RF or
sequentially in XGBoost. In this paper, we propose Adaptive Forests (AF), a
novel approach that adaptively selects the weights of the underlying CART
models. AF combines (a) the Optimal Predictive-Policy Trees (OP2T) framework to
prescribe tailored, input-dependent unequal weights to trees and (b) Mixed
Integer Optimization (MIO) to refine weight candidates dynamically, enhancing
overall performance. We demonstrate that AF consistently outperforms RF,
XGBoost, and other weighted RF in binary and multi-class classification
problems over 20+ real-world datasets.

</details>


### [216] [PAHQ: Accelerating Automated Circuit Discovery through Mixed-Precision Inference Optimization](https://arxiv.org/abs/2510.23264)
*Xinhai Wang,Shu Yang,Liangyu Wang,Lin Zhang,Huanyi Xie,Lijie Hu,Di Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Circuit discovery, which involves identifying sparse and task-relevant
subnetworks in pre-trained language models, is a cornerstone of mechanistic
interpretability. Automated Circuit Discovery (ACDC) has emerged as a pivotal
methodology in circuit discovery, but its application to large language models
is severely limited by computational inefficiency and prohibitively high memory
requirements. Although several accelerated approaches have been proposed, they
primarily rely on linear approximations to ACDC, which significantly
compromises analytical faithfulness. Our proposed method for accelerating
automated circuit discovery, Per Attention Head Quantization (PAHQ), takes a
fundamentally different approach by optimizing the efficiency of each
individual patching operation. PAHQ leverages a fundamental alignment between
activation patching and mixed-precision quantization (MPQ): interpretability
analysis through patching essentially performs targeted ablation studies.
Therefore, we can maintain high precision exclusively for investigated
components while safely reducing precision elsewhere in the network.
PAHQ-accelerated ACDC reduces runtime by up to 80\% and memory consumption by
up to 30\% compared to unaccelerated ACDC while maintaining faithfulness.
Importantly, our method readily integrates with existing edge-based circuit
discovery techniques by modifying the attention computation mechanism. This
training-free approach provides a practical and novel pathway for accelerating
mechanistic interpretability methods. Our code is available at
https://github.com/626619403/PAHQ.

</details>


### [217] [Can Language Models Compose Skills In-Context?](https://arxiv.org/abs/2510.22993)
*Zidong Liu,Zhuoyan Xu,Zhenmei Shi,Yingyu Liang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Composing basic skills from simple tasks to accomplish composite tasks is
crucial for modern intelligent systems. We investigate the in-context
composition ability of language models to perform composite tasks that combine
basic skills demonstrated in in-context examples. This is more challenging than
the standard setting, where skills and their composition can be learned in
training. We conduct systematic experiments on various representative
open-source language models, utilizing linguistic and logical tasks designed to
probe composition abilities. The results reveal that simple task examples can
have a surprising negative impact on the performance, because the models
generally struggle to recognize and assemble the skills correctly, even with
Chain-of-Thought examples. Theoretical analysis further shows that it is
crucial to align examples with the corresponding steps in the composition. This
inspires a method for the probing tasks, whose improved performance provides
positive support for our insights.

</details>


### [218] [A Novel Framework for Multi-Modal Protein Representation Learning](https://arxiv.org/abs/2510.23273)
*Runjie Zheng,Zhen Wang,Anjie Qiao,Jiancong Xie,Jiahua Rao,Yuedong Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate protein function prediction requires integrating heterogeneous
intrinsic signals (e.g., sequence and structure) with noisy extrinsic contexts
(e.g., protein-protein interactions and GO term annotations). However, two key
challenges hinder effective fusion: (i) cross-modal distributional mismatch
among embeddings produced by pre-trained intrinsic encoders, and (ii) noisy
relational graphs of extrinsic data that degrade GNN-based information
aggregation. We propose Diffused and Aligned Multi-modal Protein Embedding
(DAMPE), a unified framework that addresses these through two core mechanisms.
First, we propose Optimal Transport (OT)-based representation alignment that
establishes correspondence between intrinsic embedding spaces of different
modalities, effectively mitigating cross-modal heterogeneity. Second, we
develop a Conditional Graph Generation (CGG)-based information fusion method,
where a condition encoder fuses the aligned intrinsic embeddings to provide
informative cues for graph reconstruction. Meanwhile, our theoretical analysis
implies that the CGG objective drives this condition encoder to absorb
graph-aware knowledge into its produced protein representations. Empirically,
DAMPE outperforms or matches state-of-the-art methods such as DPFunc on
standard GO benchmarks, achieving AUPR gains of 0.002-0.013 pp and Fmax gains
0.004-0.007 pp. Ablation studies further show that OT-based alignment
contributes 0.043-0.064 pp AUPR, while CGG-based fusion adds 0.005-0.111 pp
Fmax. Overall, DAMPE offers a scalable and theoretically grounded approach for
robust multi-modal protein representation learning, substantially enhancing
protein function prediction.

</details>


### [219] [Symbolic Neural Generation with Applications to Lead Discovery in Drug Design](https://arxiv.org/abs/2510.23379)
*Ashwin Srinivasan,A Baskar,Tirtharaj Dash,Michael Bain,Sanjay Kumar Dey,Mainak Banerjee*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate a relatively underexplored class of hybrid neurosymbolic
models integrating symbolic learning with neural reasoning to construct data
generators meeting formal correctness criteria. In \textit{Symbolic Neural
Generators} (SNGs), symbolic learners examine logical specifications of
feasible data from a small set of instances -- sometimes just one. Each
specification in turn constrains the conditional information supplied to a
neural-based generator, which rejects any instance violating the symbolic
specification. Like other neurosymbolic approaches, SNG exploits the
complementary strengths of symbolic and neural methods. The outcome of an SNG
is a triple $(H, X, W)$, where $H$ is a symbolic description of feasible
instances constructed from data, $X$ a set of generated new instances that
satisfy the description, and $W$ an associated weight. We introduce a semantics
for such systems, based on the construction of appropriate \textit{base} and
\textit{fibre} partially-ordered sets combined into an overall partial order,
and outline a probabilistic extension relevant to practical applications. In
this extension, SNGs result from searching over a weighted partial ordering. We
implement an SNG combining a restricted form of Inductive Logic Programming
(ILP) with a large language model (LLM) and evaluate it on early-stage drug
design. Our main interest is the description and the set of potential inhibitor
molecules generated by the SNG. On benchmark problems -- where drug targets are
well understood -- SNG performance is statistically comparable to
state-of-the-art methods. On exploratory problems with poorly understood
targets, generated molecules exhibit binding affinities on par with leading
clinical candidates. Experts further find the symbolic specifications useful as
preliminary filters, with several generated molecules identified as viable for
synthesis and wet-lab testing.

</details>


### [220] [Sentinel: Dynamic Knowledge Distillation for Personalized Federated Intrusion Detection in Heterogeneous IoT Networks](https://arxiv.org/abs/2510.23019)
*Gurpreet Singh,Keshav Sood,P. Rajalakshmi,Yong Xiang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated learning (FL) offers a privacy-preserving paradigm for machine
learning, but its application in intrusion detection systems (IDS) within IoT
networks is challenged by severe class imbalance, non-IID data, and high
communication overhead.These challenges severely degrade the performance of
conventional FL methods in real-world network traffic classification. To
overcome these limitations, we propose Sentinel, a personalized federated IDS
(pFed-IDS) framework that incorporates a dual-model architecture on each
client, consisting of a personalized teacher and a lightweight shared student
model. This design effectively balances deep local adaptation with efficient
global model consensus while preserving client privacy by transmitting only the
compact student model, thus reducing communication costs. Sentinel integrates
three key mechanisms to ensure robust performance: bidirectional knowledge
distillation with adaptive temperature scaling, multi-faceted feature
alignment, and class-balanced loss functions. Furthermore, the server employs
normalized gradient aggregation with equal client weighting to enhance fairness
and mitigate client drift. Extensive experiments on the IoTID20 and 5GNIDD
benchmark datasets demonstrate that Sentinel significantly outperforms
state-of-the-art federated methods, establishing a new performance benchmark,
especially under extreme data heterogeneity, while maintaining communication
efficiency.

</details>


### [221] [Eigen-Value: Efficient Domain-Robust Data Valuation via Eigenvalue-Based Approach](https://arxiv.org/abs/2510.23409)
*Youngjun Choi,Joonseong Kang,Sungjun Lim,Kyungwoo Song*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data valuation has become central in the era of data-centric AI. It drives
efficient training pipelines and enables objective pricing in data markets by
assigning a numeric value to each data point. Most existing data valuation
methods estimate the effect of removing individual data points by evaluating
changes in model validation performance under in-distribution (ID) settings, as
opposed to out-of-distribution (OOD) scenarios where data follow different
patterns. Since ID and OOD data behave differently, data valuation methods
based on ID loss often fail to generalize to OOD settings, particularly when
the validation set contains no OOD data. Furthermore, although OOD-aware
methods exist, they involve heavy computational costs, which hinder practical
deployment. To address these challenges, we introduce \emph{Eigen-Value} (EV),
a plug-and-play data valuation framework for OOD robustness that uses only an
ID data subset, including during validation. EV provides a new spectral
approximation of domain discrepancy, which is the gap of loss between ID and
OOD using ratios of eigenvalues of ID data's covariance matrix. EV then
estimates the marginal contribution of each data point to this discrepancy via
perturbation theory, alleviating the computational burden. Subsequently, EV
plugs into ID loss-based methods by adding an EV term without any additional
training loop. We demonstrate that EV achieves improved OOD robustness and
stable value rankings across real-world datasets, while remaining
computationally lightweight. These results indicate that EV is practical for
large-scale settings with domain shift, offering an efficient path to
OOD-robust data valuation.

</details>


### [222] [Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts](https://arxiv.org/abs/2510.23027)
*Di Zhang,Xun Wu,Shaohan Huang,Yaru Hao,Li Dong,Zewen Chi,Zhifang Sui,Furu Wei*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in reinforcement learning (RL) have substantially improved
the training of large-scale language models, leading to significant gains in
generation quality and reasoning ability. However, most existing research
focuses on dense models, while RL training for Mixture-of-Experts (MoE)
architectures remains underexplored. To address the instability commonly
observed in MoE training, we propose a novel router-aware approach to optimize
importance sampling (IS) weights in off-policy RL. Specifically, we design a
rescaling strategy guided by router logits, which effectively reduces gradient
variance and mitigates training divergence. Experimental results demonstrate
that our method significantly improves both the convergence stability and the
final performance of MoE models, highlighting the potential of RL algorithmic
innovations tailored to MoE architectures and providing a promising direction
for efficient training of large-scale expert models.

</details>


### [223] [BBOPlace-Bench: Benchmarking Black-Box Optimization for Chip Placement](https://arxiv.org/abs/2510.23472)
*Ke Xue,Ruo-Tong Chen,Rong-Xi Tan,Xi Lin,Yunqi Shi,Siyuan Xu,Mingxuan Yuan,Chao Qian*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Chip placement is a vital stage in modern chip design as it has a substantial
impact on the subsequent processes and the overall quality of the final chip.
The use of black-box optimization (BBO) for chip placement has a history of
several decades. However, early efforts were limited by immature problem
formulations and inefficient algorithm designs. Recent progress has shown the
effectiveness and efficiency of BBO for chip placement, proving its potential
to achieve state-of-the-art results. Despite these advancements, the field
lacks a unified, BBO-specific benchmark for thoroughly assessing various
problem formulations and BBO algorithms. To fill this gap, we propose
BBOPlace-Bench, the first benchmark designed specifically for evaluating and
developing BBO algorithms for chip placement tasks. It integrates three problem
formulations of BBO for chip placement, and offers a modular, decoupled, and
flexible framework that enables users to seamlessly implement, test, and
compare their own algorithms. BBOPlace-Bench integrates a wide variety of
existing BBO algorithms, including simulated annealing (SA), evolutionary
algorithms (EAs), and Bayesian optimization (BO). Experimental results show
that the problem formulations of mask-guided optimization and hyperparameter
optimization exhibit superior performance than the sequence pair problem
formulation, while EAs demonstrate better overall performance than SA and BO,
especially in high-dimensional search spaces, and also achieve state-of-the-art
performance compared to the mainstream chip placement methods. BBOPlace-Bench
not only facilitates the development of efficient BBO-driven solutions for chip
placement but also broadens the practical application scenarios (which are
urgently needed) for the BBO community. The code of BBOPlace-Bench is available
at https://github.com/lamda-bbo/BBOPlace-Bench.

</details>


### [224] [Sublinear Sketches for Approximate Nearest Neighbor and Kernel Density Estimation](https://arxiv.org/abs/2510.23039)
*Ved Danait,Srijan Das,Sujoy Bhore*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Approximate Nearest Neighbor (ANN) search and Approximate Kernel Density
Estimation (A-KDE) are fundamental problems at the core of modern machine
learning, with broad applications in data analysis, information systems, and
large-scale decision making. In massive and dynamic data streams, a central
challenge is to design compact sketches that preserve essential structural
properties of the data while enabling efficient queries.
  In this work, we develop new sketching algorithms that achieve sublinear
space and query time guarantees for both ANN and A-KDE for a dynamic stream of
data. For ANN in the streaming model, under natural assumptions, we design a
sublinear sketch that requires only $\mathcal{O}(n^{1+\rho-\eta})$ memory by
storing only a sublinear ($n^{-\eta}$) fraction of the total inputs, where
$\rho$ is a parameter of the LSH family, and $0<\eta<1$. Our method supports
sublinear query time, batch queries, and extends to the more general Turnstile
model. While earlier works have focused on Exact NN, this is the first result
on ANN that achieves near-optimal trade-offs between memory size and
approximation error.
  Next, for A-KDE in the Sliding-Window model, we propose a sketch of size
$\mathcal{O}\left(RW \cdot \frac{1}{\sqrt{1+\epsilon} - 1} \log^2 N\right)$,
where $R$ is the number of sketch rows, $W$ is the LSH range, $N$ is the window
size, and $\epsilon$ is the approximation error. This, to the best of our
knowledge, is the first theoretical sublinear sketch guarantee for A-KDE in the
Sliding-Window model.
  We complement our theoretical results with experiments on various real-world
datasets, which show that the proposed sketches are lightweight and achieve
consistently low error in practice.

</details>


### [225] [Mixed Precision Training of Neural ODEs](https://arxiv.org/abs/2510.23498)
*Elena Celledoni,Brynjulf Owren,Lars Ruthotto,Tianjiao Nicole Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Exploiting low-precision computations has become a standard strategy in deep
learning to address the growing computational costs imposed by ever larger
models and datasets. However, naively performing all computations in low
precision can lead to roundoff errors and instabilities. Therefore, mixed
precision training schemes usually store the weights in high precision and use
low-precision computations only for whitelisted operations. Despite their
success, these principles are currently not reliable for training
continuous-time architectures such as neural ordinary differential equations
(Neural ODEs). This paper presents a mixed precision training framework for
neural ODEs, combining explicit ODE solvers with a custom backpropagation
scheme, and demonstrates its effectiveness across a range of learning tasks.
Our scheme uses low-precision computations for evaluating the velocity,
parameterized by the neural network, and for storing intermediate states, while
stability is provided by a custom dynamic adjoint scaling and by accumulating
the solution and gradients in higher precision. These contributions address two
key challenges in training neural ODE: the computational cost of repeated
network evaluations and the growth of memory requirements with the number of
time steps or layers. Along with the paper, we publish our extendable,
open-source PyTorch package rampde, whose syntax resembles that of leading
packages to provide a drop-in replacement in existing codes. We demonstrate the
reliability and effectiveness of our scheme using challenging test cases and on
neural ODE applications in image classification and generative models,
achieving approximately 50% memory reduction and up to 2x speedup while
maintaining accuracy comparable to single-precision training.

</details>


### [226] [A Deep Latent Factor Graph Clustering with Fairness-Utility Trade-off Perspective](https://arxiv.org/abs/2510.23507)
*Siamak Ghodsi,Amjad Seyedi,Tai Le Quy,Fariba Karimi,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fair graph clustering seeks partitions that respect network structure while
maintaining proportional representation across sensitive groups, with
applications spanning community detection, team formation, resource allocation,
and social network analysis. Many existing approaches enforce rigid constraints
or rely on multi-stage pipelines (e.g., spectral embedding followed by
$k$-means), limiting trade-off control, interpretability, and scalability. We
introduce \emph{DFNMF}, an end-to-end deep nonnegative tri-factorization
tailored to graphs that directly optimizes cluster assignments with a soft
statistical-parity regularizer. A single parameter $\lambda$ tunes the
fairness--utility balance, while nonnegativity yields parts-based factors and
transparent soft memberships. The optimization uses sparse-friendly alternating
updates and scales near-linearly with the number of edges. Across synthetic and
real networks, DFNMF achieves substantially higher group balance at comparable
modularity, often dominating state-of-the-art baselines on the Pareto front.
The code is available at https://github.com/SiamakGhodsi/DFNMF.git.

</details>


### [227] [TAMI: Taming Heterogeneity in Temporal Interactions for Temporal Graph Link Prediction](https://arxiv.org/abs/2510.23577)
*Zhongyi Yu,Jianqiu Wu,Zhenghao Wu,Shuhan Zhong,Weifeng Su,Chul-Ho Lee,Weipeng Zhuo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Temporal graph link prediction aims to predict future interactions between
nodes in a graph based on their historical interactions, which are encoded in
node embeddings. We observe that heterogeneity naturally appears in temporal
interactions, e.g., a few node pairs can make most interaction events, and
interaction events happen at varying intervals. This leads to the problems of
ineffective temporal information encoding and forgetting of past interactions
for a pair of nodes that interact intermittently for their link prediction.
Existing methods, however, do not consider such heterogeneity in their learning
process, and thus their learned temporal node embeddings are less effective,
especially when predicting the links for infrequently interacting node pairs.
To cope with the heterogeneity, we propose a novel framework called TAMI, which
contains two effective components, namely log time encoding function (LTE) and
link history aggregation (LHA). LTE better encodes the temporal information
through transforming interaction intervals into more balanced ones, and LHA
prevents the historical interactions for each target node pair from being
forgotten. State-of-the-art temporal graph neural networks can be seamlessly
and readily integrated into TAMI to improve their effectiveness. Experiment
results on 13 classic datasets and three newest temporal graph benchmark (TGB)
datasets show that TAMI consistently improves the link prediction performance
of the underlying models in both transductive and inductive settings. Our code
is available at https://github.com/Alleinx/TAMI_temporal_graph.

</details>


### [228] [SwiftTS: A Swift Selection Framework for Time Series Pre-trained Models via Multi-task Meta-Learning](https://arxiv.org/abs/2510.23051)
*Tengxue Zhang,Biao Ouyang,Yang Shu,Xinyang Chen,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Pre-trained models exhibit strong generalization to various downstream tasks.
However, given the numerous models available in the model hub, identifying the
most suitable one by individually fine-tuning is time-consuming. In this paper,
we propose \textbf{SwiftTS}, a swift selection framework for time series
pre-trained models. To avoid expensive forward propagation through all
candidates, SwiftTS adopts a learning-guided approach that leverages historical
dataset-model performance pairs across diverse horizons to predict model
performance on unseen datasets. It employs a lightweight dual-encoder
architecture that embeds time series and candidate models with rich
characteristics, computing patchwise compatibility scores between data and
model embeddings for efficient selection. To further enhance the generalization
across datasets and horizons, we introduce a horizon-adaptive expert
composition module that dynamically adjusts expert weights, and the
transferable cross-task learning with cross-dataset and cross-horizon task
sampling to enhance out-of-distribution (OOD) robustness. Extensive experiments
on 14 downstream datasets and 8 pre-trained models demonstrate that SwiftTS
achieves state-of-the-art performance in time series pre-trained model
selection.

</details>


### [229] [Variational Masked Diffusion Models](https://arxiv.org/abs/2510.23606)
*Yichi Zhang,Alex Schwing,Zhizhen Zhao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Masked diffusion models have recently emerged as a flexible framework for
discrete generative modeling. However, a key limitation of standard masked
diffusion is its inability to effectively capture dependencies among tokens
that are predicted concurrently, leading to degraded generation quality when
dependencies among tokens are important. To explicitly model dependencies among
tokens, we propose Variational Masked Diffusion (VMD), a framework that
introduces latent variables into the masked diffusion process. Through
controlled experiments on synthetic datasets, we demonstrate that VMD
successfully learns dependencies that conventional masked diffusion fails to
capture. We further validate the effectiveness of our approach on Sudoku
puzzles and text datasets, where learning of dependencies among tokens improves
global consistency. Across these domains, VMD enhances both generation quality
and dependency awareness, highlighting the value of integrating variational
inference into masked diffusion. Our code is available at:
https://riccizz.github.io/VMD.

</details>


### [230] [AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing](https://arxiv.org/abs/2510.23053)
*Zhiyu Wang,Suman Raj,Rajkumar Buyya*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing
(MEC) systems face critical challenges in coordinating trajectory planning,
task offloading, and resource allocation while ensuring Quality of Service
(QoS) under dynamic and uncertain environments. Existing approaches suffer from
limited scalability, slow convergence, and inefficient knowledge sharing among
UAVs, particularly when handling large-scale IoT device deployments with
stringent deadline constraints. This paper proposes AirFed, a novel federated
graph-enhanced multi-agent reinforcement learning framework that addresses
these challenges through three key innovations. First, we design dual-layer
dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal
dependencies among UAVs and IoT devices, capturing both service relationships
and collaborative interactions within the network topology. Second, we develop
a dual-Actor single-Critic architecture that jointly optimizes continuous
trajectory control and discrete task offloading decisions. Third, we propose a
reputation-based decentralized federated learning mechanism with
gradient-sensitive adaptive quantization, enabling efficient and robust
knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate
that AirFed achieves 42.9% reduction in weighted cost compared to
state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2%
IoT device coverage rate, and reduces communication overhead by 54.5%.
Scalability analysis confirms robust performance across varying UAV numbers,
IoT device densities, and system scales, validating AirFed's practical
applicability for large-scale UAV-MEC deployments.

</details>


### [231] [Sampling from Energy distributions with Target Concrete Score Identity](https://arxiv.org/abs/2510.23106)
*Sergei Kholkin,Francisco Vargas,Alexander Korotin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce the Target Concrete Score Identity Sampler (TCSIS), a method for
sampling from unnormalized densities on discrete state spaces by learning the
reverse dynamics of a Continuous-Time Markov Chain (CTMC). Our approach builds
on a forward in time CTMC with a uniform noising kernel and relies on the
proposed Target Concrete Score Identity, which relates the concrete score, the
ratio of marginal probabilities of two states, to a ratio of expectations of
Boltzmann factors under the forward uniform diffusion kernel. This formulation
enables Monte Carlo estimation of the concrete score without requiring samples
from the target distribution or computation of the partition function. We
approximate the concrete score with a neural network and propose two
algorithms: Self-Normalized TCSIS and Unbiased TCSIS. Finally, we demonstrate
the effectiveness of TCSIS on problems from statistical physics.

</details>


### [232] [Neural Emulator Superiority: When Machine Learning for PDEs Surpasses its Training Data](https://arxiv.org/abs/2510.23111)
*Felix Koehler,Nils Thuerey*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neural operators or emulators for PDEs trained on data from numerical solvers
are conventionally assumed to be limited by their training data's fidelity. We
challenge this assumption by identifying "emulator superiority," where neural
networks trained purely on low-fidelity solver data can achieve higher accuracy
than those solvers when evaluated against a higher-fidelity reference. Our
theoretical analysis reveals how the interplay between emulator inductive
biases, training objectives, and numerical error characteristics enables
superior performance during multi-step rollouts. We empirically validate this
finding across different PDEs using standard neural architectures,
demonstrating that emulators can implicitly learn dynamics that are more
regularized or exhibit more favorable error accumulation properties than their
training data, potentially surpassing training data limitations and mitigating
numerical artifacts. This work prompts a re-evaluation of emulator
benchmarking, suggesting neural emulators might achieve greater physical
fidelity than their training source within specific operational regimes.
Project Page: https://tum-pbs.github.io/emulator-superiority

</details>


### [233] [Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction](https://arxiv.org/abs/2510.23117)
*Omer Jauhar Khan,Sudais Khan,Hafeez Anwar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Physics Informed Neural Networks (PINNs) are gaining attention for their
ability to embed physical laws into deep learning models, which is particularly
useful in structural engineering tasks with limited data. This paper aims to
explore the use of PINNs to predict the weight of small scale spaghetti
bridges, a task relevant to understanding load limits and potential failure
modes in simplified structural models. Our proposed framework incorporates
physics-based constraints to the prediction model for improved performance. In
addition to standard PINNs, we introduce a novel architecture named Physics
Informed Kolmogorov Arnold Network (PIKAN), which blends universal function
approximation theory with physical insights. The structural parameters provided
as input to the model are collected either manually or through computer vision
methods. Our dataset includes 15 real bridges, augmented to 100 samples, and
our best model achieves an $R^2$ score of 0.9603 and a mean absolute error
(MAE) of 10.50 units. From applied perspective, we also provide a web based
interface for parameter entry and prediction. These results show that PINNs can
offer reliable estimates of structural weight, even with limited data, and may
help inform early stage failure analysis in lightweight bridge designs.
  The complete data and code are available at
https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.

</details>


### [234] [A method for outlier detection based on cluster analysis and visual expert criteria](https://arxiv.org/abs/2510.23136)
*Juan A. Lara,David Lizcano,Víctor Rampérez,Javier Soriano*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Outlier detection is an important problem occurring in a wide range of areas.
Outliers are the outcome of fraudulent behaviour, mechanical faults, human
error, or simply natural deviations. Many data mining applications perform
outlier detection, often as a preliminary step in order to filter out outliers
and build more representative models. In this paper, we propose an outlier
detection method based on a clustering process. The aim behind the proposal
outlined in this paper is to overcome the specificity of many existing outlier
detection techniques that fail to take into account the inherent dispersion of
domain objects. The outlier detection method is based on four criteria designed
to represent how human beings (experts in each domain) visually identify
outliers within a set of objects after analysing the clusters. This has an
advantage over other clustering-based outlier detection techniques that are
founded on a purely numerical analysis of clusters. Our proposal has been
evaluated, with satisfactory results, on data (particularly time series) from
two different domains: stabilometry, a branch of medicine studying
balance-related functions in human beings and electroencephalography (EEG), a
neurological exploration used to diagnose nervous system disorders. To validate
the proposed method, we studied method outlier detection and efficiency in
terms of runtime. The results of regression analyses confirm that our proposal
is useful for detecting outlier data in different domains, with a false
positive rate of less than 2% and a reliability greater than 99%.

</details>


### [235] [The Benchmarking Epistemology: Construct Validity for Evaluating Machine Learning Models](https://arxiv.org/abs/2510.23191)
*Timo Freiesleben,Sebastian Zezulka*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Predictive benchmarking, the evaluation of machine learning models based on
predictive performance and competitive ranking, is a central epistemic practice
in machine learning research and an increasingly prominent method for
scientific inquiry. Yet, benchmark scores alone provide at best measurements of
model performance relative to an evaluation dataset and a concrete learning
problem. Drawing substantial scientific inferences from the results, say about
theoretical tasks like image classification, requires additional assumptions
about the theoretical structure of the learning problems, evaluation functions,
and data distributions. We make these assumptions explicit by developing
conditions of construct validity inspired by psychological measurement theory.
We examine these assumptions in practice through three case studies, each
exemplifying a typical intended inference: measuring engineering progress in
computer vision with ImageNet; evaluating policy-relevant weather predictions
with WeatherBench; and examining limitations of the predictability of life
events with the Fragile Families Challenge. Our framework clarifies the
conditions under which benchmark scores can support diverse scientific claims,
bringing predictive benchmarking into perspective as an epistemological
practice and a key site of conceptual and theoretical reasoning in machine
learning.

</details>


### [236] [Grassmanian Interpolation of Low-Pass Graph Filters: Theory and Applications](https://arxiv.org/abs/2510.23235)
*Anton Savostianov,Michael T. Schaub,Benjamin Stamm*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Low-pass graph filters are fundamental for signal processing on graphs and
other non-Euclidean domains. However, the computation of such filters for
parametric graph families can be prohibitively expensive as computation of the
corresponding low-frequency subspaces, requires the repeated solution of an
eigenvalue problem. We suggest a novel algorithm of low-pass graph filter
interpolation based on Riemannian interpolation in normal coordinates on the
Grassmann manifold. We derive an error bound estimate for the subspace
interpolation and suggest two possible applications for induced parametric
graph families. First, we argue that the temporal evolution of the node
features may be translated to the evolving graph topology via a similarity
correction to adjust the homophily degree of the network. Second, we suggest a
dot product graph family induced by a given static graph which allows to infer
improved message passing scheme for node classification facilitated by the
filter interpolation.

</details>


### [237] [Robust Iterative Learning Hidden Quantum Markov Models](https://arxiv.org/abs/2510.23237)
*Ning Ning*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hidden Quantum Markov Models (HQMMs) extend classical Hidden Markov Models to
the quantum domain, offering a powerful probabilistic framework for modeling
sequential data with quantum coherence. However, existing HQMM learning
algorithms are highly sensitive to data corruption and lack mechanisms to
ensure robustness under adversarial perturbations. In this work, we introduce
the Adversarially Corrupted HQMM (AC-HQMM), which formalizes robustness
analysis by allowing a controlled fraction of observation sequences to be
adversarially corrupted. To learn AC-HQMMs, we propose the Robust Iterative
Learning Algorithm (RILA), a derivative-free method that integrates a Remove
Corrupted Rows by Entropy Filtering (RCR-EF) module with an iterative
stochastic resampling procedure for physically valid Kraus operator updates.
RILA incorporates L1-penalized likelihood objectives to enhance stability,
resist overfitting, and remain effective under non-differentiable conditions.
Across multiple HQMM and HMM benchmarks, RILA demonstrates superior convergence
stability, corruption resilience, and preservation of physical validity
compared to existing algorithms, establishing a principled and efficient
approach for robust quantum sequential learning.

</details>


### [238] [GCAO: Group-driven Clustering via Gravitational Attraction and Optimization](https://arxiv.org/abs/2510.23259)
*Qi Li,Jun Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Traditional clustering algorithms often struggle with high-dimensional and
non-uniformly distributed data, where low-density boundary samples are easily
disturbed by neighboring clusters, leading to unstable and distorted clustering
results. To address this issue, we propose a Group-driven Clustering via
Gravitational Attraction and Optimization (GCAO) algorithm. GCAO introduces a
group-level optimization mechanism that aggregates low-density boundary points
into collaboratively moving groups, replacing the traditional point-based
contraction process. By combining local density estimation with neighborhood
topology, GCAO constructs effective gravitational interactions between groups
and their surroundings, enhancing boundary clarity and structural consistency.
Using groups as basic motion units, a gravitational contraction strategy
ensures globally stable and directionally consistent convergence. Experiments
on multiple high-dimensional datasets demonstrate that GCAO outperforms 11
representative clustering methods, achieving average improvements of 37.13%,
52.08%, 44.98%, and 38.81% in NMI, ARI, Homogeneity, and ACC, respectively,
while maintaining competitive efficiency and scalability. These results
highlight GCAO's superiority in preserving cluster integrity, enhancing
boundary separability, and ensuring robust performance on complex data
distributions.

</details>


### [239] [Learning from Frustration: Torsor CNNs on Graphs](https://arxiv.org/abs/2510.23288)
*Daiyuan Li,Shreya Arya,Robert Ghrist*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Most equivariant neural networks rely on a single global symmetry, limiting
their use in domains where symmetries are instead local. We introduce Torsor
CNNs, a framework for learning on graphs with local symmetries encoded as edge
potentials-- group-valued transformations between neighboring coordinate
frames. We establish that this geometric construction is fundamentally
equivalent to the classical group synchronization problem, yielding: (1) a
Torsor Convolutional Layer that is provably equivariant to local changes in
coordinate frames, and (2) the frustration loss--a standalone geometric
regularizer that encourages locally equivariant representations when added to
any NN's training objective. The Torsor CNN framework unifies and generalizes
several architectures--including classical CNNs and Gauge CNNs on manifolds--
by operating on arbitrary graphs without requiring a global coordinate system
or smooth manifold structure. We establish the mathematical foundations of this
framework and demonstrate its applicability to multi-view 3D recognition, where
relative camera poses naturally define the required edge potentials.

</details>


### [240] [Predicting symbolic ODEs from multiple trajectories](https://arxiv.org/abs/2510.23295)
*Yakup Emre Şahin,Niki Kilbertus,Sören Becker*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce MIO, a transformer-based model for inferring symbolic ordinary
differential equations (ODEs) from multiple observed trajectories of a
dynamical system. By combining multiple instance learning with
transformer-based symbolic regression, the model effectively leverages repeated
observations of the same system to learn more generalizable representations of
the underlying dynamics. We investigate different instance aggregation
strategies and show that even simple mean aggregation can substantially boost
performance. MIO is evaluated on systems ranging from one to four dimensions
and under varying noise levels, consistently outperforming existing baselines.

</details>


### [241] [Towards Scaling Deep Neural Networks with Predictive Coding: Theory and Practice](https://arxiv.org/abs/2510.23323)
*Francesco Innocenti*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Backpropagation (BP) is the standard algorithm for training the deep neural
networks that power modern artificial intelligence including large language
models. However, BP is energy inefficient and unlikely to be implemented by the
brain. This thesis studies an alternative, potentially more efficient
brain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks
(PCNs) perform inference by iterative equilibration of neuron activities before
learning or weight updates. Recent work has suggested that this iterative
inference procedure provides a range of benefits over BP, such as faster
training. However, these advantages have not been consistently observed, the
inference and learning dynamics of PCNs are still poorly understood, and deep
PCNs remain practically untrainable. Here, we make significant progress towards
scaling PCNs by taking a theoretical approach grounded in optimisation theory.
First, we show that the learning dynamics of PC can be understood as an
approximate trust-region method using second-order information, despite
explicitly using only first-order local updates. Second, going beyond this
approximation, we show that PC can in principle make use of arbitrarily
higher-order information, such that for feedforward networks the effective
landscape on which PC learns is far more benign and robust to vanishing
gradients than the (mean squared error) loss landscape. Third, motivated by a
study of the inference dynamics of PCNs, we propose a new parameterisation
called ``$\mu$PC'', which for the first time allows stable training of 100+
layer networks with little tuning and competitive performance on simple tasks.
Overall, this thesis significantly advances our fundamental understanding of
the inference and learning dynamics of PCNs, while highlighting the need for
future research to focus on hardware co-design if PC is to compete with BP at
scale.

</details>


### [242] [GRAD: Real-Time Gated Recurrent Anomaly Detection in Autonomous Vehicle Sensors Using Reinforced EMA and Multi-Stage Sliding Window Techniques](https://arxiv.org/abs/2510.23327)
*Mohammad Hossein Jafari Naeimi,Ali Norouzi,Athena Abdi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper introduces GRAD, a real-time anomaly detection method for
autonomous vehicle sensors that integrates statistical analysis and deep
learning to ensure the reliability of sensor data. The proposed approach
combines the Reinforced Exponential Moving Average (REMA), which adapts
smoothing factors and thresholding for outlier detection, with the Multi-Stage
Sliding Window (MS-SW) technique for capturing both short- and long-term
patterns. These features are processed using a lightweight Gated Recurrent Unit
(GRU) model, which detects and classifies anomalies based on bias types, while
a recovery module restores damaged sensor data to ensure continuous system
operation. GRAD has a lightweight architecture consisting of two layers of GRU
with a limited number of neurons that make it appropriate for real-time
applications while maintaining high detection accuracy. The GRAD framework
achieved remarkable performance in anomaly detection and classification. The
model demonstrated an overall F1-score of 97.6% for abnormal data and 99.4% for
normal data, signifying its high accuracy in distinguishing between normal and
anomalous sensor data. Regarding the anomaly classification, GRAD successfully
categorized different anomaly types with high precision, enabling the recovery
module to accurately restore damaged sensor data. Relative to analogous
studies, GRAD surpasses current models by attaining a balance between elevated
detection accuracy and diminished computational expense. These results
demonstrate GRAD's potential as a reliable and efficient solution for real-time
anomaly detection in autonomous vehicle systems, guaranteeing safe vehicle
operation with minimal computational overhead.

</details>


### [243] [Block-Diagonal LoRA for Eliminating Communication Overhead in Tensor Parallel LoRA Serving](https://arxiv.org/abs/2510.23346)
*Xinyu Wang,Jonas M. Kübler,Kailash Budhathoki,Yida Wang,Matthäus Kleindessner*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: When serving a single base LLM with several different LoRA adapters
simultaneously, the adapters cannot simply be merged with the base model's
weights as the adapter swapping would create overhead and requests using
different adapters could not be batched. Rather, the LoRA computations have to
be separated from the base LLM computations, and in a multi-device setup the
LoRA adapters can be sharded in a way that is well aligned with the base
model's tensor parallel execution, as proposed in S-LoRA. However, the S-LoRA
sharding strategy encounters some communication overhead, which may be small in
theory, but can be large in practice. In this paper, we propose to constrain
certain LoRA factors to be block-diagonal, which allows for an alternative way
of sharding LoRA adapters that does not require any additional communication
for the LoRA computations. We demonstrate in extensive experiments that our
block-diagonal LoRA approach is similarly parameter efficient as standard LoRA
(i.e., for a similar number of parameters it achieves similar downstream
performance) and that it leads to significant end-to-end speed-up over S-LoRA.
For example, when serving on eight A100 GPUs, we observe up to 1.79x (1.23x)
end-to-end speed-up with 0.87x (1.74x) the number of adapter parameters for
Llama-3.1-70B, and up to 1.63x (1.3x) end-to-end speed-up with 0.86x (1.73x)
the number of adapter parameters for Llama-3.1-8B.

</details>


### [244] [Robust Non-negative Proximal Gradient Algorithm for Inverse Problems](https://arxiv.org/abs/2510.23362)
*Hanzhang Wang,Zonglin Liu,Jingyi Xu,Chenyang Wang,Zhiwei Zhong,Qiangqiang Shen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Proximal gradient algorithms (PGA), while foundational for inverse problems
like image reconstruction, often yield unstable convergence and suboptimal
solutions by violating the critical non-negativity constraint. We identify the
gradient descent step as the root cause of this issue, which introduces
negative values and induces high sensitivity to hyperparameters. To overcome
these limitations, we propose a novel multiplicative update proximal gradient
algorithm (SSO-PGA) with convergence guarantees, which is designed for
robustness in non-negative inverse problems. Our key innovation lies in
superseding the gradient descent step with a learnable sigmoid-based operator,
which inherently enforces non-negativity and boundedness by transforming
traditional subtractive updates into multiplicative ones. This design,
augmented by a sliding parameter for enhanced stability and convergence, not
only improves robustness but also boosts expressive capacity and noise
immunity. We further formulate a degradation model for multi-modal restoration
and derive its SSO-PGA-based optimization algorithm, which is then unfolded
into a deep network to marry the interpretability of optimization with the
power of deep learning. Extensive numerical and real-world experiments
demonstrate that our method significantly surpasses traditional PGA and other
state-of-the-art algorithms, ensuring superior performance and stability.

</details>


### [245] [The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N Sampling via max@k Optimisation](https://arxiv.org/abs/2510.23393)
*Farid Bagirov,Mikhail Arkhipov,Ksenia Sycheva,Evgeniy Glukhov,Egor Bogomolov*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The application of Reinforcement Learning with Verifiable Rewards (RLVR) to
mathematical and coding domains has demonstrated significant improvements in
the reasoning and problem-solving abilities of Large Language Models. Despite
its success in single generation problem solving, the reinforcement learning
fine-tuning process may harm the model's exploration ability, as reflected in
decreased diversity of generations and a resulting degradation of performance
during Best-of-N sampling for large N values. In this work, we focus on
optimizing the max@k metric, a continuous generalization of pass@k. We derive
an unbiased on-policy gradient estimate for direct optimization of this metric.
Furthermore, we extend our derivations to the off-policy updates, a common
element in modern RLVR algorithms, that allows better sample efficiency.
Empirically, we show that our objective effectively optimizes max@k metric in
off-policy scenarios, aligning the model with the Best-of-N inference strategy.

</details>


### [246] [PrivacyGuard: A Modular Framework for Privacy Auditing in Machine Learning](https://arxiv.org/abs/2510.23427)
*Luca Melis,Matthew Grange,Iden Kalemaj,Karan Chadha,Shengyuan Hu,Elena Kashtelyan,Will Bullock*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The increasing deployment of Machine Learning (ML) models in sensitive
domains motivates the need for robust, practical privacy assessment tools.
PrivacyGuard is a comprehensive tool for empirical differential privacy (DP)
analysis, designed to evaluate privacy risks in ML models through
state-of-the-art inference attacks and advanced privacy measurement techniques.
To this end, PrivacyGuard implements a diverse suite of privacy attack--
including membership inference , extraction, and reconstruction attacks --
enabling both off-the-shelf and highly configurable privacy analyses. Its
modular architecture allows for the seamless integration of new attacks, and
privacy metrics, supporting rapid adaptation to emerging research advances. We
make PrivacyGuard available at
https://github.com/facebookresearch/PrivacyGuard.

</details>


### [247] [Improving Predictions of Molecular Properties with Graph Featurisation and Heterogeneous Ensemble Models](https://arxiv.org/abs/2510.23428)
*Michael L. Parker,Samar Mahmoud,Bailey Montefiore,Mario Öeren,Himani Tandon,Charlotte Wharrick,Matthew D. Segall*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We explore a "best-of-both" approach to modelling molecular properties by
combining learned molecular descriptors from a graph neural network (GNN) with
general-purpose descriptors and a mixed ensemble of machine learning (ML)
models. We introduce a MetaModel framework to aggregate predictions from a
diverse set of leading ML models. We present a featurisation scheme for
combining task-specific GNN-derived features with conventional molecular
descriptors.
  We demonstrate that our framework outperforms the cutting-edge ChemProp model
on all regression datasets tested and 6 of 9 classification datasets. We
further show that including the GNN features derived from ChemProp boosts the
ensemble model's performance on several datasets where it otherwise would have
underperformed. We conclude that to achieve optimal performance across a wide
set of problems, it is vital to combine general-purpose descriptors with
task-specific learned features and use a diverse set of ML models to make the
predictions.

</details>


### [248] [Coresets for Clustering Under Stochastic Noise](https://arxiv.org/abs/2510.23438)
*Lingxiao Huang,Zhize Li,Nisheeth K. Vishnoi,Runkai Yang,Haoyu Zhao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the problem of constructing coresets for $(k, z)$-clustering when
the input dataset is corrupted by stochastic noise drawn from a known
distribution. In this setting, evaluating the quality of a coreset is
inherently challenging, as the true underlying dataset is unobserved. To
address this, we investigate coreset construction using surrogate error metrics
that are tractable and provably related to the true clustering cost. We analyze
a traditional metric from prior work and introduce a new error metric that more
closely aligns with the true cost. Although our metric is defined independently
of the noise distribution, it enables approximation guarantees that scale with
the noise level. We design a coreset construction algorithm based on this
metric and show that, under mild assumptions on the data and noise, enforcing
an $\varepsilon$-bound under our metric yields smaller coresets and tighter
guarantees on the true clustering cost than those obtained via classical
metrics. In particular, we prove that the coreset size can improve by a factor
of up to $\mathrm{poly}(k)$, where $n$ is the dataset size. Experiments on
real-world datasets support our theoretical findings and demonstrate the
practical advantages of our approach.

</details>


### [249] [An Information-Theoretic Analysis of Out-of-Distribution Generalization in Meta-Learning with Applications to Meta-RL](https://arxiv.org/abs/2510.23448)
*Xingtu Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we study out-of-distribution generalization in meta-learning
from an information-theoretic perspective. We focus on two scenarios: (i) when
the testing environment mismatches the training environment, and (ii) when the
training environment is broader than the testing environment. The first
corresponds to the standard distribution mismatch setting, while the second
reflects a broad-to-narrow training scenario. We further formalize the
generalization problem in meta-reinforcement learning and establish
corresponding generalization bounds. Finally, we analyze the generalization
performance of a gradient-based meta-reinforcement learning algorithm.

</details>


### [250] [Schrodinger Neural Network and Uncertainty Quantification: Quantum Machine](https://arxiv.org/abs/2510.23449)
*M. M. Hammad*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce the Schrodinger Neural Network (SNN), a principled architecture
for conditional density estimation and uncertainty quantification inspired by
quantum mechanics. The SNN maps each input to a normalized wave function on the
output domain and computes predictive probabilities via the Born rule. The SNN
departs from standard parametric likelihood heads by learning complex
coefficients of a spectral expansion (e . g ., Chebyshev polynomials) whose
squared modulus yields the conditional density $p(y|x)=\left| \psi _x(y)\right|
{}^2$ with analytic normalization. This representation confers three practical
advantages: positivity and exact normalization by construction, native
multimodality through interference among basis modes without explicit mixture
bookkeeping, and yields closed-form (or efficiently computable)
functionals$-$such as moments and several calibration diagnostics$-$as
quadratic forms in coefficient space. We develop the statistical and
computational foundations of the SNN, including (i) training by exact
maximum-likelihood with unit-sphere coefficient parameterization, (ii)
physics-inspired quadratic regularizers (kinetic and potential energies)
motivated by uncertainty relations between localization and spectral
complexity, (iii) scalable low-rank and separable extensions for multivariate
outputs, (iv) operator-based extensions that represent observables,
constraints, and weak labels as self-adjoint matrices acting on the amplitude
space, and (v) a comprehensive framework for evaluating multimodal predictions.
The SNN provides a coherent, tractable framework to elevate probabilistic
prediction from point estimates to physically inspired amplitude-based
distributions.

</details>


### [251] [SGFusion: Stochastic Geographic Gradient Fusion in Federated Learning](https://arxiv.org/abs/2510.23455)
*Khoa Nguyen,Khang Tran,NhatHai Phan,Cristian Borcea,Rouming Jin,Issa Khalil*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper proposes Stochastic Geographic Gradient Fusion (SGFusion), a novel
training algorithm to leverage the geographic information of mobile users in
Federated Learning (FL). SGFusion maps the data collected by mobile devices
onto geographical zones and trains one FL model per zone, which adapts well to
the data and behaviors of users in that zone. SGFusion models the local
data-based correlation among geographical zones as a hierarchical random graph
(HRG) optimized by Markov Chain Monte Carlo sampling. At each training step,
every zone fuses its local gradient with gradients derived from a small set of
other zones sampled from the HRG. This approach enables knowledge fusion and
sharing among geographical zones in a probabilistic and stochastic gradient
fusion process with self-attention weights, such that "more similar" zones have
"higher probabilities" of sharing gradients with "larger attention weights."
SGFusion remarkably improves model utility without introducing undue
computational cost. Extensive theoretical and empirical results using a
heart-rate prediction dataset collected across 6 countries show that models
trained with SGFusion converge with upper-bounded expected errors and
significantly improve utility in all countries compared to existing approaches
without notable cost in system scalability.

</details>


### [252] [Differential Privacy as a Perk: Federated Learning over Multiple-Access Fading Channels with a Multi-Antenna Base Station](https://arxiv.org/abs/2510.23463)
*Hao Liang,Haifeng Wen,Kaishun Wu,Hong Xing*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated Learning (FL) is a distributed learning paradigm that preserves
privacy by eliminating the need to exchange raw data during training. In its
prototypical edge instantiation with underlying wireless transmissions enabled
by analog over-the-air computing (AirComp), referred to as \emph{over-the-air
FL (AirFL)}, the inherent channel noise plays a unique role of \emph{frenemy}
in the sense that it degrades training due to noisy global aggregation while
providing a natural source of randomness for privacy-preserving mechanisms,
formally quantified by \emph{differential privacy (DP)}. It remains,
nevertheless, challenging to effectively harness such channel impairments, as
prior arts, under assumptions of either simple channel models or restricted
types of loss functions, mostly considering (local) DP enhancement with a
single-round or non-convergent bound on privacy loss. In this paper, we study
AirFL over multiple-access fading channels with a multi-antenna base station
(BS) subject to user-level DP requirements. Despite a recent study, which
claimed in similar settings that artificial noise (AN) must be injected to
ensure DP in general, we demonstrate, on the contrary, that DP can be gained as
a \emph{perk} even \emph{without} employing any AN. Specifically, we derive a
novel bound on DP that converges under general bounded-domain assumptions on
model parameters, along with a convergence bound with general smooth and
non-convex loss functions. Next, we optimize over receive beamforming and power
allocations to characterize the optimal convergence-privacy trade-offs, which
also reveal explicit conditions in which DP is achievable without compromising
training. Finally, our theoretical findings are validated by extensive
numerical results.

</details>


### [253] [Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph Neural Networks](https://arxiv.org/abs/2510.23469)
*Yuhan Yang,Xingbo Fu,Jundong Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In recent years, pre-training Graph Neural Networks (GNNs) through
self-supervised learning on unlabeled graph data has emerged as a widely
adopted paradigm in graph learning. Although the paradigm is effective for
pre-training powerful GNN models, the objective gap often exists between
pre-training and downstream tasks. To bridge this gap, graph prompting adapts
pre-trained GNN models to specific downstream tasks with extra learnable
prompts while keeping the pre-trained GNN models frozen. As recent graph
prompting methods largely focus on enhancing model utility on downstream tasks,
they often overlook fairness concerns when designing prompts for adaptation. In
fact, pre-trained GNN models will produce discriminative node representations
across demographic subgroups, as downstream graph data inherently contains
biases in both node attributes and graph structures. To address this issue, we
propose an Adaptive Dual Prompting (ADPrompt) framework that enhances fairness
for adapting pre-trained GNN models to downstream tasks. To mitigate attribute
bias, we design an Adaptive Feature Rectification module that learns customized
attribute prompts to suppress sensitive information at the input layer,
reducing bias at the source. Afterward, we propose an Adaptive Message
Calibration module that generates structure prompts at each layer, which adjust
the message from neighboring nodes to enable dynamic and soft calibration of
the information flow. Finally, ADPrompt jointly optimizes the two prompting
modules to adapt the pre-trained GNN while enhancing fairness. We conduct
extensive experiments on four datasets with four pre-training strategies to
evaluate the performance of ADPrompt. The results demonstrate that our proposed
ADPrompt outperforms seven baseline methods on node classification tasks.

</details>


### [254] [T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning](https://arxiv.org/abs/2510.23484)
*Julie Mordacq,David Loiseaux,Vicky Kalogeiton,Steve Oudot*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for
learning representations without labeled data, often by enforcing invariance to
input transformations such as rotations or blurring. Recent studies have
highlighted two pivotal properties for effective representations: (i) avoiding
dimensional collapse-where the learned features occupy only a low-dimensional
subspace, and (ii) enhancing uniformity of the induced distribution. In this
work, we introduce T-REGS, a simple regularization framework for SSL based on
the length of the Minimum Spanning Tree (MST) over the learned representation.
We provide theoretical analysis demonstrating that T-REGS simultaneously
mitigates dimensional collapse and promotes distribution uniformity on
arbitrary compact Riemannian manifolds. Several experiments on synthetic data
and on classical SSL benchmarks validate the effectiveness of our approach at
enhancing representation quality.

</details>


### [255] [Learning to Reason Efficiently with Discounted Reinforcement Learning](https://arxiv.org/abs/2510.23486)
*Alex Ayoub,Kavosh Asadi,Dale Schuurmans,Csaba Szepesvári,Karim Bouyarmane*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large reasoning models (LRMs) often consume excessive tokens, inflating
computational cost and latency. We challenge the assumption that longer
responses improve accuracy. By penalizing reasoning tokens using a discounted
reinforcement learning setup (interpretable as a small token cost) and
analyzing Blackwell optimality in restricted policy classes, we encourage
concise yet accurate reasoning. Experiments confirm our theoretical results
that this approach shortens chains of thought while preserving accuracy.

</details>


### [256] [Towards Deep Physics-Informed Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.23501)
*Spyros Rigas,Fotios Anagnostopoulos,Michalis Papachristou,Georgios Alexandridis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Since their introduction, Kolmogorov-Arnold Networks (KANs) have been
successfully applied across several domains, with physics-informed machine
learning (PIML) emerging as one of the areas where they have thrived. In the
PIML setting, Chebyshev-based physics-informed KANs (cPIKANs) have become the
standard due to their computational efficiency. However, like their multilayer
perceptron-based counterparts, cPIKANs face significant challenges when scaled
to depth, leading to training instabilities that limit their applicability to
several PDE problems. To address this, we propose a basis-agnostic, Glorot-like
initialization scheme that preserves activation variance and yields substantial
improvements in stability and accuracy over the default initialization of
cPIKANs. Inspired by the PirateNet architecture, we further introduce
Residual-Gated Adaptive KANs (RGA KANs), designed to mitigate divergence in
deep cPIKANs where initialization alone is not sufficient. Through empirical
tests and information bottleneck analysis, we show that RGA KANs successfully
traverse all training phases, unlike baseline cPIKANs, which stagnate in the
diffusion phase in specific PDE settings. Evaluations on seven standard forward
PDE benchmarks under a fixed training pipeline with adaptive components
demonstrate that RGA KANs consistently outperform parameter-matched cPIKANs and
PirateNets - often by several orders of magnitude - while remaining stable in
settings where the others diverge.

</details>


### [257] [Sequential Multi-Agent Dynamic Algorithm Configuration](https://arxiv.org/abs/2510.23535)
*Chen Lu,Ke Xue,Lei Yuan,Yao Wang,Yaoyuan Wang,Sheng Fu,Chao Qian*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dynamic algorithm configuration (DAC) is a recent trend in automated machine
learning, which can dynamically adjust the algorithm's configuration during the
execution process and relieve users from tedious trial-and-error tuning tasks.
Recently, multi-agent reinforcement learning (MARL) approaches have improved
the configuration of multiple heterogeneous hyperparameters, making various
parameter configurations for complex algorithms possible. However, many complex
algorithms have inherent inter-dependencies among multiple parameters (e.g.,
determining the operator type first and then the operator's parameter), which
are, however, not considered in previous approaches, thus leading to
sub-optimal results. In this paper, we propose the sequential multi-agent DAC
(Seq-MADAC) framework to address this issue by considering the inherent
inter-dependencies of multiple parameters. Specifically, we propose a
sequential advantage decomposition network, which can leverage action-order
information through sequential advantage decomposition. Experiments from
synthetic functions to the configuration of multi-objective optimization
algorithms demonstrate Seq-MADAC's superior performance over state-of-the-art
MARL methods and show strong generalization across problem classes. Seq-MADAC
establishes a new paradigm for the widespread dependency-aware automated
algorithm configuration. Our code is available at
https://github.com/lamda-bbo/seq-madac.

</details>


### [258] [A U-Net and Transformer Pipeline for Multilingual Image Translation](https://arxiv.org/abs/2510.23554)
*Siddharth Sahay,Radhika Agarwal*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents an end-to-end multilingual translation pipeline that
integrates a custom U-Net for text detection, the Tesseract engine for text
recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for
Neural Machine Translation (NMT). Our approach first utilizes a U-Net model,
trained on a synthetic dataset , to accurately segment and detect text regions
from an image. These detected regions are then processed by Tesseract to
extract the source text. This extracted text is fed into a custom Transformer
model trained from scratch on a multilingual parallel corpus spanning 5
languages. Unlike systems reliant on monolithic pre-trained models, our
architecture emphasizes full customization and adaptability. The system is
evaluated on its text detection accuracy, text recognition quality, and
translation performance via BLEU scores. The complete pipeline demonstrates
promising results, validating the viability of a custom-built system for
translating text directly from images.

</details>


### [259] [Lightweight Robust Direct Preference Optimization](https://arxiv.org/abs/2510.23590)
*Cheol Woo Kim,Shresth Verma,Mauricio Tec,Milind Tambe*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Direct Preference Optimization (DPO) has become a popular method for
fine-tuning large language models (LLMs) due to its stability and simplicity.
However, it is also known to be sensitive to noise in the data and prone to
overfitting. Recent works have proposed using distributionally robust
optimization (DRO) to address potential noise and distributional shift in the
data. However, these methods often suffer from excessive conservatism and high
computational cost. We propose DPO-PRO (DPO with Preference Robustness), a
robust fine-tuning algorithm based on DPO which accounts for uncertainty in the
preference distribution through a lightweight DRO formulation. Unlike prior
DRO-based variants, DPO-PRO focuses solely on uncertainty in preferences,
avoiding unnecessary conservatism and incurring negligible computational
overhead. We further show that DPO-PRO is equivalent to a regularized DPO
objective that penalizes model overconfidence under weak preference signals. We
evaluate DPO-PRO on standard alignment benchmarks and a real-world public
health task. Experimental results show that our method consistently improves
robustness to noisy preference signals compared to existing DPO variants.

</details>


### [260] [Representer Theorems for Metric and Preference Learning: Geometric Insights and Algorithms](https://arxiv.org/abs/2304.03720)
*Peyman Morteza*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We develop a mathematical framework to address a broad class of metric and
preference learning problems within a Hilbert space. We obtain a novel
representer theorem for the simultaneous task of metric and preference
learning. Our key observation is that the representer theorem for this task can
be derived by regularizing the problem with respect to the norm inherent in the
task structure. For the general task of metric learning, our framework leads to
a simple and self-contained representer theorem and offers new geometric
insights into the derivation of representer theorems for this task. In the case
of Reproducing Kernel Hilbert Spaces (RKHSs), we illustrate how our representer
theorem can be used to express the solution of the learning problems in terms
of finite kernel terms similar to classical representer theorems. Lastly, our
representer theorem leads to a novel nonlinear algorithm for metric and
preference learning. We compare our algorithm against challenging baseline
methods on real-world rank inference benchmarks, where it achieves competitive
performance. Notably, our approach significantly outperforms vanilla ideal
point methods and surpasses strong baselines across multiple datasets. Code
available at: https://github.com/PeymanMorteza/Metric-Preference-Learning-RKHS

</details>


### [261] [Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection](https://arxiv.org/abs/2305.05239)
*Jiajun Fan,Yuzheng Zhuang,Yuecheng Liu,Jianye Hao,Bin Wang,Jiangcheng Zhu,Hao Wang,Shu-Tao Xia*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The exploration problem is one of the main challenges in deep reinforcement
learning (RL). Recent promising works tried to handle the problem with
population-based methods, which collect samples with diverse behaviors derived
from a population of different exploratory policies. Adaptive policy selection
has been adopted for behavior control. However, the behavior selection space is
largely limited by the predefined policy population, which further limits
behavior diversity. In this paper, we propose a general framework called
Learnable Behavioral Control (LBC) to address the limitation, which a) enables
a significantly enlarged behavior selection space via formulating a hybrid
behavior mapping from all policies; b) constructs a unified learnable process
for behavior selection. We introduce LBC into distributed off-policy
actor-critic methods and achieve behavior control via optimizing the selection
of the behavior mappings with bandit-based meta-controllers. Our agents have
achieved 10077.52% mean human normalized score and surpassed 24 human world
records within 1B training frames in the Arcade Learning Environment, which
demonstrates our significant state-of-the-art (SOTA) performance without
degrading the sample efficiency.

</details>


### [262] [Graph Neural Architecture Search with GPT-4](https://arxiv.org/abs/2310.01436)
*Haishuai Wang,Yang Gao,Xin Zheng,Peng Zhang,Jiajun Bu,Philip S. Yu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Neural Architecture Search (GNAS) has shown promising results in
finding the best graph neural network architecture on a given graph dataset.
However, existing GNAS methods still require intensive human labor and rich
domain knowledge when designing the search space and search strategy. To this
end, we integrate Large Language Models (LLMs) into GNAS and present a new GNAS
model based on LLMs (GNAS-LLM for short). The basic idea of GNAS-LLM is to
design a new class of GNAS prompts for LLMs to guide LLMs towards understanding
the generative task of graph neural architectures. The prompts consist of
descriptions of the search space, search strategy, and search feedback of GNAS.
By iteratively running LLMs with the prompts, GNAS-LLM generates more accurate
graph neural network architectures with fast convergence. Experimental results
show that GNAS-LLM outperforms the state-of-the-art GNAS methods on four
benchmark graph datasets, with an average improvement of 0.7% on the validation
sets and 0.3% on the test sets. Besides, GNAS-LLM achieves an average
improvement of 1.0% on the test sets based on the search space from AutoGEL.

</details>


### [263] [Diffusion Models Meet Contextual Bandits](https://arxiv.org/abs/2402.10028)
*Imad Aouali*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Efficient decision-making in contextual bandits with large action spaces is
challenging, as methods lacking additional prior information may suffer from
computational and statistical inefficiencies. In this work, we leverage
pre-trained diffusion models as priors to capture complex action distributions
and introduce a diffusion-based decision framework for contextual bandits. We
develop practical algorithms to efficiently approximate posteriors under
diffusion priors, enabling flexible decision-making strategies. Empirical
evaluations demonstrate the effectiveness and versatility of our approach
across diverse contextual bandit settings.

</details>


### [264] [REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning](https://arxiv.org/abs/2406.04772)
*Sungho Jeon,Xinyue Ma,Kwang In Kim,Myeongjae Jeon*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent rehearsal-free continual learning (CL) methods guided by prompts
achieve strong performance on vision tasks with non-stationary data but remain
resource-intensive, hindering real-world edge deployment. We introduce
resource-efficient prompting (REP), which improves the computational and memory
efficiency of prompt-based rehearsal-free continual learning methods while
minimizing accuracy trade-offs. Our approach employs swift prompt selection to
refine input data using a carefully provisioned model and introduces adaptive
token merging (AToM) and adaptive layer dropping (ALD) for efficient prompt
updates. AToM and ALD selectively skip data and model layers while preserving
task-specific features during the learning of new tasks. Extensive experiments
on multiple image classification datasets demonstrate REP's superior resource
efficiency over state-of-the-art rehearsal-free CL methods.

</details>


### [265] [R-SFLLM: Jamming Resilient Framework for Split Federated Learning with Large Language Models](https://arxiv.org/abs/2407.11654)
*Aladin Djuhera,Vlad C. Andrei,Xinyang Li,Ullrich J. Mönich,Holger Boche,Walid Saad*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Split federated learning (SFL) is a compute-efficient paradigm in distributed
machine learning (ML), where components of large ML models are outsourced to
remote servers. A significant challenge in SFL, particularly when deployed over
wireless channels, is the susceptibility of transmitted model parameters to
adversarial jamming that could jeopardize the learning process. This is
particularly pronounced for embedding parameters in large language models
(LLMs) and vision language models (VLMs), which are learned feature vectors
essential for domain understanding. In this paper, rigorous insights are
provided into the influence of jamming embeddings in SFL by deriving an
expression for the ML training loss divergence and showing that it is
upper-bounded by the mean squared error (MSE). Based on this analysis, a
physical layer framework is developed for resilient SFL with LLMs (R-SFLLM)
over wireless networks. R-SFLLM leverages wireless sensing data to gather
information on the jamming directions-of-arrival (DoAs) for the purpose of
devising a novel, sensing-assisted anti-jamming strategy while jointly
optimizing beamforming, user scheduling, and resource allocation. Extensive
experiments using both LLMs and VLMs demonstrate R-SFLLM's effectiveness,
achieving close-to-baseline performance across various natural language
processing (NLP) and computer vision (CV) tasks, datasets, and modalities. The
proposed methodology further introduces an adversarial training component,
where controlled noise exposure significantly enhances the model's resilience
to perturbed parameters during training. The results show that more
noise-sensitive models, such as RoBERTa, benefit from this feature, especially
when resource allocation is unfair. It is also shown that worst-case jamming in
particular translates into worst-case model outcomes, thereby necessitating the
need for jamming-resilient SFL protocols.

</details>


### [266] [Centralized Reward Agent for Knowledge Sharing and Transfer in Multi-Task Reinforcement Learning](https://arxiv.org/abs/2408.10858)
*Haozhe Ma,Zhengding Luo,Thanh Vinh Vo,Kuankuan Sima,Tze-Yun Leong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reward shaping is effective in addressing the sparse-reward challenge in
reinforcement learning (RL) by providing immediate feedback through auxiliary,
informative rewards. Based on the reward shaping strategy, we propose a novel
multi-task reinforcement learning framework that integrates a centralized
reward agent (CRA) and multiple distributed policy agents. The CRA functions as
a knowledge pool, aimed at distilling knowledge from various tasks and
distributing it to individual policy agents to improve learning efficiency.
Specifically, the shaped rewards serve as a straightforward metric for encoding
knowledge. This framework not only enhances knowledge sharing across
established tasks but also adapts to new tasks by transferring meaningful
reward signals. We validate the proposed method on both discrete and continuous
domains, including the representative Meta-World benchmark, demonstrating its
robustness in multi-task sparse-reward settings and its effective
transferability to unseen tasks.

</details>


### [267] [Painless Federated Learning: An Interplay of Line-Search and Extrapolation](https://arxiv.org/abs/2408.17145)
*Geetika,Somya Tyagi,Bapi Chatterjee*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The classical line search for learning rate (LR) tuning in the stochastic
gradient descent (SGD) algorithm can tame the convergence slowdown due to
data-sampling noise. In a federated setting, wherein the client heterogeneity
introduces a slowdown to the global convergence, line search can be relevantly
adapted. In this work, we show that a stochastic variant of line search tames
the heterogeneity in federated optimization in addition to that due to
client-local gradient noise. To this end, we introduce Federated Stochastic
Line Search (FedSLS) algorithm and show that it achieves deterministic rates in
expectation. Specifically, FedSLS offers linear convergence for strongly convex
objectives even with partial client participation. Recently, the extrapolation
of the server's LR has shown promise for improved empirical performance for
federated learning. To benefit from extrapolation, we extend FedSLS to
Federated Extrapolated Stochastic Line Search (FedExpSLS) and prove its
convergence. Our extensive empirical results show that the proposed methods
perform at par or better than the popular federated learning algorithms across
many convex and non-convex problems.

</details>


### [268] [AI for Water Sustainability: Global Water Quality Assessment and Prediction with Explainable AI with LLM Chatbot for Insights](https://arxiv.org/abs/2409.10898)
*Biplov Paneru,Bishwash Paneru*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ensuring safe water supplies requires effective water quality monitoring,
especially in developing countries like Nepal, where contamination risks are
high. This paper introduces various hybrid deep learning models to predict on
the CCME dataset with multiple water quality parameters from Canada, China, the
UK, the USA, and Ireland, with 2.82 million data records feature-engineered and
evaluated using them. Models such as CatBoost, XGBoost, and Extra Trees, along
with neural networks combining CNN and LSTM layers, are used to capture
temporal and spatial patterns in the data. The model demonstrated notable
accuracy improvements, aiding proactive water quality control. CatBoost,
XGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values
with an average RMSE of 1.2 and an R squared score of 0.99. Additionally,
classifiers achieved 99% accuracy, cross-validated across models. SHAP analysis
showed the importance of indicators like F.R.C. and orthophosphate levels in
hybrid architectures' classification decisions. The practical application is
demonstrated along with a chatbot application for water quality insights.

</details>


### [269] [MIBP-Cert: Certified Training against Data Perturbations with Mixed-Integer Bilinear Programs](https://arxiv.org/abs/2412.10186)
*Tobias Lorenz,Marta Kwiatkowska,Mario Fritz*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data errors, corruptions, and poisoning attacks during training pose a major
threat to the reliability of modern AI systems. While extensive effort has gone
into empirical mitigations, the evolving nature of attacks and the complexity
of data require a more principled, provable approach to robustly learn on such
data - and to understand how perturbations influence the final model. Hence, we
introduce MIBP-Cert, a novel certification method based on mixed-integer
bilinear programming (MIBP) that computes sound, deterministic bounds to
provide provable robustness even under complex threat models. By computing the
set of parameters reachable through perturbed or manipulated data, we can
predict all possible outcomes and guarantee robustness. To make solving this
optimization problem tractable, we propose a novel relaxation scheme that
bounds each training step without sacrificing soundness. We demonstrate the
applicability of our approach to continuous and discrete data, as well as
different threat models - including complex ones that were previously out of
reach.

</details>


### [270] [Efficient Semi-Supervised Adversarial Training via Latent Clustering-Based Data Reduction](https://arxiv.org/abs/2501.10466)
*Somrita Ghosh,Yuelin Xu,Xiao Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Achieving high model robustness under adversarial settings is widely
recognized as demanding considerable training samples. Recent works propose
semi-supervised adversarial training (SSAT) methods with external unlabeled or
synthetically generated data, which are the current state-of-the-art. However,
SSAT requires substantial extra data to attain high robustness, resulting in
prolonged training time and increased memory usage. In this paper, we propose
unlabeled data reduction strategies to improve the efficiency of SSAT.
Specifically, we design novel latent clustering-based techniques to select or
generate a small critical subset of data samples near the model's decision
boundary. While focusing on boundary-adjacent points, our methods maintain a
balanced ratio between boundary and non-boundary data points to avoid
overfitting. Comprehensive experiments on benchmark datasets demonstrate that
our methods can significantly reduce SSAT's data requirement and computation
costs while preserving its strong robustness advantages. In particular, our
latent-space selection scheme based on k-means clustering and our guided DDPM
fine-tuning approach with LCG-KM are the most effective, achieving nearly
identical robust accuracies with 5x to 10x less unlabeled data and
approximately 4x less total runtime.

</details>


### [271] [FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks](https://arxiv.org/abs/2502.04465)
*Luca Della Libera,Francesco Paissan,Cem Subakan,Mirco Ravanelli*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models have revolutionized natural language processing through
self-supervised pretraining on massive datasets. Inspired by this success,
researchers have explored adapting these methods to speech by discretizing
continuous audio into tokens using neural audio codecs. However, existing
approaches face limitations, including high bitrates, the loss of either
semantic or acoustic information, and the reliance on multi-codebook designs
when trying to capture both, which increases architectural complexity for
downstream tasks. To address these challenges, we introduce FocalCodec, an
efficient low-bitrate codec based on focal modulation that utilizes a single
binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec
delivers competitive performance in speech resynthesis and voice conversion at
lower bitrates than the current state-of-the-art, while effectively handling
multilingual speech and noisy environments. Evaluation on downstream tasks
shows that FocalCodec successfully preserves sufficient semantic and acoustic
information, while also being well-suited for generative modeling. Demo samples
and code are available at https://lucadellalib.github.io/focalcodec-web/.

</details>


### [272] [Technical Debt in In-Context Learning: Diminishing Efficiency in Long Context](https://arxiv.org/abs/2502.04580)
*Taejong Joo,Diego Klabjan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Transformers have demonstrated remarkable in-context learning (ICL)
capabilities, adapting to new tasks by simply conditioning on demonstrations
without parameter updates. Compelling empirical and theoretical evidence
suggests that ICL, as a general-purpose learner, could outperform task-specific
models. However, it remains unclear to what extent the transformers optimally
learn in-context compared to principled learning algorithms. To investigate
this, we employ a meta ICL framework in which each prompt defines a distinctive
regression task whose target function is drawn from a hierarchical
distribution, requiring inference over both the latent model class and
task-specific parameters. Within this setup, we benchmark sample complexity of
ICL against principled learning algorithms, including the Bayes optimal
estimator, under varying performance requirements. Our findings reveal a
striking dichotomy: while ICL initially matches the efficiency of a Bayes
optimal estimator, its efficiency significantly deteriorates in long context.
Through an information-theoretic analysis, we show that the diminishing
efficiency is inherent to ICL. These results clarify the trade-offs in adopting
ICL as a universal problem solver, motivating a new generation of on-the-fly
adaptive methods without the diminishing efficiency.

</details>


### [273] [From Contextual Combinatorial Semi-Bandits to Bandit List Classification: Improved Sample Complexity with Sparse Rewards](https://arxiv.org/abs/2502.09257)
*Liad Erez,Tomer Koren*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the problem of contextual combinatorial semi-bandits, where input
contexts are mapped into subsets of size $m$ of a collection of $K$ possible
actions. In each round, the learner observes the realized reward of the
predicted actions. Motivated by prototypical applications of contextual
bandits, we focus on the $s$-sparse regime where we assume that the sum of
rewards is bounded by some value $s\ll K$. For example, in recommendation
systems the number of products purchased by any customer is significantly
smaller than the total number of available products. Our main result is for the
$(\epsilon,\delta)$-PAC variant of the problem for which we design an algorithm
that returns an $\epsilon$-optimal policy with high probability using a sample
complexity of $\tilde{O}((poly(K/m)+sm/\epsilon^2) \log(|\Pi|/\delta))$ where
$\Pi$ is the underlying (finite) class and $s$ is the sparsity parameter. This
bound improves upon known bounds for combinatorial semi-bandits whenever $s\ll
K$, and in the regime where $s=O(1)$, the leading terms in our bound match the
corresponding full-information rates, implying that bandit feedback essentially
comes at no cost. Our algorithm is also computationally efficient given access
to an ERM oracle for $\Pi$. Our framework generalizes the list multiclass
classification problem with bandit feedback, which can be seen as a special
case with binary reward vectors. In the special case of single-label
classification corresponding to $s=m=1$, we prove an
$O((K^7+1/\epsilon^2)\log(|H|/\delta))$ sample complexity bound, which improves
upon recent results in this scenario. Additionally, we consider the regret
minimization setting where data can be generated adversarially, and establish a
regret bound of $\tilde O(|\Pi|+\sqrt{smT\log |\Pi|})$, extending the result of
Erez et al. (2024) who consider the simpler single label classification
setting.

</details>


### [274] [On Vanishing Gradients, Over-Smoothing, and Over-Squashing in GNNs: Bridging Recurrent and Graph Learning](https://arxiv.org/abs/2502.10818)
*Álvaro Arroyo,Alessio Gravina,Benjamin Gutteridge,Federico Barbero,Claudio Gallicchio,Xiaowen Dong,Michael Bronstein,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Neural Networks (GNNs) are models that leverage the graph structure to
transmit information between nodes, typically through the message-passing
operation. While widely successful, this approach is well known to suffer from
the over-smoothing and over-squashing phenomena, which result in
representational collapse as the number of layers increases and insensitivity
to the information contained at distant and poorly connected nodes,
respectively. In this paper, we present a unified view of these problems
through the lens of vanishing gradients, using ideas from linear control theory
for our analysis. We propose an interpretation of GNNs as recurrent models and
empirically demonstrate that a simple state-space formulation of a GNN
effectively alleviates over-smoothing and over-squashing at no extra trainable
parameter cost. Further, we show theoretically and empirically that (i) GNNs
are by design prone to extreme gradient vanishing even after a few layers; (ii)
Over-smoothing is directly related to the mechanism causing vanishing
gradients; (iii) Over-squashing is most easily alleviated by a combination of
graph rewiring and vanishing gradient mitigation. We believe our work will help
bridge the gap between the recurrent and graph neural network literature and
will unlock the design of new deep and performant GNNs.

</details>


### [275] [Shortcuts and Identifiability in Concept-based Models from a Neuro-Symbolic Lens](https://arxiv.org/abs/2502.11245)
*Samuele Bortolotti,Emanuele Marconato,Paolo Morettin,Andrea Passerini,Stefano Teso*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Concept-based Models are neural networks that learn a concept extractor to
map inputs to high-level concepts and an inference layer to translate these
into predictions. Ensuring these modules produce interpretable concepts and
behave reliably in out-of-distribution is crucial, yet the conditions for
achieving this remain unclear. We study this problem by establishing a novel
connection between Concept-based Models and reasoning shortcuts (RSs), a common
issue where models achieve high accuracy by learning low-quality concepts, even
when the inference layer is fixed and provided upfront. Specifically, we extend
RSs to the more complex setting of Concept-based Models and derive theoretical
conditions for identifying both the concepts and the inference layer. Our
empirical results highlight the impact of RSs and show that existing methods,
even combined with multiple natural mitigation strategies, often fail to meet
these conditions in practice.

</details>


### [276] [KL Penalty Control via Perturbation for Direct Preference Optimization](https://arxiv.org/abs/2502.13177)
*Sangkyu Lee,Janghoon Han,Hosung Song,Stanley Jungkyu Choi,Honglak Lee,Youngjae Yu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Direct Preference Optimization (DPO) demonstrates the advantage of aligning a
large language model with human preference using only an offline dataset.
However, DPO has the limitation that the KL penalty, which prevents excessive
deviation from the reference model, is static throughout the training process.
Several methods claim to change this static KL penalty of DPO into a dynamic
one, but no approach can adaptively assign different KL penalties for each
preference pair. In this paper, we propose $\varepsilon$-Direct Preference
Optimization ($\varepsilon$-DPO), which allows adaptive control of the KL
penalty strength $\beta$ for each preference pair. Specifically,
$\varepsilon$-DPO adaptively controls $\beta$ for each preference pair based on
the monotonicity of logits as a preference model under the perturbation of
$\beta$ during training. This is equivalent to adjusting the KL penalty by
checking whether the change in training-time temperature can lead to better
preference confidence as preference models by simply reusing the logit of the
current policy and the reference policy. Experimental results show that the
simple criterion of $\varepsilon$-DPO for KL penalty relaxation significantly
improves DPO compared to most existing direct alignment algorithms on general
chatbot benchmarks and reveal that this KL penalty control criterion can
reflect confusion as a preference model and provide an efficient KL trade-off,
highlighting the significance of instance-level adaptive KL penalty control in
DPO.

</details>


### [277] [Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting](https://arxiv.org/abs/2502.14704)
*Yuxuan Yang,Dalin Zhang,Yuxuan Liang,Hua Lu,Gang Chen,Huan Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time Series Forecasting (TSF) is a crucial task in various domains, yet
existing TSF models rely heavily on high-quality data and insufficiently
exploit all available data. This paper explores a novel self-supervised
approach to re-label time series datasets by inherently constructing candidate
datasets. During the optimization of a simple reconstruction network,
intermediates are used as pseudo labels in a self-supervised paradigm,
improving generalization for any predictor. We introduce the Self-Correction
with Adaptive Mask (SCAM), which discards overfitted components and selectively
replaces them with pseudo labels generated from reconstructions. Additionally,
we incorporate Spectral Norm Regularization (SNR) to further suppress
overfitting from a loss landscape perspective. Our experiments on eleven
real-world datasets demonstrate that SCAM consistently improves the performance
of various backbone models. This work offers a new perspective on constructing
datasets and enhancing the generalization of TSF models through self-supervised
learning. The code is available at https://github.com/SuDIS-ZJU/SCAM.

</details>


### [278] [Reducing the Representation Error of GAN Image Priors Using the Deep Decoder](https://arxiv.org/abs/2001.08747)
*Mara Daniels,Paul Hand,Reinhard Heckel*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative models, such as GANs, learn an explicit low-dimensional
representation of a particular class of images, and so they may be used as
natural image priors for solving inverse problems such as image restoration and
compressive sensing. GAN priors have demonstrated impressive performance on
these tasks, but they can exhibit substantial representation error for both
in-distribution and out-of-distribution images, because of the mismatch between
the learned, approximate image distribution and the data generating
distribution. In this paper, we demonstrate a method for reducing the
representation error of GAN priors by modeling images as the linear combination
of a GAN prior with a Deep Decoder. The deep decoder is an underparameterized
and most importantly unlearned natural signal model similar to the Deep Image
Prior. No knowledge of the specific inverse problem is needed in the training
of the GAN underlying our method. For compressive sensing and image
superresolution, our hybrid model exhibits consistently higher PSNRs than both
the GAN priors and Deep Decoder separately, both on in-distribution and
out-of-distribution images. This model provides a method for extensibly and
cheaply leveraging both the benefits of learned and unlearned image recovery
priors in inverse problems.

</details>


### [279] [RePO: Understanding Preference Learning Through ReLU-Based Optimization](https://arxiv.org/abs/2503.07426)
*Junkang Wu,Kexin Huang,Xue Wang,Jinyang Gao,Bolin Ding,Jiancan Wu,Xiangnan He,Xiang Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Aligning large language models (LLMs) with human preferences is critical for
real-world deployment, yet existing methods like RLHF face computational and
stability challenges. While DPO establishes an offline paradigm with single
hyperparameter $\beta$, subsequent methods like SimPO reintroduce complexity
through dual parameters ($\beta$, $\gamma$). We propose {ReLU-based Preference
Optimization (RePO)}, a streamlined algorithm that eliminates $\beta$ via two
advances: (1) retaining SimPO's reference-free margins but removing $\beta$
through gradient analysis, and (2) adopting a ReLU-based max-margin loss that
naturally filters trivial pairs. Theoretically, RePO is characterized as
SimPO's limiting case ($\beta \to \infty$), where the logistic weighting
collapses to binary thresholding, forming a convex envelope of the 0-1 loss.
Empirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO
and SimPO across multiple base models, requiring only one hyperparameter to
tune.

</details>


### [280] [OMPQ: Orthogonal Mixed Precision Quantization](https://arxiv.org/abs/2109.07865)
*Yuexiao Ma,Taisong Jin,Xiawu Zheng,Yan Wang,Huixia Li,Yongjian Wu,Guannan Jiang,Wei Zhang,Rongrong Ji*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: To bridge the ever increasing gap between deep neural networks' complexity
and hardware capability, network quantization has attracted more and more
research attention. The latest trend of mixed precision quantization takes
advantage of hardware's multiple bit-width arithmetic operations to unleash the
full potential of network quantization. However, this also results in a
difficult integer programming formulation, and forces most existing approaches
to use an extremely time-consuming search process even with various
relaxations. Instead of solving a problem of the original integer programming,
we propose to optimize a proxy metric, the concept of network orthogonality,
which is highly correlated with the loss of the integer programming but also
easy to optimize with linear programming. This approach reduces the search time
and required data amount by orders of magnitude, with little compromise on
quantization accuracy. Specifically, we achieve 72.08% Top-1 accuracy on
ResNet-18 with 6.7Mb, which does not require any searching iterations. Given
the high efficiency and low data dependency of our algorithm, we used it for
the post-training quantization, which achieve 71.27% Top-1 accuracy on
MobileNetV2 with only 1.5Mb. Our code is available at
https://github.com/MAC-AutoML/OMPQ.

</details>


### [281] [Identifying Trustworthiness Challenges in Deep Learning Models for Continental-Scale Water Quality Prediction](https://arxiv.org/abs/2503.09947)
*Xiaobo Xia,Xiaofeng Liu,Jiale Liu,Kuai Fang,Lu Lu,Samet Oymak,William S. Currie,Tongliang Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Water quality is foundational to environmental sustainability, ecosystem
resilience, and public health. Deep learning offers transformative potential
for large-scale water quality prediction and scientific insights generation.
However, their widespread adoption in high-stakes operational decision-making,
such as pollution mitigation and equitable resource allocation, is prevented by
unresolved trustworthiness challenges, including performance disparity,
robustness, uncertainty, interpretability, generalizability, and
reproducibility. In this work, we present a multi-dimensional, quantitative
evaluation of trustworthiness benchmarking three state-of-the-art deep learning
architectures: recurrent (LSTM), operator-learning (DeepONet), and
transformer-based (Informer), trained on 37 years of data from 482 U.S. basins
to predict 20 water quality variables. Our investigation reveals systematic
performance disparities tied to process complexity, data availability, and
basin heterogeneity. Management-critical variables remain the least predictable
and most uncertain. Robustness tests reveal pronounced sensitivity to outliers
and corrupted targets; notably, the architecture with the strongest baseline
performance (LSTM) proves most vulnerable under data corruption. Attribution
analyses align for simple variables but diverge for nutrients, underscoring the
need for multi-method interpretability. Spatial generalization to ungauged
basins remains poor across all models. This work serves as a timely call to
action for advancing trustworthy data-driven methods for water resources
management and provides a pathway to offering critical insights for
researchers, decision-makers, and practitioners seeking to leverage artificial
intelligence (AI) responsibly in environmental management.

</details>


### [282] [Score-based Generative Neural Networks for Large-Scale Optimal Transport](https://arxiv.org/abs/2110.03237)
*Mara Daniels,Tyler Maunu,Paul Hand*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We consider the fundamental problem of sampling the optimal transport
coupling between given source and target distributions. In certain cases, the
optimal transport plan takes the form of a one-to-one mapping from the source
support to the target support, but learning or even approximating such a map is
computationally challenging for large and high-dimensional datasets due to the
high cost of linear programming routines and an intrinsic curse of
dimensionality. We study instead the Sinkhorn problem, a regularized form of
optimal transport whose solutions are couplings between the source and the
target distribution. We introduce a novel framework for learning the Sinkhorn
coupling between two distributions in the form of a score-based generative
model. Conditioned on source data, our procedure iterates Langevin Dynamics to
sample target data according to the regularized optimal coupling. Key to this
approach is a neural network parametrization of the Sinkhorn problem, and we
prove convergence of gradient descent with respect to network parameters in
this formulation. We demonstrate its empirical success on a variety of large
scale optimal transport tasks.

</details>


### [283] [Out-of-Distribution Generalization in Time Series: A Survey](https://arxiv.org/abs/2503.13868)
*Xin Wu,Fei Teng,Xingwang Li,Ji Zhang,Tianrui Li,Qiang Duan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time series frequently manifest distribution shifts, diverse latent features,
and non-stationary learning dynamics, particularly in open and evolving
environments. These characteristics pose significant challenges for
out-of-distribution (OOD) generalization. While substantial progress has been
made, a systematic synthesis of advancements remains lacking. To address this
gap, we present the first comprehensive review of OOD generalization
methodologies for time series, organized to delineate the field's evolutionary
trajectory and contemporary research landscape. We organize our analysis across
three foundational dimensions: data distribution, representation learning, and
OOD evaluation. For each dimension, we present several popular algorithms in
detail. Furthermore, we highlight key application scenarios, emphasizing their
real-world impact. Finally, we identify persistent challenges and propose
future research directions. A detailed summary of the methods reviewed for the
generalization of OOD in time series can be accessed at
https://tsood-generalization.com.

</details>


### [284] [DMol: A Schedule-Driven Diffusion Model for Highly Efficient and Versatile Molecule Generation](https://arxiv.org/abs/2504.06312)
*Peizhi Niu,Yu-Hsiang Wang,Vishal Rana,Chetan Rupakheti,Abhishek Pandey,Olgica Milenkovic*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a new graph diffusion model for small molecule generation, DMol,
which outperforms the state-of-the-art DiGress model in terms of validity by
roughly 1.5% across all benchmarking datasets while reducing the number of
diffusion steps by at least 10-fold, and the running time to roughly one half.
The performance improvements are a result of a careful change in the objective
function and a graph noise scheduling approach which, at each diffusion step,
allows one to only change a subset of nodes of varying size in the molecule
graph. Another relevant property of the method is that it can be easily
combined with junction-tree-like graph representations that arise by
compressing a collection of relevant ring structures into supernodes. Unlike
classical junction-tree techniques that involve VAEs and require complicated
reconstruction steps, compressed DMol directly performs graph diffusion on a
graph that compresses only a carefully selected set of frequent carbon rings
into supernodes, which results in straightforward sample generation. This
compressed DMol method offers additional validity improvements over generic
DMol of roughly 2%, increases the novelty of the method, and further improves
the running time due to reductions in the graph size.

</details>


### [285] [Censoring chemical data to mitigate dual use risk](https://arxiv.org/abs/2304.10510)
*Quintina L. Campbell,Jonathan Herington,Andrew D. White*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine learning models have dual-use potential, potentially serving both
beneficial and malicious purposes. The development of open-source models in
chemistry has specifically surfaced dual-use concerns around toxicological data
and chemical warfare agents. We discuss a chain risk framework identifying
three misuse pathways and corresponding mitigation strategies: inference-level,
model-level, and data-level. At the data level, we introduce a model-agnostic
noising method to increase prediction error in specific desired regions
(sensitive regions). Our results show that selective noise induces variance and
attenuation bias, whereas simply omitting sensitive data fails to prevent
extrapolation. These findings hold for both molecular feature multilayer
perceptrons and graph neural networks. Thus, noising molecular structures can
enable open sharing of potential dual-use molecular data.

</details>


### [286] [Measuring the (Un)Faithfulness of Concept-Based Explanations](https://arxiv.org/abs/2504.10833)
*Shubham Kumar,Narendra Ahuja*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) translate
a vision model's internal reasoning into human-understandable concepts, leading
to interpretable explanations. However, we find that many state-of-the-art
(SOTA) U-CBEMs are not faithful: their concepts seem interpretable but fail to
reproduce the model's predictions. We argue that this deficiency has gone
unnoticed due to fragmented evaluation - each paper proposes its own
faithfulness measure, with no measure-over-measure comparison or broad
benchmarking. We close this gap by (i) organizing prior metrics in a unified
framework, discussing their limitations, and identifying desiderata for a
faithfulness measure; (ii) introducing the Surrogate Faithfulness (SURF)
measure, which quantifies faithfulness via the predictive loss of a surrogate
that maps explanations to the model's outputs; and (iii) delivering the first
comprehensive U-CBEM faithfulness benchmark across diverse tasks and
architectures. In a controlled setting, SURF outperforms prior faithfulness
measures in measure-over-measure comparisons, and applying SURF to SOTA U-CBEMs
reveals that many visually appealing U-CBEMs are surprisingly unfaithful. We
demonstrate SURF applicability in two downstream settings - (i) faithfulness
versus the number of concepts used in the explanation and (ii) U-CBEM
robustness to adversarial attacks - underscoring SURF's value as a reliable
faithfulness measure. Code to be released.

</details>


### [287] [FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA](https://arxiv.org/abs/2505.12805)
*Seanie Lee,Sangwoo Park,Dong Bok Lee,Dominik Wagner,Haebin Seong,Tobias Bocklet,Juho Lee,Sung Ju Hwang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Low-Rank Adaptation (LoRA), which introduces a product of two trainable
low-rank matrices into frozen pre-trained weights, is widely used for efficient
fine-tuning of language models in federated learning (FL). However, when
combined with differentially private stochastic gradient descent (DP-SGD), LoRA
faces substantial noise amplification: DP-SGD perturbs per-sample gradients,
and the matrix multiplication of the LoRA update ($BA$) intensifies this
effect. Freezing one matrix (e.g., $A$) reduces the noise but restricts model
expressiveness, often resulting in suboptimal adaptation. To address this, we
propose $\texttt{FedSVD}$, a simple yet effective method that introduces a
global reparameterization based on singular value decomposition (SVD). In our
approach, each client optimizes only the $B$ matrix and transmits it to the
server. The server aggregates the $B$ matrices, computes the product $BA$ using
the previous $A$, and refactorizes the result via SVD. This yields a new
adaptive $A$ composed of the orthonormal right singular vectors of $BA$, and an
updated $B$ containing the remaining SVD components. This reparameterization
avoids quadratic noise amplification, while allowing $A$ to better capture the
principal directions of the aggregate updates. Moreover, the orthonormal
structure of $A$ bounds the gradient norms of $B$ and preserves more signal
under DP-SGD, as confirmed by our theoretical analysis. As a result,
$\texttt{FedSVD}$ consistently improves stability and performance across a
variety of privacy settings and benchmarks, outperforming relevant baselines
under both private and non-private regimes.

</details>


### [288] [torchgfn: A PyTorch GFlowNet library](https://arxiv.org/abs/2305.14594)
*Joseph D. Viviano,Omar G. Younis,Sanghyeok Choi,Victor Schmidt,Yoshua Bengio,Salem Lahlou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The growing popularity of generative flow networks (GFlowNets or GFNs) from a
range of researchers with diverse backgrounds and areas of expertise
necessitates a library that facilitates the testing of new features (e.g.,
training losses and training policies) against standard benchmark
implementations, or on a set of common environments. We present torchgfn, a
PyTorch library that aims to address this need. Its core contribution is a
modular and decoupled architecture which treats environments, neural network
modules, and training objectives as interchangeable components. This provides
users with a simple yet powerful API to facilitate rapid prototyping and novel
research. Multiple examples are provided, replicating and unifying published
results. The library is available on GitHub
(https://github.com/GFNOrg/torchgfn) and on pypi
(https://pypi.org/project/torchgfn/).

</details>


### [289] [RLVR-World: Training World Models with Reinforcement Learning](https://arxiv.org/abs/2505.13934)
*Jialong Wu,Shaofeng Yin,Ningya Feng,Mingsheng Long*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: World models predict state transitions in response to actions and are
increasingly developed across diverse modalities. However, standard training
objectives such as maximum likelihood estimation (MLE) often misalign with
task-specific goals of world models, i.e., transition prediction metrics like
accuracy or perceptual quality. In this paper, we present RLVR-World, a unified
framework that leverages reinforcement learning with verifiable rewards (RLVR)
to directly optimize world models for such metrics. Despite formulating world
modeling as autoregressive prediction of tokenized sequences, RLVR-World
evaluates metrics of decoded predictions as verifiable rewards. We demonstrate
substantial performance gains on both language- and video-based world models
across domains, including text games, web navigation, and robot manipulation.
Our work indicates that, beyond recent advances in reasoning language models,
RLVR offers a promising post-training paradigm for enhancing the utility of
generative models more broadly. Code, datasets, models, and video samples are
available at the project website: https://thuml.github.io/RLVR-World.

</details>


### [290] [Optimal and Fair Encouragement Policy Evaluation and Learning](https://arxiv.org/abs/2309.07176)
*Angela Zhou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In consequential domains, it is often impossible to compel individuals to
take treatment, so that optimal policy rules are merely suggestions in the
presence of human non-adherence to treatment recommendations. Under
heterogeneity, covariates may predict take-up of treatment and final outcome,
but differently. While optimal treatment rules optimize causal outcomes across
the population, access parity constraints or other fairness considerations on
who receives treatment can be important. For example, in social services, a
persistent puzzle is the gap in take-up of beneficial services among those who
may benefit from them the most. We study causal identification and robust
estimation of optimal treatment rules, including under potential violations of
positivity. We consider fairness constraints such as demographic parity in
treatment take-up, and other constraints, via constrained optimization. Our
framework can be extended to handle algorithmic recommendations under an
often-reasonable covariate-conditional exclusion restriction, using our
robustness checks for lack of positivity in the recommendation. We develop a
two-stage algorithm for solving over parametrized policy classes under general
constraints to obtain variance-sensitive regret bounds. We illustrate the
methods in three case studies based on data from reminders of SNAP benefits
recertification, randomized encouragement to enroll in insurance, and from
pretrial supervised release with electronic monitoring. While the specific
remedy to inequities in algorithmic allocation is context-specific, it requires
studying both take-up of decisions and downstream outcomes of them.

</details>


### [291] [Adaptive Inference-Time Scaling via Cyclic Diffusion Search](https://arxiv.org/abs/2505.14036)
*Gyubin Lee,Truong Nhat Nguyen Bao,Jaesik Yoon,Dongwoo Lee,Minsu Kim,Yoshua Bengio,Sungjin Ahn*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion models have demonstrated strong generative capabilities across
domains ranging from image synthesis to complex reasoning tasks. However, most
inference-time scaling methods rely on fixed denoising schedules, limiting
their ability to allocate computation based on instance difficulty or
task-specific demands adaptively. We introduce the challenge of adaptive
inference-time scaling-dynamically adjusting computational effort during
inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a
flexible, search-based inference framework. ABCD refines outputs through
bi-directional diffusion cycles while adaptively controlling exploration depth
and termination. It comprises three components: Cyclic Diffusion Search,
Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.
Experiments show that ABCD improves performance across diverse tasks while
maintaining computational efficiency.

</details>


### [292] [COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection](https://arxiv.org/abs/2505.17701)
*Jaewon Cheon,Pilsung Kang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The growing size of large language models has created significant
computational inefficiencies. To address this challenge, sparse activation
methods selectively deactivates non-essential parameters during inference,
reducing computational costs in FFNN layers. While existing methods focus on
non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN
layer lies globally in the form of a linear combination over its internal down
projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,
leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct
coefficients of the linear combination. Experimental results demonstrate that
D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%
ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%
better performance preservation compared to existing methods. Our specialized
kernel implementations effectively realize these theoretical gains into
substantial real-world acceleration.

</details>


### [293] [Generalization Bounds for Robust Contrastive Learning: From Theory to Practice](https://arxiv.org/abs/2311.09671)
*Ngoc N. Tran,Lam Tran,Hoang Phan,Anh Bui,Tung Pham,Toan Tran,Dinh Phung,Trung Le*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Contrastive Learning first extracts features from unlabeled data, followed by
linear probing with labeled data. Adversarial Contrastive Learning (ACL)
integrates Adversarial Training into the first phase to enhance feature
robustness against attacks in the probing phase. While ACL has shown strong
empirical results, its theoretical understanding remains limited. Furthermore,
while a fair amount of theoretical works analyze how the unsupervised loss can
support the supervised loss in the probing phase, none has examined its role to
the robust supervised loss. To fill this gap, our work develops rigorous
theories to identify which components in the unsupervised training can help
improve the robust supervised loss. Specifically, besides the adversarial
contrastive loss, we reveal that the benign one, along with a global divergence
between benign and adversarial examples can also improve robustness. Proper
experiments are conducted to justify our findings.

</details>


### [294] [Performance and Generalizability Impacts of Incorporating Location Encoders into Deep Learning for Dynamic PM2.5 Estimation](https://arxiv.org/abs/2505.18461)
*Morteza Karimzadeh,Zhongying Wang,James L. Crooks*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep learning has shown strong performance in geospatial prediction tasks,
but the role of geolocation information in improving accuracy and
generalizability remains underexamined. Recent work has introduced location
encoders that aim to represent spatial context in a transferable way, yet most
evaluations have focused on static mapping tasks. Here, we study the effect of
incorporating geolocation into deep learning for a dynamic and spatially
heterogeneous application: estimating daily surface-level PM2.5 across the
contiguous United States using satellite and ground-based observations. We
compare three strategies for representing location: excluding geolocation,
using raw latitude and longitude, and using pretrained location encoders. We
evaluate each under within-region and out-of-region generalization settings.
Results show that raw coordinates can improve performance within regions by
supporting spatial interpolation, but can reduce generalizability across
regions. In contrast, pretrained location encoders such as GeoCLIP improve both
predictive accuracy and geographic transfer. However, we also observe spatial
artifacts linked to encoder characteristics, and performance varies across
encoder types (e.g., SatCLIP vs. GeoCLIP). This work provides the first
systematic evaluation of location encoders in a dynamic environmental
estimation context and offers guidance for incorporating geolocation into deep
learning models for geospatial prediction.

</details>


### [295] [Improving Model Fusion by Training-time Neuron Alignment with Fixed Neuron Anchors](https://arxiv.org/abs/2402.01342)
*Zexi Li,Zhiqi Li,Jie Lin,Tao Shen,Jun Xiao,Yike Guo,Tao Lin,Chao Wu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Model fusion aims to integrate several deep neural network (DNN) models'
knowledge into one by fusing parameters, and it has promising applications,
such as improving the generalization of foundation models and parameter
averaging in federated learning. However, models under different settings
(data, hyperparameter, etc.) have diverse neuron permutations; in other words,
from the perspective of loss landscape, they reside in different loss basins,
thus hindering model fusion performances. To alleviate this issue, previous
studies highlighted the role of permutation invariance and have developed
methods to find correct network permutations for neuron alignment after
training. Orthogonal to previous attempts, this paper studies training-time
neuron alignment, improving model fusion without the need for post-matching.
Training-time alignment is cheaper than post-alignment and is applicable in
various model fusion scenarios. Starting from fundamental hypotheses and
theorems, a simple yet lossless algorithm called TNA-PFN is introduced. TNA-PFN
utilizes partially fixed neuron weights as anchors to reduce the potential of
training-time permutations, and it is empirically validated in reducing the
barriers of linear mode connectivity and multi-model fusion. It is also
validated that TNA-PFN can improve the fusion of pretrained models under the
setting of model soup (vision transformers) and ColD fusion (pretrained
language models). Based on TNA-PFN, two federated learning methods, FedPFN and
FedPNU, are proposed, showing the prospects of training-time neuron alignment.
FedPFN and FedPNU reach state-of-the-art performances in federated learning
under heterogeneous settings and can be compatible with the server-side
algorithm.

</details>


### [296] [Preference Optimization by Estimating the Ratio of the Data Distribution](https://arxiv.org/abs/2505.19601)
*Yeongmin Kim,Heesun Bae,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Direct preference optimization (DPO) is widely used as a simple and stable
method for aligning large language models (LLMs) with human preferences. This
paper investigates a generalized DPO loss that enables a policy model to match
the target policy from a likelihood ratio estimation perspective. The ratio of
the target policy provides a unique identification of the policy distribution
without relying on reward models or partition functions. This allows the
generalized loss to retain both simplicity and theoretical guarantees, which
prior work such as $f$-PO fails to achieve simultaneously. We propose Bregman
preference optimization (BPO), a generalized framework for ratio matching that
provides a family of objective functions achieving target policy optimality.
BPO subsumes DPO as a special case and offers tractable forms for all
instances, allowing implementation with a few lines of code. We further develop
scaled Basu's power divergence (SBA), a gradient scaling method that can be
used for BPO instances. The BPO framework complements other DPO variants and is
applicable to target policies defined by these variants. In experiments, unlike
other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a
trade-off between generation fidelity and diversity, instances of BPO improve
both win rate and entropy compared with DPO. When applied to
Llama-3-8B-Instruct, BPO achieves state-of-the-art performance among Llama-3-8B
backbones, with a 55.9\% length-controlled win rate on AlpacaEval2. Project
page: https://github.com/aailab-kaist/BPO.

</details>


### [297] [MobilityGPT: Enhanced Human Mobility Modeling with a GPT model](https://arxiv.org/abs/2402.03264)
*Ammar Haydari,Dongjie Chen,Zhengfeng Lai,Michael Zhang,Chen-Nee Chuah*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative models have shown promising results in capturing human mobility
characteristics and generating synthetic trajectories. However, it remains
challenging to ensure that the generated geospatial mobility data is
semantically realistic, including consistent location sequences, and reflects
real-world characteristics, such as constraining on geospatial limits. We
reformat human mobility modeling as an autoregressive generation task to
address these issues, leveraging the Generative Pre-trained Transformer (GPT)
architecture. To ensure its controllable generation to alleviate the above
challenges, we propose a geospatially-aware generative model, MobilityGPT. We
propose a gravity-based sampling method to train a transformer for semantic
sequence similarity. Then, we constrained the training process via a road
connectivity matrix that provides the connectivity of sequences in trajectory
generation, thereby keeping generated trajectories in geospatial limits.
Lastly, we proposed to construct a preference dataset for fine-tuning
MobilityGPT via Reinforcement Learning from Trajectory Feedback (RLTF)
mechanism, which minimizes the travel distance between training and the
synthetically generated trajectories. Experiments on real-world datasets
demonstrate MobilityGPT's superior performance over state-of-the-art methods in
generating high-quality mobility trajectories that are closest to real data in
terms of origin-destination similarity, trip length, travel radius, link, and
gravity distributions. We release the source code and reference links to
datasets at https://github.com/ammarhydr/MobilityGPT.

</details>


### [298] [On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling](https://arxiv.org/abs/2505.22491)
*Moritz Haas,Sebastian Bordt,Ulrike von Luxburg,Leena Chennuru Vankadara*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scaling limits, such as infinite-width limits, serve as promising theoretical
tools to study large-scale models. However, it is widely believed that existing
infinite-width theory does not faithfully explain the behavior of practical
networks, especially those trained in standard parameterization (SP) meaning He
initialization with a global learning rate. For instance, existing theory for
SP predicts instability at large learning rates and vanishing feature learning
at stable ones. In practice, however, optimal learning rates decay slower than
theoretically predicted and networks exhibit both stable training and
non-trivial feature learning, even at very large widths. Here, we show that
this discrepancy is not fully explained by finite-width phenomena.
  Instead, we find a resolution through a finer-grained analysis of the regime
previously considered unstable and therefore uninteresting. In particular, we
show that, under cross-entropy (CE) loss, the unstable regime comprises two
distinct sub-regimes: a catastrophically unstable regime and a more benign
controlled divergence regime, where logits diverge but gradients and
activations remain stable. Moreover, under large learning rates at the edge of
the controlled divergence regime, there exists a well-defined infinite width
limit where features continue to evolve in all the hidden layers. In
experiments across optimizers, architectures, and data modalities, we validate
that neural networks operate in this controlled divergence regime under CE loss
but not under MSE loss. Our empirical evidence suggests that width-scaling
considerations are surprisingly useful for predicting empirically maximal
stable learning rate exponents which provide useful guidance on optimal
learning rate exponents. Finally, our analysis clarifies the effectiveness and
limitations of recently proposed layerwise learning rate scaling for standard
initialization.

</details>


### [299] [Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration](https://arxiv.org/abs/2505.23859)
*Wenju Sun,Qingyong Li,Wen Wang,Yang Liu,Yangli-ao Geng,Boyang Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-task model merging aims to consolidate knowledge from multiple
fine-tuned task-specific experts into a unified model while minimizing
performance degradation. Existing methods primarily approach this by minimizing
differences between task-specific experts and the unified model, either from a
parameter-level or a task-loss perspective. However, parameter-level methods
exhibit a significant performance gap compared to the upper bound, while
task-loss approaches entail costly secondary training procedures. In contrast,
we observe that performance degradation closely correlates with feature drift,
i.e., differences in feature representations of the same sample caused by model
merging. Motivated by this observation, we propose Layer-wise Optimal Task
Vector Merging (LOT Merging), a technique that explicitly minimizes feature
drift between task-specific experts and the unified model in a layer-by-layer
manner. LOT Merging can be formulated as a convex quadratic optimization
problem, enabling us to analytically derive closed-form solutions for the
parameters of linear and normalization layers. Consequently, LOT Merging
achieves efficient model consolidation through basic matrix operations.
Extensive experiments across vision and vision-language benchmarks demonstrate
that LOT Merging significantly outperforms baseline methods, achieving
improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches. The
source code is available at https://github.com/SunWenJu123/model-merging.

</details>


### [300] [FlowPrecision: Advancing FPGA-Based Real-Time Fluid Flow Estimation with Linear Quantization](https://arxiv.org/abs/2403.01922)
*Tianheng Ling,Julian Hoever,Chao Qian,Gregor Schiele*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In industrial and environmental monitoring, achieving real-time and precise
fluid flow measurement remains a critical challenge. This study applies linear
quantization in FPGA-based soft sensors for fluid flow estimation,
significantly enhancing Neural Network model precision by overcoming the
limitations of traditional fixed-point quantization. Our approach achieves up
to a 10.10% reduction in Mean Squared Error and a notable 9.39% improvement in
inference speed through targeted hardware optimizations. Validated across
multiple data sets, our findings demonstrate that the optimized FPGA-based
quantized models can provide efficient, accurate real-time inference, offering
a viable alternative to cloud-based processing in pervasive autonomous systems.

</details>


### [301] [Machine learning augmented diagnostic testing to identify sources of variability in test performance](https://arxiv.org/abs/2404.03678)
*Christopher J. Banks,Aeron Sanchez,Vicki Stewart,Kate Bowen,Thomas Doherty,Oliver Tearne,Graham Smith,Rowland R. Kao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diagnostic tests that can detect pre-clinical or sub-clinical infection, are
one of the most powerful tools in our armoury of weapons to control infectious
diseases. Considerable effort has been paid to improving diagnostic testing for
human, plant and animal diseases, including strategies for targeting the use of
diagnostic tests towards individuals who are more likely to be infected. We use
machine learning to assess the surrounding risk landscape under which a
diagnostic test is applied to augment its interpretation. We develop this to
predict the occurrence of bovine tuberculosis incidents in cattle herds,
exploiting the availability of exceptionally detailed testing records. We show
that, without compromising test specificity, test sensitivity can be improved
so that the proportion of infected herds detected improves by over 5 percentage
points, or 240 additional infected herds detected in one year beyond those
detected by the skin test alone. We also use feature importance testing for
assessing the weighting of risk factors. While many factors are associated with
increased risk of incidents, of note are several factors that suggest that in
some herds there is a higher risk of infection going undetected.

</details>


### [302] [Mixture-of-Experts Meets In-Context Reinforcement Learning](https://arxiv.org/abs/2506.05426)
*Wenhao Wu,Fuhong Liu,Haoru Li,Zican Hu,Daoyi Dong,Chunlin Chen,Zhi Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In-context reinforcement learning (ICRL) has emerged as a promising paradigm
for adapting RL agents to downstream tasks through prompt conditioning.
However, two notable challenges remain in fully harnessing in-context learning
within RL domains: the intrinsic multi-modality of the state-action-reward data
and the diverse, heterogeneous nature of decision tasks. To tackle these
challenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an
innovative framework that introduces architectural advances of
mixture-of-experts (MoE) into transformer-based decision models. T2MIR
substitutes the feedforward layer with two parallel layers: a token-wise MoE
that captures distinct semantics of input tokens across multiple modalities,
and a task-wise MoE that routes diverse tasks to specialized experts for
managing a broad task distribution with alleviated gradient conflicts. To
enhance task-wise routing, we introduce a contrastive learning method that
maximizes the mutual information between the task and its router
representation, enabling more precise capture of task-relevant information. The
outputs of two MoE components are concatenated and fed into the next layer.
Comprehensive experiments show that T2MIR significantly facilitates in-context
learning capacity and outperforms various types of baselines. We bring the
potential and promise of MoE to ICRL, offering a simple and scalable
architectural enhancement to advance ICRL one step closer toward achievements
in language and vision communities. Our code is available at
https://github.com/NJU-RL/T2MIR.

</details>


### [303] [KV-weights are all you need for skipless transformers](https://arxiv.org/abs/2404.12362)
*Nils Graef*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the
V and P (post-attention projection) linear layers, which reduces the total
number of weights. However, this scheme is only applicable to MHA (multi-head
attention), but not for MQA (multi-query attention) and GQA (grouped-query
attention). The latter schemes are used by many popular LLMs such as Llama 2,
Mistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes
mathematically equivalent versions that are suitable for MQA and GQA. For
example, removing Q and P from a skipless version of Mistral-7B would remove
15% of its weights (and thus reduce its compute and memory complexity). Watch
our explainer video https://youtu.be/Tx_lMpphd2g and see
https://github.com/OpenMachine-ai/transformer-tricks for code and more
transformer tricks.

</details>


### [304] [Zero-shot protein stability prediction by inverse folding models: a free energy interpretation](https://arxiv.org/abs/2506.05596)
*Jes Frellsen,Maher M. Kassem,Tone Bengtsen,Lars Olsen,Kresten Lindorff-Larsen,Jesper Ferkinghoff-Borg,Wouter Boomsma*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Inverse folding models have proven to be highly effective zero-shot
predictors of protein stability. Despite this success, the link between the
amino acid preferences of an inverse folding model and the free-energy
considerations underlying thermodynamic stability remains incompletely
understood. A better understanding would be of interest not only from a
theoretical perspective, but also potentially provide the basis for stronger
zero-shot stability prediction. In this paper, we take steps to clarify the
free-energy foundations of inverse folding models. Our derivation reveals the
standard practice of likelihood ratios as a simplistic approximation and
suggests several paths towards better estimates of the relative stability. We
empirically assess these approaches and demonstrate that considerable gains in
zero-shot performance can be achieved with fairly simple means.

</details>


### [305] [WaveCastNet: Rapid Wavefield Forecasting for Earthquake Early Warning via Deep Sequence to Sequence Learning](https://arxiv.org/abs/2405.20516)
*Dongwei Lyu,Rie Nakata,Pu Ren,Michael W. Mahoney,Arben Pitarka,Nori Nakata,N. Benjamin Erichson*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose a new deep learning model, WaveCastNet, to forecast
high-dimensional wavefields. WaveCastNet integrates a convolutional long
expressive memory architecture into a sequence-to-sequence forecasting
framework, enabling it to model long-term dependencies and multiscale patterns
in both space and time. By sharing weights across spatial and temporal
dimensions, WaveCastNet requires significantly fewer parameters than more
resource-intensive models such as transformers, resulting in faster inference
times. Crucially, WaveCastNet also generalizes better than transformers to rare
and critical seismic scenarios, such as high-magnitude earthquakes. Here, we
show the ability of the model to predict the intensity and timing of
destructive ground motions in real time, using simulated data from the San
Francisco Bay Area. Furthermore, we demonstrate its zero-shot capabilities by
evaluating WaveCastNet on real earthquake data. Our approach does not require
estimating earthquake magnitudes and epicenters, steps that are prone to error
in conventional methods, nor does it rely on empirical ground-motion models,
which often fail to capture strongly heterogeneous wave propagation effects.

</details>


### [306] [Distributional Training Data Attribution: What do Influence Functions Sample?](https://arxiv.org/abs/2506.12965)
*Bruno Mlodozeniec,Isaac Reid,Sam Power,David Krueger,Murat Erdogdu,Richard E. Turner,Roger Grosse*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Randomness is an unavoidable part of training deep learning models, yet
something that traditional training data attribution algorithms fail to
rigorously account for. They ignore the fact that, due to stochasticity in the
initialisation and batching, training on the same dataset can yield different
models. In this paper, we address this shortcoming through introducing
distributional training data attribution (d-TDA), the goal of which is to
predict how the distribution of model outputs (over training runs) depends upon
the dataset. Intriguingly, we find that influence functions (IFs), a popular
data attribution tool, are 'secretly distributional': they emerge from our
framework as the limit to unrolled differentiation, without requiring
restrictive convexity assumptions. This provides a new perspective on the
effectiveness of IFs in deep learning. We demonstrate the practical utility of
d-TDA in experiments, including improving data pruning for vision transformers
and identifying influential examples with diffusion models.

</details>


### [307] [Identifiability of Deep Polynomial Neural Networks](https://arxiv.org/abs/2506.17093)
*Konstantin Usevich,Ricardo Borsoi,Clara Dérand,Marianne Clausel*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric
structure. However, their identifiability -- a key property for ensuring
interpretability -- remains poorly understood. In this work, we present a
comprehensive analysis of the identifiability of deep PNNs, including
architectures with and without bias terms. Our results reveal an intricate
interplay between activation degrees and layer widths in achieving
identifiability. As special cases, we show that architectures with
non-increasing layer widths are generically identifiable under mild conditions,
while encoder-decoder networks are identifiable when the decoder widths do not
grow too rapidly compared to the activation degrees. Our proofs are
constructive and center on a connection between deep PNNs and low-rank tensor
decompositions, and Kruskal-type uniqueness theorems. We also settle an open
conjecture on the dimension of PNN's neurovarieties, and provide new bounds on
the activation degrees required for it to reach the expected dimension.

</details>


### [308] [Thought Anchors: Which LLM Reasoning Steps Matter?](https://arxiv.org/abs/2506.19143)
*Paul C. Bogdan,Uzay Macar,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current frontier large-language models rely on reasoning to achieve
state-of-the-art performance. Many existing interpretability are limited in
this area, as standard methods have been designed to study single forward
passes of a model rather than the multi-token computational steps that unfold
during reasoning. We argue that analyzing reasoning traces at the sentence
level is a promising approach to understanding reasoning processes. We
introduce a black-box method that measures each sentence's counterfactual
importance by repeatedly sampling replacement sentences from the model,
filtering for semantically different ones, and continuing the chain of thought
from that point onwards to quantify the sentence's impact on the distribution
of final answers. We discover that certain sentences can have an outsized
impact on the trajectory of the reasoning trace and final answer. We term these
sentences \textit{thought anchors}. These are generally planning or uncertainty
management sentences, and specialized attention heads consistently attend from
subsequent sentences to thought anchors. We further show that examining
sentence-sentence causal links within a reasoning trace gives insight into a
model's behavior. Such information can be used to predict a problem's
difficulty and the extent different question domains involve sequential or
diffuse reasoning. As a proof-of-concept, we demonstrate that our techniques
together provide a practical toolkit for analyzing reasoning models by
conducting a detailed case study of how the model solves a difficult math
problem, finding that our techniques yield a consistent picture of the
reasoning trace's structure. We provide an open-source tool
(thought-anchors.com) for visualizing the outputs of our methods on further
problems. The convergence across our methods shows the potential of
sentence-level analysis for a deeper understanding of reasoning models.

</details>


### [309] [Efficient Federated Learning against Byzantine Attacks and Data Heterogeneity via Aggregating Normalized Gradients](https://arxiv.org/abs/2408.09539)
*Shiyuan Zuo,Xingrun Yan,Rongfei Fan,Li Shen,Puning Zhao,Jie Xu,Han Hu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated Learning (FL) enables multiple clients to collaboratively train
models without sharing raw data, but is vulnerable to Byzantine attacks and
data heterogeneity, which can severely degrade performance. Existing
Byzantine-robust approaches tackle data heterogeneity, but incur high
computational overhead during gradient aggregation, thereby slowing down the
training process. To address this issue, we propose a simple yet effective
Federated Normalized Gradients Algorithm (Fed-NGA), which performs aggregation
by merely computing the weighted mean of the normalized gradients from each
client. This approach yields a favorable time complexity of $\mathcal{O}(pM)$,
where $p$ is the model dimension and $M$ is the number of clients. We
rigorously prove that Fed-NGA is robust to both Byzantine faults and data
heterogeneity. For non-convex loss functions, Fed-NGA achieves convergence to a
neighborhood of stationary points under general assumptions, and further
attains zero optimality gap under some mild conditions, which is an outcome
rarely achieved in existing literature. In both cases, the convergence rate is
$\mathcal{O}(1/T^{\frac{1}{2} - \delta})$, where $T$ denotes the number of
iterations and $\delta \in (0, 1/2)$. Experimental results on benchmark
datasets confirm the superior time efficiency and convergence performance of
Fed-NGA over existing methods.

</details>


### [310] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu,Xuan Wu,Yizhun Tian,Songhan Fan,Yali Fang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Flight trajectory prediction (FTP) and similar time series tasks typically
require capturing smooth latent dynamics hidden within noisy signals. However,
existing deep learning models face significant challenges of high computational
cost and insufficient interpretability due to their complex black-box nature.
This paper introduces FlightKooba, a novel modeling approach designed to
extract such underlying dynamics analytically. Our framework uniquely
integrates HiPPO theory, Koopman operator theory, and control theory. By
leveraging Legendre polynomial bases, it constructs Koopman operators
analytically, thereby avoiding large-scale parameter training. The method's
core strengths lie in its exceptional computational efficiency and inherent
interpretability. Experiments on multiple public datasets validate our design
philosophy: for signals exhibiting strong periodicity or clear physical laws
(e.g., in aviation, meteorology, and traffic flow), FlightKooba delivers
competitive prediction accuracy while reducing trainable parameters by several
orders of magnitude and achieving the fastest training speed. Furthermore, we
analyze the model's theoretical boundaries, clarifying its inherent low-pass
filtering characteristics that render it unsuitable for sequences dominated by
high-frequency noise. In summary, FlightKooba offers a powerful, efficient, and
interpretable new alternative for time series analysis, particularly in
resource-constrained environments.

</details>


### [311] [PerturBench: Benchmarking Machine Learning Models for Cellular Perturbation Analysis](https://arxiv.org/abs/2408.10609)
*Yan Wu,Esther Wershof,Sebastian M Schmon,Marcel Nassar,Błażej Osiński,Ridvan Eksi,Zichao Yan,Rory Stark,Kun Zhang,Thore Graepel*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a comprehensive framework for modeling single cell
transcriptomic responses to perturbations, aimed at standardizing benchmarking
in this rapidly evolving field. Our approach includes a modular and
user-friendly model development and evaluation platform, a collection of
diverse perturbational datasets, and a set of metrics designed to fairly
compare models and dissect their performance. Through extensive evaluation of
both published and baseline models across diverse datasets, we highlight the
limitations of widely used models, such as mode collapse. We also demonstrate
the importance of rank metrics which complement traditional model fit measures,
such as RMSE, for validating model effectiveness. Notably, our results show
that while no single model architecture clearly outperforms others, simpler
architectures are generally competitive and scale well with larger datasets.
Overall, this benchmarking exercise sets new standards for model evaluation,
supports robust model development, and furthers the use of these models to
simulate genetic and chemical screens for therapeutic discovery.

</details>


### [312] [Curious Causality-Seeking Agents Learn Meta Causal World](https://arxiv.org/abs/2506.23068)
*Zhiyu Zhao,Haoxuan Li,Haifeng Zhang,Jun Wang,Francesco Faccio,Jürgen Schmidhuber,Mengyue Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: When building a world model, a common assumption is that the environment has
a single, unchanging underlying causal rule, like applying Newton's laws to
every situation. In reality, what appears as a drifting causal mechanism is
often the manifestation of a fixed underlying mechanism seen through a narrow
observational window. This brings about a problem that, when building a world
model, even subtle shifts in policy or environment states can alter the very
observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal
Graph} as world models, a minimal unified representation that efficiently
encodes the transformation rules governing how causal structures shift across
different latent world states. A single Meta-Causal Graph is composed of
multiple causal subgraphs, each triggered by meta state, which is in the latent
state space. Building on this representation, we introduce a
\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta
states that trigger each subgraph, (2) discover the corresponding causal
relationships by agent curiosity-driven intervention policy, and (3)
iteratively refine the Meta-Causal Graph through ongoing curiosity-driven
exploration and agent experiences. Experiments on both synthetic tasks and a
challenging robot arm manipulation task demonstrate that our method robustly
captures shifts in causal dynamics and generalizes effectively to previously
unseen contexts.

</details>


### [313] [Reasoning as an Adaptive Defense for Safety](https://arxiv.org/abs/2507.00971)
*Taeyoun Kim,Fahim Tajwar,Aditi Raghunathan,Aviral Kumar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reasoning methods that adaptively allocate test-time compute have advanced
LLM performance on easy to verify domains such as math and code. In this work,
we study how to utilize this approach to train models that exhibit a degree of
robustness to safety vulnerabilities, and show that doing so can provide
benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners
for Safety), a reinforcement learning (RL) approach that trains models to
reason about safety using chain-of-thought traces and a reward signal that
balances safety with task completion. To build TARS, we identify three critical
design choices: (1) a ``lightweight'' warmstart SFT stage, (2) a mix of
harmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as
too many refusals, and (3) a reward function to prevent degeneration of
reasoning capabilities during training. Models trained with TARS exhibit
adaptive behaviors by spending more compute on ambiguous queries, leading to
better safety-refusal trade-offs. They also internally learn to better
distinguish between safe and unsafe prompts and attain greater robustness to
both white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our
work provides an effective, open recipe for training LLMs against jailbreaks
and harmful requests by reasoning per prompt.

</details>


### [314] [Mitigating Distribution Shift in Model-based Offline RL via Shifts-aware Reward Learning](https://arxiv.org/abs/2408.12830)
*Wang Luo,Haoran Li,Zicheng Zhang,Congying Han,Chi Zhou,Jiayu Lv,Tiande Guo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Model-based offline reinforcement learning trains policies using
pre-collected datasets and learned environment models, eliminating the need for
direct real-world environment interaction. However, this paradigm is inherently
challenged by distribution shift~(DS). Existing methods address this issue by
leveraging off-policy mechanisms and estimating model uncertainty, but they
often result in inconsistent objectives and lack a unified theoretical
foundation. This paper offers a comprehensive analysis that disentangles the
problem into two fundamental components: model bias and policy shift. Our
theoretical and empirical investigations reveal how these factors distort value
estimation and restrict policy optimization. To tackle these challenges, we
derive a novel shifts-aware reward through a unified probabilistic inference
framework, which modifies the vanilla reward to refine value learning and
facilitate policy training. Building on this, we develop a practical
implementation that leverages classifier-based techniques to approximate the
adjusted reward for effective policy optimization. Empirical results across
multiple benchmarks demonstrate that the proposed approach mitigates
distribution shift and achieves superior or comparable performance, validating
our theoretical insights.

</details>


### [315] [Echo State Transformer: Attention Over Finite Memories](https://arxiv.org/abs/2507.02917)
*Yannis Bendi-Ouis,Xavier Hinaut*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While Large Language Models and their underlying Transformer architecture are
remarkably efficient, they do not reflect how our brain processes and learns a
diversity of cognitive tasks such as language and working memory. Furthermore,
sequential data processing with Transformers encounters a fundamental barrier:
quadratic complexity growth with sequence length. Motivated by these
limitations, our ambition is to create more efficient models that are less
reliant on intensive computations. We introduce Echo State Transformers (EST),
a hybrid architecture that elegantly resolves this challenge while
demonstrating exceptional performance in classification and detection tasks.
EST integrates the Transformer attention mechanisms with principles from
Reservoir Computing to create a fixed-size window distributed memory system.
Drawing inspiration from Echo State Networks, the most prominent instance of
the Reservoir Computing paradigm, our approach leverages reservoirs (random
recurrent networks) as a lightweight and efficient memory. Our architecture
integrates a new module called ''Working Memory'' based on several reservoirs
working in parallel. These reservoirs work as independent working memory units
with distinct internal dynamics. A novelty here is that the classical reservoir
hyperparameters, controlling the dynamics, are now trained. Thus, the EST
dynamically adapts the reservoir memory/non-linearity trade-off. Thanks to
these working memory units, EST achieves constant computational complexity at
each processing step, effectively breaking the quadratic scaling problem of
standard Transformers. We evaluate ESTs on a recent challenging timeseries
benchmark: the Time Series Library, which comprises 69 tasks across five
categories. Results show that ESTs ranks first overall in two of five
categories, outperforming strong state-of-the-art baselines on classification
and anomaly detection tasks, while remaining competitive on short-term
forecasting. These results position ESTs as a compelling alternative for
time-series classification and anomaly detection, and a practical complement to
transformer-style models in applications that prioritize robust representations
and sensitive event detection.

</details>


### [316] [Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy](https://arxiv.org/abs/2507.06969)
*Bogdan Kulynych,Juan Felipe Gomez,Georgios Kaissis,Jamie Hayes,Borja Balle,Flavio du Pin Calmon,Jean Louis Raisaro*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary,
including worst-case, levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., an accuracy increase
from 52% to 70% in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.

</details>


### [317] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [318] [Ground-Compose-Reinforce: Grounding Language in Agentic Behaviours using Limited Data](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Grounding language in perception and action is a key challenge when building
situated agents that can interact with humans, or other agents, via language.
In the past, addressing this challenge has required manually designing the
language grounding or curating massive datasets that associate language with
the environment. We propose Ground-Compose-Reinforce, an end-to-end,
neurosymbolic framework for training RL agents directly from high-level task
specifications--without manually designed reward functions or other
domain-specific oracles, and without massive datasets. These task
specifications take the form of Reward Machines, automata-based representations
that capture high-level task structure and are in some cases autoformalizable
from natural language. Critically, we show that Reward Machines can be grounded
using limited data by exploiting compositionality. Experiments in a custom
Meta-World domain with only 350 labelled pretraining trajectories show that our
framework faithfully elicits complex behaviours from high-level
specifications--including behaviours that never appear in pretraining--while
non-compositional approaches fail.

</details>


### [319] [DeepVigor+: Scalable and Accurate Semi-Analytical Fault Resilience Analysis for Deep Neural Network](https://arxiv.org/abs/2410.15742)
*Mohammad Hasan Ahmadilivani,Jaan Raik,Masoud Daneshtalab,Maksim Jenihhin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The growing exploitation of Machine Learning (ML) in safety-critical
applications necessitates rigorous safety analysis. Hardware reliability
assessment is a major concern with respect to measuring the level of safety in
ML-based systems. Quantifying the reliability of emerging ML models, including
Convolutional Neural Networks (CNNs), is highly complex due to their enormous
size in terms of the number of parameters and computations. Conventionally,
Fault Injection (FI) is applied to perform a reliability measurement. However,
performing FI on modern-day CNNs is prohibitively time-consuming if an
acceptable confidence level is to be achieved. To speed up FI for large CNNs,
statistical FI (SFI) has been proposed, but its runtimes are still considerably
long.
  In this work, we introduce DeepVigor+, a scalable, fast, and accurate
semi-analytical method as an efficient alternative for reliability measurement
in CNNs. DeepVigor+ implements a fault propagation analysis model and attempts
to acquire Vulnerability Factors (VFs) as reliability metrics in an optimal
way. The results indicate that DeepVigor+ obtains VFs for CNN models with an
error less than $1\%$, i.e., the objective in SFI, but with $14.9$ up to $26.9$
times fewer simulations than the best-known state-of-the-art SFI. DeepVigor+
enables an accurate reliability analysis for large and deep CNNs within a few
minutes, rather than achieving the same results in days or weeks.

</details>


### [320] [A Lightweight Gradient-based Causal Discovery Framework with Applications to Complex Industrial Processes](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [321] [Efficient Adaptive Federated Optimization](https://arxiv.org/abs/2410.18117)
*Su Hyeong Lee,Sidharth Sharma,Manzil Zaheer,Tian Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Adaptive optimization is critical in federated learning, where enabling
adaptivity on both the server and client sides has proven essential for
achieving optimal performance. However, the scalability of such jointly
adaptive systems is often hindered by resource limitations in communication and
memory. In this paper, we introduce a class of efficient adaptive algorithms,
named $FedAda^2$ and its enhanced version $FedAda^2$++, designed specifically
for large-scale, cross-device federated environments. $FedAda^2$ optimizes
communication efficiency by avoiding the transfer of preconditioners between
the server and clients. Additionally, $FedAda^2$++ extends this approach by
incorporating memory-efficient adaptive optimizers on the client side, further
reducing on-device memory usage. Theoretically, we demonstrate that $FedAda^2$
and $FedAda^2$++ achieve the same convergence rates for general, non-convex
objectives as its more resource-intensive counterparts that directly integrate
joint adaptivity. Extensive empirical evaluations on image and text datasets
demonstrate both the advantages of joint adaptivity and the effectiveness of
$FedAda^2$/$FedAda^2$++.

</details>


### [322] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce \textsc{PhysGym}, a novel
benchmark suite and simulation platform for rigorously assessing LLM-based
scientific reasoning in interactive physics environments. \textsc{PhysGym}'s
primary contribution lies in its sophisticated control over the level of prior
knowledge provided to the agent. This allows researchers to dissect agent
performance along axes including the complexity of the problem and the prior
knowledge levels. The benchmark comprises a suite of interactive simulations,
where agents must actively probe environments, gather data sequentially under
constraints and formulate hypotheses about underlying physical laws.
\textsc{PhysGym} provides standardized evaluation protocols and metrics for
assessing hypothesis accuracy and model fidelity. We demonstrate the
benchmark's utility by presenting results from baseline LLMs, showcasing its
ability to differentiate capabilities based on varying priors and task
complexity.

</details>


### [323] [DNN Modularization via Activation-Driven Training](https://arxiv.org/abs/2411.01074)
*Tuan Ngo,Abid Hassan,Saad Shafiq,Nenad Medvidovic*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep Neural Networks (DNNs) tend to accrue technical debt and suffer from
significant retraining costs when adapting to evolving requirements.
Modularizing DNNs offers the promise of improving their reusability. Previous
work has proposed techniques to decompose DNN models into modules both during
and after training. However, these strategies yield several shortcomings,
including significant weight overlaps and accuracy losses across modules,
restricted focus on convolutional layers only, and added complexity and
training time by introducing auxiliary masks to control modularity. In this
work, we propose MODA, an activation-driven modular training approach. MODA
promotes inherent modularity within a DNN model by directly regulating the
activation outputs of its layers based on three modular objectives: intra-class
affinity, inter-class dispersion, and compactness. MODA is evaluated using
three well-known DNN models and five datasets with varying sizes. This
evaluation indicates that, compared to the existing state-of-the-art, using
MODA yields several advantages: (1) MODA accomplishes modularization with 22%
less training time; (2) the resultant modules generated by MODA comprise up to
24x fewer weights and 37x less weight overlap while (3) preserving the original
model's accuracy without additional fine-tuning; in module replacement
scenarios, (4) MODA improves the accuracy of a target class by 12% on average
while ensuring minimal impact on the accuracy of other classes.

</details>


### [324] [DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning](https://arxiv.org/abs/2507.20499)
*Linh Le Pham Van,Minh Hoang Nguyen,Duc Kieu,Hung Le,Hung The Tran,Sunil Gupta*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cross-domain offline reinforcement learning (RL) seeks to enhance sample
efficiency in offline RL by utilizing additional offline source datasets. A key
challenge is to identify and utilize source samples that are most relevant to
the target domain. Existing approaches address this challenge by measuring
domain gaps through domain classifiers, target transition dynamics modeling, or
mutual information estimation using contrastive loss. However, these methods
often require large target datasets, which is impractical in many real-world
scenarios. In this work, we address cross-domain offline RL under a limited
target data setting, identifying two primary challenges: (1) Dataset imbalance,
which is caused by large source and small target datasets and leads to
overfitting in neural network-based domain gap estimators, resulting in
uninformative measurements; and (2) Partial domain overlap, where only a subset
of the source data is closely aligned with the target domain. To overcome these
issues, we propose DmC, a novel framework for cross-domain offline RL with
limited target samples. Specifically, DmC utilizes $k$-nearest neighbor
($k$-NN) based estimation to measure domain proximity without neural network
training, effectively mitigating overfitting. Then, by utilizing this domain
proximity, we introduce a nearest-neighbor-guided diffusion model to generate
additional source samples that are better aligned with the target domain, thus
enhancing policy learning with more effective source samples. Through
theoretical analysis and extensive experiments in diverse MuJoCo environments,
we demonstrate that DmC significantly outperforms state-of-the-art cross-domain
offline RL methods, achieving substantial performance gains.

</details>


### [325] [Hopfield-Fenchel-Young Networks: A Unified Framework for Associative Memory Retrieval](https://arxiv.org/abs/2411.08590)
*Saul Santos,Vlad Niculae,Daniel McNamee,André F. T. Martins*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Associative memory models, such as Hopfield networks and their modern
variants, have garnered renewed interest due to advancements in memory capacity
and connections with self-attention in transformers. In this work, we introduce
a unified framework-Hopfield-Fenchel-Young networks-which generalizes these
models to a broader family of energy functions. Our energies are formulated as
the difference between two Fenchel-Young losses: one, parameterized by a
generalized entropy, defines the Hopfield scoring mechanism, while the other
applies a post-transformation to the Hopfield output. By utilizing Tsallis and
norm entropies, we derive end-to-end differentiable update rules that enable
sparse transformations, uncovering new connections between loss margins,
sparsity, and exact retrieval of single memory patterns. We further extend this
framework to structured Hopfield networks using the SparseMAP transformation,
allowing the retrieval of pattern associations rather than a single pattern.
Our framework unifies and extends traditional and modern Hopfield networks and
provides an energy minimization perspective for widely used
post-transformations like $\ell_2$-normalization and layer normalization-all
through suitable choices of Fenchel-Young losses and by using convex analysis
as a building block. Finally, we validate our Hopfield-Fenchel-Young networks
on diverse memory recall tasks, including free and sequential recall.
Experiments on simulated data, image retrieval, multiple instance learning, and
text rationalization demonstrate the effectiveness of our approach.

</details>


### [326] [Generalized EXTRA stochastic gradient Langevin dynamics](https://arxiv.org/abs/2412.01993)
*Mert Gurbuzbalaban,Mohammad Rafiqul Islam,Xiaoyu Wang,Lingjiong Zhu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Langevin algorithms are popular Markov Chain Monte Carlo methods for Bayesian
learning, particularly when the aim is to sample from the posterior
distribution of a parametric model, given the input data and the prior
distribution over the model parameters. Their stochastic versions such as
stochastic gradient Langevin dynamics (SGLD) allow iterative learning based on
randomly sampled mini-batches of large datasets and are scalable to large
datasets. However, when data is decentralized across a network of agents
subject to communication and privacy constraints, standard SGLD algorithms
cannot be applied. Instead, we employ decentralized SGLD (DE-SGLD) algorithms,
where Bayesian learning is performed collaboratively by a network of agents
without sharing individual data. Nonetheless, existing DE-SGLD algorithms
induce a bias at every agent that can negatively impact performance; this bias
persists even when using full batches and is attributable to network effects.
Motivated by the EXTRA algorithm and its generalizations for decentralized
optimization, we propose the generalized EXTRA stochastic gradient Langevin
dynamics, which eliminates this bias in the full-batch setting. Moreover, we
show that, in the mini-batch setting, our algorithm provides performance bounds
that significantly improve upon those of standard DE-SGLD algorithms in the
literature. Our numerical results also demonstrate the efficiency of the
proposed approach.

</details>


### [327] [Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery](https://arxiv.org/abs/2508.12650)
*Jiyeon Kang,Songseong Kim,Chanhui Lee,Doyeong Hwang,Joanie Hayoun Chung,Yunkyung Ko,Sumin Lee,Sungwoong Kim,Sungbin Lim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ordering-based approaches to causal discovery identify topological orders of
causal graphs, providing scalable alternatives to combinatorial search methods.
Under the Additive Noise Model (ANM) assumption, recent causal ordering methods
based on score matching require an accurate estimation of the Hessian diagonal
of the log-densities. In this paper, we aim to improve the approximation of the
Hessian diagonal of the log-densities, thereby enhancing the performance of
ordering-based causal discovery algorithms. Existing approaches that rely on
Stein gradient estimators are computationally expensive and memory-intensive,
while diffusion-model-based methods remain unstable due to the second-order
derivatives of score models. To alleviate these problems, we propose
Score-informed Neural Operator (SciNO), a probabilistic generative model in
smooth function spaces designed to stably approximate the Hessian diagonal and
to preserve structural information during the score modeling. Empirical results
show that SciNO reduces order divergence by 42.7% on synthetic graphs and by
31.5% on real-world datasets on average compared to DiffAN, while maintaining
memory efficiency and scalability. Furthermore, we propose a probabilistic
control algorithm for causal reasoning with autoregressive models that
integrates SciNO's probability estimates with autoregressive model priors,
enabling reliable data-driven causal ordering informed by semantic information.
Consequently, the proposed method enhances causal reasoning abilities of LLMs
without additional fine-tuning or prompt engineering.

</details>


### [328] [No Free Lunch From Random Feature Ensembles: Scaling Laws and Near-Optimality Conditions](https://arxiv.org/abs/2412.05418)
*Benjamin S. Ruben,William L. Tong,Hamza Tahir Chaudhry,Cengiz Pehlevan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Given a fixed budget for total model size, one must choose between training a
single large model or combining the predictions of multiple smaller models. We
investigate this trade-off for ensembles of random-feature ridge regression
models in both the overparameterized and underparameterized regimes. Using
deterministic equivalent risk estimates, we prove that when a fixed number of
parameters is distributed among $K$ independently trained models, the
ridge-optimized test risk increases with $K$. Consequently, a single large
model achieves optimal performance. We then ask when ensembles can achieve
\textit{near}-optimal performance. In the overparameterized regime, we show
that, to leading order, the test error depends on ensemble size and model size
only through the total feature count, so that overparameterized ensembles
consistently achieve near-optimal performance. To understand underparameterized
ensembles, we derive scaling laws for the test risk as a function of total
parameter count when the ensemble size and parameters per ensemble member are
jointly scaled according to a ``growth exponent'' $\ell$. While the optimal
error scaling is always achieved by increasing model size with a fixed ensemble
size, our analysis identifies conditions on the kernel and task eigenstructure
under which near-optimal scaling laws can be obtained by joint scaling of
ensemble size and model size.

</details>


### [329] [Deriving Transformer Architectures as Implicit Multinomial Regression](https://arxiv.org/abs/2509.04653)
*Jonas A. Actor,Anthony Gruber,Eric C. Cyr*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While attention has been empirically shown to improve model performance, it
lacks a rigorous mathematical justification. This short paper establishes a
novel connection between attention mechanisms and multinomial regression.
Specifically, we show that in a fixed multinomial regression setting,
optimizing over latent features yields solutions that align with the dynamics
induced on features by attention blocks. In other words, the evolution of
representations through a transformer can be interpreted as a trajectory that
recovers the optimal features for classification.

</details>


### [330] [EvoBrain: Dynamic Multi-Channel EEG Graph Modeling for Time-Evolving Brain Networks](https://arxiv.org/abs/2509.15857)
*Rikuto Kotoge,Zheng Chen,Tasuku Kimura,Yasuko Matsubara,Takufumi Yanagisawa,Haruhiko Kishima,Yasushi Sakurai*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dynamic GNNs, which integrate temporal and spatial features in
Electroencephalography (EEG) data, have shown great potential in automating
seizure detection. However, fully capturing the underlying dynamics necessary
to represent brain states, such as seizure and non-seizure, remains a
non-trivial task and presents two fundamental challenges. First, most existing
dynamic GNN methods are built on temporally fixed static graphs, which fail to
reflect the evolving nature of brain connectivity during seizure progression.
Second, current efforts to jointly model temporal signals and graph structures
and, more importantly, their interactions remain nascent, often resulting in
inconsistent performance. To address these challenges, we present the first
theoretical analysis of these two problems, demonstrating the effectiveness and
necessity of explicit dynamic modeling and time-then-graph dynamic GNN method.
Building on these insights, we propose EvoBrain, a novel seizure detection
model that integrates a two-stream Mamba architecture with a GCN enhanced by
Laplacian Positional Encoding, following neurological insights. Moreover,
EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes
and edges to evolve over time. Our contributions include (a) a theoretical
analysis proving the expressivity advantage of explicit dynamic modeling and
time-then-graph over other approaches, (b) a novel and efficient model that
significantly improves AUROC by 23% and F1 score by 30%, compared with the
dynamic GNN baseline, and (c) broad evaluations of our method on the
challenging early seizure prediction tasks.

</details>


### [331] [Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise](https://arxiv.org/abs/2509.18001)
*Haocheng Luo,Mehrtash Harandi,Dinh Phung,Trung Le*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Sharpness-aware minimization (SAM) has emerged as a highly effective
technique for improving model generalization, but its underlying principles are
not fully understood. We investigated the phenomenon known as m-sharpness,
where the performance of SAM improves monotonically as the micro-batch size for
computing perturbations decreases. In practice, the empirical m-sharpness
effect underpins the deployment of SAM in distributed training, yet a rigorous
theoretical account has remained lacking. To provide a theoretical explanation
for m-sharpness, we leverage an extended Stochastic Differential Equation (SDE)
framework and analyze the structure of stochastic gradient noise (SGN) to
characterize the dynamics of various SAM variants, including n-SAM and m-SAM.
Our findings reveal that the stochastic noise introduced during SAM
perturbations inherently induces a variance-based sharpness regularization
effect. Motivated by our theoretical insights, we introduce Reweighted SAM
(RW-SAM), which employs sharpness-weighted sampling to mimic the generalization
benefits of m-SAM while remaining parallelizable. Comprehensive experiments
validate the effectiveness of our theoretical analysis and proposed method.

</details>


### [332] [Automatic Discovery of One Parameter Subgroups of $SO(n)$](https://arxiv.org/abs/2509.22219)
*Pavan Karjol,Vivek V Kashyap,Rohan Kashyap,Prathosh A P*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a novel framework for the automatic discovery of one-parameter
subgroups ($H_{\gamma}$) of $SO(3)$ and, more generally, $SO(n)$. One-parameter
subgroups of $SO(n)$ are crucial in a wide range of applications, including
robotics, quantum mechanics, and molecular structure analysis. Our method
utilizes the standard Jordan form of skew-symmetric matrices, which define the
Lie algebra of $SO(n)$, to establish a canonical form for orbits under the
action of $H_{\gamma}$. This canonical form is then employed to derive a
standardized representation for $H_{\gamma}$-invariant functions. By learning
the appropriate parameters, the framework uncovers the underlying one-parameter
subgroup $H_{\gamma}$. The effectiveness of the proposed approach is
demonstrated through tasks such as double pendulum modeling, moment of inertia
prediction, top quark tagging and invariant polynomial regression, where it
successfully recovers meaningful subgroup structure and produces interpretable,
symmetry-aware representations.

</details>


### [333] [Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints](https://arxiv.org/abs/2510.04951)
*Jayanta Mandi,Marianne Defresne,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: When some parameters of a constrained optimization problem (COP) are
uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising
two stages: the prediction of the unknown parameters from contextual
information and the subsequent optimization using those predicted parameters.
Decision-focused learning (DFL) implements the first stage by training a
machine learning (ML) model to optimize the quality of the decisions made using
the predicted parameters. When the predicted parameters occur in the
constraints, they can lead to infeasible solutions. Therefore, it is important
to simultaneously manage both feasibility and decision quality. We develop a
DFL framework for predicting constraint parameters in a generic COP. While
prior works typically assume that the underlying optimization problem is a
linear program (LP) or integer LP (ILP), our approach makes no such assumption.
We derive two novel loss functions based on maximum likelihood estimation
(MLE): the first one penalizes infeasibility (by penalizing predicted
parameters that lead to infeasible solutions), while the second one penalizes
suboptimal decisions (by penalizing predicted parameters that make the true
optimal solution infeasible). We introduce a single tunable parameter to form a
weighted average of the two losses, allowing decision-makers to balance
suboptimality and feasibility. We experimentally demonstrate that adjusting
this parameter provides decision-makers control over this trade-off. Moreover,
across several COP instances, we show that adjusting the tunable parameter
allows a decision-maker to prioritize either suboptimality or feasibility,
outperforming the performance of existing baselines in either objective.

</details>


### [334] [E2Former: An Efficient and Equivariant Transformer with Linear-Scaling Tensor Products](https://arxiv.org/abs/2501.19216)
*Yunyang Li,Lin Huang,Zhihao Ding,Chu Wang,Xinran Wei,Han Yang,Zun Wang,Chang Liu,Yu Shi,Peiran Jin,Tao Qin,Mark Gerstein,Jia Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Equivariant Graph Neural Networks (EGNNs) have demonstrated significant
success in modeling microscale systems, including those in chemistry, biology
and materials science. However, EGNNs face substantial computational challenges
due to the high cost of constructing edge features via spherical tensor
products, making them impractical for large-scale systems. To address this
limitation, we introduce E2Former, an equivariant and efficient transformer
architecture that incorporates the Wigner $6j$ convolution (Wigner $6j$ Conv).
By shifting the computational burden from edges to nodes, the Wigner $6j$ Conv
reduces the complexity from $O(|\mathcal{E}|)$ to $ O(| \mathcal{V}|)$ while
preserving both the model's expressive power and rotational equivariance. We
show that this approach achieves a 7x-30x speedup compared to conventional
$\mathrm{SO}(3)$ convolutions. Furthermore, our empirical results demonstrate
that the derived E2Former mitigates the computational challenges of existing
approaches without compromising the ability to capture detailed geometric
information. This development could suggest a promising direction for scalable
and efficient molecular modeling.

</details>


### [335] [Covering Multiple Objectives with a Small Set of Solutions Using Bayesian Optimization](https://arxiv.org/abs/2501.19342)
*Natalie Maus,Kyurae Kim,Yimeng Zeng,Haydn Thomas Jones,Fangping Wan,Marcelo Der Torossian Torres,Cesar de la Fuente-Nunez,Jacob R. Gardner*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In multi-objective black-box optimization, the goal is typically to find
solutions that optimize a set of $T$ black-box objective functions, $f_1,
\ldots f_T$, simultaneously. Traditional approaches often seek a single
Pareto-optimal set that balances trade-offs among all objectives. In contrast,
we consider a problem setting that departs from this paradigm: finding a small
set of $K < T$ solutions, that collectively "cover" the $T$ objectives. A set
of solutions is defined as "covering" if, for each objective $f_1, \ldots f_T$,
there is at least one good solution. A motivating example for this problem
setting occurs in drug design. For example, we may have $T$ pathogens and aim
to identify a set of $K < T$ antibiotics such that at least one antibiotic can
be used to treat each pathogen. This problem, known as coverage optimization,
has yet to be tackled with the Bayesian optimization (BO) framework. To fill
this void, we develop Multi-Objective Coverage Bayesian Optimization (MOCOBO),
a BO algorithm for solving coverage optimization. Our approach is based on a
new acquisition function reminiscent of expected improvement in the vanilla BO
setup. We demonstrate the performance of our method on high-dimensional
black-box optimization tasks, including applications in peptide and molecular
design. Results show that the coverage of the $K < T$ solutions found by MOCOBO
matches or nearly matches the coverage of $T$ solutions obtained by optimizing
each objective individually. Furthermore, in in vitro experiments, the peptides
found by MOCOBO exhibited high potency against drug-resistant pathogens,
further demonstrating the potential of MOCOBO for drug discovery. All of our
code is publicly available at the following link:
https://github.com/nataliemaus/mocobo.

</details>


### [336] [MIN-Merging: Merge the Important Neurons for Model Merging](https://arxiv.org/abs/2510.17890)
*Yunfei Liang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in deep learning have led to a surge of open-source models
across diverse domains. While model merging offers a promising way to combine
their strengths, existing approaches often suffer from parameter conflicts that
degrade performance on domain-specific tasks. We propose MIN-Merging, a
router-based framework that selectively merges the most important neurons to
reduce such conflicts. Extensive experiments on Computer Vision(CV) and Natural
Language Processing(NLP) benchmarks show that MIN-Merging achieves consistent
gains on in-domain tasks while retaining the generalization ability of
pretrained models on out-of-domain tasks. These results highlight its
effectiveness as a practical solution to the parameter conflict problem in
model merging.

</details>


### [337] [Regularized Langevin Dynamics for Combinatorial Optimization](https://arxiv.org/abs/2502.00277)
*Shengyu Feng,Yiming Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work proposes a simple yet effective sampling framework for
combinatorial optimization (CO). Our method builds on discrete Langevin
dynamics (LD), an efficient gradient-guided generative paradigm. However, we
observe that directly applying LD often leads to limited exploration. To
overcome this limitation, we propose the Regularized Langevin Dynamics (RLD),
which enforces an expected distance between the sampled and current solutions,
effectively avoiding local minima. We develop two CO solvers on top of RLD, one
based on simulated annealing (SA), and the other one based on neural network
(NN). Empirical results on three classic CO problems demonstrate that both of
our methods can achieve comparable or better performance against the previous
state-of-the-art (SOTA) SA- and NN-based solvers. In particular, our SA
algorithm reduces the runtime of the previous SOTA SA method by up to 80\%,
while achieving equal or superior performance. In summary, RLD offers a
promising framework for enhancing both traditional heuristics and NN models to
solve CO problems. Our code is available at
https://github.com/Shengyu-Feng/RLD4CO.

</details>


### [338] [ADPO: Anchored Direct Preference Optimization](https://arxiv.org/abs/2510.18913)
*Wang Zixian*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Direct Preference Optimization (DPO) is an efficient alternative to
reinforcement learning from human feedback (RLHF), yet it typically assumes
hard binary labels and pairwise comparisons. Such assumptions can be brittle
under noisy or distribution-shifted supervision. We present Anchored Direct
Preference Optimization (ADPO), which (i) incorporates soft preference
probabilities, (ii) aligns policy updates through reference anchoring that
induces an implicit trust region, and (iii) extends to listwise learning via
Plackett-Luce modeling. In controlled synthetic setups covering 12 scenarios (4
noise types x 3 severities) and 3 model scales, ADPO exhibits relative
improvements ranging from 12% to 79% over a standard DPO baseline (10-seed
means; 95% CIs in the Appendix). Hard labels tend to fare better under severe
noise, whereas soft labels yield better calibration under distribution shift;
listwise variants achieve the highest WinMass (expected probability mass on the
ground-truth best item) in 9/12 scenarios. Larger models amplify ADPO's
benefits (0.718 vs. 0.416 at hidden=256), suggesting that anchoring acts as an
effective trust-region regularizer. We release code and configurations to
facilitate reproducibility.

</details>


### [339] [FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation](https://arxiv.org/abs/2502.01068)
*Dongwon Jo,Jiwon Song,Yulhwa Kim,Jae-Joon Kim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While large language models (LLMs) excel at handling long-context sequences,
they require substantial prefill computation and key-value (KV) cache, which
can heavily burden computational efficiency and memory usage in both prefill
and decoding stages. Recent works that compress KV caches with prefill
acceleration reduce this cost but inadvertently tie the prefill compute
reduction to the decoding KV budget. This coupling arises from overlooking the
layer-dependent variation of critical context, often leading to accuracy
degradation. To address this issue, we introduce FastKV, a KV cache compression
framework designed to reduce latency in both prefill and decoding by leveraging
the stabilization of token importance in later layers. FastKV performs
full-context computation until a Token-Selective Propagation (TSP) layer, which
forwards only the most informative tokens to subsequent layers. From these
propagated tokens, FastKV independently selects salient KV entries for caching,
thereby decoupling KV budget from the prefill compute reduction based on the
TSP decision. This independent control of the TSP rate and KV retention rate
enables flexible optimization of efficiency and accuracy. Experimental results
show that FastKV achieves speedups of up to 1.82$\times$ in prefill and
2.87$\times$ in decoding compared to the full-context baseline, while matching
the accuracy of the baselines that only accelerate the decoding stage. Our code
is available at https://github.com/dongwonjo/FastKV.

</details>


### [340] [Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients](https://arxiv.org/abs/2510.18924)
*Omar El Mansouri,Mohamed El Amine Seddik,Salem Lahlou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning from human feedback (RLHF) or verifiable rewards
(RLVR), the standard paradigm for aligning LLMs or building recent SOTA
reasoning models, is highly sensitive to noise from inconsistent or erroneous
rewards. Yet, the interaction between such noise and widely used group-based
policy optimization methods remains underexplored. We introduce a noise-robust
Group Relative Policy Optimization (GRPO) and Done Right GRPO (Dr.GRPO)
framework that explicitly models reward corruption as Bernoulli noise. Our
method applies noise correction after estimating reward flip probabilities to
debias the learning signal, yielding provably unbiased gradient estimates.
Theoretical analysis shows that group-based methods inherently mitigate
individual-level noise, and our correction strategy amplifies this robustness.
Empirically, we observe consistent improvements across math and code tasks when
applying our noise correction to standard reward model usage, with particular
gains of up to 6.7 percentage points in accuracy on math tasks and 1.5 on code
tasks under realistic reward model conditions. This work bridges label-noise
correction from supervised learning with modern RLHF, offering both theoretical
insights and a practical algorithm for noisy real-world deployment.

</details>


### [341] [SubTrack++ : Gradient Subspace Tracking for Scalable LLM Training](https://arxiv.org/abs/2502.01586)
*Sahar Rajabi,Nayeema Nonta,Sirisha Rambhatla*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Training large language models (LLMs) is highly resource-intensive due to
their massive number of parameters and the overhead of optimizer states. While
recent work has aimed to reduce memory consumption, such efforts often entail
trade-offs among memory efficiency, training time, and model performance. Yet,
true democratization of LLMs requires simultaneous progress across all three
dimensions. To this end, we propose SubTrack++ that leverages Grassmannian
gradient subspace tracking combined with projection-aware optimizers, enabling
Adam's internal statistics to adapt to subspace changes. Additionally,
employing recovery scaling, a technique that restores information lost through
low-rank projections, further enhances model performance. Our method
demonstrates SOTA convergence by exploiting Grassmannian geometry, reducing
pre-training wall-time by up to 65% and fine-tuning time by 36% compared to
existing SOTA methods, while maintaining the same memory footprint.

</details>


### [342] [What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning](https://arxiv.org/abs/2510.19099)
*Yaning Jia,Chunhui Zhang,Xingjian Diao,Xiangchi Yuan,Zhongyu Ouyang,Chiyu Ma,Soroush Vosoughi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Curriculum learning (CL) - ordering training data from easy to hard - has
become a popular strategy for improving reasoning in large language models
(LLMs). Yet prior work employs disparate difficulty metrics and training
setups, leaving open fundamental questions: When does curriculum help? Which
direction - forward or reverse - is better? And does the answer depend on what
we measure? We address these questions through a unified offline evaluation
framework that decomposes curriculum difficulty into five complementary
dimensions: Problem Difficulty, Model Surprisal, Confidence Margin, Predictive
Uncertainty, and Decision Variability. Through controlled post-training
experiments on mathematical reasoning benchmarks with Llama3.1-8B, Mistral-7B,
and Gemma3-4B, we find that (i) no curriculum strategy dominates universally -
the relative effectiveness of forward versus reverse CL depends jointly on
model capability and task complexity; (ii) even within a single metric, samples
at different difficulty levels produce distinct gains depending on task
demands; and (iii) task-aligned curricula focus on shaping the model's final
representations and generalization, whereas inner-state curricula modulate
internal states such as confidence and uncertainty. Our findings challenge the
notion of a universal curriculum strategy and offer actionable guidance across
model and task regimes, with some metrics indicating that prioritizing
decision-uncertain samples can further enhance learning outcomes.

</details>


### [343] [Diffusion Generative Modeling on Lie Group Representations](https://arxiv.org/abs/2502.02513)
*Marco Bertolini,Tuan Le,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a novel class of score-based diffusion processes that operate
directly in the representation space of Lie groups. Leveraging the framework of
Generalized Score Matching, we derive a class of Langevin dynamics that
decomposes as a direct sum of Lie algebra representations, enabling the
modeling of any target distribution on any (non-Abelian) Lie group. Standard
score-matching emerges as a special case of our framework when the Lie group is
the translation group. We prove that our generalized generative processes arise
as solutions to a new class of paired stochastic differential equations (SDEs),
introduced here for the first time. We validate our approach through
experiments on diverse data types, demonstrating its effectiveness in
real-world applications such as SO(3)-guided molecular conformer generation and
modeling ligand-specific global SE(3) transformations for molecular docking,
showing improvement in comparison to Riemannian diffusion on the group itself.
We show that an appropriate choice of Lie group enhances learning efficiency by
reducing the effective dimensionality of the trajectory space and enables the
modeling of transitions between complex data distributions.

</details>


### [344] [Imbalanced Gradients in RL Post-Training of Multi-Task LLMs](https://arxiv.org/abs/2510.19178)
*Runzhe Wu,Ankur Samanta,Ayush Jain,Scott Fujimoto,Jeongyeol Kwon,Ben Kretzu,Youliang Yu,Kaveh Hassani,Boris Vidolov,Yonathan Efroni*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-task post-training of large language models (LLMs) is typically
performed by mixing datasets from different tasks and optimizing them jointly.
This approach implicitly assumes that all tasks contribute gradients of similar
magnitudes; when this assumption fails, optimization becomes biased toward
large-gradient tasks. In this paper, however, we show that this assumption
fails in RL post-training: certain tasks produce significantly larger
gradients, thus biasing updates toward those tasks. Such gradient imbalance
would be justified only if larger gradients implied larger learning gains on
the tasks (i.e., larger performance improvements) -- but we find this is not
true. Large-gradient tasks can achieve similar or even much lower learning
gains than small-gradient ones. Further analyses reveal that these gradient
imbalances cannot be explained by typical training statistics such as training
rewards or advantages, suggesting that they arise from the inherent differences
between tasks. This cautions against naive dataset mixing and calls for future
work on principled gradient-level corrections for LLMs.

</details>


### [345] [Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning](https://arxiv.org/abs/2502.02770)
*Chaofan Lin,Jiaming Tang,Shuo Yang,Hanshuo Wang,Tian Tang,Boyu Tian,Ion Stoica,Mingyu Gao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Leveraging attention sparsity to accelerate long-context large language
models (LLMs) has been a hot research topic. However, current algorithms such
as sparse attention or key-value (KV) cache compression tend to use a fixed
budget, which presents a significant challenge during deployment because it
fails to account for the dynamic nature of real-world scenarios, where the
optimal balance between accuracy and efficiency can vary greatly. In this
paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse
attention can surprisingly achieve adaptive budgeting. Based on this, we
propose Twilight, a framework to bring adaptive sparsity to any existing sparse
attention algorithm without sacrificing their accuracy. Empirical results show
that Twilight can adaptively prune at most 98% of redundant tokens, leading to
$15.4\times$ acceleration in self-attention operations and $3.9\times$
acceleration in end-to-end per token latency in long context LLM decoding.

</details>


### [346] [ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models](https://arxiv.org/abs/2510.20084)
*Bosong Huang,Ming Jin,Yuxuan Liang,Johan Barthelemy,Debo Cheng,Qingsong Wen,Chenghao Liu,Shirui Pan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.

</details>


### [347] [Bilevel ZOFO: Bridging Parameter-Efficient and Zeroth-Order Techniques for Efficient LLM Fine-Tuning and Meta-Training](https://arxiv.org/abs/2502.03604)
*Reza Shirkavand,Peiran Yu,Qi He,Heng Huang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fine-tuning pre-trained Large Language Models (LLMs) for downstream tasks
using First-Order (FO) optimizers presents significant computational
challenges. Parameter-Efficient Fine-Tuning (PEFT) methods address these by
freezing most model parameters and training only a small subset. However, PEFT
often underperforms compared to full fine-tuning when high task-specific
accuracy is required. Zeroth-Order (ZO) methods fine-tune the entire
pre-trained model without back-propagation, estimating gradients through
forward passes only. While memory-efficient, ZO methods suffer from slow
convergence and high sensitivity to prompt selection. We bridge these two
worlds with Bilevel-ZOFO, a bilevel optimization method that couples fast,
local FO-PEFT adaptation at the inner level with stable, memory-efficient ZO
updates of the full backbone at the outer level. The FO-PEFT inner loop
performs fast, low-memory local adaptation that reduces the variance of ZO
estimates and stabilizes the search, guiding the outer ZO updates of the full
backbone and reducing prompt sensitivity. In the mean time, the outer ZO
provides better generalization ability for PEFT. We provide theoretical
convergence guarantees and empirically demonstrate that Bilevel-ZOFO
significantly outperforms existing ZO and FO-PEFT methods, achieving 2-4 times
faster training while maintaining similar memory efficiency. Additionally, we
show by updating the backbone with ZO and adapting only a tiny FO-PEFT block
per task, Bilevel-ZOFO combines full-model capacity with few-shot efficiency,
making it a very efficient meta-learning algorithm that quickly adapts to new
tasks.

</details>


### [348] [Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset](https://arxiv.org/abs/2510.20209)
*Shumin Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The development of accessible screening tools for early cancer detection in
dogs represents a significant challenge in veterinary medicine. Routine
laboratory data offer a promising, low-cost source for such tools, but their
utility is hampered by the non-specificity of individual biomarkers and the
severe class imbalance inherent in screening populations. This study assesses
the feasibility of cancer risk classification using the Golden Retriever
Lifetime Study (GRLS) cohort under real-world constraints, including the
grouping of diverse cancer types and the inclusion of post-diagnosis samples. A
comprehensive benchmark evaluation was conducted, systematically comparing 126
analytical pipelines that comprised various machine learning models, feature
selection methods, and data balancing techniques. Data were partitioned at the
patient level to prevent leakage. The optimal model, a Logistic Regression
classifier with class weighting and recursive feature elimination, demonstrated
moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical
classification performance (F1-score = 0.25, Positive Predictive Value = 0.15).
While a high Negative Predictive Value (0.98) was achieved, insufficient recall
(0.79) precludes its use as a reliable rule-out test. Interpretability analysis
with SHapley Additive exPlanations (SHAP) revealed that predictions were driven
by non-specific features like age and markers of inflammation and anemia. It is
concluded that while a statistically detectable cancer signal exists in routine
lab data, it is too weak and confounded for clinically reliable discrimination
from normal aging or other inflammatory conditions. This work establishes a
critical performance ceiling for this data modality in isolation and
underscores that meaningful progress in computational veterinary oncology will
require integration of multi-modal data sources.

</details>


### [349] [Efficient Randomized Experiments Using Foundation Models](https://arxiv.org/abs/2502.04262)
*Piersilvio De Bartolomeis,Javier Abad,Guanbo Wang,Konstantin Donhauser,Raymond M. Duch,Fanny Yang,Issa J. Dahabreh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Randomized experiments are the preferred approach for evaluating the effects
of interventions, but they are costly and often yield estimates with
substantial uncertainty. On the other hand, in silico experiments leveraging
foundation models offer a cost-effective alternative that can potentially
attain higher statistical precision. However, the benefits of in silico
experiments come with a significant risk: statistical inferences are not valid
if the models fail to accurately predict experimental responses to
interventions. In this paper, we propose a novel approach that integrates the
predictions from multiple foundation models with experimental data while
preserving valid statistical inference. Our estimator is consistent and
asymptotically normal, with asymptotic variance no larger than the standard
estimator based on experimental data alone. Importantly, these statistical
properties hold even when model predictions are arbitrarily biased. Empirical
results across several randomized experiments show that our estimator offers
substantial precision gains, equivalent to a reduction of up to 20% in the
sample size needed to match the same precision as the standard estimator based
on experimental data alone.

</details>


### [350] [DB-FGA-Net: Dual Backbone Frequency Gated Attention Network for Multi-Class Brain Tumor Classification with Grad-CAM Interpretability](https://arxiv.org/abs/2510.20299)
*Saraf Anzum Shreya,MD. Abu Ismail Siddique,Sharaf Tasnim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Brain tumors are a challenging problem in neuro-oncology, where early and
precise diagnosis is important for successful treatment. Deep learning-based
brain tumor classification methods often rely on heavy data augmentation which
can limit generalization and trust in clinical applications. In this paper, we
propose a double-backbone network integrating VGG16 and Xception with a
Frequency-Gated Attention (FGA) Block to capture complementary local and global
features. Unlike previous studies, our model achieves state-of-the-art
performance without augmentation which demonstrates robustness to variably
sized and distributed datasets. For further transparency, Grad-CAM is
integrated to visualize the tumor regions based on which the model is giving
prediction, bridging the gap between model prediction and clinical
interpretability. The proposed framework achieves 99.24\% accuracy on the 7K-DS
dataset for the 4-class setting, along with 98.68\% and 99.85\% in the 3-class
and 2-class settings, respectively. On the independent 3K-DS dataset, the model
generalizes with 95.77\% accuracy, outperforming baseline and state-of-the-art
methods. To further support clinical usability, we developed a graphical user
interface (GUI) that provides real-time classification and Grad-CAM-based tumor
localization. These findings suggest that augmentation-free, interpretable, and
deployable deep learning models such as DB-FGA-Net hold strong potential for
reliable clinical translation in brain tumor diagnosis.

</details>


### [351] [Provable Sample-Efficient Transfer Learning Conditional Diffusion Models via Representation Learning](https://arxiv.org/abs/2502.04491)
*Ziheng Cheng,Tianyu Xie,Shiyue Zhang,Cheng Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While conditional diffusion models have achieved remarkable success in
various applications, they require abundant data to train from scratch, which
is often infeasible in practice. To address this issue, transfer learning has
emerged as an essential paradigm in small data regimes. Despite its empirical
success, the theoretical underpinnings of transfer learning conditional
diffusion models remain unexplored. In this paper, we take the first step
towards understanding the sample efficiency of transfer learning conditional
diffusion models through the lens of representation learning. Inspired by
practical training procedures, we assume that there exists a low-dimensional
representation of conditions shared across all tasks. Our analysis shows that
with a well-learned representation from source tasks, the samplecomplexity of
target tasks can be reduced substantially. In addition, we investigate the
practical implications of our theoretical results in several real-world
applications of conditional diffusion models. Numerical experiments are also
conducted to verify our results.

</details>


### [352] [Breaking the Frozen Subspace: Importance Sampling for Low-Rank Optimization in LLM Pretraining](https://arxiv.org/abs/2502.05790)
*Haochen Zhang,Junze Yin,Guanchu Wang,Zirui Liu,Lin F. Yang,Tianyi Zhang,Anshumali Shrivastava,Vladimir Braverman*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Low-rank optimization has emerged as a promising approach to enabling
memory-efficient training of large language models (LLMs). Existing low-rank
optimization methods typically project gradients onto a low-rank subspace,
reducing the memory cost of storing optimizer states. A key challenge in these
methods is selecting suitable subspaces to ensure an effective optimization
trajectory. Most existing approaches select the dominant subspace to preserve
gradient information, as this intuitively provides the best approximation.
However, we find that in practice, the dominant subspace stops changing during
pretraining, thereby constraining weight updates to similar subspaces. In this
paper, we propose importance sampling for low-rank optimization in LLM
pretraining with a provable convergence guarantee, which the dominant subspace
approach does not have. Empirically, we demonstrate that our method
significantly outperforms previous methods in LLM pretraining tasks.

</details>


### [353] [Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions](https://arxiv.org/abs/2502.06309)
*Zhaoxian Wu,Quan Xiao,Tayfun Gokmen,Omobayode Fagbohungbe,Tianyi Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As the economic and environmental costs of training and deploying large
vision or language models increase dramatically, analog in-memory computing
(AIMC) emerges as a promising energy-efficient solution. However, the training
perspective, especially its training dynamic, is underexplored. In AIMC
hardware, the trainable weights are represented by the conductance of resistive
elements and updated using consecutive electrical pulses. While the conductance
changes by a constant in response to each pulse, in reality, the change is
scaled by asymmetric and non-linear response functions, leading to a non-ideal
training dynamic. This paper provides a theoretical foundation for
gradient-based training on AIMC hardware with non-ideal response functions. We
demonstrate that asymmetric response functions negatively impact Analog SGD by
imposing an implicit penalty on the objective. To overcome the issue, we
propose Residual Learning algorithm, which provably converges exactly to a
critical point by solving a bilevel optimization problem. We demonstrate that
the proposed method can be extended to address other hardware imperfections,
such as limited response granularity. As we know, it is the first paper to
investigate the impact of a class of generic non-ideal response functions. The
conclusion is supported by simulations validating our theoretical insights.

</details>


### [354] [Structure-preserving contrastive learning for spatial time series](https://arxiv.org/abs/2502.06380)
*Yiru Jiao,Sander van Cranenburgh,Simeon Calvert,Hans van Lint*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The effectiveness of neural network models largely relies on learning
meaningful latent patterns from data, where self-supervised learning of
informative representations can enhance model performance and generalisability.
However, self-supervised representation learning for spatially characterised
time series, which are ubiquitous in transportation domain, poses unique
challenges due to the necessity of maintaining fine-grained spatio-temporal
similarities in the latent space. In this study, we introduce two
structure-preserving regularisers for the contrastive learning of spatial time
series: one regulariser preserves the topology of similarities between
instances, and the other preserves the graph geometry of similarities across
spatial and temporal dimensions. To balance the contrastive learning objective
and the need for structure preservation, we propose a dynamic weighting
mechanism that adaptively manages this trade-off and stabilises training. We
validate the proposed method through extensive experiments, including
multivariate time series classification to demonstrate its general
applicability, as well as macroscopic and microscopic traffic prediction to
highlight its particular usefulness in encoding traffic interactions. Across
all tasks, our method preserves the similarity structures more effectively and
improves state-of-the-art task performances. This method can be integrated with
an arbitrary neural network model and is particularly beneficial for time
series data with spatial or geographical features. Furthermore, our findings
suggest that well-preserved similarity structures in the latent space indicate
more informative and useful representations. This provides insights to design
more effective neural networks for data-driven transportation research. Our
code is made openly accessible with all resulting data at
https://github.com/yiru-jiao/spclt

</details>


### [355] [Universal Sequence Preconditioning](https://arxiv.org/abs/2502.06545)
*Annie Marsden,Elad Hazan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the problem of preconditioning in sequential prediction. From the
theoretical lens of linear dynamical systems, we show that convolving the input
sequence corresponds to applying a polynomial to the hidden transition matrix.
Building on this insight, we propose a universal preconditioning method that
convolves the input with coefficients from orthogonal polynomials such as
Chebyshev or Legendre. We prove that this approach reduces regret for two
distinct prediction algorithms and yields the first ever sublinear and
hidden-dimension independent regret bounds (up to logarithmic factors) that
hold for systems with marginally stable and asymmetric transition matrices.
Finally, extensive synthetic and real-world experiments show that this simple
preconditioning strategy improves the performance of a diverse range of
algorithms, including recurrent neural networks, and generalizes to signals
beyond linear dynamical systems.

</details>


### [356] [Provably Efficient Online RLHF with One-Pass Reward Modeling](https://arxiv.org/abs/2502.07193)
*Long-Fei Li,Yu-Yang Qian,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning from Human Feedback (RLHF) has shown remarkable
success in aligning Large Language Models (LLMs) with human preferences.
Traditional RLHF methods rely on a fixed dataset, which often suffers from
limited coverage. To this end, online RLHF has emerged as a promising
direction, enabling iterative data collection and refinement. Despite its
potential, this paradigm faces a key bottleneck: the requirement to
continuously integrate new data into the dataset and re-optimize the model from
scratch at each iteration, resulting in computational and storage costs that
grow linearly with the number of iterations. In this work, we address this
challenge by proposing a one-pass reward modeling method that eliminates the
need to store historical data and achieves constant-time updates per iteration.
Specifically, we first formalize RLHF as a contextual preference bandit and
develop a new algorithm based on online mirror descent with a tailored local
norm, replacing the standard maximum likelihood estimation for reward modeling.
We then apply it to various online RLHF settings, including passive data
collection, active data collection, and deployment-time adaptation. We provide
theoretical guarantees showing that our method enhances both statistical and
computational efficiency. Finally, we design practical algorithms for LLMs and
conduct experiments with the Llama-3-8B-Instruct and Qwen2.5-7B-Instruct models
on Ultrafeedback and Mixture2 datasets, validating the effectiveness of our
approach.

</details>


### [357] [Mixing It Up: Exploring Mixer Networks for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2502.11816)
*Christian Klötergens,Vijaya Krishna Yalavarthi,Tim Dernedde,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Forecasting Irregular Multivariate Time Series (IMTS) has recently emerged as
a distinct research field, necessitating specialized models to address its
unique challenges. While most forecasting literature assumes regularly spaced
observations without missing values, many real-world datasets - particularly in
healthcare, climate research, and biomechanics - violate these assumptions.
Time Series (TS)-mixer models have achieved remarkable success in regular
multivariate time series forecasting. However, they remain unexplored for IMTS
due to their requirement for complete and evenly spaced observations. To bridge
this gap, we introduce IMTS-Mixer, a novel forecasting architecture designed
specifically for IMTS. Our approach retains the core principles of TS mixer
models while introducing innovative methods to transform IMTS into fixed-size
matrix representations, enabling their seamless integration with mixer modules.
We evaluate IMTS-Mixer on a benchmark of four real-world datasets from various
domains. Our results demonstrate that IMTS-Mixer establishes a new
state-of-the-art in forecasting accuracy while also improving computational
efficiency.

</details>


### [358] [TimeXL: Explainable Multi-modal Time Series Prediction with LLM-in-the-Loop](https://arxiv.org/abs/2503.01013)
*Yushan Jiang,Wenchao Yu,Geon Lee,Dongjin Song,Kijung Shin,Wei Cheng,Yanchi Liu,Haifeng Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time series analysis provides essential insights for real-world system
dynamics and informs downstream decision-making, yet most existing methods
often overlook the rich contextual signals present in auxiliary modalities. To
bridge this gap, we introduce TimeXL, a multi-modal prediction framework that
integrates a prototype-based time series encoder with three collaborating Large
Language Models (LLMs) to deliver more accurate predictions and interpretable
explanations. First, a multi-modal prototype-based encoder processes both time
series and textual inputs to generate preliminary forecasts alongside
case-based rationales. These outputs then feed into a prediction LLM, which
refines the forecasts by reasoning over the encoder's predictions and
explanations. Next, a reflection LLM compares the predicted values against the
ground truth, identifying textual inconsistencies or noise. Guided by this
feedback, a refinement LLM iteratively enhances text quality and triggers
encoder retraining. This closed-loop workflow-prediction, critique (reflect),
and refinement-continuously boosts the framework's performance and
interpretability. Empirical evaluations on four real-world datasets demonstrate
that TimeXL achieves up to 8.9% improvement in AUC and produces human-centric,
multi-modal explanations, highlighting the power of LLM-driven reasoning for
time series prediction.

</details>


### [359] [Memory Injection Attacks on LLM Agents via Query-Only Interaction](https://arxiv.org/abs/2503.03704)
*Shen Dong,Shaochen Xu,Pengfei He,Yige Li,Jiliang Tang,Tianming Liu,Hui Liu,Zhen Xiang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Agents powered by large language models (LLMs) have demonstrated strong
capabilities in a wide range of complex, real-world applications. However, LLM
agents with a compromised memory bank may easily produce harmful outputs when
the past records retrieved for demonstration are malicious. In this paper, we
propose a novel Memory INJection Attack, MINJA, without assuming that the
attacker can directly modify the memory bank of the agent. The attacker injects
malicious records into the memory bank by only interacting with the agent via
queries and output observations. These malicious records are designed to elicit
a sequence of malicious reasoning steps corresponding to a different target
query during the agent's execution of the victim user's query. Specifically, we
introduce a sequence of bridging steps to link victim queries to the malicious
reasoning steps. During the memory injection, we propose an indication prompt
that guides the agent to autonomously generate similar bridging steps, with a
progressive shortening strategy that gradually removes the indication prompt,
such that the malicious record will be easily retrieved when processing later
victim queries. Our extensive experiments across diverse agents demonstrate the
effectiveness of MINJA in compromising agent memory. With minimal requirements
for execution, MINJA enables any user to influence agent memory, highlighting
the risk.

</details>


### [360] [Validating LLM-as-a-Judge Systems under Rating Indeterminacy](https://arxiv.org/abs/2503.05965)
*Luke Guerdan,Solon Barocas,Kenneth Holstein,Hanna Wallach,Zhiwei Steven Wu,Alexandra Chouldechova*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The LLM-as-a-judge paradigm, in which a judge LLM system replaces human
raters in rating the outputs of other generative AI (GenAI) systems, plays a
critical role in scaling and standardizing GenAI evaluations. To validate such
judge systems, evaluators assess human--judge agreement by first collecting
multiple human ratings for each item in a validation corpus, then aggregating
the ratings into a single, per-item gold label rating. For many items, however,
rating criteria may admit multiple valid interpretations, so a human or LLM
rater may deem multiple ratings "reasonable" or "correct." We call this
condition rating indeterminacy. Problematically, many rating tasks that contain
rating indeterminacy rely on forced-choice elicitation, whereby raters are
instructed to select only one rating for each item. In this paper, we introduce
a framework for validating LLM-as-a-judge systems under rating indeterminacy.
We draw theoretical connections between different measures of judge system
performance under different human--judge agreement metrics, and different
rating elicitation and aggregation schemes. We demonstrate that differences in
how humans and LLMs resolve rating indeterminacy when responding to
forced-choice rating instructions can heavily bias LLM-as-a-judge validation.
Through extensive experiments involving 11 real-world rating tasks and 9
commercial LLMs, we show that standard validation approaches that rely upon
forced-choice ratings select judge systems that are highly suboptimal,
performing as much as 31% worse than judge systems selected by our approach
that uses multi-label "response set" ratings to account for rating
indeterminacy. We conclude with concrete recommendations for more principled
approaches to LLM-as-a-judge validation.

</details>


### [361] [Revisiting Agnostic Boosting](https://arxiv.org/abs/2503.09384)
*Arthur da Cunha,Mikael Møller Høgsgaard,Andrea Paudice,Yuxin Sun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Boosting is a key method in statistical learning, allowing for converting
weak learners into strong ones. While well studied in the realizable case, the
statistical properties of weak-to-strong learning remain less understood in the
agnostic setting, where there are no assumptions on the distribution of the
labels. In this work, we propose a new agnostic boosting algorithm with
substantially improved sample complexity compared to prior works under very
general assumptions. Our approach is based on a reduction to the realizable
case, followed by a margin-based filtering of high-quality hypotheses.
Furthermore, we show a nearly-matching lower bound, settling the sample
complexity of agnostic boosting up to logarithmic factors.

</details>


### [362] [Federated Structured Sparse PCA for Anomaly Detection in IoT Networks](https://arxiv.org/abs/2503.23981)
*Chenyi Huang,Xinrong Li,Xianchao Xiu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Although federated learning has gained prominence as a privacy-preserving
framework tailored for distributed Internet of Things (IoT) environments,
current federated principal component analysis (PCA) methods lack integration
of sparsity, a critical feature for robust anomaly detection. To address this
limitation, we propose a novel federated structured sparse PCA (FedSSP)
approach for anomaly detection in IoT networks. The proposed model uniquely
integrates double sparsity regularization: (1) row-wise sparsity governed by
$\ell_{2,p}$-norm with $p\in[0,1)$ to eliminate redundant feature dimensions,
and (2) element-wise sparsity via $\ell_{q}$-norm with $q\in[0,1)$ to suppress
noise-sensitive components. To efficiently solve this non-convex optimization
problem in a distributed setting, we devise a proximal alternating minimization
(PAM) algorithm with rigorous theoretical proofs establishing its convergence
guarantees. Experiments on real datasets validate that incorporating structured
sparsity enhances both model interpretability and detection accuracy.

</details>


### [363] [TianQuan-S2S: A Subseasonal-to-Seasonal Global Weather Model via Incorporate Climatology State](https://arxiv.org/abs/2504.09940)
*Guowen Li,Xintong Liu,Yang Liu,Mengxuan Chen,Shilei Cao,Xuehe Wang,Juepeng Zheng,Jinxiao Zhang,Haoyuan Liang,Lixian Zhang,Jiuke Wang,Meng Jin,Hong Cheng,Haohuan Fu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate Subseasonal-to-Seasonal (S2S) forecasting is vital for
decision-making in agriculture, energy production, and emergency management.
However, it remains a challenging and underexplored problem due to the chaotic
nature of the weather system. Recent data-driven studies have shown promising
results, but their performance is limited by the inadequate incorporation of
climate states and a model tendency to degrade, progressively losing fine-scale
details and yielding over-smoothed forecasts. To overcome these limitations, we
propose TianQuan-S2S, a global S2S forecasting model that integrates initial
weather states with climatological means via incorporating climatology into
patch embedding and enhancing variability capture through an
uncertainty-augmented Transformer. Extensive experiments on the Earth
Reanalysis 5 (ERA5) reanalysis dataset demonstrate that our model yields a
significant improvement in both deterministic and ensemble forecasting over the
climatology mean, traditional numerical methods, and data-driven models.
Ablation studies empirically show the effectiveness of our model designs.
Remarkably, our model outperforms skillful numerical ECMWF-S2S and advanced
data-driven Fuxi-S2S in key meteorological variables.

</details>


### [364] [Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models](https://arxiv.org/abs/2504.17660)
*Julius Vetter,Manuel Gloeckler,Daniel Gedon,Jakob H. Macke*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Simulation-based inference (SBI) offers a flexible and general approach to
performing Bayesian inference: In SBI, a neural network is trained on synthetic
data simulated from a model and used to rapidly infer posterior distributions
for observed data. A key goal for SBI is to achieve accurate inference with as
few simulations as possible, especially for expensive simulators. In this work,
we address this challenge by repurposing recent probabilistic foundation models
for tabular data: We show how tabular foundation models -- specifically TabPFN
-- can be used as pre-trained autoregressive conditional density estimators for
SBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks
(NPE-PFN) and show that it is competitive with current SBI approaches in terms
of accuracy for both benchmark tasks and two complex scientific inverse
problems. Crucially, it often substantially outperforms them in terms of
simulation efficiency, sometimes requiring orders of magnitude fewer
simulations. NPE-PFN eliminates the need for inference network selection,
training, and hyperparameter tuning. We also show that it exhibits superior
robustness to model misspecification and can be scaled to simulation budgets
that exceed the context size limit of TabPFN. NPE-PFN provides a new direction
for SBI, where training-free, general-purpose inference models offer efficient,
easy-to-use, and flexible solutions for a wide range of stochastic inverse
problems.

</details>


### [365] [A critical assessment of reinforcement learning methods for microswimmer navigation in complex flows](https://arxiv.org/abs/2505.05525)
*Selim Mecanna,Aurore Loisy,Christophe Eloy*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Navigating in a fluid flow while being carried by it, using only information
accessible from on-board sensors, is a problem commonly faced by small
planktonic organisms. It is also directly relevant to autonomous robots
deployed in the oceans. In the last ten years, the fluid mechanics community
has widely adopted reinforcement learning, often in the form of its simplest
implementations, to address this challenge. But it is unclear how good are the
strategies learned by these algorithms. In this paper, we perform a
quantitative assessment of reinforcement learning methods applied to navigation
in partially observable flows. We first introduce a well-posed problem of
directional navigation for which a quasi-optimal policy is known analytically.
We then report on the poor performance and robustness of commonly used
algorithms (Q-Learning, Advantage Actor Critic) in flows regularly encountered
in the literature: Taylor-Green vortices, Arnold-Beltrami-Childress flow, and
two-dimensional turbulence. We show that they are vastly surpassed by PPO
(Proximal Policy Optimization), a more advanced algorithm that has established
dominance across a wide range of benchmarks in the reinforcement learning
community. In particular, our custom implementation of PPO matches the
theoretical quasi-optimal performance in turbulent flow and does so in a robust
manner. Reaching this result required the use of several additional techniques,
such as vectorized environments and generalized advantage estimation, as well
as hyperparameter optimization. This study demonstrates the importance of
algorithm selection, implementation details, and fine-tuning for discovering
truly smart autonomous navigation strategies in complex flows.

</details>


### [366] [Analog Foundation Models](https://arxiv.org/abs/2505.09663)
*Julian Büchel,Iason Chalas,Giovanni Acampa,An Chen,Omobayode Fagbohungbe,Sidney Tsai,Kaoutar El Maghraoui,Manuel Le Gallo,Abbas Rahimi,Abu Sebastian*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models.

</details>


### [367] [Koopman Eigenfunction-Based Identification and Optimal Nonlinear Control of Turbojet Engine](https://arxiv.org/abs/2505.10438)
*David Grasev*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Gas turbine engines are complex and highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging because it requires
performance characteristics that are not always available, often leading to
many simplifying assumptions. This paper discusses the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models, and addresses these issues by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics are estimated using the
sparse identification of nonlinear dynamics. Subsequently, the autonomous part
of the dynamics is mapped into an optimally constructed Koopman eigenfunction
space. This process involves eigenvalue optimization using metaheuristic
algorithms and temporal projection, followed by gradient-based eigenfunction
identification. The resulting Koopman model is validated against an in-house
reference component-level model. A globally optimal nonlinear feedback
controller and a Kalman estimator are then designed within the eigenfunction
space and compared to traditional and gain-scheduled proportional-integral
controllers, as well as a proposed internal model control approach. The
eigenmode structure enables targeting individual modes during optimization,
leading to improved performance tuning. Results demonstrate that the
Koopman-based controller surpasses other benchmark controllers in both
reference tracking and disturbance rejection under sea-level and varying flight
conditions, due to its global nature.

</details>


### [368] [Approximation and Generalization Abilities of Score-based Neural Network Generative Models for Sub-Gaussian Distributions](https://arxiv.org/abs/2505.10880)
*Guoji Fu,Wee Sun Lee*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper studies the approximation and generalization abilities of
score-based neural network generative models (SGMs) in estimating an unknown
distribution $P_0$ from $n$ i.i.d. observations in $d$ dimensions. Assuming
merely that $P_0$ is $\alpha$-sub-Gaussian, we prove that for any time step $t
\in [t_0, n^{\mathcal{O}(1)}]$, where $t_0 > \mathcal{O}(\alpha^2n^{-2/d}\log
n)$, there exists a deep ReLU neural network with width $\leq
\mathcal{O}(n^{\frac{3}{d}}\log_2n)$ and depth $\leq \mathcal{O}(\log^2n)$ that
can approximate the scores with $\tilde{\mathcal{O}}(n^{-1})$ mean square error
and achieve a nearly optimal rate of $\tilde{\mathcal{O}}(n^{-1}t_0^{-d/2})$
for score estimation, as measured by the score matching loss. Our framework is
universal and can be used to establish convergence rates for SGMs under milder
assumptions than previous work. For example, assuming further that the target
density function $p_0$ lies in Sobolev or Besov classes, with an appropriately
early stopping strategy, we demonstrate that neural network-based SGMs can
attain nearly minimax convergence rates up to logarithmic factors. Our analysis
removes several crucial assumptions, such as Lipschitz continuity of the score
function or a strictly positive lower bound on the target density.

</details>


### [369] [Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schrödinger Bridge](https://arxiv.org/abs/2505.11197)
*Zhenyi Zhang,Zihan Wang,Yuhao Sun,Tiejun Li,Peijie Zhou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modeling the dynamics from sparsely time-resolved snapshot data is crucial
for understanding complex cellular processes and behavior. Existing methods
leverage optimal transport, Schr\"odinger bridge theory, or their variants to
simultaneously infer stochastic, unbalanced dynamics from snapshot data.
However, these approaches remain limited in their ability to account for
cell-cell interactions. This integration is essential in real-world scenarios
since intercellular communications are fundamental life processes and can
influence cell state-transition dynamics. To address this challenge, we
formulate the Unbalanced Mean-Field Schr\"odinger Bridge (UMFSB) framework to
model unbalanced stochastic interaction dynamics from snapshot data. Inspired
by this framework, we further propose CytoBridge, a deep learning algorithm
designed to approximate the UMFSB problem. By explicitly modeling cellular
transitions, proliferation, and interactions through neural networks,
CytoBridge offers the flexibility to learn these processes directly from data.
The effectiveness of our method has been extensively validated using both
synthetic gene regulatory data and real scRNA-seq datasets. Compared to
existing methods, CytoBridge identifies growth, transition, and interaction
patterns, eliminates false transitions, and reconstructs the developmental
landscape with greater accuracy. Code is available at:
https://github.com/zhenyiizhang/CytoBridge-NeurIPS.

</details>


### [370] [Minimizing False-Positive Attributions in Explanations of Non-Linear Models](https://arxiv.org/abs/2505.11210)
*Anders Gjølbye,Stefan Haufe,Lars Kai Hansen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Suppressor variables can influence model predictions without being dependent
on the target outcome, and they pose a significant challenge for Explainable AI
(XAI) methods. These variables may cause false-positive feature attributions,
undermining the utility of explanations. Although effective remedies exist for
linear models, their extension to non-linear models and instance-based
explanations has remained limited. We introduce PatternLocal, a novel XAI
technique that addresses this gap. PatternLocal begins with a locally linear
surrogate, e.g., LIME, KernelSHAP, or gradient-based methods, and transforms
the resulting discriminative model weights into a generative representation,
thereby suppressing the influence of suppressor variables while preserving
local fidelity. In extensive hyperparameter optimization on the XAI-TRIS
benchmark, PatternLocal consistently outperformed other XAI methods and reduced
false-positive attributions when explaining non-linear tasks, thereby enabling
more reliable and actionable insights. We further evaluate PatternLocal on an
EEG motor imagery dataset, demonstrating physiologically plausible
explanations.

</details>


### [371] [Variational Regularized Unbalanced Optimal Transport: Single Network, Least Action](https://arxiv.org/abs/2505.11823)
*Yuhao Sun,Zhenyi Zhang,Zihan Wang,Tiejun Li,Peijie Zhou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recovering the dynamics from a few snapshots of a high-dimensional system is
a challenging task in statistical physics and machine learning, with important
applications in computational biology. Many algorithms have been developed to
tackle this problem, based on frameworks such as optimal transport and the
Schr\"odinger bridge. A notable recent framework is Regularized Unbalanced
Optimal Transport (RUOT), which integrates both stochastic dynamics and
unnormalized distributions. However, since many existing methods do not
explicitly enforce optimality conditions, their solutions often struggle to
satisfy the principle of least action and meet challenges to converge in a
stable and reliable way. To address these issues, we propose Variational RUOT
(Var-RUOT), a new framework to solve the RUOT problem. By incorporating the
optimal necessary conditions for the RUOT problem into both the
parameterization of the search space and the loss function design, Var-RUOT
only needs to learn a scalar field to solve the RUOT problem and can search for
solutions with lower action. We also examined the challenge of selecting a
growth penalty function in the widely used Wasserstein-Fisher-Rao metric and
proposed a solution that better aligns with biological priors in Var-RUOT. We
validated the effectiveness of Var-RUOT on both simulated data and real
single-cell datasets. Compared with existing algorithms, Var-RUOT can find
solutions with lower action while exhibiting faster convergence and improved
training stability. Our code is available at
https://github.com/ZerooVector/VarRUOT.

</details>


### [372] [OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction](https://arxiv.org/abs/2505.13071)
*Jie Yan,Jing Liu,Zhong-Yuan Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated clustering (FC) aims to discover global cluster structures across
decentralized clients without sharing raw data, making privacy preservation a
fundamental requirement. There are two critical challenges: (1) privacy leakage
during collaboration, and (2) robustness degradation due to aggregation of
proxy information from non-independent and identically distributed (Non-IID)
local data, leading to inaccurate or inconsistent global clustering. Existing
solutions typically rely on model-specific local proxies, which are sensitive
to data heterogeneity and inherit inductive biases from their centralized
counterparts, thus limiting robustness and generality. We propose Omni
Federated Clustering (OmniFC), a unified and model-agnostic framework.
Leveraging Lagrange coded computing, our method enables clients to share only
encoded data, allowing exact reconstruction of the global distance matrix--a
fundamental representation of sample relationships--without leaking private
information, even under client collusion. This construction is naturally
resilient to Non-IID data distributions. This approach decouples FC from
model-specific proxies, providing a unified extension mechanism applicable to
diverse centralized clustering methods. Theoretical analysis confirms both
reconstruction fidelity and privacy guarantees, while comprehensive experiments
demonstrate OmniFC's superior robustness, effectiveness, and generality across
various benchmarks compared to state-of-the-art methods. Code will be released.

</details>


### [373] [Unlabeled Data vs. Pre-trained Knowledge: Rethinking SSL in the Era of Large Models](https://arxiv.org/abs/2505.13317)
*Song-Lin Lv,Rui Zhu,Tong Wei,Yu-Feng Li,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Semi-supervised learning (SSL) alleviates the cost of data labeling process
by exploiting unlabeled data and has achieved promising results. Meanwhile,
with the development of large foundation models, exploiting pre-trained models
becomes a promising way to address the label scarcity in the downstream tasks,
such as various parameter-efficient fine-tuning techniques. This raises a
natural yet critical question: When labeled data is limited, should we rely on
unlabeled data or pre-trained models? To investigate this issue, we conduct a
fair comparison between SSL methods and pre-trained models (e.g., CLIP) on
representative image classification tasks under a controlled supervision
budget. Experiments reveal that SSL has met its ``Waterloo" in the era of large
models, as pre-trained models show both high efficiency and strong performance
on widely adopted SSL benchmarks. This underscores the urgent need for SSL
researchers to explore new avenues, such as deeper integration between the SSL
and pre-trained models. Furthermore, we investigate the potential of
Multi-Modal Large Language Models (MLLMs) in image classification tasks.
Results show that, despite their massive parameter scales, MLLMs still face
significant performance limitations, highlighting that even a seemingly
well-studied task remains highly challenging.

</details>


### [374] [Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers](https://arxiv.org/abs/2505.14595)
*Nima Hosseini Dashtbayaz,Hesam Salehipour,Adrian Butscher,Nigel Morris*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reduced-order modeling (ROM) of time-dependent and parameterized differential
equations aims to accelerate the simulation of complex high-dimensional systems
by learning a compact latent manifold representation that captures the
characteristics of the solution fields and their time-dependent dynamics.
Although high-fidelity numerical solvers generate the training datasets, they
have thus far been excluded from the training process, causing the learned
latent dynamics to drift away from the discretized governing physics. This
mismatch often limits generalization and forecasting capabilities. In this
work, we propose Physics-informed ROM ($\Phi$-ROM) by incorporating
differentiable PDE solvers into the training procedure. Specifically, the
latent space dynamics and its dependence on PDE parameters are shaped directly
by the governing physics encoded in the solver, ensuring a strong
correspondence between the full and reduced systems. Our model outperforms
state-of-the-art data-driven ROMs and other physics-informed strategies by
accurately generalizing to new dynamics arising from unseen parameters,
enabling long-term forecasting beyond the training horizon, maintaining
continuity in both time and space, and reducing the data cost. Furthermore,
$\Phi$-ROM learns to recover and forecast the solution fields even when trained
or evaluated with sparse and irregular observations of the fields, providing a
flexible framework for field reconstruction and data assimilation. We
demonstrate the framework's robustness across various PDE solvers and highlight
its broad applicability by providing an open-source JAX implementation that is
readily extensible to other PDE systems and differentiable solvers, available
at https://phi-rom.github.io.

</details>


### [375] [HOPSE: Scalable Higher-Order Positional and Structural Encoder for Combinatorial Representations](https://arxiv.org/abs/2505.15405)
*Martin Carrasco,Guillermo Bernardez,Marco Montagna,Nina Miolane,Lev Telyatnikov*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While Graph Neural Networks (GNNs) have proven highly effective at modeling
relational data, pairwise connections cannot fully capture multi-way
relationships naturally present in complex real-world systems. In response to
this, Topological Deep Learning (TDL) leverages more general combinatorial
representations -- such as simplicial or cellular complexes -- to accommodate
higher-order interactions. Existing TDL methods often extend GNNs through
Higher-Order Message Passing (HOMP), but face critical \emph{scalability
challenges} due to \textit{(i)} a combinatorial explosion of message-passing
routes, and \textit{(ii)} significant complexity overhead from the propagation
mechanism. This work presents HOPSE (Higher-Order Positional and Structural
Encoder), an alternative method to solve tasks involving higher-order
interactions \emph{without message passing}. Instead, HOPSE breaks
\emph{arbitrary higher-order domains} into their neighborhood relationships
using a Hasse graph decomposition. This method shows that decoupling the
representation learning of neighborhood topology from that of attributes
results in lower computational complexity, casting doubt on the need for HOMP.
The experiments on molecular graph tasks and topological benchmarks show that
HOPSE matches performance on traditional TDL datasets and outperforms HOMP
methods on topological tasks, achieving up to $7\times$ speedups over
HOMP-based models, opening a new path for scalable TDL.

</details>


### [376] [Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes](https://arxiv.org/abs/2505.15496)
*Hossein Zakerinia,Christoph H. Lampert*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present new fast-rate PAC-Bayesian generalization bounds for multi-task
and meta-learning in the unbalanced setting, i.e. when the tasks have training
sets of different sizes, as is typically the case in real-world scenarios.
Previously, only standard-rate bounds were known for this situation, while
fast-rate bounds were limited to the setting where all training sets are of
equal size. Our new bounds are numerically computable as well as interpretable,
and we demonstrate their flexibility in handling a number of cases where they
give stronger guarantees than previous bounds. Besides the bounds themselves,
we also make conceptual contributions: we demonstrate that the unbalanced
multi-task setting has different statistical properties than the balanced
situation, specifically that proofs from the balanced situation do not carry
over to the unbalanced setting. Additionally, we shed light on the fact that
the unbalanced situation allows two meaningful definitions of multi-task risk,
depending on whether all tasks should be considered equally important or if
sample-rich tasks should receive more weight than sample-poor ones.

</details>


### [377] [A Temporal Difference Method for Stochastic Continuous Dynamics](https://arxiv.org/abs/2505.15544)
*Haruki Settai,Naoya Takeishi,Takehisa Yairi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: For continuous systems modeled by dynamical equations such as ODEs and SDEs,
Bellman's Principle of Optimality takes the form of the Hamilton-Jacobi-Bellman
(HJB) equation, which provides the theoretical target of reinforcement learning
(RL). Although recent advances in RL successfully leverage this formulation,
the existing methods typically assume the underlying dynamics are known a
priori because they need explicit access to the coefficient functions of
dynamical equations to update the value function following the HJB equation. We
address this inherent limitation of HJB-based RL; we propose a model-free
approach still targeting the HJB equation and propose the corresponding
temporal difference method. We establish exponential convergence of the
idealized continuous-time dynamics and empirically demonstrate its potential
advantages over transition-kernel-based formulations. The proposed formulation
paves the way toward bridging stochastic control and model-free reinforcement
learning.

</details>


### [378] [Fairness under Competition](https://arxiv.org/abs/2505.16291)
*Ronen Gradwohl,Eilam Shapira,Moshe Tennenholtz*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Algorithmic fairness has emerged as a central issue in ML, and it has become
standard practice to adjust ML algorithms so that they will satisfy fairness
requirements such as Equal Opportunity. In this paper we consider the effects
of adopting such fair classifiers on the overall level of ecosystem fairness.
Specifically, we introduce the study of fairness with competing firms, and
demonstrate the failure of fair classifiers in yielding fair ecosystems. Our
results quantify the loss of fairness in systems, under a variety of
conditions, based on classifiers' correlation and the level of their data
overlap. We show that even if competing classifiers are individually fair, the
ecosystem's outcome may be unfair; and that adjusting biased algorithms to
improve their individual fairness may lead to an overall decline in ecosystem
fairness. In addition to these theoretical results, we also provide supporting
experimental evidence. Together, our model and results provide a novel and
essential call for action.

</details>


### [379] [Adversarial Robustness of Nonparametric Regression](https://arxiv.org/abs/2505.17356)
*Parsa Moradi,Hanzaleh Akabrinodehi,Mohammad Ali Maddah-Ali*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we investigate the adversarial robustness of nonparametric
regression, a fundamental problem in machine learning, under the setting where
an adversary can arbitrarily corrupt a subset of the input data. While the
robustness of parametric regression has been extensively studied, its
nonparametric counterpart remains largely unexplored. We characterize the
adversarial robustness in nonparametric regression, assuming the regression
function belongs to the second-order Sobolev space (i.e., it is square
integrable up to its second derivative).
  The contribution of this paper is two-fold: (i) we establish a minimax lower
bound on the estimation error, revealing a fundamental limit that no estimator
can overcome, and (ii) we show that, perhaps surprisingly, the classical
smoothing spline estimator, when properly regularized, exhibits robustness
against adversarial corruption. These results imply that if $o(n)$ out of $n$
samples are corrupted, the estimation error of the smoothing spline vanishes as
$n \to \infty$. On the other hand, when a constant fraction of the data is
corrupted, no estimator can guarantee vanishing estimation error, implying the
optimality of the smoothing spline in terms of maximum tolerable number of
corrupted samples.

</details>


### [380] [MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention](https://arxiv.org/abs/2505.18698)
*Can Yaras,Alec S. Xu,Pierre Abillama,Changwoo Lee,Laura Balzano*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Transformers have achieved state-of-the-art performance across various tasks,
but suffer from a notable quadratic complexity in sequence length due to the
attention mechanism. In this work, we propose MonarchAttention -- a novel
approach to sub-quadratic attention approximation via Monarch matrices, an
expressive class of structured matrices. Based on the variational form of
softmax, we describe an efficient optimization-based algorithm to compute an
approximate projection of softmax attention onto the class of Monarch matrices
with $\Theta(N\sqrt{N} d)$ computational complexity and $\Theta(Nd)$ memory/IO
complexity. Unlike previous approaches, MonarchAttention is both (1)
transferable, yielding minimal performance loss with no additional training,
even when replacing every attention layer of the Transformer, and (2)
hardware-efficient, utilizing the highest-throughput tensor core units on
modern GPUs. With optimized kernels, MonarchAttention achieves substantial
speed-ups in wall-time over FlashAttention-2: $1.4\times$ for shorter sequences
$(N=256)$, $4.5\times$ for medium-length sequences $(N=4K)$, and $8.2\times$
for longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention
on diverse tasks and architectures in vision and language problems, showing
that it flexibly and accurately approximates softmax attention in a variety of
contexts. Our code is available at
https://github.com/cjyaras/monarch-attention.

</details>


### [381] [Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data](https://arxiv.org/abs/2505.19043)
*Jingyuan Liu,Zeyu Zhang,Xuchuang Wang,Xutong Liu,John C. S. Lui,Mohammad Hajiesmaili,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Contextual multi-armed bandit is a fundamental learning framework for making
a sequence of decisions, e.g., advertising recommendations for a sequence of
arriving users. Recent works have shown that clustering these users based on
the similarity of their learned preferences can accelerate the learning.
However, prior work has primarily focused on the online setting, which requires
continually collecting user data, ignoring the offline data widely available in
many applications. To tackle these limitations, we study the offline clustering
of bandits (Off-ClusBand) problem, which studies how to use the offline dataset
to learn cluster properties and improve decision-making. The key challenge in
Off-ClusBand arises from data insufficiency for users: unlike the online case
where we continually learn from online data, in the offline case, we have a
fixed, limited dataset to work from and thus must determine whether we have
enough data to confidently cluster users together. To address this challenge,
we propose two algorithms: Off-C2LUB, which we show analytically and
experimentally outperforms existing methods under limited offline user data,
and Off-CLUB, which may incur bias when data is sparse but performs well and
nearly matches the lower bound when data is sufficient. We experimentally
validate these results on both real and synthetic datasets.

</details>


### [382] [Regret Analysis of Average-Reward Unichain MDPs via an Actor-Critic Approach](https://arxiv.org/abs/2505.19986)
*Swetha Ganesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Actor-Critic methods are widely used for their scalability, yet existing
theoretical guarantees for infinite-horizon average-reward Markov Decision
Processes (MDPs) often rely on restrictive ergodicity assumptions. We propose
NAC-B, a Natural Actor-Critic with Batching, that achieves order-optimal regret
of $\tilde{O}(\sqrt{T})$ in infinite-horizon average-reward MDPs under the
unichain assumption, which permits both transient states and periodicity. This
assumption is among the weakest under which the classic policy gradient theorem
remains valid for average-reward settings. NAC-B employs function approximation
for both the actor and the critic, enabling scalability to problems with large
state and action spaces. The use of batching in our algorithm helps mitigate
potential periodicity in the MDP and reduces stochasticity in gradient
estimates, and our analysis formalizes these benefits through the introduction
of the constants $C_{\text{hit}}$ and $C_{\text{tar}}$, which characterize the
rate at which empirical averages over Markovian samples converge to the
stationary distribution.

</details>


### [383] [Towards Fully FP8 GEMM LLM Training at Scale](https://arxiv.org/abs/2505.20524)
*Alejandro Hernández-Cano,Dhia Garbaya,Imanol Schlag,Martin Jaggi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite the significant potential of FP8 data formats for large language
model (LLM) pre-training, their adoption has been limited due to challenges in
maintaining stability at scale. Existing approaches often rely on suboptimal
fine-grained FP8 kernels or fall back to higher-precision matrix
multiplications (GEMMs) in sensitive components, such as attention projections,
compromising potential throughput gains. We introduce a new class of LLM
architectures that, for the first time, support FP8 computation for all GEMMs
within transformer blocks during both forward and backward passes. This enables
unprecedented throughput gains, particularly at scale, while matching the
downstream performance of standard BF16 training. Our architecture design
reduces large outlier activations, promoting stable long-term FP8 training. In
addition, we identify key metrics to monitor low-precision training and predict
potential future divergences.

</details>


### [384] [Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework](https://arxiv.org/abs/2505.21251)
*Mustafa Hajij,Lennart Bastian,Sarah Osentoski,Hardik Kabaria,John L. Davenport,Sheik Dawood,Balaji Cherukuri,Joseph G. Kocheemoolayil,Nastaran Shahmansouri,Adrian Lew,Theodore Papamarkou,Tolga Birdal*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce copresheaf topological neural networks (CTNNs), a powerful
unifying framework that encapsulates a wide spectrum of deep learning
architectures, designed to operate on structured data, including images, point
clouds, graphs, meshes, and topological manifolds. While deep learning has
profoundly impacted domains ranging from digital assistants to autonomous
systems, the principled design of neural architectures tailored to specific
tasks and data types remains one of the field's most persistent open
challenges. CTNNs address this gap by formulating model design in the language
of copresheaves, a concept from algebraic topology that generalizes most
practical deep learning models in use today. This abstract yet constructive
formulation yields a rich design space from which theoretically sound and
practically effective solutions can be derived to tackle core challenges in
representation learning, such as long-range dependencies, oversmoothing,
heterophily, and non-Euclidean domains. Our empirical results on structured
data benchmarks demonstrate that CTNNs consistently outperform conventional
baselines, particularly in tasks requiring hierarchical or localized
sensitivity. These results establish CTNNs as a principled multi-scale
foundation for the next generation of deep learning architectures.

</details>


### [385] [OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models](https://arxiv.org/abs/2505.21347)
*Ziheng Cheng,Yixiao Huang,Hui Xu,Somayeh Sojoudi,Xuandong Zhao,Dawn Song,Song Mei*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Text-to-Image (T2I) models have achieved remarkable success in generating
visual content from text inputs. Although multiple safety alignment strategies
have been proposed to prevent harmful outputs, they often lead to overly
cautious behavior -- rejecting even benign prompts -- a phenomenon known as
$\textit{over-refusal}$ that reduces the practical utility of T2I models.
Despite over-refusal having been observed in practice, there is no large-scale
benchmark that systematically evaluates this phenomenon for T2I models. In this
paper, we present an automatic workflow to construct synthetic evaluation data,
resulting in OVERT ($\textbf{OVE}$r-$\textbf{R}$efusal evaluation on
$\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing
over-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful
but benign prompts across nine safety-related categories, along with 1,785
genuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility
trade-off. Using OVERT, we evaluate several leading T2I models and find that
over-refusal is a widespread issue across various categories (Figure 1),
underscoring the need for further research to enhance the safety alignment of
T2I models without compromising their functionality. As a preliminary attempt
to reduce over-refusal, we explore prompt rewriting; however, we find it often
compromises faithfulness to the meaning of the original prompts. Finally, we
demonstrate the flexibility of our generation framework in accommodating
diverse safety requirements by generating customized evaluation data adapting
to user-defined policies.

</details>


### [386] [LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing](https://arxiv.org/abs/2505.21732)
*Ruijie Zhang,Ziyue Liu,Zhengyang Wang,Zheng Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Training foundation models such as ViTs and LLMs requires tremendous
computing cost. Low-rank matrix or tensor factorization offers a
parameter-efficient alternative, but often downgrades performance due to the
restricted parameter space. In this work, we introduce {\textbf{Latent Crossing
(LaX)}} -- a simple yet effective plug-and-play module that enhances the
capacity of low-rank models by enabling information flow across low-rank
subspaces. We extensively validate the benefits of LaX on pre-training tasks
with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.
LaX boosts low-rank model performance to match or exceed the full-rank
baselines while using 2-3\(\times\) fewer parameters. When equipped with
low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently
improves performance on arithmetic and common sense reasoning tasks with
negligible cost.

</details>


### [387] [ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods](https://arxiv.org/abs/2505.22494)
*Michal Kmicikiewicz,Vincent Fortuin,Ewa Szczurek*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Designing protein sequences of both high fitness and novelty is a challenging
task in data-efficient protein engineering. Exploration beyond wild-type
neighborhoods often leads to biologically implausible sequences or relies on
surrogate models that lose fidelity in novel regions. Here, we propose
ProSpero, an active learning framework in which a frozen pre-trained generative
model is guided by a surrogate updated from oracle feedback. By integrating
fitness-relevant residue selection with biologically-constrained Sequential
Monte Carlo sampling, our approach enables exploration beyond wild-type
neighborhoods while preserving biological plausibility. We show that our
framework remains effective even when the surrogate is misspecified. ProSpero
consistently outperforms or matches existing methods across diverse protein
engineering tasks, retrieving sequences of both high fitness and novelty.

</details>


### [388] [SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training](https://arxiv.org/abs/2505.24749)
*Yehonathan Refael,Guy Smorodinsky,Tom Tirer,Ofir Lindenbaum*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Low-rank gradient-based optimization methods have significantly improved
memory efficiency during the training of large language models (LLMs), enabling
operations within constrained hardware without sacrificing performance.
However, these methods primarily emphasize memory savings, often overlooking
potential acceleration in convergence due to their reliance on standard
isotropic steepest descent techniques, which can perform suboptimally in the
highly anisotropic landscapes typical of deep networks, particularly LLMs. In
this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an
optimizer that employs exact singular value decomposition (SVD) for moment
orthogonalization within a dynamically adapted low-dimensional subspace,
enabling norm-inducing steepest descent optimization steps. By explicitly
aligning optimization steps with the spectral characteristics of the loss
landscape, SUMO effectively mitigates approximation errors associated with
commonly used methods like Newton-Schulz orthogonalization approximation. We
theoretically establish an upper bound on these approximation errors, proving
their dependence on the condition numbers of moments, conditions we
analytically demonstrate are encountered during LLM training. Furthermore, we
both theoretically and empirically illustrate that exact orthogonalization via
SVD substantially improves convergence rates while reducing overall complexity.
Empirical evaluations confirm that SUMO accelerates convergence, enhances
stability, improves performance, and reduces memory requirements by up to 20%
compared to state-of-the-art methods.

</details>


### [389] [Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs](https://arxiv.org/abs/2506.00846)
*Mana Sakai,Ryo Karakida,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In modern theoretical analyses of neural networks, the infinite-width limit
is often invoked to justify Gaussian approximations of neuron preactivations
(e.g., via neural network Gaussian processes or Tensor Programs). However,
these Gaussian-based asymptotic theories have so far been unable to capture the
behavior of attention layers, except under special regimes such as infinitely
many heads or tailored scaling schemes. In this paper, leveraging the Tensor
Programs framework, we rigorously identify the infinite-width limit
distribution of variables within a single attention layer under realistic
architectural dimensionality and standard $1/\sqrt{n}$-scaling with $n$
dimensionality. We derive the exact form of this limit law without resorting to
infinite-head approximations or tailored scalings, demonstrating that it
departs fundamentally from Gaussianity. This limiting distribution exhibits
non-Gaussianity from a hierarchical structure, being Gaussian conditional on
the random similarity scores. Numerical experiments validate our theoretical
predictions, confirming the effectiveness of our theory at finite width and
accurate description of finite-head attentions. Beyond characterizing a
standalone attention layer, our findings lay the groundwork for developing a
unified theory of deep Transformer architectures in the infinite-width regime.

</details>


### [390] [On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective](https://arxiv.org/abs/2506.01213)
*Ning Zhang,Henry Kenlay,Li Zhang,Mihai Cucuringu,Xiaowen Dong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph convolutional neural networks (GCNNs) have emerged as powerful tools
for analyzing graph-structured data, achieving remarkable success across
diverse applications. However, the theoretical understanding of the stability
of these models, i.e., their sensitivity to small changes in the graph
structure, remains in rather limited settings, hampering the development and
deployment of robust and trustworthy models in practice. To fill this gap, we
study how perturbations in the graph topology affect GCNN outputs and propose a
novel formulation for analyzing model stability. Unlike prior studies that
focus only on worst-case perturbations, our distribution-aware formulation
characterizes output perturbations across a broad range of input data. This
way, our framework enables, for the first time, a probabilistic perspective on
the interplay between the statistical properties of the node data and
perturbations in the graph topology. We conduct extensive experiments to
validate our theoretical findings and demonstrate their benefits over existing
baselines, in terms of both representation stability and adversarial attacks on
downstream tasks. Our results demonstrate the practical significance of the
proposed formulation and highlight the importance of incorporating data
distribution into stability analysis.

</details>


### [391] [Assessing the Completeness of Traffic Scenario Categories for Automated Highway Driving Functions via Cluster-based Analysis](https://arxiv.org/abs/2506.02599)
*Niklas Roßberg,Marion Neumeier,Sinan Hasirlioglu,Mohamed Essayed Bouzouraa,Michael Botsch*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The ability to operate safely in increasingly complex traffic scenarios is a
fundamental requirement for Automated Driving Systems (ADS). Ensuring the safe
release of ADS functions necessitates a precise understanding of the occurring
traffic scenarios. To support this objective, this work introduces a pipeline
for traffic scenario clustering and the analysis of scenario category
completeness. The Clustering Vector Quantized - Variational Autoencoder
(CVQ-VAE) is employed for the clustering of highway traffic scenarios and
utilized to create various catalogs with differing numbers of traffic scenario
categories. Subsequently, the impact of the number of categories on the
completeness considerations of the traffic scenario categories is analyzed. The
results show an outperforming clustering performance compared to previous work.
The trade-off between cluster quality and the amount of required data to
maintain completeness is discussed based on the publicly available highD
dataset.

</details>


### [392] [Statistically Valid Post-Deployment Monitoring Should Be Standard for AI-Based Digital Health](https://arxiv.org/abs/2506.05701)
*Pavel Dolin,Weizhi Li,Gautam Dasarathy,Visar Berisha*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This position paper argues that post-deployment monitoring in clinical AI is
underdeveloped and proposes statistically valid and label-efficient testing
frameworks as a principled foundation for ensuring reliability and safety in
real-world deployment. A recent review found that only 9% of FDA-registered
AI-based healthcare tools include a post-deployment surveillance plan. Existing
monitoring approaches are often manual, sporadic, and reactive, making them
ill-suited for the dynamic environments in which clinical models operate. We
contend that post-deployment monitoring should be grounded in label-efficient
and statistically valid testing frameworks, offering a principled alternative
to current practices. We use the term "statistically valid" to refer to methods
that provide explicit guarantees on error rates (e.g., Type I/II error), enable
formal inference under pre-defined assumptions, and support
reproducibility--features that align with regulatory requirements.
Specifically, we propose that the detection of changes in the data and model
performance degradation should be framed as distinct statistical hypothesis
testing problems. Grounding monitoring in statistical rigor ensures a
reproducible and scientifically sound basis for maintaining the reliability of
clinical AI systems. Importantly, it also opens new research directions for the
technical community--spanning theory, methods, and tools for statistically
principled detection, attribution, and mitigation of post-deployment model
failures in real-world settings.

</details>


### [393] [Membership Inference Attacks for Unseen Classes](https://arxiv.org/abs/2506.06488)
*Pratiksha Thaker,Neil Kale,Zhiwei Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The state-of-the-art for membership inference attacks on machine learning
models is a class of attacks based on shadow models that mimic the behavior of
the target model on subsets of held-out nonmember data. However, we find that
this class of attacks is fundamentally limited because of a key assumption --
that the shadow models can replicate the target model's behavior on the
distribution of interest. As a result, we show that attacks relying on shadow
models can fail catastrophically on critical AI safety applications where data
access is restricted due to legal, ethical, or logistical constraints, so that
the shadow models have no reasonable signal on the query examples. Although
this problem seems intractable within the shadow model paradigm, we find that
quantile regression attacks are a promising approach in this setting, as these
models learn features of member examples that can generalize to unseen classes.
We demonstrate this both empirically and theoretically, showing that quantile
regression attacks achieve up to 11x the TPR of shadow model-based approaches
in practice, and providing a theoretical model that outlines the generalization
properties required for this approach to succeed. Our work identifies an
important failure mode in existing MIAs and provides a cautionary tale for
practitioners that aim to directly use existing tools for real-world
applications of AI safety.

</details>


### [394] [Optimal Rates in Continual Linear Regression via Increasing Regularization](https://arxiv.org/abs/2506.06501)
*Ran Levinstein,Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study realizable continual linear regression under random task orderings,
a common setting for developing continual learning theory. In this setup, the
worst-case expected loss after $k$ learning iterations admits a lower bound of
$\Omega(1/k)$. However, prior work using an unregularized scheme has only
established an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our
paper proves that this gap can be narrowed, or even closed, using two
frequently used regularization schemes: (1) explicit isotropic $\ell_2$
regularization, and (2) implicit regularization via finite step budgets. We
show that these approaches, which are used in practice to mitigate forgetting,
reduce to stochastic gradient descent (SGD) on carefully defined surrogate
losses. Through this lens, we identify a fixed regularization strength that
yields a near-optimal rate of $O(\log k / k)$. Moreover, formalizing and
analyzing a generalized variant of SGD for time-varying functions, we derive an
increasing regularization strength schedule that provably achieves an optimal
rate of $O(1/k)$. This suggests that schedules that increase the regularization
coefficient or decrease the number of steps per task are beneficial, at least
in the worst case.

</details>


### [395] [TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval](https://arxiv.org/abs/2506.09114)
*Jialin Chen,Ziyu Zhao,Gaukhar Nurbek,Aosong Feng,Ali Maatouk,Leandros Tassiulas,Yifeng Gao,Rex Ying*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The ubiquity of dynamic data in domains such as weather, healthcare, and
energy underscores a growing need for effective interpretation and retrieval of
time-series data. These data are inherently tied to domain-specific contexts,
such as clinical notes or weather narratives, making cross-modal retrieval
essential not only for downstream tasks but also for developing robust
time-series foundation models by retrieval-augmented generation (RAG). Despite
the increasing demand, time-series retrieval remains largely underexplored.
Existing methods often lack semantic grounding, struggle to align heterogeneous
modalities, and have limited capacity for handling multi-channel signals. To
address this gap, we propose TRACE, a generic multimodal retriever that grounds
time-series embeddings in aligned textual context. TRACE enables fine-grained
channel-level alignment and employs hard negative mining to facilitate
semantically meaningful retrieval. It supports flexible cross-modal retrieval
modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking
linguistic descriptions with complex temporal patterns. By retrieving
semantically relevant pairs, TRACE enriches downstream models with informative
context, leading to improved predictive accuracy and interpretability. Beyond a
static retrieval engine, TRACE also serves as a powerful standalone encoder,
with lightweight task-specific tuning that refines context-aware
representations while maintaining strong cross-modal alignment. These
representations achieve state-of-the-art performance on downstream forecasting
and classification tasks. Extensive experiments across multiple domains
highlight its dual utility, as both an effective encoder for downstream
applications and a general-purpose retriever to enhance time-series models.

</details>


### [396] [Boost Post-Training Quantization via Null Space Optimization for Large Language Models](https://arxiv.org/abs/2506.11044)
*Jiaqi Zhao,Miao Zhang,Deng Xiang,Ming Li,Weili Guan,Liqiang Nie*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Existing post-training quantization methods for large language models (LLMs)
offer remarkable success. However, the increasingly marginal performance gains
suggest that existing quantization strategies are insufficient to support the
development of more compressed models. To inspire new directions for future
research, this paper introduces the concept of null space into LLMs
quantization. We argue that the quantization error can be effectively
alleviated by constraining the post-quantization weight perturbation to lie
within the null space of input activations. To prove this idea, we propose a
plug-and-play null space projection module for existing milestone PTQ baselines
named Q2N. Specifically, we first design an efficient and accurate null space
projection approximation method tailored to the characteristics of LLMs.
Subsequently, we theoretically derive a closed-form solution for an equivalent
vector of the obtained projection matrix, which satisfies practical inference
condition while avoiding additional memory overhead. Extensive experiments are
conducted on various state-of-the-art LLMs (LLaMA3, DeepSeek, Qwen3) and
baselines, demonstrating the effectiveness of both our Q2N and the perspective
of null space optimization for LLMs quantization. We view this paper the first
step to further alleviate the quantization error based on the insights of null
space, hoping it inspiring future researchers to design more advanced
quantization methods. Codes are available at https://github.com/zjq0455/q2n.

</details>


### [397] [In Defense of Defensive Forecasting](https://arxiv.org/abs/2506.11848)
*Juan Carlos Perdomo,Benjamin Recht*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This tutorial provides a survey of algorithms for Defensive Forecasting,
where predictions are derived not by prognostication but by correcting past
mistakes. Pioneered by Vovk, Defensive Forecasting frames the goal of
prediction as a sequential game, and derives predictions to minimize metrics no
matter what outcomes occur. We present an elementary introduction to this
general theory and derive simple, near-optimal algorithms for online learning,
calibration, prediction with expert advice, and online conformal prediction.

</details>


### [398] [Path-specific effects for pulse-oximetry guided decisions in critical care](https://arxiv.org/abs/2506.12371)
*Kevin Zhang,Yonghan Jung,Divyat Mahajan,Karthikeyan Shanmugam,Shalmali Joshi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Identifying and measuring biases associated with sensitive attributes is a
crucial consideration in healthcare to prevent treatment disparities. One
prominent issue is inaccurate pulse oximeter readings, which tend to
overestimate oxygen saturation for dark-skinned patients and misrepresent
supplemental oxygen needs. Most existing research has revealed statistical
disparities linking device measurement errors to patient outcomes in intensive
care units (ICUs) without causal formalization. This study causally
investigates how racial discrepancies in oximetry measurements affect invasive
ventilation in ICU settings. We employ a causal inference-based approach using
path-specific effects to isolate the impact of bias by race on clinical
decision-making. To estimate these effects, we leverage a doubly robust
estimator, propose its self-normalized variant for improved sample efficiency,
and provide novel finite-sample guarantees. Our methodology is validated on
semi-synthetic data and applied to two large real-world health datasets:
MIMIC-IV and eICU. Contrary to prior work, our analysis reveals minimal impact
of racial discrepancies on invasive ventilation rates. However, path-specific
effects mediated by oxygen saturation disparity are more pronounced on
ventilation duration, and the severity differs by dataset. Our work provides a
novel pipeline for investigating potential disparities in clinical
decision-making and, more importantly, highlights the necessity of causal
methods to robustly assess fairness in healthcare.

</details>


### [399] [Complexity Scaling Laws for Neural Models using Combinatorial Optimization](https://arxiv.org/abs/2506.12932)
*Lowell Weissman,Michael Krumdick,A. Lynn Abbott*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent work on neural scaling laws demonstrates that model performance scales
predictably with compute budget, model size, and dataset size. In this work, we
develop scaling laws based on problem complexity. We analyze two fundamental
complexity measures: solution space size and representation space size. Using
the Traveling Salesman Problem (TSP) as a case study, we show that
combinatorial optimization promotes smooth cost trends, and therefore
meaningful scaling laws can be obtained even in the absence of an interpretable
loss. We then show that suboptimality grows predictably for fixed-size models
when scaling the number of TSP nodes or spatial dimensions, independent of
whether the model was trained with reinforcement learning or supervised
fine-tuning on a static dataset. We conclude with an analogy to problem
complexity scaling in local search, showing that a much simpler gradient
descent of the cost landscape produces similar trends.

</details>


### [400] [MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation](https://arxiv.org/abs/2506.14436)
*Shen Yuan,Yin Zheng,Taifeng Wang,Binbin Liu,Hongteng Xu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Adapting large-scale foundation models in multi-task scenarios often suffers
from task conflict and oblivion. To mitigate such issues, we propose a novel
''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant
multi-task adaptation method. Given a weight matrix of a pre-trained model, our
method applies SVD to it and introduces a learnable router to adjust its
singular values based on tasks and samples. Accordingly, the weight matrix
becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert
corresponds to the outer product of a left singular vector and the
corresponding right one. We can improve the model capacity by imposing a
learnable orthogonal transform on the right singular vectors. Unlike low-rank
adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts'
orthogonality and maintains the column space of the original weight matrix.
These two properties make the adapted model resistant to the conflicts among
the new tasks and the oblivion of its original tasks, respectively. Experiments
on various datasets demonstrate that MoORE outperforms existing multi-task
adaptation methods consistently, showing its superiority in terms of conflict-
and oblivion-resistance. The code of the experiments is available at
https://github.com/DaShenZi721/MoORE.

</details>


### [401] [Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration](https://arxiv.org/abs/2506.15721)
*Junqi Gao,Zhichang Guo,Dazhi Zhang,Dong Li,Runze Liu,Pengfei Li,Kai Tian,Biqing Qi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of
multiple source LLMs with different architectures into a target LLM with low
computational overhead. While promising, existing methods suffer from two major
limitations: 1) reliance on real data from limited domain for knowledge fusion,
preventing the target LLM from fully acquiring knowledge across diverse
domains, and 2) fixed data allocation proportions across domains, failing to
dynamically adjust according to the target LLM's varying capabilities across
domains, leading to a capability imbalance. To overcome these limitations, we
propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.
Through the organization of knowledge domains into a hierarchical tree
structure, Bohdi enables automatic domain exploration and multi-domain data
generation through multi-model collaboration, thereby comprehensively
extracting knowledge from source LLMs. By formalizing domain expansion and data
sampling proportion allocation on the knowledge tree as a Hierarchical
Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism
to adaptively adjust sampling proportions based on the target LLM's performance
feedback across domains. Integrated with our proposed Introspection-Rebirth
(IR) mechanism, DynaBranches dynamically tracks capability shifts during target
LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),
further enhancing its online adaptation capability. Comparative experimental
results on a comprehensive suite of benchmarks demonstrate that Bohdi
significantly outperforms existing baselines on multiple target LLMs, exhibits
higher data efficiency, and virtually eliminates the imbalance in the target
LLM's capabilities. Our code is available at
https://github.com/gjq100/Bohdi.git.

</details>


### [402] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/abs/2506.20324)
*Torben Berndt,Benjamin Walker,Tiexin Qin,Jan Stühmer,Andrey Kormilitzin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [403] [Wearable Sensor-Based IoT XAI Framework for Predicting Freezing of Gait in Parkinsons Disease](https://arxiv.org/abs/2507.01068)
*Biplov Paneru*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This research discusses the critical need for early detection and treatment
for early prediction of Freezing of Gaits (FOG) utilizing a wearable sensor
technology powered with LoRa communication. The system consisted of an Esp-32
microcontroller, in which the trained model is utilized utilizing the
Micromlgen Python library. The research investigates accurate FOG
classification based on pertinent clinical data by utilizing machine learning
(ML) algorithms like Catboost, XGBoost, and Extra Tree classifiers. The XGBoost
could classify with approximately 97% accuracy, along with 96% for the catboost
and 90% for the Extra Trees Classifier model. The SHAP analysis
interpretability shows that GYR SI degree is the most affecting factor in the
prediction of the diseases. These results show the possibility of monitoring
and identifying the affected person with tracking location on GPS and providing
aid as an assistive technology for aiding the affected. The developed
sensor-based technology has great potential for real-world problem solving in
the field of healthcare and biomedical technology enhancements.

</details>


### [404] [BLaST: High Performance Inference and Pretraining using BLock Sparse Transformers](https://arxiv.org/abs/2507.03117)
*Patrik Okanovic,Sameer Deshmukh,Grzegorz Kwasniewski,Yi Zhu,Haruto Fujii,Sakina Fatima,Maciej Besta,Kentaro Katayama,Takumi Honda,Yusuke Nagasaka,Torsten Hoefler*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The energy consumption of large-scale ML models is dominated by data
movement, shuffling billions of parameters across memory hierarchies and data
centers. Sparsification offers a principled way to mitigate these costs by
pruning redundant weights and activations, thereby reducing data movement.
Effective sparsification to prune redundant parameters is still challenging:
existing methods incur significant accuracy degradation, performance overhead,
or both. We introduce (Bl)ock (a)nd (S)parse (T)ransformers (BLaST), a general,
robust, and reliable method for sparsification, applicable to linear layers in
all settings. Our method iteratively sparsifies weight matrices into a block
sparsity pattern suitable for efficient sparse matrix-matrix (SpMM)
multiplication. BLaST achieves up to 95% sparsity in MLP weights with
negligible accuracy loss (majority <2.25%). We show a 2.2x inference speedup
for Llama 3.2 with 16 GPUs, and up to 4.45x reduction in inference memory
footprint resulting in a 2.9x reduction in GPU setup and operating costs.

</details>


### [405] [MPX: Mixed Precision Training for JAX](https://arxiv.org/abs/2507.03312)
*Alexander Gräfe,Sebastian Trimpe*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mixed-precision training has emerged as an indispensable tool for enhancing
the efficiency of neural network training in recent years. Concurrently, JAX
has grown in popularity as a versatile machine learning toolbox. However, it
currently lacks robust support for mixed-precision training. We propose MPX, a
mixed-precision training toolbox for JAX that simplifies and accelerates the
training of large-scale neural networks while preserving model accuracy. MPX
seamlessly integrates with popular toolboxes such as Equinox and Flax, allowing
users to convert full-precision pipelines to mixed-precision versions with
minimal modifications. By casting both inputs and outputs to half precision,
and introducing a dynamic loss-scaling mechanism, MPX alleviates issues like
gradient underflow and overflow that commonly arise in half precision
computations. Its design inherits critical features from JAX's type-promotion
behavior, ensuring that operations take place in the correct precision and
allowing for selective enforcement of full precision where needed (e.g., sums,
means, or softmax). MPX further provides wrappers for automatic creation and
management of mixed-precision gradients and optimizers, enabling
straightforward integration into existing JAX training pipelines. MPX's source
code, documentation, and usage examples are available at
github.com/Data-Science-in-Mechanical-Engineering/mixed_precision_for_JAX .

</details>


### [406] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim Bouaziz,Manon Revel,Jack Kussman,Yasha Sheynin,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [407] [Continental-scale habitat distribution modelling with multimodal earth observation foundation models](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Yoann Cartier,Borja Jiménez-Alfaro,Fabio Attorre,Jens-Christian Svenning,Wilfried Thuiller*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Habitats integrate the abiotic conditions, vegetation composition and
structure that support biodiversity and sustain nature's contributions to
people. Most habitats face mounting pressures from human activities, which
requires accurate, high-resolution habitat mapping for effective conservation
and restoration. Yet, current habitat maps often fall short in thematic or
spatial resolution because they must (1) model several mutually exclusive
habitat types that co-occur across landscapes and (2) cope with severe class
imbalance that complicates exhaustive multi-class training. Here, we evaluated
how high-resolution remote sensing (RS) data and Artificial Intelligence (AI)
tools can improve habitat mapping across large geographical extents at fine
spatial and thematic resolution. Using vegetation plots from the European
Vegetation Archive, we modelled the distribution of Level 3 EUNIS habitat types
across Europe and assessed multiple modelling strategies against independent
validation datasets. Strategies that exploited the hierarchical nature of
habitat classifications resolved classification ambiguities, especially in
fragmented habitats. Integrating satellite-borne multispectral and radar
imagery, particularly through Earth Observation (EO) Foundation models
(EO-FMs), enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
predictive accuracy even further. Our methodological framework is transferable
beyond Europe and adaptable to other classification systems. Future research
should advance temporal modelling of habitat dynamics, extend to habitat
segmentation and quality assessment, and exploit next-generation EO data paired
with higher-quality in situ observations.

</details>


### [408] [PICore: Physics-Informed Unsupervised Coreset Selection for Data Efficient Neural Operator Training](https://arxiv.org/abs/2507.17151)
*Anirudh Satheesh,Anant Khandelwal,Mucong Ding,Radu Balan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neural operators offer a powerful paradigm for solving partial differential
equations (PDEs) that cannot be solved analytically by learning mappings
between function spaces. However, there are two main bottlenecks in training
neural operators: they require a significant amount of training data to learn
these mappings, and this data needs to be labeled, which can only be accessed
via expensive simulations with numerical solvers. To alleviate both of these
issues simultaneously, we propose PICore, an unsupervised coreset selection
framework that identifies the most informative training samples without
requiring access to ground-truth PDE solutions. PICore leverages a
physics-informed loss to select unlabeled inputs by their potential
contribution to operator learning. After selecting a compact subset of inputs,
only those samples are simulated using numerical solvers to generate labels,
reducing annotation costs. We then train the neural operator on the reduced
labeled dataset, significantly decreasing training time as well. Across four
diverse PDE benchmarks and multiple coreset selection strategies, PICore
achieves up to 78% average increase in training efficiency relative to
supervised coreset selection methods with minimal changes in accuracy. We
provide code at https://github.com/Asatheesh6561/PICore.

</details>


### [409] [Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods](https://arxiv.org/abs/2507.18242)
*Fabian Akkerman,Julien Ferry,Christian Artigues,Emmanuel Hebrard,Thibaut Vidal*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite their theoretical appeal, totally corrective boosting methods based
on linear programming have received limited empirical attention. In this paper,
we conduct the first large-scale experimental study of six LP-based boosting
formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20
diverse datasets. We evaluate the use of both heuristic and optimal base
learners within these formulations, and analyze not only accuracy, but also
ensemble sparsity, margin distribution, anytime performance, and hyperparameter
sensitivity. We show that totally corrective methods can outperform or match
state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,
while producing significantly sparser ensembles. We further show that these
methods can thin pre-trained ensembles without sacrificing performance, and we
highlight both the strengths and limitations of using optimal decision trees in
this context.

</details>


### [410] [Language Model Guided Reinforcement Learning in Quantitative Trading](https://arxiv.org/abs/2508.02366)
*Adam Darmanin,Vince Vella*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Algorithmic trading requires short-term tactical decisions consistent with
long-term financial objectives. Reinforcement Learning (RL) has been applied to
such problems, but adoption is limited by myopic behaviour and opaque policies.
Large Language Models (LLMs) offer complementary strategic reasoning and
multi-modal signal interpretation when guided by well-structured prompts. This
paper proposes a hybrid framework in which LLMs generate high-level trading
strategies to guide RL agents. We evaluate (i) the economic rationale of
LLM-generated strategies through expert review, and (ii) the performance of
LLM-guided agents against unguided RL baselines using Sharpe Ratio (SR) and
Maximum Drawdown (MDD). Empirical results indicate that LLM guidance improves
both return and risk metrics relative to standard RL.

</details>


### [411] [Learning Robust Satellite Attitude Dynamics with Physics-Informed Normalising Flow](https://arxiv.org/abs/2508.07841)
*Carlo Cena,Mauro Martini,Marcello Chiaberge*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Attitude control is a fundamental aspect of spacecraft operations. Model
Predictive Control (MPC) has emerged as a powerful strategy for these tasks,
relying on accurate models of the system dynamics to optimize control actions
over a prediction horizon. In scenarios where physics models are incomplete,
difficult to derive, or computationally expensive, machine learning offers a
flexible alternative by learning the system behavior directly from data.
However, purely data-driven models often struggle with generalization and
stability, especially when applied to inputs outside their training domain. To
address these limitations, we investigate the benefits of incorporating
Physics-Informed Neural Networks (PINNs) into the learning of spacecraft
attitude dynamics, comparing their performance with that of purely data-driven
approaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network
architecture with a self-attention mechanism, we trained several models on
simulated data generated with the Basilisk simulator. Two training strategies
were considered: a purely data-driven baseline and a physics-informed variant
to improve robustness and stability. Our results demonstrate that the inclusion
of physics-based information significantly enhances the performance in terms of
the mean relative error with the best architectures found by 27.08%. These
advantages are particularly evident when the learned models are integrated into
an MPC framework, where PINN-based models consistently outperform their purely
data-driven counterparts in terms of control accuracy and robustness, and
achieve improved settling times when compared to traditional MPC approaches,
yielding improvements of up to 62%, when subject to observation noise and RWs
friction.

</details>


### [412] [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221)
*Zihe Liu,Jiashun Liu,Yancheng He,Weixun Wang,Jiaheng Liu,Ling Pan,Xinyu Hu,Shaopan Xiong,Ju Huang,Jian Hu,Shengyi Huang,Johan Obando-Ceron,Siran Yang,Jiamang Wang,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent
research area, marked by a significant surge in related studies on both
algorithmic innovations and practical applications. Despite this progress,
several critical challenges remain, including the absence of standardized
guidelines for employing RL techniques and a fragmented understanding of their
underlying mechanisms. Additionally, inconsistent experimental settings,
variations in training data, and differences in model initialization have led
to conflicting conclusions, obscuring the key characteristics of these
techniques and creating confusion among practitioners when selecting
appropriate techniques. This paper systematically reviews widely adopted RL
techniques through rigorous reproductions and isolated evaluations within a
unified open-source framework. We analyze the internal mechanisms, applicable
scenarios, and core principles of each technique through fine-grained
experiments, including datasets of varying difficulty, model sizes, and
architectures. Based on these insights, we present clear guidelines for
selecting RL techniques tailored to specific setups, and provide a reliable
roadmap for practitioners navigating the RL for the LLM domain. Finally, we
reveal that a minimalist combination of two techniques can unlock the learning
capability of critic-free policies using vanilla PPO loss. The results
demonstrate that our simple combination consistently improves performance,
surpassing strategies like GRPO and DAPO.

</details>


### [413] [Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491)
*Reza Shirkavand,Shangqian Gao,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study cost-aware routing for large language models across diverse and
dynamic pools of models. Existing approaches often overlook prompt-specific
context, rely on expensive model profiling, assume a fixed set of experts, or
use inefficient trial-and-error strategies. We introduce Cost-Spectrum
Contrastive Routing (CSCR), a lightweight framework that maps both prompts and
models into a shared embedding space to enable fast, cost-sensitive selection.
CSCR uses compact, fast-to-compute logit footprints for open-source models and
perplexity fingerprints for black-box APIs. A contrastive encoder is trained to
favor the cheapest accurate expert within adaptive cost bands. At inference
time, routing reduces to a single k-NN lookup via a FAISS index, requiring no
retraining when the expert pool changes and enabling microsecond latency.
Across multiple benchmarks, CSCR consistently outperforms baselines, improving
the accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen
LLMs and out-of-distribution prompts.

</details>


### [414] [RotaTouille: Rotation Equivariant Deep Learning for Contours](https://arxiv.org/abs/2508.16359)
*Odin Hoff Gardaa,Nello Blaser*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Contours or closed planar curves are common in many domains. For example,
they appear as object boundaries in computer vision, isolines in meteorology,
and the orbits of rotating machinery. In many cases when learning from contour
data, planar rotations of the input will result in correspondingly rotated
outputs. It is therefore desirable that deep learning models be rotationally
equivariant. In addition, contours are typically represented as an ordered
sequence of edge points, where the choice of starting point is arbitrary. It is
therefore also desirable for deep learning methods to be equivariant under
cyclic shifts. We present RotaTouille, a deep learning framework for learning
from contour data that achieves both rotation and cyclic shift equivariance
through complex-valued circular convolution. We further introduce and
characterize equivariant non-linearities, coarsening layers, and global pooling
layers to obtain invariant representations for downstream tasks. Finally, we
demonstrate the effectiveness of RotaTouille through experiments in shape
classification, reconstruction, and contour regression.

</details>


### [415] [DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches](https://arxiv.org/abs/2509.05663)
*Lucas Correia,Jan-Christoph Goos,Thomas Bäck,Anna V. Kononova*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Truly unsupervised approaches for time series anomaly detection are rare in
the literature. Those that exist suffer from a poorly set threshold, which
hampers detection performance, while others, despite claiming to be
unsupervised, need to be calibrated using a labelled data subset, which is
often not available in the real world. This work integrates active learning
with an existing unsupervised anomaly detection method by selectively querying
the labels of multivariate time series, which are then used to refine the
threshold selection process. To achieve this, we introduce a novel query
strategy called the dissimilarity-based query strategy (DQS). DQS aims to
maximise the diversity of queried samples by evaluating the similarity between
anomaly scores using dynamic time warping. We assess the detection performance
of DQS in comparison to other query strategies and explore the impact of
mislabelling, a topic that is underexplored in the literature. Our findings
indicate that DQS performs best in small-budget scenarios, though the others
appear to be more robust when faced with mislabelling. Therefore, in the real
world, the choice of query strategy depends on the expertise of the oracle and
the number of samples they are willing to label. Regardless, all query
strategies outperform the unsupervised threshold even in the presence of
mislabelling. Thus, whenever it is feasible to query an oracle, employing an
active learning-based threshold is recommended.

</details>


### [416] [Foundational theory for optimal decision tree problems. I. Algorithmic and geometric foundations](https://arxiv.org/abs/2509.11226)
*Xi He*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In the first paper (part I) of this series of two, we introduce four novel
definitions of the ODT problems: three for size-constrained trees and one for
depth-constrained trees. These definitions are stated unambiguously through
executable recursive programs, satisfying all criteria we propose for a formal
specification. In this sense, they resemble the "standard form" used in the
study of general-purpose solvers.
  Grounded in algebraic programming theory-a relational formalism for deriving
correct-by-construction algorithms from specifications-we can not only
establish the existence or nonexistence of dynamic programming solutions but
also derive them constructively whenever they exist. Consequently, the four
generic problem definitions yield four novel optimal algorithms for ODT
problems with arbitrary splitting rules that satisfy the axioms and objective
functions of a given form. These algorithms encompass the known
depth-constrained, axis-parallel ODT algorithm as the special case, while
providing a unified, efficient, and elegant solution for the general ODT
problem.
  In Part II, we present the first optimal hypersurface decision tree algorithm
and provide comprehensive experiments against axis-parallel decision tree
algorithms, including heuristic CART and state-of-the-art optimal methods. The
results demonstrate the significant potential of decision trees with flexible
splitting rules. Moreover, our framework is readily extendable to support
algorithms for constructing even more flexible decision trees, including those
with mixed splitting rules.

</details>


### [417] [On Linear Mode Connectivity of Mixture-of-Experts Architectures](https://arxiv.org/abs/2509.11348)
*Viet-Hoang Tran,Van Hoan Trinh,Khanh Vinh Bui,Tan M. Nguyen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Linear Mode Connectivity (LMC) is a notable phenomenon in the loss landscapes
of neural networks, wherein independently trained models have been observed to
be connected--up to permutation symmetries--by linear paths in parameter space
along which the loss remains consistently low. This observation challenges
classical views of non-convex optimization and has implications for model
ensembling, generalization, and our understanding of neural loss geometry.
Inspired by recent studies on LMC in standard neural networks, we
systematically investigate this phenomenon within Mixture-of-Experts (MoE)
architectures--a class of models known for their scalability and computational
efficiency, which combine traditional neural networks--referred to as
experts--through a learnable gating mechanism. We begin by conducting a
comprehensive analysis of both dense and sparse gating regimes, demonstrating
that the symmetries inherent to MoE architectures are fully characterized by
permutations acting on both the expert components and the gating function.
Building on these foundational findings, we propose a matching algorithm that
enables alignment between independently trained MoEs, thereby facilitating the
discovery of LMC. Finally, we empirically validate the presence of LMC using
our proposed algorithm across diverse MoE configurations--including dense,
sparse, and shared-expert variants--under a wide range of model settings and
datasets of varying scales and modalities. Our results confirm the existence of
LMC in MoE architectures and offer fundamental insights into the functional
landscape and optimization dynamics of deep learning models.

</details>


### [418] [Foundational theory for optimal decision tree problems. II. Optimal hypersurface decision tree algorithm](https://arxiv.org/abs/2509.12057)
*Xi He*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Decision trees are a ubiquitous model for classification and regression tasks
due to their interpretability and efficiency. However, solving the optimal
decision tree (ODT) problem remains a challenging combinatorial optimization
task. Even for the simplest splitting rules--axis-parallel hyperplanes--it is
NP-hard to optimize. In Part I of this series, we rigorously defined the proper
decision tree model through four axioms and, based on these, introduced four
formal definitions of the ODT problem. From these definitions, we derived four
generic algorithms capable of solving ODT problems for arbitrary decision trees
satisfying the axioms. We also analyzed the combinatorial geometric properties
of hypersurfaces, showing that decision trees defined by polynomial
hypersurface splitting rules satisfy the proper axioms that we proposed.
  In this second paper (Part II) of this two-part series, building on the
algorithmic and geometric foundations established in Part I, we introduce the
first hypersurface decision tree (HODT) algorithm. To the best of our
knowledge, existing optimal decision tree methods are, to date, limited to
hyperplane splitting rules--a special case of hypersurfaces--and rely on
general-purpose solvers. In contrast, our HODT algorithm addresses the general
hypersurface decision tree model without requiring external solvers.
  Using synthetic datasets generated from ground-truth hyperplane decision
trees, we vary tree size, data size, dimensionality, and label and feature
noise. Results showing that our algorithm recovers the ground truth more
accurately than axis-parallel trees and exhibits greater robustness to noise.
We also analyzed generalization performance across 30 real-world datasets,
showing that HODT can achieve up to 30% higher accuracy than the
state-of-the-art optimal axis-parallel decision tree algorithm when tree
complexity is properly controlled.

</details>


### [419] [Kuramoto Orientation Diffusion Models](https://arxiv.org/abs/2509.15328)
*Yue Song,T. Anderson Keller,Sevan Brodjian,Takeru Miyato,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Orientation-rich images, such as fingerprints and textures, often exhibit
coherent angular directional patterns that are challenging to model using
standard generative approaches based on isotropic Euclidean diffusion.
Motivated by the role of phase synchronization in biological systems, we
propose a score-based generative model built on periodic domains by leveraging
stochastic Kuramoto dynamics in the diffusion process. In neural and physical
systems, Kuramoto models capture synchronization phenomena across coupled
oscillators -- a behavior that we re-purpose here as an inductive bias for
structured image generation. In our framework, the forward process performs
\textit{synchronization} among phase variables through globally or locally
coupled oscillator interactions and attraction to a global reference phase,
gradually collapsing the data into a low-entropy von Mises distribution. The
reverse process then performs \textit{desynchronization}, generating diverse
patterns by reversing the dynamics with a learned score function. This approach
enables structured destruction during forward diffusion and a hierarchical
generation process that progressively refines global coherence into fine-scale
details. We implement wrapped Gaussian transition kernels and periodicity-aware
networks to account for the circular geometry. Our method achieves competitive
results on general image benchmarks and significantly improves generation
quality on orientation-dense datasets like fingerprints and textures.
Ultimately, this work demonstrates the promise of biologically inspired
synchronization dynamics as structured priors in generative modeling.

</details>


### [420] [Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria](https://arxiv.org/abs/2509.16040)
*Jorge-Humberto Urrea-Quintero,David Anton,Laura De Lorenzis,Henning Wessels*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The automated discovery of constitutive models from data has recently emerged
as a promising alternative to the traditional model calibration paradigm. In
this work, we present a fully automated framework for constitutive model
discovery that systematically pairs three sparse regression algorithms (Least
Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression
(LARS), and Orthogonal Matching Pursuit (OMP)) with three model selection
criteria: $K$-fold cross-validation (CV), Akaike Information Criterion (AIC),
and Bayesian Information Criterion (BIC). This pairing yields nine distinct
algorithms for model discovery and enables a systematic exploration of the
trade-off between sparsity, predictive performance, and computational cost.
While LARS serves as an efficient path-based solver for the
$\ell_1$-constrained problem, OMP is introduced as a tractable heuristic for
$\ell_0$-regularized selection. The framework is applied to both isotropic and
anisotropic hyperelasticity, utilizing both synthetic and experimental
datasets. Results reveal that all nine algorithm-criterion combinations perform
consistently well in discovering isotropic and anisotropic materials, yielding
highly accurate constitutive models. These findings broaden the range of viable
discovery algorithms beyond $\ell_1$-based approaches such as LASSO.

</details>


### [421] [On the Fragility of Contribution Score Computation in Federated Learning](https://arxiv.org/abs/2509.19921)
*Balazs Pejo,Marcell Frank,Krisztian Varga,Peter Veliczky,Gergely Biczok*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper investigates the fragility of contribution evaluation in federated
learning, a critical mechanism for ensuring fairness and incentivizing
participation. We argue that contribution scores are susceptible to significant
distortions from two fundamental perspectives: architectural sensitivity and
intentional manipulation. First, we explore how different model aggregation
methods impact these scores. While most research assumes a basic averaging
approach, we demonstrate that advanced techniques, including those designed to
handle unreliable or diverse clients, can unintentionally yet significantly
alter the final scores. Second, we explore vulnerabilities posed by poisoning
attacks, where malicious participants strategically manipulate their model
updates to inflate their own contribution scores or reduce the importance of
other participants. Through extensive experiments across diverse datasets and
model architectures, implemented within the Flower framework, we rigorously
show that both the choice of aggregation method and the presence of attackers
are potent vectors for distorting contribution scores, highlighting a critical
need for more robust evaluation schemes.

</details>


### [422] [Policy Compatible Skill Incremental Learning via Lazy Learning Interface](https://arxiv.org/abs/2509.20612)
*Daehee Lee,Dongsu Lee,TaeYoon Kwack,Wonje Choi,Honguk Woo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Skill Incremental Learning (SIL) is the process by which an embodied agent
expands and refines its skill set over time by leveraging experience gained
through interaction with its environment or by the integration of additional
data. SIL facilitates efficient acquisition of hierarchical policies grounded
in reusable skills for downstream tasks. However, as the skill repertoire
evolves, it can disrupt compatibility with existing skill-based policies,
limiting their reusability and generalization. In this work, we propose SIL-C,
a novel framework that ensures skill-policy compatibility, allowing
improvements in incrementally learned skills to enhance the performance of
downstream policies without requiring policy re-training or structural
adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to
dynamically align the subtask space referenced by policies with the skill space
decoded into agent behaviors. This enables each subtask, derived from the
policy's decomposition of a complex task, to be executed by selecting an
appropriate skill based on trajectory distribution similarity. We evaluate
SIL-C across diverse SIL scenarios and demonstrate that it maintains
compatibility between evolving skills and downstream policies while ensuring
efficiency throughout the learning process.

</details>


### [423] [Differentiable Structure Learning and Causal Discovery for General Binary Data](https://arxiv.org/abs/2509.21658)
*Chang Deng,Bryon Aragam*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Existing methods for differentiable structure learning in discrete data
typically assume that the data are generated from specific structural equation
models. However, these assumptions may not align with the true data-generating
process, which limits the general applicability of such methods. Furthermore,
current approaches often ignore the complex dependence structure inherent in
discrete data and consider only linear effects. We propose a differentiable
structure learning framework that is capable of capturing arbitrary
dependencies among discrete variables. We show that although general discrete
models are unidentifiable from purely observational data, it is possible to
characterize the complete set of compatible parameters and structures.
Additionally, we establish identifiability up to Markov equivalence under mild
assumptions. We formulate the learning problem as a single differentiable
optimization task in the most general form, thereby avoiding the unrealistic
simplifications adopted by previous methods. Empirical results demonstrate that
our approach effectively captures complex relationships in discrete data.

</details>


### [424] [Differentiable Sparsity via $D$-Gating: Simple and Versatile Structured Penalization](https://arxiv.org/abs/2509.23898)
*Chris Kolb,Laetitia Frost,Bernd Bischl,David Rügamer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Structured sparsity regularization offers a principled way to compact neural
networks, but its non-differentiability breaks compatibility with conventional
stochastic gradient descent and requires either specialized optimizers or
additional post-hoc pruning without formal guarantees. In this work, we propose
$D$-Gating, a fully differentiable structured overparameterization that splits
each group of weights into a primary weight vector and multiple scalar gating
factors. We prove that any local minimum under $D$-Gating is also a local
minimum using non-smooth structured $L_{2,2/D}$ penalization, and further show
that the $D$-Gating objective converges at least exponentially fast to the
$L_{2,2/D}$-regularized loss in the gradient flow limit. Together, our results
show that $D$-Gating is theoretically equivalent to solving the original group
sparsity problem, yet induces distinct learning dynamics that evolve from a
non-sparse regime into sparse optimization. We validate our theory across
vision, language, and tabular tasks, where $D$-Gating consistently delivers
strong performance-sparsity tradeoffs and outperforms both direct optimization
of structured penalties and conventional pruning baselines.

</details>


### [425] [Detecting and Rectifying Noisy Labels: A Similarity-based Approach](https://arxiv.org/abs/2509.23964)
*Dang Huu-Tien,Minh-Phuong Nguyen,Naoya Inoue*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Label noise in datasets could significantly damage the performance and
robustness of deep neural networks (DNNs) trained on these datasets. As the
size of modern DNNs grows, there is a growing demand for automated tools for
detecting such errors. In this paper, we propose post-hoc, model-agnostic noise
detection and rectification methods utilizing the penultimate feature from a
DNN. Our idea is based on the observation that the similarity between the
penultimate feature of a mislabeled data point and its true class data points
is higher than that for data points from other classes, making the probability
of label occurrence within a tight, similar cluster informative for detecting
and rectifying errors. Through theoretical and empirical analyses, we
demonstrate that our approach achieves high detection performance across
diverse, realistic noise scenarios and can automatically rectify these errors
to improve dataset quality. Our implementation is available at
https://anonymous.4open.science/r/noise-detection-and-rectification-AD8E.

</details>


### [426] [Transfer Learning on Edge Connecting Probability Estimation under Graphon Model](https://arxiv.org/abs/2510.05527)
*Yuyao Wang,Yu-Hung Cheng,Debarghya Mukherjee,Huimin Cheng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graphon models provide a flexible nonparametric framework for estimating
latent connectivity probabilities in networks, enabling a range of downstream
applications such as link prediction and data augmentation. However, accurate
graphon estimation typically requires a large graph, whereas in practice, one
often only observes a small-sized network. One approach to addressing this
issue is to adopt a transfer learning framework, which aims to improve
estimation in a small target graph by leveraging structural information from a
larger, related source graph. In this paper, we propose a novel method, namely
GTRANS, a transfer learning framework that integrates neighborhood smoothing
and Gromov-Wasserstein optimal transport to align and transfer structural
patterns between graphs. To prevent negative transfer, GTRANS includes an
adaptive debiasing mechanism that identifies and corrects for target-specific
deviations via residual smoothing. We provide theoretical guarantees on the
stability of the estimated alignment matrix and demonstrate the effectiveness
of GTRANS in improving the accuracy of target graph estimation through
extensive synthetic and real data experiments. These improvements translate
directly to enhanced performance in downstream applications, such as the graph
classification task and the link prediction task.

</details>


### [427] [Accelerated Evolving Set Processes for Local PageRank Computation](https://arxiv.org/abs/2510.08010)
*Binbin Huang,Luo Luo,Yanghua Xiao,Deqing Yang,Baojian Zhou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work proposes a novel framework based on nested evolving set processes
to accelerate Personalized PageRank (PPR) computation. At each stage of the
process, we employ a localized inexact proximal point iteration to solve a
simplified linear system. We show that the time complexity of such localized
methods is upper bounded by $\min\{\tilde{\mathcal{O}}(R^2/\epsilon^2),
\tilde{\mathcal{O}}(m)\}$ to obtain an $\epsilon$-approximation of the PPR
vector, where $m$ denotes the number of edges in the graph and $R$ is a
constant defined via nested evolving set processes. Furthermore, the algorithms
induced by our framework require solving only
$\tilde{\mathcal{O}}(1/\sqrt{\alpha})$ such linear systems, where $\alpha$ is
the damping factor. When $1/\epsilon^2\ll m$, this implies the existence of an
algorithm that computes an $\ epsilon $-approximation of the PPR vector with an
overall time complexity of $\tilde{\mathcal{O}}\left(R^2 /
(\sqrt{\alpha}\epsilon^2)\right)$, independent of the underlying graph size.
Our result resolves an open conjecture from existing literature. Experimental
results on real-world graphs validate the efficiency of our methods,
demonstrating significant convergence in the early stages.

</details>


### [428] [Efficient Resource-Constrained Training of Vision Transformers via Subspace Optimization](https://arxiv.org/abs/2510.09160)
*Le-Trung Nguyen,Enzo Tartaglione,Van-Tam Nguyen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As AI increasingly shapes daily life, energy consumption and data privacy
have become pressing concerns. On-device learning trains models directly on
edge devices, cutting energy consumption and safeguarding data privacy.
However, the expanding scale of modern neural networks creates a major obstacle
for on-device training. Although prior work has concentrated on compact
convolutional architectures, we instead apply subspace-based training to
transformer models. Motivated by the idea that a model's essential information
lies in a fixed subspace, we introduce Weight-Activation Subspace Iteration
(WASI), a method that mitigates the memory bottleneck of backpropagation and
boosts inference efficiency in transformer models by restricting training to
this subspace. Our results demonstrate that WASI maintains accuracy comparable
to vanilla training while reducing memory usage by up to $62\times$ and
computational cost (FLOPs) by up to $2\times$. On a Raspberry Pi 5, WASI
achieves roughly $1.5\times$ faster training and inference than vanilla
training.

</details>


### [429] [Multi-Agent Regime-Conditioned Diffusion (MARCD) for CVaR-Constrained Portfolio Decisions](https://arxiv.org/abs/2510.10807)
*Ali Atiah Alzahrani*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We examine whether regime-conditioned generative scenarios combined with a
convex CVaR allocator improve portfolio decisions under regime shifts. We
present MARCD, a generative-to-decision framework with: (i) a Gaussian HMM to
infer latent regimes; (ii) a diffusion generator that produces
regime-conditioned scenarios; (iii) signal extraction via blended, shrunk
moments; and (iv) a governed CVaR epigraph quadratic program. Contributions:
Within the Scenario stage we introduce a tail-weighted diffusion objective that
up-weights low-quantile outcomes relevant for drawdowns and a regime-expert
(MoE) denoiser whose gate increases with crisis posteriors; both are evaluated
end-to-end through the allocator. Under strict walk-forward on liquid
multi-asset ETFs (2005-2025), MARCD exhibits stronger scenario calibration and
materially smaller drawdowns: MaxDD 9.3% versus 14.1% for BL (a 34% reduction)
over 2020-2025 out-of-sample. The framework provides an auditable pipeline with
explicit budget, box, and turnover constraints, demonstrating the value of
decision-aware generative modeling in finance.

</details>


### [430] [Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand](https://arxiv.org/abs/2510.12328)
*Kiattikun Chobtham,Kanoksri Sarinnapakorn,Kritanai Torsri,Prattana Deeprasertkul,Jirawan Kamma*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate rainfall forecasting, particularly for extreme events, remains a
significant challenge in climatology and the Earth system. This paper presents
novel physics-informed Graph Neural Networks (GNNs) combined with extreme-value
analysis techniques to improve gauge-station rainfall predictions across
Thailand. The model leverages a graph-structured representation of gauge
stations to capture complex spatiotemporal patterns, and it offers
explainability through teleconnections. We preprocess relevant climate indices
that potentially influence regional rainfall. The proposed Graph Attention
Network with Long Short-Term Memory (Attention-LSTM) applies the attention
mechanism using initial edge features derived from simple
orographic-precipitation physics formulation. The embeddings are subsequently
processed by LSTM layers. To address extremes, we perform Peak-Over-Threshold
(POT) mapping using the novel Spatial Season-aware Generalized Pareto
Distribution (GPD) method, which overcomes limitations of traditional
machine-learning models. Experiments demonstrate that our method outperforms
well-established baselines across most regions, including areas prone to
extremes, and remains strongly competitive with the state of the art. Compared
with the operational forecasting system SEAS5, our real-world application
improves extreme-event prediction and offers a practical enhancement to produce
high-resolution maps that support decision-making in long-term water
management.

</details>


### [431] [Tahakom LLM Guidelines and Recipes: From Pre-training Data to an Arabic LLM](https://arxiv.org/abs/2510.13481)
*Areej AlOtaibi,Lina Alyahya,Raghad Alshabanah,Shahad Alfawzan,Shuruq Alarefei,Reem Alsabti,Nouf Alsubaie,Abdulaziz Alhuzaymi,Lujain Alkhelb,Majd Alsayari,Waad Alahmed,Omar Talabay,Jalal Alowibdi,Salem Alelyani,Adel Bibi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have significantly advanced the field of natural
language processing, enhancing capabilities in both language understanding and
generation across diverse domains. However, developing LLMs for Arabic presents
unique challenges. This paper explores these challenges by focusing on critical
aspects such as data curation, tokenizer design, and evaluation. We detail our
approach to the collection and filtration of Arabic pre-training datasets,
assess the impact of various tokenizer designs on model performance, and
examine the limitations of existing Arabic evaluation frameworks, for which we
propose a systematic corrective methodology. To promote transparency and
facilitate collaborative development, we share our data and methodologies,
contributing to the advancement of language modeling, particularly for the
Arabic language.

</details>


### [432] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [433] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement. Our code is available on
GitHub, and the datasets and trained models are available on Hugging Face.

</details>


### [434] [Enhancing Graph Neural Networks: A Mutual Learning Approach](https://arxiv.org/abs/2510.19223)
*Paul Agbaje,Arkajyoti Mitra,Afia Anjum,Pranali Khose,Ebelechukwu Nwafor,Habeeb Olufowobi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Knowledge distillation (KD) techniques have emerged as a powerful tool for
transferring expertise from complex teacher models to lightweight student
models, particularly beneficial for deploying high-performance models in
resource-constrained devices. This approach has been successfully applied to
graph neural networks (GNNs), harnessing their expressive capabilities to
generate node embeddings that capture structural and feature-related
information. In this study, we depart from the conventional KD approach by
exploring the potential of collaborative learning among GNNs. In the absence of
a pre-trained teacher model, we show that relatively simple and shallow GNN
architectures can synergetically learn efficient models capable of performing
better during inference, particularly in tackling multiple tasks. We propose a
collaborative learning framework where ensembles of student GNNs mutually teach
each other throughout the training process. We introduce an adaptive logit
weighting unit to facilitate efficient knowledge exchange among models and an
entropy enhancement technique to improve mutual learning. These components
dynamically empower the models to adapt their learning strategies during
training, optimizing their performance for downstream tasks. Extensive
experiments conducted on three datasets each for node and graph classification
demonstrate the effectiveness of our approach.

</details>


### [435] [Convergence Analysis of SGD under Expected Smoothness](https://arxiv.org/abs/2510.20608)
*Yuta Kawamoto,Hideaki Iiduka*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Stochastic gradient descent (SGD) is the workhorse of large-scale learning,
yet classical analyses rely on assumptions that can be either too strong
(bounded variance) or too coarse (uniform noise). The expected smoothness (ES)
condition has emerged as a flexible alternative that ties the second moment of
stochastic gradients to the objective value and the full gradient. This paper
presents a self-contained convergence analysis of SGD under ES. We (i) refine
ES with interpretations and sampling-dependent constants; (ii) derive bounds of
the expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates
with explicit residual errors for various step-size schedules. All proofs are
given in full detail in the appendix. Our treatment unifies and extends recent
threads (Khaled and Richt\'arik, 2020; Umeda and Iiduka, 2025).

</details>


### [436] [On Uncertainty Calibration for Equivariant Functions](https://arxiv.org/abs/2510.21691)
*Edward Berman,Jacob Ginesin,Marco Pacini,Robin Walters*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data-sparse settings such as robotic manipulation, molecular physics, and
galaxy morphology classification are some of the hardest domains for deep
learning. For these problems, equivariant networks can help improve modeling
across undersampled parts of the input space, and uncertainty estimation can
guard against overconfidence. However, until now, the relationships between
equivariance and model confidence, and more generally equivariance and model
calibration, has yet to be studied. Since traditional classification and
regression error terms show up in the definitions of calibration error, it is
natural to suspect that previous work can be used to help understand the
relationship between equivariance and calibration error. In this work, we
present a theory relating equivariance to uncertainty estimation. By proving
lower and upper bounds on uncertainty calibration errors (ECE and ENCE) under
various equivariance conditions, we elucidate the generalization limits of
equivariant models and illustrate how symmetry mismatch can result in
miscalibration in both classification and regression. We complement our
theoretical framework with numerical experiments that clarify the relationship
between equivariance and uncertainty using a variety of real and simulated
datasets, and we comment on trends with symmetry mismatch, group size, and
aleatoric and epistemic uncertainties.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [437] [High-Harmonic Optical Vortex Generation from a Plasma Aperture](https://arxiv.org/abs/2510.22091)
*Runze Li,Wenchao Yan,Longqing Yi*

Main category: physics.plasm-ph

TL;DR: 本研究通过数值模拟，在更真实的条件下（特别是激光斜入射）研究了飞秒圆偏振激光脉冲与固体靶微米孔径相互作用产生高阶谐波涡旋的机制。研究发现，增大斜入射角、减小靶厚和提高激光对比度可提高谐波转换效率。非理想条件下产生的谐波可能包含拉盖尔-高斯（LG）涡旋和非LG分量，但它们可以通过发散角差异进行分离。本研究为未来实验的设计和实施提供了有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 现有的关于高功率飞秒圆偏振激光脉冲作用于固体靶微米孔径产生表面等离子体振荡，进而生成衍射光中高阶谐波涡旋的机制，仅限于理想条件下的理论研究。本工作旨在弥补这一空白，通过数值模拟研究更真实的场景，尤其是考虑了激光斜入射以避免反射光对光学元件的潜在损伤，这对于实际实验设计至关重要。

Method: 本研究采用数值模拟方法，重点关注激光斜入射到靶表面的场景。具体分析了增大斜入射角、减小靶厚和提高激光对比度对谐波产生的影响。同时，对产生的谐波束中的拉盖尔-高斯（LG）涡旋分量和非LG分量进行了研究，并通过其发散角差异（涡旋分量发散角更小）提出了分离方法。此外，还计算分析了不同条件下谐波的发散角和涡旋高阶谐波的拓扑荷谱。

Result: 研究结果表明，增大斜入射角、减小靶厚和提高激光对比度均能有效提高谐波转换效率。在非理想条件下，产生的谐波束可能同时包含拉盖尔-高斯（LG）涡旋分量和非LG分量。通过分析发现，这些分量可以通过其发散角的差异进行有效分离，其中涡旋分量的发散角较小。计算分析还提供了不同条件下谐波发散角和拓扑荷谱的详细信息，证明了纯LG模式高阶谐波有可能被筛选出来。

Conclusion: 本研究通过数值模拟，在接近实际的实验条件下（特别是激光斜入射）深入探讨了高阶谐波涡旋的产生机制。研究结果不仅揭示了影响谐波转换效率的关键因素，还提出了一种分离LG涡旋和非LG谐波分量的方法，即利用它们发散角的差异。这些发现为未来利用高功率飞秒激光产生高阶谐波涡旋的实验设计和实施提供了宝贵的理论指导和实用见解，有助于在基础和应用物理研究中筛选出纯LG模式谐波。

Abstract: When a high-power, femtosecond, circularly polarized (CP) laser pulse is
incident on a micrometer-scale aperture in a solid foil target, it drives
surface plasma oscillation, generating high-order harmonic vortices in the
diffracted light. However, this mechanism has so far only been studied
theoretically under ideal conditions. In this work, we perform numerical
studies on more realistic situations. In particular, we focus on a scenario
where the laser is obliquely incident on the target surface to avoid the
potential damage of the optics by the reflected light. We demonstrate that
increasing oblique incidence angle, reducing target thickness, and improving
laser contrast can enhance the harmonic conversion efficiency. However, the
generated harmonic beams may contain both Laguerre-Gaussian (LG) (vortex) and
non-LG components under non-ideal conditions. We show that they can be
separated by their divergence, as the vortex components has smaller diverging
angle. In addition, we have performed computational analyses on the harmonic
divergence angles and topological charge spectra of vortex high-order harmonics
under different conditions. These high-order harmonic pure LG modes can
potentially be filtered out for wide range of fundamental and applied physics
researches. This study provides valuable insights for the design and
implementation of future experiments.

</details>


### [438] [Global linear drift-wave eigenmode structures on flux surfaces in stellarators: ion temperature gradient mode](https://arxiv.org/abs/2506.12948)
*Hongxuan Zhu,H. Chen,Z. Lin,A. Bhattacharjee*

Main category: physics.plasm-ph

TL;DR: 该研究通过全局回旋动理学粒子在单元格代码GTC模拟了仿星器中的线性静电离子温度梯度模式。发现本征模结构在磁通面上沿场线不均匀且局限于离子逆磁漂移的下游，这可以用复数横向波数的非零虚部来解释。研究强调，理解仿星器中线性漂移波本征模结构需要考虑表面-全局效应产生的复数波数谱。


<details>
  <summary>Details</summary>
Motivation: 湍流输运严重影响仿星器磁约束装置的性能。尽管数值方面取得了显著进展，但仿星器中湍流的理论理解仍然不足，特别是由于非轴对称性导致的场线在磁通面内的耦合效应尚未得到充分研究。本研究旨在填补这一理论空白，深入理解仿星器中的湍流机制。

Method: 研究采用全局回旋动理学粒子在单元格代码GTC对仿星器中的线性静电离子温度梯度（ITG）模式进行了数值模拟。为了解释发现的局域化现象，研究基于Zocco等人提出的一个简单模型进行了分析。此外，研究还展示了如何通过局部回旋动理学代码stella和GX构建局域化的表面-全局本征模，其方法是首先在每条场线上用实数波数求解局部色散关系，然后进行解析延拓到复数波数平面。

Result: 研究发现线性本征模结构在磁通面上沿场线分布不均匀，并且局限于离子逆磁漂移的下游。基于Zocco等人的模型，研究表明这种局域化现象可以通过横向波数的非零虚部来解释。进一步的成果是，通过在每条场线上用实数波数求解局部色散关系，然后解析延拓到复数波数平面，可以利用局部回旋动理学代码stella和GX构建出局域化的表面-全局本征模。

Conclusion: 研究结果表明，理解仿星器中线性漂移波本征模结构需要考虑表面-全局效应产生的复数波数谱。这强调了将复数波数分析纳入仿星器湍流理论的重要性，为未来更准确地预测和控制仿星器中的湍流输运提供了关键理论基础。

Abstract: Turbulent transport greatly impacts the performance of stellarator magnetic
confinement devices. While significant progress has been made on the numerical
front, theoretical understanding of turbulence in stellarators is still
lacking. In particular, due to nonaxisymmetry, different field lines couple
within flux surfaces, the effects from which have yet to be adequately studied.
In this work, we numerically simulate the linear electrostatic
ion-temperature-gradient modes in stellarators using the global gyrokinetic
particle-in-cell code GTC. We find that the linear eigenmode structures are
nonuniform across field lines on flux surfaces and are localized at the
downstream of the ion diamagnetic drift. Based on a simple model from Zocco et
al. [Phys. Plasmas 23, 082516 (2016); 27, 022507 (2020)], we show that the
localization can be explained from the nonzero imaginary part of the binormal
wavenumber. We further demonstrate that a localized surface-global eigenmode
can be constructed from local gyrokinetic codes stella and GX, only if we first
solve the local dispersion relation with real wavenumbers on each field line,
and then do an analytic continuation to the complex-wavenumber plane. These
results suggest that the complex-wavenumber spectra from surface-global effects
are required to understand the linear drift-wave eigenmode structures in
stellarators.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [439] [Collaborative Task Assignment, Sequencing and Multi-agent Path-finding](https://arxiv.org/abs/2510.21738)
*Yifan Bai,Shruti Kotpalliwar,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.MA

TL;DR: 本文提出了一种名为CBS-TS的算法，用于解决协同任务分配、排序和多智能体路径规划（TSPF）问题，该算法通过结合混合整数线性规划（MILP）和基于冲突搜索（CBS）的方法，实现了在最小化流时间的同时，确保无碰撞路径和所有任务完成。实验结果表明，CBS-TS在大多数测试场景中优于基线方法CBSS，能够持续获得最优解。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决协同任务分配、排序和多智能体路径规划（TSPF）问题。该问题要求一个智能体团队在避免碰撞的前提下，访问一系列任务地点，并最小化总流时间。TSPF问题考虑了智能体与任务之间的兼容性约束，并确保所有任务都必须完成。这是一个复杂的优化问题，涉及到多个智能体的协调、路径规划以及任务的高效分配和执行，对于提高多智能体系统的效率和鲁棒性具有重要意义。

Method: 本文提出了一种名为“基于冲突搜索的任务排序”（CBS-TS）的算法，这是一种最优且完备的算法。CBS-TS通过交替进行任务序列的发现和解决当前序列路径中的冲突来运作。具体来说，CBS-TS利用混合整数线性规划（MILP）来优化任务排序，并且仅在需要时调用MILP来寻找下一个最佳序列，从而有效地限制了搜索空间，提高了计算效率。对于无碰撞的路径规划，CBS-TS在搜索森林中结合了基于冲突搜索（CBS）和多标签A*（MLA*）算法。为了评估其性能，本文将CBS-TS与经过修改可解决TSPF问题的基线方法“基于冲突的施泰纳搜索”（CBSS）进行了比较。

Result: 实验结果表明，CBS-TS在大多数测试场景中表现优于CBSS。CBS-TS实现了更高的成功率，并能持续获得最优解。相比之下，CBSS在某些情况下能达到接近最优的解决方案，但不如CBS-TS稳定和高效。

Conclusion: 本文提出的CBS-TS算法为协同任务分配、排序和多智能体路径规划（TSPF）问题提供了一个最优且计算高效的解决方案。通过智能地结合MILP进行任务排序和CBS-MLA*进行无碰撞路径规划，CBS-TS在保持最优性的同时，显著提升了计算效率。实验结果明确证实了CBS-TS相对于现有基线方法CBSS的优越性，展现了其在复杂多智能体系统中的强大应用潜力。

Abstract: In this article, we address the problem of collaborative task assignment,
sequencing, and multi-agent pathfinding (TSPF), where a team of agents must
visit a set of task locations without collisions while minimizing flowtime.
TSPF incorporates agent-task compatibility constraints and ensures that all
tasks are completed. We propose a Conflict-Based Search with Task Sequencing
(CBS-TS), an optimal and complete algorithm that alternates between finding new
task sequences and resolving conflicts in the paths of current sequences.
CBS-TS uses a mixed-integer linear program (MILP) to optimize task sequencing
and employs Conflict-Based Search (CBS) with Multi-Label A* (MLA*) for
collision-free path planning within a search forest. By invoking MILP for the
next-best sequence only when needed, CBS-TS efficiently limits the search
space, enhancing computational efficiency while maintaining optimality. We
compare the performance of our CBS-TS against Conflict-based Steiner Search
(CBSS), a baseline method that, with minor modifications, can address the TSPF
problem. Experimental results demonstrate that CBS-TS outperforms CBSS in most
testing scenarios, achieving higher success rates and consistently optimal
solutions, whereas CBSS achieves near-optimal solutions in some cases. The
supplementary video is available at https://youtu.be/QT8BYgvefmU.

</details>


### [440] [LLM-augmented empirical game theoretic simulation for social-ecological systems](https://arxiv.org/abs/2510.21965)
*Jennifer Shi,Christopher K. Frantz,Christian Kimmich,Saba Siddiki,Atrisha Sarkar*

Main category: cs.MA

TL;DR: 本研究比较了四种结合大型语言模型（LLM）的框架（程序化ABM、生成式ABM、LLM-EGTA和专家引导LLM-EGTA），用于模拟社会生态系统中的制度设计。研究发现，不同方法产生了截然不同的集体行为模式，并且在专家引导的EGTA模型中，通过参数化收益来塑造行为比通过LLM系统提示更有效。


<details>
  <summary>Details</summary>
Motivation: 为社会生态系统设计制度需要能够捕捉异质性、不确定性和战略互动的模型。尽管已存在多种建模方法（如经验博弈论分析EGTA和LLM驱动的模拟），但这些方法如何相互整合以及其模拟结果是否能为现实世界的社会生态治理提供可信的行为范围尚不明确。本研究旨在填补这一空白。

Method: 研究通过比较四种LLM增强框架来解决上述问题：程序化Agent-Based Models (ABMs)、生成式ABMs、LLM-Empirical Game-Theoretic Analysis (EGTA) 和专家引导的LLM-EGTA。这些框架在一个真实的案例研究中进行评估，即阿姆河盆地在集中式和分散式治理下的灌溉与渔业问题。

Result: 结果显示：首先，程序化ABM、生成式ABM和LLM增强的EGTA模型产生了截然不同的集体行为模式，这突显了方法多样性的价值。其次，通过LLM中的系统提示来诱导行为，其效果不如在专家引导的基于EGTA的模型中通过参数化收益来塑造行为。

Conclusion: 本研究揭示了在社会生态系统建模中采用不同方法的重要性。它还表明，在整合LLM与博弈论模型时，通过结构化参数（如收益）来引导行为可能比仅仅依赖LLM的提示工程更为有效。这些发现为未来在社会生态系统治理背景下开发更稳健、更可信的LLM增强模拟提供了指导。

Abstract: Designing institutions for social-ecological systems requires models that
capture heterogeneity, uncertainty, and strategic interaction. Multiple
modeling approaches have emerged to meet this challenge, including empirical
game-theoretic analysis (EGTA), which merges ABM's scale and diversity with
game-theoretic models' formal equilibrium analysis. The newly popular class of
LLM-driven simulations provides yet another approach, and it is not clear how
these approaches can be integrated with one another, nor whether the resulting
simulations produce a plausible range of behaviours for real-world
social-ecological governance. To address this gap, we compare four
LLM-augmented frameworks: procedural ABMs, generative ABMs, LLM-EGTA, and
expert guided LLM-EGTA, and evaluate them on a real-world case study of
irrigation and fishing in the Amu Darya basin under centralized and
decentralized governance. Our results show: first, procedural ABMs, generative
ABMs, and LLM-augmented EGTA models produce strikingly different patterns of
collective behaviour, highlighting the value of methodological diversity.
Second, inducing behaviour through system prompts in LLMs is less effective
than shaping behaviour through parameterized payoffs in an expert-guided
EGTA-based model.

</details>


### [441] [CreditXAI: A Multi-Agent System for Explainable Corporate Credit Rating](https://arxiv.org/abs/2510.22222)
*Yumeng Shi,Zhongliang Yang,Yisi Wang,Linna Zhou*

Main category: cs.MA

TL;DR: 该研究提出了CreditXAI，一个模拟专业信用分析师协作决策过程的多智能体系统（MAS）框架，用于企业信用评级。该框架专注于业务、财务和治理风险维度，以生成一致且可解释的信用评估。实验结果表明，多智能体协作将预测准确率提高了7%以上，超越了最佳的单一智能体基线，证明了其在企业信用风险评估中的显著协同优势。


<details>
  <summary>Details</summary>
Motivation: 企业信用评级中的传统深度学习方法虽然提高了预测准确性，但存在固有的“黑箱”问题和有限的可解释性。虽然纳入非财务信息丰富了数据并提供了部分可解释性，但模型仍然缺乏分层推理机制，限制了其综合分析能力。本研究的动机是解决这些挑战，开发一种既智能又可解释、且具备分层推理能力的信用评级模型。

Method: 我们提出了CreditXAI，一个多智能体系统（MAS）框架，该框架模拟了专业信用分析师的协作决策过程。该框架侧重于业务、财务和治理风险维度，以生成一致且可解释的信用评估。

Result: 实验结果表明，多智能体协作将预测准确率提高了7%以上，超越了最佳的单一智能体基线。这证实了多智能体系统在企业信用风险评估中具有显著的协同优势。

Conclusion: 本研究为构建智能且可解释的信用评级模型提供了一条新的技术路径。CreditXAI框架通过模拟分析师的协作决策过程，不仅提高了预测准确性，还增强了模型的可解释性，有效解决了传统深度学习模型的“黑箱”问题和缺乏分层推理能力的问题。

Abstract: In the domain of corporate credit rating, traditional deep learning methods
have improved predictive accuracy but still suffer from the inherent
'black-box' problem and limited interpretability. While incorporating
non-financial information enriches the data and provides partial
interpretability, the models still lack hierarchical reasoning mechanisms,
limiting their comprehensive analytical capabilities. To address these
challenges, we propose CreditXAI, a Multi-Agent System (MAS) framework that
simulates the collaborative decision-making process of professional credit
analysts. The framework focuses on business, financial, and governance risk
dimensions to generate consistent and interpretable credit assessments.
Experimental results demonstrate that multi-agent collaboration improves
predictive accuracy by more than 7% over the best single-agent baseline,
confirming its significant synergistic advantage in corporate credit risk
evaluation. This study provides a new technical pathway to build intelligent
and interpretable credit rating models.

</details>


### [442] [CGoT: A Novel Inference Mechanism for Embodied Multi-Agent Systems Using Composable Graphs of Thoughts](https://arxiv.org/abs/2510.22235)
*Yixiao Nie,Yang Zhang,Yingjie Jin,Zhepeng Wang,Xiu Li,Xiang Li*

Main category: cs.MA

TL;DR: 本文提出了一种结合自动驾驶汽车和服务机器人的新型系统，该系统利用大型语言模型（LLMs）提升操作效率和协作潜力。研究引入了一种名为CGOT的新型推理机制，专门用于一个智能体携带另一个智能体的场景。实验结果验证了所提出方法的性能。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶汽车和服务机器人在工业应用和日常生活中日益普及，以及大型语言模型（LLMs）的快速发展，如何将这些技术有效整合以解决复杂任务、提升系统整体效率和协作能力成为一个重要的研究问题。特别是在一个智能体（如自动驾驶汽车）需要运输另一个智能体（如服务机器人）去执行任务的场景中，如何优化这种协同机制以最大限度地发挥其潜力，是本研究旨在解决的关键问题。

Method: 本研究提出了一种新颖的车辆-机器人系统。在该系统中，两辆自动驾驶车辆负责将服务机器人运输到办公园区内的指定地点，由服务机器人在这些地点执行一系列任务。为了增强系统的操作效率并最大化车辆与机器人之间的合作潜力，研究探索了将大型语言模型（LLMs）融入到该系统中的可行性与潜在益处。此外，本文提出了一种名为CGOT的新型推理机制，专门用于处理一个智能体可以携带另一个智能体这类系统的推理问题。

Result: 实验结果表明，所提出的方法在性能上得到了验证。这证实了将大型语言模型整合到车辆-机器人系统中，并结合CGOT推理机制，能够有效提升系统的运行效率和协作能力。

Conclusion: 本文成功引入了一种将自动驾驶车辆、服务机器人与大型语言模型相结合的新型系统，并提出了一种创新的CGOT推理机制，以优化智能体之间携带与被携带的合作模式。研究验证了该方法在提升系统操作效率和协作潜力方面的有效性，为未来多智能体系统的设计和LLMs的应用提供了有价值的参考。

Abstract: The integration of self-driving cars and service robots is becoming
increasingly prevalent across a wide array of fields, playing a crucial and
expanding role in both industrial applications and everyday life. In parallel,
the rapid advancements in Large Language Models (LLMs) have garnered
substantial attention and interest within the research community. This paper
introduces a novel vehicle-robot system that leverages the strengths of both
autonomous vehicles and service robots. In our proposed system, two autonomous
ego-vehicles transports service robots to locations within an office park,
where they perform a series of tasks. The study explores the feasibility and
potential benefits of incorporating LLMs into this system, with the aim of
enhancing operational efficiency and maximizing the potential of the
cooperative mechanisms between the vehicles and the robots. This paper proposes
a novel inference mechanism which is called CGOT toward this type of system
where an agent can carry another agent. Experimental results are presented to
validate the performance of the proposed method.

</details>


### [443] [IFS: Information Flow Structure for Multi-agent Ad Hoc System](https://arxiv.org/abs/2510.22320)
*Yanqing Fu,Chenrun Wang,Chao Huang,Zhuping Wang*

Main category: cs.MA

TL;DR: 本文针对多智能体特设系统中信息流不足和信息处理能力有限的问题，提出了一种信息流结构（IFS），通过通信和信息融合来解决这些挑战。在星际争霸II的实验结果表明，IFS显著改善了信息流和处理能力，展现出强大的泛化能力，并在复杂的特设团队协作场景中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体特设系统是动态协作系统，其中多个自主智能体必须在开放环境中与已知和未知队友合作，且不依赖预先协调的策略。这些系统在不确定性和部分可观察性条件下运行，团队构成、智能体行为和环境因素在执行过程中可能会发生变化。通过对这些系统中信息流的分析，现有研究存在两个关键局限性：信息流不足和信息处理能力有限。解决这些问题对于提高多智能体特设系统的协作效率和性能至关重要。

Method: 为了解决信息流不足和信息处理能力有限的问题，本文提出了一种适用于多智能体特设系统的信息流结构（IFS）。IFS从通信和信息融合的角度入手，旨在优化智能体之间信息的有效传递和综合处理。具体方法包括设计一种新的信息传递机制以确保关键信息能够充分流通，以及开发先进的信息融合技术以提高智能体对多源信息的理解和利用能力，从而应对特设系统的动态性和不确定性挑战。

Result: 在星际争霸II环境中的实验结果表明，IFS显著改善了系统的信息流和信息处理能力。这不仅体现在智能体能够更有效地获取和利用信息，还体现在整体系统性能的提升。此外，IFS展现出强大的泛化能力，这意味着它在面对不同的特设团队协作场景时也能保持优异表现。与基线方法相比，IFS在复杂的特设团队协作任务中表现出更高的性能，验证了其在解决多智能体特设系统信息挑战方面的有效性。

Conclusion: 综上所述，本文提出的信息流结构（IFS）成功解决了多智能体特设系统中信息流不足和信息处理能力有限的关键问题。通过优化通信和信息融合机制，IFS在实际应用（如星际争霸II）中取得了显著的性能提升和强大的泛化能力。这项研究为未来设计更高效、更鲁棒的多智能体特设系统提供了宝贵的见解和有效的方法。未来的工作可以进一步探索IFS在更广泛、更复杂的应用场景中的表现，并进一步优化其信息处理机制。

Abstract: Multi-agent ad hoc systems are dynamic collaborative systems in which
multiple autonomous agents must cooperate with both known and unknown teammates
in open environments, without relying on pre-coordinated strategies. These
systems operate under conditions of uncertainty and partial observability,
where team composition, agent behaviors, and environmental factors may change
during execution. Through an analysis of information flow in such systems, we
identify two key limitations in existing research: insufficient information
flow and limited information processing capacity. To address these issues, we
propose an information flow structure for multi-agent ad hoc systems (IFS),
which tackles these challenges from the perspectives of communication and
information fusion. Experimental results in StarCraft II demonstrate that IFS
significantly improves both information flow and processing capacity, while
exhibiting strong generalization capabilities and outperforming baseline
methods in complex ad hoc teamwork scenarios.

</details>


### [444] [Group size effects and collective misalignment in LLM multi-agent systems](https://arxiv.org/abs/2510.22422)
*Ariel Flint,Luca Maria Aiello,Romualdo Pastor-Satorras,Andrea Baronchelli*

Main category: cs.MA

TL;DR: 本研究系统性探索了大型语言模型（LLM）多智能体系统中群体规模对动态的影响。发现集体偏差比之前认为的更深远，群体规模以非线性方式影响动态，并且在超过临界规模后，系统行为趋于确定性预测。这表明群体规模是多智能体动态的关键驱动因素。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）多智能体系统正在迅速扩展，但现有研究主要集中于单个智能体与固定规模集体的对比，未能充分揭示群体规模如何塑造系统动态。特别是在智能体交互可能导致集体偏差（在个体模型中不存在的偏差）的背景下，理解群体规模的影响至关重要，因为这关系到多智能体系统部署的风险和行为预测。

Method: 本研究超越了单一智能体与固定规模集体的二分法，系统性地探索了不同群体规模下的结果。研究重点关注多智能体未对齐问题，并以LLMs玩一个简单的协调博弈作为基础。此外，本研究开发了一种平均场分析方法，以理论化和预测系统在不同群体规模下的行为。

Result: 首先，研究发现集体偏差是一个比之前评估的更深层的现象：智能体间的交互可以放大个体偏差，引入新的偏差，或者覆盖模型层面的偏好。其次，研究表明群体规模以非线性的方式影响系统动态，揭示了依赖于模型的动态机制。最后，通过开发的平均场分析方法，研究发现当群体规模超过一个临界值时，模拟结果收敛于确定性预测，从而揭示了竞争平衡的吸引域。

Conclusion: 这些发现确立了群体规模作为多智能体动态的关键驱动因素。研究强调，在部署大规模基于LLM的系统时，必须考虑群体层面的效应，以更好地理解、预测和管理系统的行为和潜在风险。

Abstract: Multi-agent systems of large language models (LLMs) are rapidly expanding
across domains, introducing dynamics not captured by single-agent evaluations.
Yet, existing work has mostly contrasted the behavior of a single agent with
that of a collective of fixed size, leaving open a central question: how does
group size shape dynamics? Here, we move beyond this dichotomy and
systematically explore outcomes across the full range of group sizes. We focus
on multi-agent misalignment, building on recent evidence that interacting LLMs
playing a simple coordination game can generate collective biases absent in
individual models. First, we show that collective bias is a deeper phenomenon
than previously assessed: interaction can amplify individual biases, introduce
new ones, or override model-level preferences. Second, we demonstrate that
group size affects the dynamics in a non-linear way, revealing model-dependent
dynamical regimes. Finally, we develop a mean-field analytical approach and
show that, above a critical population size, simulations converge to
deterministic predictions that expose the basins of attraction of competing
equilibria. These findings establish group size as a key driver of multi-agent
dynamics and highlight the need to consider population-level effects when
deploying LLM-based systems at scale.

</details>


### [445] [Hollywood Town: Long-Video Generation via Cross-Modal Multi-Agent Orchestration](https://arxiv.org/abs/2510.22431)
*Zheng Wei,Mingchen Li,Zeqian Zhang,Ruibin Yuan,Pan Hui,Huamin Qu,James Evans,Maneesh Agrawala,Anyi Rao*

Main category: cs.MA

TL;DR: 本文提出了三项创新，包括OmniAgent分层图基多智能体框架、基于超图节点的临时群组讨论机制以及带有有限重试的循环图，旨在提升多智能体系统在长视频生成等创意任务中的协作能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在增强长视频生成等创意任务表现方面展现出巨大潜力。然而，如何有效提升智能体间的协作，实现模块化专业分工、高效上下文共享以及迭代式优化，是当前面临的关键挑战。本研究旨在通过引入创新的协作机制来解决这些问题，从而更充分地发挥多智能体系统在复杂创意任务中的能力。

Method: 本研究引入了三项关键创新来改进多智能体协作：1. **OmniAgent框架**：提出一个分层、基于图的多智能体框架，其灵感来源于电影制作流程，旨在实现模块化专业分工和可扩展的智能体间协作，特别适用于长视频生成。2. **超图节点**：受上下文工程启发，引入超图节点，使缺乏足够上下文信息的智能体能够进行临时群组讨论，这在减少单个智能体内存需求的同时，确保了充足的上下文信息。3. **带有有限重试的循环图**：从有向无环图（DAGs）转向带有有限重试的有向循环图，允许智能体迭代地反思和完善输出，并通过后续节点的反馈来改进早期阶段的成果。

Result: 本研究的贡献为在创意任务中开发更强大的多智能体系统奠定了基础。通过引入OmniAgent框架、超图节点和带有有限重试的循环图，预期能够显著提升多智能体系统在复杂创意任务（如长视频生成）中的协作效率、上下文管理能力和输出质量，从而使其更加鲁棒和高效。

Conclusion: 本研究通过引入OmniAgent分层框架、超图节点和带有有限重试的循环图，为提升多智能体系统在创意任务中的协作能力和鲁棒性做出了重要贡献。这些创新为未来开发更先进、更高效的多智能体系统以应对复杂创意挑战提供了坚实的基础。抽象中未提及具体的局限性和未来的工作。

Abstract: Recent advancements in multi-agent systems have demonstrated significant
potential for enhancing creative task performance, such as long video
generation. This study introduces three innovations to improve multi-agent
collaboration. First, we propose OmniAgent, a hierarchical, graph-based
multi-agent framework for long video generation that leverages a
film-production-inspired architecture to enable modular specialization and
scalable inter-agent collaboration. Second, inspired by context engineering, we
propose hypergraph nodes that enable temporary group discussions among agents
lacking sufficient context, reducing individual memory requirements while
ensuring adequate contextual information. Third, we transition from directed
acyclic graphs (DAGs) to directed cyclic graphs with limited retries, allowing
agents to reflect and refine outputs iteratively, thereby improving earlier
stages through feedback from subsequent nodes. These contributions lay the
groundwork for developing more robust multi-agent systems in creative tasks.

</details>


### [446] [Agent-GSPO: Communication-Efficient Multi-Agent Systems via Group Sequence Policy Optimization](https://arxiv.org/abs/2510.22477)
*Yijia Fan,Jusheng Zhang,Jing Yang,Keze Wang*

Main category: cs.MA

TL;DR: Agent-GSPO是一个通过序列级强化学习优化多智能体系统（MAS）中通信成本的框架。它利用GSPO算法和惩罚冗余的奖励机制，在七个推理基准测试中以极低的令牌消耗实现了新的最先进性能，并促成了“策略性沉默”等新兴策略。


<details>
  <summary>Details</summary>
Motivation: 现有“自由发挥”的多智能体系统（MAS）面临高昂的通信成本问题，这严重限制了系统的可扩展性和经济可行性。本研究旨在解决这一关键挑战，通过优化令牌经济来降低通信开销，从而开发出更高效、更实用的多智能体系统。

Method: 本研究引入了Agent-GSPO框架，该框架直接使用序列级强化学习来优化令牌经济。Agent-GSPO利用稳定且内存高效的群序列策略优化（GSPO）算法来训练智能体。关键在于其通信感知奖励机制，该机制明确惩罚冗余通信，鼓励智能体采用更高效的交流方式，例如“策略性沉默”。

Result: Agent-GSPO在七个推理基准测试中不仅取得了新的最先进性能，而且其令牌消耗仅为现有方法的一小部分。这表明该方法在实现高性能的同时，显著降低了通信成本，展现了其卓越的效率。

Conclusion: Agent-GSPO提供了一个开发可扩展且经济可行的多智能体系统的实用蓝图。通过优化令牌经济和鼓励高效通信策略（如“策略性沉默”），该方法有效地解决了多智能体系统中的通信成本难题，为未来多智能体系统的设计和部署开辟了新途径。

Abstract: To combat the prohibitive communication costs of ``free-for-all" multi-agent
systems (MAS), we introduce \textbf{Agent-GSPO}, a framework that directly
optimizes for token economy using sequence-level reinforcement learning.
Agent-GSPO leverages the stable and memory-efficient Group Sequence Policy
Optimization (GSPO) algorithm to train agents on a communication-aware reward
that explicitly penalizes verbosity. Across seven reasoning benchmarks,
Agent-GSPO not only achieves new state-of-the-art performance but does so with
a fraction of the token consumption of existing methods. By fostering emergent
strategies like ``strategic silence," our approach provides a practical
blueprint for developing scalable and economically viable multi-agent systems.

</details>


### [447] [ColorEcosystem: Powering Personalized, Standardized, and Trustworthy Agentic Service in massive-agent Ecosystem](https://arxiv.org/abs/2510.21566)
*Fangwen Wu,Zheng Wu,Jihong Wang,Yunku Chen,Ruiguang Pei,Heyuan Huang,Xin Liao,Xingyu Lou,Huarong Deng,Zhihui Fu,Weiwen Liu,Zhuosheng Zhang,Weinan Zhang,Jun Wang*

Main category: cs.MA

TL;DR: 当前大规模智能体生态系统面临非个性化服务、缺乏标准化和不信任行为的挑战。本文提出了ColorEcosystem，一个旨在通过智能体载体（个性化服务）、智能体商店（标准化平台）和智能体审计（确保信任）来实现规模化个性化、标准化和可信赖的智能体服务的新蓝图。部分功能已实现并开源。


<details>
  <summary>Details</summary>
Motivation: 随着（多模态）大型语言模型智能体技术的快速发展，智能体服务管理已从单智能体系统演变为多智能体系统，进而发展到大规模智能体生态系统。然而，当前的大规模智能体生态系统面临日益严峻的挑战，包括非个性化的服务体验、缺乏标准化以及不值得信赖的行为。这些问题阻碍了智能体服务的进一步发展和广泛应用，因此需要一个创新的解决方案来应对这些挑战，从而实现规模化的个性化、标准化和可信赖的智能体服务。

Method: 为了解决现有大规模智能体生态系统面临的问题，本文提出了ColorEcosystem，一个新颖的蓝图。ColorEcosystem由三个关键组件构成：智能体载体（agent carrier）、智能体商店（agent store）和智能体审计（agent audit）。智能体载体通过利用用户特定数据并创建数字孪生，提供个性化的服务体验。智能体商店作为一个中心化、标准化的平台，用于管理多样化的智能体服务。智能体审计则基于对开发者和用户活动的监督，确保服务提供商和用户的完整性和可信度。通过这三个组件的协同工作，ColorEcosystem旨在实现大规模智能体服务的个性化、标准化和可信赖性。

Result: 通过对现有挑战、过渡形式和实际考虑因素的深入分析，ColorEcosystem被定位为能够在大规模智能体生态系统中提供个性化、标准化和可信赖的智能体服务。此外，本研究已经实现了ColorEcosystem的部分功能，并已将相关代码开源，验证了其概念的可行性和实用性。

Conclusion: ColorEcosystem为解决当前大规模智能体生态系统面临的非个性化服务、缺乏标准化和不信任行为等核心挑战提供了一个全面的解决方案。通过引入智能体载体、智能体商店和智能体审计这三个创新组件，该蓝图有望推动智能体服务向个性化、标准化和可信赖的方向发展。已开源的部分功能实现进一步证明了其潜力，为未来大规模智能体生态系统的发展奠定了基础，并为实现更高效、更安全、更用户友好的智能体服务指明了方向。

Abstract: With the rapid development of (multimodal) large language model-based agents,
the landscape of agentic service management has evolved from single-agent
systems to multi-agent systems, and now to massive-agent ecosystems. Current
massive-agent ecosystems face growing challenges, including impersonal service
experiences, a lack of standardization, and untrustworthy behavior. To address
these issues, we propose ColorEcosystem, a novel blueprint designed to enable
personalized, standardized, and trustworthy agentic service at scale.
Concretely, ColorEcosystem consists of three key components: agent carrier,
agent store, and agent audit. The agent carrier provides personalized service
experiences by utilizing user-specific data and creating a digital twin, while
the agent store serves as a centralized, standardized platform for managing
diverse agentic services. The agent audit, based on the supervision of
developer and user activities, ensures the integrity and credibility of both
service providers and users. Through the analysis of challenges, transitional
forms, and practical considerations, the ColorEcosystem is poised to power
personalized, standardized, and trustworthy agentic service across
massive-agent ecosystems. Meanwhile, we have also implemented part of
ColorEcosystem's functionality, and the relevant code is open-sourced at
https://github.com/opas-lab/color-ecosystem.

</details>
