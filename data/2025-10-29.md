<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 49]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [physics.acc-ph](#physics.acc-ph) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Long-Term Memory for Long-Context Question Answering](https://arxiv.org/abs/2510.23730)
*Alessandra Terranova,Björn Ross,Alexandra Birch*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）需要记忆来实现对话的连贯性和体验式学习。本研究系统评估了不同记忆增强方法在长对话任务中的效果。研究发现，记忆增强方法可将token使用量减少90%以上，同时保持准确性。模型能力应与记忆架构复杂度相匹配，小模型更适合检索增强生成（RAG），而强大的指令微调模型则受益于 the episodic learning（通过反思和更复杂的agentic semantic memory）。特别是，episodic memory能帮助LLM认识到自身知识的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了使大型语言模型（LLM）能够实现真正的对话连贯性并从体验式学习中受益，它们需要记忆。尽管已有研究致力于开发复杂的记忆系统，但对于哪种类型的记忆对于长上下文对话任务最有效，目前尚不清楚。因此，本研究旨在系统地评估不同的记忆增强方法，以解决这一关键问题，并为LLM在长对话场景下的记忆机制提供指导。

Method: 本研究提出并评估了多种记忆增强方法，包括：1. 全上下文提示（full-context prompting）；2. 通过检索增强生成（RAG）实现的语义记忆（semantic memory）；3. 通过上下文学习（in-context learning）实现的 the episodic memory；4. 通过提示优化（prompt optimization）实现的程序化记忆（procedural memory）。研究使用LoCoMo基准数据集，该数据集包含合成的长上下文对话，并针对需要多种推理策略的问答任务进行了标注。研究人员分析了这些方法在LoCoMo基准上的表现。

Result: 研究结果表明，与不使用记忆增强的方法相比，记忆增强的方法可以将token使用量减少90%以上，同时保持具有竞争力的准确性。此外，研究发现记忆架构的复杂性应与模型的性能相匹配：小型基础模型从RAG中受益最多；而强大的指令微调模型则从 the episodic learning（通过反思和更复杂的agentic semantic memory）中获得更多益处。具体而言，the episodic memory有助于LLM认识到其自身知识的局限性。

Conclusion: 本研究系统地评估了不同的记忆增强方法在长上下文对话任务中的有效性，并提出了LoCoMo基准数据集。研究发现，记忆增强方法在显著减少token使用量的同时，能够保持准确性。模型能力与记忆架构复杂度的匹配至关重要，小型模型更适合RAG，而强大模型则受益于 the episodic learning。The episodic memory尤其有助于LLM识别自身知识的不足。这项工作为设计更有效的长对话LLM提供了重要的见解，并指出了未来研究的方向，例如进一步探索不同记忆机制的组合以及其在更广泛任务中的应用。

Abstract: In order for large language models to achieve true conversational continuity
and benefit from experiential learning, they need memory. While research has
focused on the development of complex memory systems, it remains unclear which
types of memory are most effective for long-context conversational tasks. We
present a systematic evaluation of memory-augmented methods using LoCoMo, a
benchmark of synthetic long-context dialogues annotated for question-answering
tasks that require diverse reasoning strategies. We analyse full-context
prompting, semantic memory through retrieval-augmented generation and agentic
memory, episodic memory through in-context learning, and procedural memory
through prompt optimization. Our findings show that memory-augmented approaches
reduce token usage by over 90% while maintaining competitive accuracy. Memory
architecture complexity should scale with model capability, with small
foundation models benefitting most from RAG, and strong instruction-tuned
reasoning model gaining from episodic learning through reflections and more
complex agentic semantic memory. In particular, episodic memory can help LLMs
recognise the limits of their own knowledge.

</details>


### [2] [BitSkip: An Empirical Analysis of Quantization and Early Exit Composition](https://arxiv.org/abs/2510.23766)
*Ramshankar Bhuvaneswaran,Handan Liu*

Main category: cs.CL

TL;DR: BitSkip是一个混合架构框架，用于探索LLM中的极端量化和动态路由等技术之间的组合效应。实验发现，简单的8位量化模型（BitSkip-V1）优于更复杂的4位量化和Hadamard变换模型，并且在性能上可与全精度基线相媲美。Hadamard变换在8位精度下会严重降低性能。BitSkip-V1在早期退出方面表现出色，在18层实现32.5%的速度提升，仅损失4%的质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究过度关注如极端量化和动态路由等复杂技术，但这些技术的组合效应研究不足。本研究旨在系统性地探索这些技术组合对LLM性能的影响，解决当前研究的不足。

Method: 提出BitSkip混合架构框架，用于系统地探索LLM中量化（如8位和4位）和Hadamard变换等技术的组合效应。实验通过比较不同配置（BitSkip-V1：8位量化无Hadamard变换；4位量化模型；8位量化加Hadamard变换）的性能来评估组合效应。

Result: BitSkip-V1（8位量化无Hadamard变换）的困惑度为1.13，优于4位量化模型和8位量化加Hadamard变换模型。8位量化加Hadamard变换模型性能下降超过37000%，表明存在训练不稳定的根本问题。BitSkip-V1在第18层实现32.5%的速度增益，仅损失4%的质量，显示出优越的早期退出特性。

Conclusion: BitSkip框架为探索LLM中的技术组合提供了有效途径。研究表明，简单技术（如8位量化）的组合可能比复杂技术更有效。BitSkip-V1在速度和质量之间取得了良好的平衡，特别是在早期退出方面。未来的工作可以进一步优化BitSkip框架，探索更多技术组合，并解决Hadamard变换引入的训练不稳定性问题。

Abstract: The pursuit of efficient Large Language Models (LLMs) has led to increasingly
complex techniques like extreme quantization and dynamic routing. While
individual benefits of these methods are well-documented, their compositional
effects remain poorly understood. This paper introduces BitSkip, a hybrid
architectural framework for systematically exploring these interactions.
Counter-intuitively, our findings reveal that a simple 8-bit quantized model
without Hadamard transform (BitSkip-V1) not only outperforms its more complex
4-bit and Hadamard-enhanced counterparts but also competes the full-precision
baseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard
transforms, even at 8-bit precision, catastrophically degraded performance by
over 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe
demonstrates superior early-exit characteristics, with layer 18 providing
optimal 32.5% speed gain for minimal 4% quality loss.

</details>


### [3] [Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language](https://arxiv.org/abs/2510.23828)
*Mena Attia,Aashiq Muhamed,Mai Alkhamissi,Thamar Solorio,Mona Diab*

Main category: cs.CL

TL;DR: 该研究全面评估了大型语言模型（LLM）在理解和运用包含地方知识和文化细微差别的比喻性语言方面的能力，特别关注阿拉伯语和英语。研究发现，LLM在处理阿拉伯语习语和谚语方面表现不如英语，并且在实际运用比喻性语言时面临挑战。研究还发布了一个新的阿拉伯语习语数据集 Kinayat，以促进未来的研究。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在各种语言任务中越来越强大，理解它们在处理文化特异性语言方面的能力变得至关重要。比喻性语言，如习语和谚语，因其蕴含地方知识和文化细微差别而成为衡量LLM文化理解能力的一个有效指标。然而，目前对于LLM在理解和恰当运用这类语言方面的表现缺乏全面评估，尤其是在非英语语言中。本研究旨在弥合这一差距，通过对阿拉伯语和英语的比喻性语言进行深入评估，揭示LLM在文化推理方面的优势和劣势。

Method: 该研究设计了一系列评估任务，用于测试LLM在理解、实际运用和内涵解释三个方面的能力。研究人员使用了阿拉伯语（埃及方言习语、多方言阿拉伯语谚语）和英语（英语谚语）的比喻性语言。具体来说，评估了22个开源和闭源LLM在这些任务上的表现。研究中，模型首先被要求理解比喻性表达的含义，然后是实际运用这些表达，最后是解释其内涵。为了量化模型表现，研究使用了准确率作为主要评估指标，并与人类标注者进行比较。此外，研究还发布了一个名为 Kinayat 的新数据集，包含埃及阿拉伯语习语，用于评估比喻性理解和实际运用。

Result: 研究结果显示，LLM在处理阿拉伯语比喻性语言时普遍面临更大挑战。阿拉伯语谚语的平均准确率比英语谚语低4.29%，埃及阿拉伯语习语的准确率则比阿拉伯语谚语低10.28%。在实际运用任务中，准确率比理解任务下降了14.07%，但提供包含习语的上下文句子可以将准确率提高10.66%。在解释内涵方面，模型表现不佳，在解释习语的内涵时，与人类标注者的一致性最高仅达到85.58%，而人类标注者之间的一致性为100%。这表明LLM虽然能够部分理解比喻性含义，但在恰当运用方面存在显著困难。

Conclusion: 本研究通过对LLM在理解和运用阿拉伯语及英语比喻性语言方面的能力进行全面评估，证明了比喻性语言是诊断LLM文化推理能力的有效工具。研究结果揭示了LLM在处理文化特异性语言，特别是阿拉伯语时存在的挑战，尤其是在实际运用和内涵解释方面。尽管LLM在一定程度上能够理解比喻性含义，但它们在将其恰当地融入语境中仍面临困难。为了推动相关领域的研究，研究者发布了 Kinayat 数据集。未来的工作可以集中于改进LLM的文化理解和生成能力，以及扩展对更多语言和文化背景的评估。

Abstract: We present a comprehensive evaluation of the ability of large language models
(LLMs) to process culturally grounded language, specifically to understand and
pragmatically use figurative expressions that encode local knowledge and
cultural nuance. Using figurative language as a proxy for cultural nuance and
local knowledge, we design evaluation tasks for contextual understanding,
pragmatic use, and connotation interpretation in Arabic and English. We
evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,
multidialectal Arabic proverbs, and English proverbs. Our results show a
consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower
than for English proverbs, and performance for Egyptian idioms is 10.28% lower
than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%
relative to understanding, though providing contextual idiomatic sentences
improves accuracy by 10.66%. Models also struggle with connotative meaning,
reaching at most 85.58% agreement with human annotators on idioms with 100%
inter-annotator agreement. These findings demonstrate that figurative language
serves as an effective diagnostic for cultural reasoning: while LLMs can often
interpret figurative meaning, they face challenges in using it appropriately.
To support future research, we release Kinayat, the first dataset of Egyptian
Arabic idioms designed for both figurative understanding and pragmatic use
evaluation.

</details>


### [4] [How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse](https://arxiv.org/abs/2510.23842)
*Saki Imai,Lee Kezar,Laurel Aichler,Mert Inan,Erin Walker,Alicia Wooten,Lorna Quandt,Malihe Alikhani*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Most state-of-the-art sign language models are trained on interpreter or
isolated vocabulary data, which overlooks the variability that characterizes
natural dialogue. However, human communication dynamically adapts to contexts
and interlocutors through spatiotemporal changes and articulation style. This
specifically manifests itself in educational settings, where novel vocabularies
are used by teachers, and students. To address this gap, we collect a motion
capture dataset of American Sign Language (ASL) STEM (Science, Technology,
Engineering, and Mathematics) dialogue that enables quantitative comparison
between dyadic interactive signing, solo signed lecture, and interpreted
articles. Using continuous kinematic features, we disentangle dialogue-specific
entrainment from individual effort reduction and show spatiotemporal changes
across repeated mentions of STEM terms. On average, dialogue signs are
24.6%-44.6% shorter in duration than the isolated signs, and show significant
reductions absent in monologue contexts. Finally, we evaluate sign embedding
models on their ability to recognize STEM signs and approximate how entrained
the participants become over time. Our study bridges linguistic analysis and
computational modeling to understand how pragmatics shape sign articulation and
its representation in sign language technologies.

</details>


### [5] [CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection](https://arxiv.org/abs/2510.23845)
*Grace Byun,Rebecca Lipschutz,Sean T. Minton,Abigail Lott,Jinho D. Choi*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Detecting mental health crisis situations such as suicide ideation, rape,
domestic violence, child abuse, and sexual harassment is a critical yet
underexplored challenge for language models. When such situations arise during
user--model interactions, models must reliably flag them, as failure to do so
can have serious consequences. In this work, we introduce CRADLE BENCH, a
benchmark for multi-faceted crisis detection. Unlike previous efforts that
focus on a limited set of crisis types, our benchmark covers seven types
defined in line with clinical standards and is the first to incorporate
temporal labels. Our benchmark provides 600 clinician-annotated evaluation
examples and 420 development examples, together with a training corpus of
around 4K examples automatically labeled using a majority-vote ensemble of
multiple language models, which significantly outperforms single-model
annotation. We further fine-tune six crisis detection models on subsets defined
by consensus and unanimous ensemble agreement, providing complementary models
trained under different agreement criteria.

</details>


### [6] [Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content](https://arxiv.org/abs/2510.24438)
*Abdullah Mushtaq,Rafay Naeem,Ezieddin Elmahjub,Ibrahim Ghaznavi,Shawqi Al-Maliki,Mohamed Abdallah,Ala Al-Fuqaha,Junaid Qadir*

Main category: cs.CL

TL;DR: 大型语言模型在伊斯兰指导方面存在误引、曲解教法和文化不一致的风险。本研究评估了 GPT-4o、Ansari AI 和 Fanar 在伊斯兰博客提示下的表现。采用包含定量（引用验证、六维度评分）和定性（侧边比较、五维度评分）的混合评估框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在伊斯兰指导领域的应用日益广泛，但存在引用不准确、教法应用错误以及文化响应不兼容等风险，这在宗教指导领域是不可接受的。因此，有必要评估现有 LLM 在此敏感领域的表现，并为未来改进提供方向。

Method: 本研究采用了一个双主体框架来评估 GPT-4o、Ansari AI 和 Fanar 三种大型语言模型。定量主体负责进行引文验证，并从结构、伊斯兰一致性、引文等六个维度进行评分。定性主体则进行并列比较，从语调、深度、原创性等五个维度进行评估。评估的提示语来自真实的伊斯兰博客。

Result: 在伊斯兰准确性和引文方面，GPT-4o 的得分最高（分别为 3.93 和 3.38），其次是 Ansari AI（分别为 3.68 和 3.32），Fanar 的得分最低（分别为 2.76 和 1.82）。GPT-4o 的平均定量得分为 3.90/5，而 Ansari AI 在定性比较中获胜次数最多（116/200）。尽管 Fanar 表现落后，但其在伊斯兰和阿拉伯语境方面具有创新性。

Conclusion: 尽管 GPT-4o 和 Ansari AI 在伊斯兰指导方面表现出相对较强的能力，但目前的大型语言模型在可靠生成准确的伊斯兰内容和引用方面仍有不足，而这恰恰是信仰敏感型写作所必需的。本研究强调了建立以穆斯林视角为中心的、社区驱动的基准测试的必要性，并为 AI 在伊斯兰知识及医学、法律、新闻等高风险领域的应用迈出了初步探索。

Abstract: Large language models are increasingly used for Islamic guidance, but risk
misquoting texts, misapplying jurisprudence, or producing culturally
inconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar
on prompts from authentic Islamic blogs. Our dual-agent framework uses a
quantitative agent for citation verification and six-dimensional scoring (e.g.,
Structure, Islamic Consistency, Citations) and a qualitative agent for
five-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).
GPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI
followed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong
performance, models still fall short in reliably producing accurate Islamic
content and citations -- a paramount requirement in faith-sensitive writing.
GPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led
qualitative pairwise wins (116/200). Fanar, though trailing, introduces
innovations for Islamic and Arabic contexts. This study underscores the need
for community-driven benchmarks centering Muslim perspectives, offering an
early step toward more reliable AI in Islamic knowledge and other high-stakes
domains such as medicine, law, and journalism.

</details>


### [7] [Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs](https://arxiv.org/abs/2510.23854)
*Jyotika Singh,Weiyi Sun,Amit Agarwal,Viji Krishnamurthy,Yassine Benajiba,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: 本文提出了一种名为Combo-Eval的新型评估方法，用于评估文本到SQL（Text-to-SQL）技术中从数据库结果生成自然语言表示（NLR）的质量。该方法结合了多种现有方法的优点，提高了评估的准确性，并显著减少了对大型语言模型（LLM）的调用次数（25-61%）。同时，研究人员还发布了NLR-BIRD，这是第一个专门用于NLR基准测试的数据集。实验证明，Combo-Eval与人类判断高度一致，适用于有无真实参考两种场景。


<details>
  <summary>Details</summary>
Motivation: 尽管Text-to-SQL技术在现代工业系统（如多轮聊天机器人）中至关重要，能够将自然语言问题转化为数据库查询，但将查询结果从表格形式转换为自然语言表示（NLR）的过程仍面临信息丢失或错误的问题。目前主要依赖大型语言模型（LLM）处理NLR生成，但对其生成内容的评估方法尚不完善，信息损失和错误呈现的问题未得到充分研究，这阻碍了聊天交互的质量和可靠性。

Method: 本文提出了一种名为Combo-Eval的新型评估方法，用于衡量LLM生成的NLR的质量。Combo-Eval整合了多种现有评估方法的优点，旨在优化评估的准确性和效率。为了支持NLR的评估和基准测试，研究人员创建了一个名为NLR-BIRD的新数据集，这是第一个专门为此目的设计的数据集。通过与人类判断进行比较，验证了Combo-Eval的有效性，该方法在有无真实参考的情况下均表现出优越的性能。

Result: 通过与人类判断进行比较，Combo-Eval在评估NLR的准确性方面显示出优越的性能，与人类的判断高度一致。此外，Combo-Eval能够显著减少LLM的调用次数，减少幅度在25%到61%之间，从而提高了评估效率。该方法在有无真实参考的场景下均适用。

Conclusion: 本文提出的Combo-Eval评估方法和NLR-BIRD数据集，为Text-to-SQL技术中的NLR生成评估提供了一个更准确、更高效的解决方案。Combo-Eval通过结合多种方法的优点，提高了评估的准确性，并减少了对LLM的依赖。NLR-BIRD数据集的发布填补了NLR基准测试领域的空白。研究结果表明，Combo-Eval与人类判断高度一致，并且在不同场景下均表现良好，预示着其在实际应用中的巨大潜力。未来的工作可以进一步探索Combo-Eval在更广泛的NLP任务中的应用，以及进一步优化数据集和评估指标。

Abstract: In modern industry systems like multi-turn chat agents, Text-to-SQL
technology bridges natural language (NL) questions and database (DB) querying.
The conversion of tabular DB results into NL representations (NLRs) enables the
chat-based interaction. Currently, NLR generation is typically handled by large
language models (LLMs), but information loss or errors in presenting tabular
results in NL remains largely unexplored. This paper introduces a novel
evaluation method - Combo-Eval - for judgment of LLM-generated NLRs that
combines the benefits of multiple existing methods, optimizing evaluation
fidelity and achieving a significant reduction in LLM calls by 25-61%.
Accompanying our method is NLR-BIRD, the first dedicated dataset for NLR
benchmarking. Through human evaluations, we demonstrate the superior alignment
of Combo-Eval with human judgments, applicable across scenarios with and
without ground truth references.

</details>


### [8] [OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning](https://arxiv.org/abs/2510.23870)
*Marianne Menglin Liu,Sai Ashish Somayajula,Syed Fahad Allam Shah,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: OraPlan-SQL 在 Archer NL2SQL 评估挑战赛 2025 中排名第一，在英语和中文上分别达到 55.0% 和 56.7% 的执行准确率 (EX)，同时保持超过 99% 的 SQL 有效性 (VA)。该系统采用基于智能体 (agent) 的框架，包含生成自然语言计划的 Planner agent 和将计划转换为 SQL 的 SQL agent。通过反馈引导的元提示策略优化单个 Planner agent，并通过实体链接和计划多样化来处理多语言和提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前的 NL2SQL 系统在处理需要复杂推理（如算术、常识和假设推理）的双语基准时面临挑战。Archer NL2SQL 评估挑战赛 2025 旨在解决这一问题，对需要高级理解和推理能力的系统提出了更高的要求。

Method: OraPlan-SQL 采用基于智能体的框架，由 Planner agent 和 SQL agent 组成。Planner agent 负责生成逐步的自然语言计划，SQL agent 则将这些计划转换为可执行的 SQL。为了优化 Planner agent，引入了一种反馈引导的元提示策略，通过分析失败案例并将其提炼成纠正性指南来改进系统提示。为了处理多语言场景，集成了实体链接指南，生成实体的不同表面形式并将其纳入计划。最后，通过计划多样化来增强可靠性，即为每个查询生成多个候选计划，并使用多数投票选择最终输出。

Result: OraPlan-SQL 在 Archer NL2SQL 评估挑战赛 2025 中取得了领先地位，在英语上实现了 55.0% 的执行准确率 (EX)，在中文上实现了 56.7% 的执行准确率 (EX)，平均准确率超过第二名 6%。同时，系统的 SQL 有效性 (VA) 保持在 99% 以上。

Conclusion: OraPlan-SQL 通过创新的反馈引导元提示策略、实体链接方法和计划多样化技术，在复杂的双语 NL2SQL 任务上取得了显著的成功。该系统在 Archer NL2SQL 评估挑战赛 2025 中名列前茅，证明了其在处理需要复杂推理和多语言能力的方面的有效性。未来的工作可以进一步探索元提示策略的泛化能力，以及在更广泛的多语言和领域特定场景下的应用。

Abstract: We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge
2025, a bilingual benchmark requiring complex reasoning such as arithmetic,
commonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding
the second-best system by more than 6% in execution accuracy (EX), with 55.0%
in English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).
Our system follows an agentic framework with two components: Planner agent that
generates stepwise natural language plans, and SQL agent that converts these
plans into executable SQL. Since SQL agent reliably adheres to the plan, our
refinements focus on the planner. Unlike prior methods that rely on multiple
sub-agents for planning and suffer from orchestration overhead, we introduce a
feedback-guided meta-prompting strategy to refine a single planner. Failure
cases from a held-out set are clustered with human input, and an LLM distills
them into corrective guidelines that are integrated into the planner's system
prompt, improving generalization without added complexity. For the multilingual
scenario, to address transliteration and entity mismatch issues, we incorporate
entity-linking guidelines that generate alternative surface forms for entities
and explicitly include them in the plan. Finally, we enhance reliability
through plan diversification: multiple candidate plans are generated for each
query, with the SQL agent producing a query for each plan, and final output
selected via majority voting over their executions.

</details>


### [9] [Language Models for Longitudinal Clinical Prediction](https://arxiv.org/abs/2510.23884)
*Tananun Songdechakraiwut,Michael Lutz*

Main category: cs.CL

TL;DR: 提出了一种轻量级框架，通过整合患者病史和上下文信息，在不进行模型微调的情况下，利用冻结的大型语言模型分析纵向临床数据，并成功应用于神经心理学评估，在早期阿尔茨海默病监测方面显示出巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的临床数据分析方法在处理纵向数据时面临挑战，尤其是在需要整合患者历史和上下文信息以进行准确预测的情况下。此外，对大型语言模型进行微调以适应特定临床任务通常需要大量标注数据和计算资源，这在大规模临床应用中可能难以实现。因此，研究一种能够有效利用现有大型语言模型能力，同时克服数据和计算资源限制的轻量级分析框架具有重要意义。

Method: 本研究提出了一种轻量级框架，该框架能够适应已冻结的大型语言模型，用于分析纵向临床数据。该方法的核心在于将患者的历史信息和上下文信息整合到语言模型的向量空间中，从而无需对模型进行微调即可生成准确的预测。具体来说，该框架能够处理非结构化的临床文本数据，提取关键的患者信息，并将其编码为模型能够理解的表示形式。在神经心理学评估任务中，该框架被应用于分析纵向数据，以监测疾病进展和预测未来趋势。

Result: 在神经心理学评估任务的应用中，该框架取得了准确且可靠的性能。即使在仅使用少量训练数据的情况下，该方法也能够有效地进行预测。实验结果表明，该框架在早期阿尔茨海默病监测方面表现出巨大的潜力，能够准确识别与疾病相关的模式和趋势。

Conclusion: 本研究提出的轻量级框架为利用大型语言模型分析纵向临床数据提供了一种有效且资源高效的解决方案。通过在不进行模型微调的情况下整合患者历史和上下文信息，该框架在神经心理学评估任务中取得了优异的性能，尤其是在早期阿尔茨海默病监测方面。这表明该方法在实际临床应用中具有广阔的前景。未来的工作可以探索该框架在其他临床领域的可扩展性，并进一步优化其在处理不同类型临床数据时的性能。

Abstract: We explore a lightweight framework that adapts frozen large language models
to analyze longitudinal clinical data. The approach integrates patient history
and context within the language model space to generate accurate forecasts
without model fine-tuning. Applied to neuropsychological assessments, it
achieves accurate and reliable performance even with minimal training data,
showing promise for early-stage Alzheimer's monitoring.

</details>


### [10] [AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages](https://arxiv.org/abs/2510.23896)
*Kosei Uemura,Miaoran Zhang,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Text embeddings are an essential building component of several NLP tasks such
as retrieval-augmented generation which is crucial for preventing
hallucinations in LLMs. Despite the recent release of massively multilingual
MTEB (MMTEB), African languages remain underrepresented, with existing tasks
often repurposed from translation benchmarks such as FLORES clustering or
SIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB
covering 59 languages, 14 tasks, and 38 datasets, including six newly added
datasets. Unlike many MMTEB datasets that include fewer than five languages,
the new additions span 14 to 56 African languages and introduce entirely new
tasks, such as hate speech detection, intent detection, and emotion
classification, which were not previously covered. Complementing this, we
present AfriE5, an adaptation of the instruction-tuned mE5 model to African
languages through cross-lingual contrastive distillation. Our evaluation shows
that AfriE5 achieves state-of-the-art performance, outperforming strong
baselines such as Gemini-Embeddings and mE5.

</details>


### [11] [Breaking the Benchmark: Revealing LLM Bias via Minimal Contextual Augmentation](https://arxiv.org/abs/2510.23921)
*Kaveh Eskandari Miandoab,Mahammed Kamruzzaman,Arshia Gharooni,Gene Louis Kim,Vasanth Sarathy,Ninareh Mehrabi*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models have been shown to demonstrate stereotypical biases in
their representations and behavior due to the discriminative nature of the data
that they have been trained on. Despite significant progress in the development
of methods and models that refrain from using stereotypical information in
their decision-making, recent work has shown that approaches used for bias
alignment are brittle. In this work, we introduce a novel and general
augmentation framework that involves three plug-and-play steps and is
applicable to a number of fairness evaluation benchmarks. Through application
of augmentation to a fairness evaluation dataset (Bias Benchmark for Question
Answering (BBQ)), we find that Large Language Models (LLMs), including
state-of-the-art open and closed weight models, are susceptible to
perturbations to their inputs, showcasing a higher likelihood to behave
stereotypically. Furthermore, we find that such models are more likely to have
biased behavior in cases where the target demographic belongs to a community
less studied by the literature, underlining the need to expand the fairness and
safety research to include more diverse communities.

</details>


### [12] [Agent-based Automated Claim Matching with Instruction-following LLMs](https://arxiv.org/abs/2510.23924)
*Dina Pisarevskaya,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a novel agent-based approach for the automated claim matching task
with instruction-following LLMs. We propose a two-step pipeline that first
generates prompts with LLMs, to then perform claim matching as a binary
classification task with LLMs. We demonstrate that LLM-generated prompts can
outperform SOTA with human-generated prompts, and that smaller LLMs can do as
well as larger ones in the generation process, allowing to save computational
resources. We also demonstrate the effectiveness of using different LLMs for
each step of the pipeline, i.e. using an LLM for prompt generation, and another
for claim matching. Our investigation into the prompt generation process in
turn reveals insights into the LLMs' understanding of claim matching.

</details>


### [13] [Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs](https://arxiv.org/abs/2510.23941)
*Soham Satyadharma,Fatemeh Sheikholeslami,Swati Kaul,Aziz Umit Batur,Suleiman A. Khan*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a novel, training free cascade for auto-prompting Large Language
Models (LLMs) to assess product quality in e-commerce. Our system requires no
training labels or model fine-tuning, instead automatically generating and
refining prompts for evaluating attribute quality across tens of thousands of
product category-attribute pairs. Starting from a seed of human-crafted
prompts, the cascade progressively optimizes instructions to meet
catalog-specific requirements. This approach bridges the gap between general
language understanding and domain-specific knowledge at scale in complex
industrial catalogs. Our extensive empirical evaluations shows the auto-prompt
cascade improves precision and recall by $8-10\%$ over traditional
chain-of-thought prompting. Notably, it achieves these gains while reducing
domain expert effort from 5.1 hours to 3 minutes per attribute - a $99\%$
reduction. Additionally, the cascade generalizes effectively across five
languages and multiple quality assessment tasks, consistently maintaining
performance gains.

</details>


### [14] [Leveraging LLMs for Early Alzheimer's Prediction](https://arxiv.org/abs/2510.23946)
*Tananun Songdechakraiwut*

Main category: cs.CL

TL;DR: 提出了一种结合连接组学和大型语言模型（LLM）的框架，用于通过动态功能磁共振成像（fMRI）连接数据进行早期阿尔茨海默病检测。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）的早期检测至关重要，但现有方法在敏感性和临床应用方面存在局限性。开发一种能够有效分析复杂神经影像数据并进行精准预测的新方法具有重要意义。

Method: 该方法将动态fMRI连接数据编码为时间序列，进行稳健的归一化处理，并将其映射为适合冻结的预训练LLM的表示。然后，利用该LLM进行临床预测。

Result: 在早期阿尔茨海默病检测任务中，该方法实现了远低于临床公认阈值的错误率，显示出高度的预测敏感性。

Conclusion: 该研究提出的连接组学信息LLM框架为早期阿尔茨海默病检测提供了一种有前景的新方法，其高预测精度预示着在阿尔茨海默病早期干预方面具有重要潜力。未来的工作可以进一步探索该框架在其他神经退行性疾病中的应用，并优化模型以提高可解释性。

Abstract: We present a connectome-informed LLM framework that encodes dynamic fMRI
connectivity as temporal sequences, applies robust normalization, and maps
these data into a representation suitable for a frozen pre-trained LLM for
clinical prediction. Applied to early Alzheimer's detection, our method
achieves sensitive prediction with error rates well below clinically recognized
margins, with implications for timely Alzheimer's intervention.

</details>


### [15] [Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning in Multilingual LLMs](https://arxiv.org/abs/2510.23949)
*Kyomin Hwang,Hyeonjin Kim,Seungyeon Kim,Sunghyun Wee,Nojun Kwak*

Main category: cs.CL

TL;DR: 以往的研究表明，仅使用英语数据来擦除多语言大型语言模型（LLM）的多语言知识是不够的。本文从评估角度切入，聚焦于多语言LLM在完全使用并行多语言数据集进行微调后进行“遗忘”时出现的“语言混淆”问题。研究发现，这种现象导致标准的基于参考的评估指标失效，并提出了新的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有关于擦除多语言LLM知识的研究主要关注性能，忽略了在完全微调后的遗忘过程中可能出现的“语言混淆”问题。语言混淆是指模型回应的语言与输入提示的语言不一致，这使得标准的基于参考的评估指标（如BLEU、ROUGE等）无法准确衡量遗忘效果，是一个亟待解决的评估盲点。

Method: 1. 提出N-gram-based Language-Mix (N-Mix)得分，量化评估语言混淆的普遍性和一致性。2. 通过实验证明，当N-Mix得分较高时，基于参考的评估指标会出现假阴性结果。3. 论证了需要一种新的评估方法，可以直接评估生成句子的语义内容，即提出“基于语义的指标”。

Result: 研究表明，语言混淆在多语言LLM中普遍存在且一致。基于参考的评估指标在语言混淆严重时会产生误导性的低评估结果（假阴性）。N-Mix得分能够有效揭示语言混淆的程度。

Conclusion: 本文指出了当前多语言LLM遗忘研究中评估方法的局限性，特别是语言混淆问题对标准指标的干扰。研究提出了量化语言混淆的新指标N-Mix，并强调了开发直接评估语义内容的“基于语义的指标”的必要性，为未来多语言LLM的遗忘评估研究提供了新的方向。

Abstract: There have been a couple of studies showing that attempting to erase
multilingual knowledge using only English data is insufficient for multilingual
LLMs. However, their analyses remain highly performance-oriented. In this
paper, we switch the point of view to evaluation, and address an additional
blind spot which reveals itself when the multilingual LLM is fully finetuned
with parallel multilingual dataset before unlearning. Here, language confusion
occurs whereby a model responds in language different from that of the input
prompt. Language confusion is a problematic phenomenon in unlearning, causing
the standard reference-based metrics to fail. We tackle this phenomenon in
three steps: (1) introduce N-gram-based Language-Mix (N-Mix) score to
quantitatively show the language confusion is pervasive and consistent in
multilingual LLMs, (2) demonstrate that reference-based metrics result in false
negatives when N-Mix score is high, and(3) suggest the need of new type of
unlearning evaluation that can directly assess the content of the generated
sentences. We call this type of metrics as semantic-based metric.

</details>


### [16] [M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems](https://arxiv.org/abs/2510.23995)
*Mengzhou Sun,Sendong Zhao,Jianyu Chen,Haochun Wang,Bin Qin*

Main category: cs.CL

TL;DR: 提出了一种名为M-Eval的新方法，通过借鉴循证医学（EBM）中的异质性分析，利用多源证据来检查检索增强生成（RAG）在医学问答系统中生成响应的事实准确性和证据可靠性，从而提高了准确性（最高提升23.31%），减少了错误和诊断误差。


<details>
  <summary>Details</summary>
Motivation: 当前的检索增强生成（RAG）技术在医学问答系统中虽然利用了大型语言模型（LLMs）和外部医学文献，但仍存在生成不正确信息（如幻觉）和未能正确使用外部知识的问题。这严重影响了其在专业领域的应用可靠性，并可能导致严重的后果。

Method: 提出M-Eval方法，该方法受循证医学（EBM）中异质性分析的启发。具体步骤包括：1. 从外部知识库提取额外的医学文献。2. 检索RAG系统生成的证据文档。3. 利用异质性分析技术，检查提取的证据是否支持响应中存在的不同观点。4. 评估RAG系统提供证据的可靠性，以验证响应的准确性。

Result: M-Eval方法在多个大型语言模型（LLMs）上进行了测试，结果显示准确性提高了23.31%。该方法能够有效检测当前基于RAG的医学系统中的错误，并提高了LLM应用的可靠性。

Conclusion: M-Eval通过引入基于EBM异质性分析的方法，有效解决了RAG在医学问答中存在的事实错误和知识使用不当的问题。该方法提高了系统的准确性和可靠性，减少了潜在的诊断误差，为LLM在医疗领域的安全应用奠定了基础。未来工作可进一步探索其在更广泛医疗场景下的应用和优化。

Abstract: Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing
medical question-answering systems through the integration of large language
models (LLMs) with external medical literature. LLMs can retrieve relevant
medical articles to generate more professional responses efficiently. However,
current RAG applications still face problems. They generate incorrect
information, such as hallucinations, and they fail to use external knowledge
correctly. To solve these issues, we propose a new method named M-Eval. This
method is inspired by the heterogeneity analysis approach used in
Evidence-Based Medicine (EBM). Our approach can check for factual errors in RAG
responses using evidence from multiple sources. First, we extract additional
medical literature from external knowledge bases. Then, we retrieve the
evidence documents generated by the RAG system. We use heterogeneity analysis
to check whether the evidence supports different viewpoints in the response. In
addition to verifying the accuracy of the response, we also assess the
reliability of the evidence provided by the RAG system. Our method shows an
improvement of up to 23.31% accuracy across various LLMs. This work can help
detect errors in current RAG-based medical systems. It also makes the
applications of LLMs more reliable and reduces diagnostic errors.

</details>


### [17] [PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine](https://arxiv.org/abs/2510.23998)
*Mengzhou Sun,Sendong Zhao,Jianyu Chen,Bin Qin*

Main category: cs.CL

TL;DR: 本研究提出了一种名为PICOs-RAG的改进检索增强生成（RAG）方法，用于解决在循证医学（EBM）领域中处理复杂、不精确的临床查询的挑战。通过将用户查询扩展并格式化为PICO（Population, Intervention, Comparison, Outcome）标准，PICOs-RAG显著提高了证据检索的效率和相关性，相较于基线模型，性能提升高达8.8%，使大型语言模型能更好地作为医学助手。


<details>
  <summary>Details</summary>
Motivation: 循证医学（EBM）的研究至关重要，旨在为医生和患者提供可靠的医疗理论支持，以减少医疗事故。传统的人工文献检索方法效率低下且缺乏客观性。现有的检索增强生成（RAG）方法在处理现实临床场景中复杂、信息不全或表述不精确的查询时存在困难，导致检索到的证据不相关，生成的回答无帮助。因此，有必要改进RAG方法以应对这些挑战。

Method: 本研究提出的PICOs-RAG方法，通过查询扩展和标准化来优化用户输入。具体来说，它将用户的查询扩展成更专业的格式，并利用PICO（Population, Intervention, Comparison, Outcome）格式，一种循证医学中常用的搜索策略工具，来提取用于检索的最关键信息。这种方法旨在提高检索的效率和相关性。

Result: PICOs-RAG方法在评估中展现出显著的性能提升，与基线方法相比，检索效率和相关性提高了高达8.8%。这一结果表明，PICOs-RAG能够有效地处理复杂和不精确的临床查询，提升了证据检索的准确性。

Conclusion: PICOs-RAG方法的提出，有效解决了现有RAG方法在处理复杂临床查询时的不足，通过引入PICO格式优化了信息检索过程。实验结果表明，该方法能够显著提升检索效率和相关性，使大型语言模型在循证医学领域成为更可靠、更有帮助的医学助手。未来的工作可以进一步探索该方法在更多样化的临床场景和更大规模数据集上的应用。

Abstract: Evidence-based medicine (EBM) research has always been of paramount
importance. It is important to find appropriate medical theoretical support for
the needs from physicians or patients to reduce the occurrence of medical
accidents. This process is often carried out by human querying relevant
literature databases, which lacks objectivity and efficiency. Therefore,
researchers utilize retrieval-augmented generation (RAG) to search for evidence
and generate responses automatically. However, current RAG methods struggle to
handle complex queries in real-world clinical scenarios. For example, when
queries lack certain information or use imprecise language, the model may
retrieve irrelevant evidence and generate unhelpful answers. To address this
issue, we present the PICOs-RAG to expand the user queries into a better
format. Our method can expand and normalize the queries into professional ones
and use the PICO format, a search strategy tool present in EBM, to extract the
most important information used for retrieval. This approach significantly
enhances retrieval efficiency and relevance, resulting in up to an 8.8\%
improvement compared to the baseline evaluated by our method. Thereby the
PICOs-RAG improves the performance of the large language models into a helpful
and reliable medical assistant in EBM.

</details>


### [18] [META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for Retrieval-Augmented Generation in Evidence-Based Medicine](https://arxiv.org/abs/2510.24003)
*Mengzhou Sun,Sendong Zhao,Jianyu Chen,Haochun Wang,Bin Qin*

Main category: cs.CL

TL;DR: 本研究提出了一种新的基于元分析思想的重排和过滤医学证据的方法，以提高检索增强生成（RAG）在循证医学（EBM）任务中的证据质量，实验证明该方法能显著提高诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 循证医学（EBM）在临床中至关重要，能减少误诊。尽管大型语言模型（LLMs）结合检索增强生成（RAG）在EBM任务中效率很高，但EBM对证据质量有严格要求，而现有RAG方法难以有效筛选高质量证据。

Method: 提出了一种借鉴EBM元分析思想的新方法，结合可靠性分析、异质性分析和外推分析，对医学证据进行重排和过滤，为LLMs提供最佳证据。该方法旨在模拟元分析过程，以检索最优医学证据。

Result: 在PubMed数据集上进行实验评估，结果显示所提出的方法能够显著提高LLMs在EBM任务中的准确性，最高可提升11.4%。该方法成功使RAG能够提取更高质量、更可靠的证据，减少错误知识的注入，从而提供更有效的回复。

Conclusion: 该研究成功提出了一种有效的方法来提升RAG在EBM任务中的证据质量，通过模拟元分析过程，显著提高了诊断准确率。未来工作可进一步探索该方法的泛化能力和在其他医学领域的应用。

Abstract: Evidence-based medicine (EBM) holds a crucial role in clinical application.
Given suitable medical articles, doctors effectively reduce the incidence of
misdiagnoses. Researchers find it efficient to use large language models (LLMs)
techniques like RAG for EBM tasks. However, the EBM maintains stringent
requirements for evidence, and RAG applications in EBM struggle to efficiently
distinguish high-quality evidence. Therefore, inspired by the meta-analysis
used in EBM, we provide a new method to re-rank and filter the medical
evidence. This method presents multiple principles to filter the best evidence
for LLMs to diagnose. We employ a combination of several EBM methods to emulate
the meta-analysis, which includes reliability analysis, heterogeneity analysis,
and extrapolation analysis. These processes allow the users to retrieve the
best medical evidence for the LLMs. Ultimately, we evaluate these high-quality
articles and show an accuracy improvement of up to 11.4% in our experiments and
results. Our method successfully enables RAG to extract higher-quality and more
reliable evidence from the PubMed dataset. This work can reduce the infusion of
incorrect knowledge into responses and help users receive more effective
replies.

</details>


### [19] [Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward](https://arxiv.org/abs/2510.24020)
*Hao An,Yang Xu*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）的幻觉问题是其可靠部署的关键。现有方法通常通过整体置信度或不确定性分数来指导LLMs，但这可能导致对模型知识边界的认识不精确。本文提出了一种新颖的强化学习框架——细粒度语义置信奖励（Ours），通过样本特定的置信度来指导LLMs进行推断。该方法通过采样多个候选答案并进行语义聚类，然后训练LLM保留高置信度聚类中的答案并丢弃低置信度聚类中的答案，从而实现精确的事后推断。此外，还提出了一种新的评估指标来更全面地评估推断微调任务的可靠性。该方法在模型内和模型外基准测试中都显著提高了可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成文本方面表现出色，但容易产生“幻觉”，即生成不准确或捏造的信息。这阻碍了它们在现实世界中的可靠应用。目前的缓解方法通常依赖于粗粒度的信号，如整体置信度或对多个样本答案的不确定性，这限制了模型对其知识边界的精确感知。因此，需要一种更精细的方法来提高LLMs在何时应该回答和何时应该推断的准确性。

Method: 本文提出了一种名为“细粒度语义置信奖励（Ours）”的强化学习框架。该框架通过以下步骤指导LLMs进行推断：1. 采样多个候选答案。2. 对这些候选答案进行语义聚类。3. 训练LLM保留高置信度聚类中的答案，并丢弃低置信度聚类中的答案。这种方法能够实现更精确的事后推断。此外，研究者还引入了一种新的评估指标，用于更全面地衡量推断微调任务的可靠性。

Result: 通过细粒度语义置信奖励（Ours）框架，该研究在模型内（in-domain）和模型外（out-of-distribution）的基准测试中均显著提高了LLMs的可靠性。新的评估指标也为评估推断微调任务的有效性提供了更全面的视角。

Conclusion: 本文提出的细粒度语义置信奖励（Ours）框架，通过样本特定的置信度，有效解决了大型语言模型（LLMs）的幻觉问题，提高了其可靠性。该方法通过语义聚类和样本特定的置信度奖励，实现了比现有方法更精确的推断。新的评估指标也为该领域的研究提供了有价值的工具。未来的工作可以探索该框架在更广泛的应用场景中的有效性，并进一步优化其性能。

Abstract: Mitigating hallucinations in Large Language Models (LLMs) is critical for
their reliable deployment. Existing methods typically fine-tune LLMs to abstain
from answering questions beyond their knowledge scope. However, these methods
often rely on coarse-grained signals to guide LLMs to abstain, such as overall
confidence or uncertainty scores on multiple sampled answers, which may result
in an imprecise awareness of the model's own knowledge boundaries. To this end,
we propose a novel reinforcement learning framework built on
$\textbf{\underline{Fi}ne-grained \underline{S}emantic \underline{Co}nfidence
\underline{Re}ward (\Ours)}$, which guides LLMs to abstain via sample-specific
confidence. Specifically, our method operates by sampling multiple candidate
answers and conducting semantic clustering, then training the LLM to retain
answers within high-confidence clusters and discard those within low-confidence
ones, thereby promoting accurate post-hoc abstention. Additionally, we propose
a new metric for evaluating the reliability of abstention fine-tuning tasks
more comprehensively. Our method significantly enhances reliability in both
in-domain and out-of-distribution benchmarks.

</details>


### [20] [SpecKD: Speculative Decoding for Effective Knowledge Distillation of LLMs](https://arxiv.org/abs/2510.24021)
*Haiduo Huang,Jiangcheng Song,Yadong Zhang,Pengju Ren*

Main category: cs.CL

TL;DR: 知识蒸馏（KD）是压缩大型语言模型（LLM）的关键技术，但传统方法对所有 token 统一应用蒸馏损失，可能引入噪声并损害学生模型性能。本文提出了一种名为“推测性知识蒸馏”（SpecKD）的新型即插即用框架，引入了动态的、 token 级别的门控机制。该机制借鉴了推测解码的“提出-验证”范式，仅对“接受”的 token 应用蒸馏损失，而“拒绝”的 token 则被屏蔽。实验证明 SpecKD 在各种文本生成任务上均显著优于现有 KD 方法，提高了训练稳定性，使学生模型更强大，并达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏（KD）方法在压缩大型语言模型（LLM）时，通常会对所有 token 统一应用蒸馏损失，而不考虑教师模型对不同 token 的置信度。这种做法可能导致学生模型学习教师模型的不确定性或高熵预测，引入噪声，从而损害学生模型的性能，尤其是在教师模型远大于学生模型时。因此，需要一种更智能的蒸馏方法，能够区分 token 的重要性，选择性地应用蒸馏损失，以提高蒸馏效率和学生模型的性能。

Method: 本文提出了一种名为“推测性知识蒸馏”（SpecKD）的框架。该框架是一个即插即用的模块，引入了一个动态的、 token 级别的门控机制。该机制受到推测解码中“提出-验证”范式的启发。在每个步骤中，学生模型提出的 token 会与教师模型的分布进行比较和验证。只有当教师模型的预测被认为是“接受”时，才会对该 token 应用蒸馏损失；反之，被“拒绝”的 token 将被屏蔽，不用于蒸馏。这种选择性蒸馏有助于学生模型专注于学习教师模型真正确信的部分。实验在多种文本生成任务上进行了广泛的评估，并与强基线 KD 方法进行了比较，以验证 SpecKD 的有效性。

Result: 在多种文本生成任务上的广泛实验表明，SpecKD 持续且显著地优于强大的知识蒸馏（KD）基线。该方法不仅提高了学生模型的性能，还带来了更稳定的训练过程。SpecKD 实现了最先进的结果，证明了其在 LLM 压缩和学生模型能力提升方面的有效性。

Conclusion: 本文提出的推测性知识蒸馏（SpecKD）框架通过引入 token 级别的动态门控机制，有效解决了传统 KD 方法中不区分 token 置信度的问题。SpecKD 实现了选择性蒸馏，仅在教师模型高度置信时才应用蒸馏损失，从而提高了蒸馏效率，增强了学生模型的性能，并优化了训练稳定性。实验结果表明，SpecKD 在各项文本生成任务上均取得了最先进的成果，为 LLM 的高效压缩提供了一种创新的解决方案。未来的工作可以进一步探索该机制在不同模型架构和任务上的泛化能力，以及优化门控机制的参数和策略。

Abstract: Knowledge Distillation (KD) has become a cornerstone technique for
compressing Large Language Models (LLMs) into smaller, more efficient student
models. However, conventional KD approaches typically apply the distillation
loss uniformly across all tokens, regardless of the teacher's confidence. This
indiscriminate mimicry can introduce noise, as the student is forced to learn
from the teacher's uncertain or high-entropy predictions, which may ultimately
harm student performance-especially when the teacher is much larger and more
powerful. To address this, we propose Speculative Knowledge Distillation
(SpecKD), a novel, plug-and-play framework that introduces a dynamic,
token-level gating mechanism inspired by the "propose-and-verify" paradigm of
speculative decoding. At each step, the student's token proposal is verified
against the teacher's distribution; the distillation loss is selectively
applied only to "accepted" tokens, while "rejected" tokens are masked out.
Extensive experiments on diverse text generation tasks show that SpecKD
consistently and significantly outperforms strong KD baselines, leading to more
stable training and more capable student models, and achieving state-of-the-art
results.

</details>


### [21] [Success and Cost Elicit Convention Formation for Efficient Communication](https://arxiv.org/abs/2510.24023)
*Saujas Vaduguru,Yilun Hua,Yoav Artzi,Daniel Fried*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Humans leverage shared conversational context to become increasingly
successful and efficient at communicating over time. One manifestation of this
is the formation of ad hoc linguistic conventions, which allow people to
coordinate on short, less costly utterances that are understood using shared
conversational context. We present a method to train large multimodal models to
form conventions, enabling efficient communication. Our approach uses simulated
reference games between models, and requires no additional human-produced data.
In repeated reference games involving photographs and tangram images, our
method enables models to communicate efficiently with people: reducing the
message length by up to 41% while increasing success by 15% over the course of
the interaction. Human listeners respond faster when interacting with our model
that forms conventions. We also show that training based on success or cost
alone is insufficient - both are necessary to elicit convention formation.

</details>


### [22] [Pie: A Programmable Serving System for Emerging LLM Applications](https://arxiv.org/abs/2510.24051)
*In Gim,Zhiyao Ma,Seung-seob Lee,Lin Zhong*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Emerging large language model (LLM) applications involve diverse reasoning
strategies and agentic workflows, straining the capabilities of existing
serving systems built on a monolithic token generation loop. This paper
introduces Pie, a programmable LLM serving system designed for flexibility and
efficiency. Pie decomposes the traditional generation loop into fine-grained
service handlers exposed via an API and delegates control of the generation
process to user-provided programs, called inferlets. This enables applications
to implement new KV cache strategies, bespoke generation logic, and seamlessly
integrate computation and I/O-entirely within the application, without
requiring modifications to the serving system. Pie executes inferlets using
WebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows
Pie matches state-of-the-art performance on standard tasks (3-12% latency
overhead) while significantly improving latency and throughput (1.3x-3.4x
higher) on agentic workflows by enabling application-specific optimizations.

</details>


### [23] [Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling Hallucination in Translation](https://arxiv.org/abs/2510.24073)
*Xinwei Wu,Heng Liu,Jiang Zhou,Xiaohu Zhao,Linlong Xu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have advanced machine translation but remain
vulnerable to hallucinations. Unfortunately, existing MT benchmarks are not
capable of exposing failures in multilingual LLMs. To disclose hallucination in
multilingual LLMs, we introduce a diagnostic framework with a taxonomy that
separates Instruction Detachment from Source Detachment. Guided by this
taxonomy, we create HalloMTBench, a multilingual, human-verified benchmark
across 11 English-to-X directions. We employed 4 frontier LLMs to generate
candidates and scrutinize these candidates with an ensemble of LLM judges, and
expert validation. In this way, we curate 5,435 high-quality instances. We have
evaluated 17 LLMs on HalloMTBench. Results reveal distinct ``hallucination
triggers'' -- unique failure patterns reflecting model scale, source length
sensitivity, linguistic biases, and Reinforcement-Learning (RL) amplified
language mixing. HalloMTBench offers a forward-looking testbed for diagnosing
LLM translation failures. HalloMTBench is available in
https://huggingface.co/collections/AIDC-AI/marco-mt.

</details>


### [24] [Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures](https://arxiv.org/abs/2510.24081)
*Tyler A. Chang,Catherine Arnett,Abdelrahman Eldesokey,Abdelrahman Sadallah,Abeer Kashar,Abolade Daud,Abosede Grace Olanihun,Adamu Labaran Mohammed,Adeyemi Praise,Adhikarinayum Meerajita Sharma,Aditi Gupta,Afitab Iyigun,Afonso Simplício,Ahmed Essouaied,Aicha Chorana,Akhil Eppa,Akintunde Oladipo,Akshay Ramesh,Aleksei Dorkin,Alfred Malengo Kondoro,Alham Fikri Aji,Ali Eren Çetintaş,Allan Hanbury,Alou Dembele,Alp Niksarli,Álvaro Arroyo,Amin Bajand,Amol Khanna,Ana Chkhaidze,Ana Condez,Andiswa Mkhonto,Andrew Hoblitzell,Andrew Tran,Angelos Poulis,Anirban Majumder,Anna Vacalopoulou,Annette Kuuipolani Kanahele Wong,Annika Simonsen,Anton Kovalev,Ashvanth. S,Ayodeji Joseph Lana,Barkin Kinay,Bashar Alhafni,Benedict Cibalinda Busole,Bernard Ghanem,Bharti Nathani,Biljana Stojanovska Đurić,Bola Agbonile,Bragi Bergsson,Bruce Torres Fischer,Burak Tutar,Burcu Alakuş Çınar,Cade J. Kanoniakapueo Kane,Can Udomcharoenchaikit,Catherine Arnett,Chadi Helwe,Chaithra Reddy Nerella,Chen Cecilia Liu,Chiamaka Glory Nwokolo,Cristina España-Bonet,Cynthia Amol,DaeYeop Lee,Dana Arad,Daniil Dzenhaliou,Daria Pugacheva,Dasol Choi,Daud Abolade,David Liu,David Semedo,Deborah Popoola,Deividas Mataciunas,Delphine Nyaboke,Dhyuthy Krishna Kumar,Diogo Glória-Silva,Diogo Tavares,Divyanshu Goyal,DongGeon Lee,Ebele Nwamaka Anajemba,Egonu Ngozi Grace,Elena Mickel,Elena Tutubalina,Elias Herranen,Emile Anand,Emmanuel Habumuremyi,Emuobonuvie Maria Ajiboye,Eryawan Presma Yulianrifat,Esther Adenuga,Ewa Rudnicka,Faith Olabisi Itiola,Faran Taimoor Butt,Fathima Thekkekara,Fatima Haouari,Filbert Aurelian Tjiaranata,Firas Laakom,Francesca Grasso,Francesco Orabona,Francesco Periti,Gbenga Kayode Solomon,Gia Nghia Ngo,Gloria Udhehdhe-oze,Gonçalo Martins,Gopi Naga Sai Ram Challagolla,Guijin Son,Gulnaz Abdykadyrova,Hafsteinn Einarsson,Hai Hu,Hamidreza Saffari,Hamza Zaidi,Haopeng Zhang,Harethah Abu Shairah,Harry Vuong,Hele-Andra Kuulmets,Houda Bouamor,Hwanjo Yu,Iben Nyholm Debess,İbrahim Ethem Deveci,Ikhlasul Akmal Hanif,Ikhyun Cho,Inês Calvo,Inês Vieira,Isaac Manzi,Ismail Daud,Itay Itzhak,Iuliia,Alekseenko,Ivan Belashkin,Ivan Spada,Ivan Zhelyazkov,Jacob Brinton,Jafar Isbarov,Jaka Čibej,Jan Čuhel,Jan Kocoń,Jauza Akbar Krito,Jebish Purbey,Jennifer Mickel,Jennifer Za,Jenny Kunz,Jihae Jeong,Jimena Tena Dávalos,Jinu Lee,João Magalhães,John Yi,Jongin Kim,Joseph Chataignon,Joseph Marvin Imperial,Jubeerathan Thevakumar,Judith Land,Junchen Jiang,Jungwhan Kim,Kairit Sirts,Kamesh R,Kamesh V,Kanda Patrick Tshinu,Kätriin Kukk,Kaustubh Ponkshe,Kavsar Huseynova,Ke He,Kelly Buchanan,Kengatharaiyer Sarveswaran,Kerem Zaman,Khalil Mrini,Kian Kyars,Krister Kruusmaa,Kusum Chouhan,Lainitha Krishnakumar,Laura Castro Sánchez,Laura Porrino Moscoso,Leshem Choshen,Levent Sencan,Lilja Øvrelid,Lisa Alazraki,Lovina Ehimen-Ugbede,Luheerathan Thevakumar,Luxshan Thavarasa,Mahnoor Malik,Mamadou K. Keita,Mansi Jangid,Marco De Santis,Marcos García,Marek Suppa,Mariam D'Ciofalo,Marii Ojastu,Maryam Sikander,Mausami Narayan,Maximos Skandalis,Mehak Mehak,Mehmet İlteriş Bozkurt,Melaku Bayu Workie,Menan Velayuthan,Michael Leventhal,Michał Marcińczuk,Mirna Potočnjak,Mohammadamin Shafiei,Mridul Sharma,Mrityunjaya Indoria,Muhammad Ravi Shulthan Habibi,Murat Kolić,Nada Galant,Naphat Permpredanun,Narada Maugin,Nicholas Kluge Corrêa,Nikola Ljubešić,Nirmal Thomas,Nisansa de Silva,Nisheeth Joshi,Nitish Ponkshe,Nizar Habash,Nneoma C. Udeze,Noel Thomas,Noémi Ligeti-Nagy,Nouhoum Coulibaly,Nsengiyumva Faustin,Odunayo Kareemat Buliaminu,Odunayo Ogundepo,Oghojafor Godswill Fejiro,Ogundipe Blessing Funmilola,Okechukwu God'spraise,Olanrewaju Samuel,Olaoye Deborah Oluwaseun,Olasoji Akindejoye,Olga Popova,Olga Snissarenko,Onyinye Anulika Chiemezie,Orkun Kinay,Osman Tursun,Owoeye Tobiloba Moses,Oyelade Oluwafemi Joshua,Oyesanmi Fiyinfoluwa,Pablo Gamallo,Pablo Rodríguez Fernández,Palak Arora,Pedro Valente,Peter Rupnik,Philip Oghenesuowho Ekiugbo,Pramit Sahoo,Prokopis Prokopidis,Pua Niau-Puhipau,Quadri Yahya,Rachele Mignone,Raghav Singhal,Ram Mohan Rao Kadiyala,Raphael Merx,Rapheal Afolayan,Ratnavel Rajalakshmi,Rishav Ghosh,Romina Oji,Ron Kekeha Solis,Rui Guerra,Rushikesh Zawar,Sa'ad Nasir Bashir,Saeed Alzaabi,Sahil Sandeep,Sai Pavan Batchu,SaiSandeep Kantareddy,Salsabila Zahirah Pranida,Sam Buchanan,Samuel Rutunda,Sander Land,Sarah Sulollari,Sardar Ali,Saroj Sapkota,Saulius Tautvaisas,Sayambhu Sen,Sayantani Banerjee,Sebastien Diarra,SenthilNathan. M,Sewoong Lee,Shaan Shah,Shankar Venkitachalam,Sharifa Djurabaeva,Sharon Ibejih,Shivanya Shomir Dutta,Siddhant Gupta,Silvia Paniagua Suárez,Sina Ahmadi,Sivasuthan Sukumar,Siyuan Song,Snegha A.,Sokratis Sofianopoulos,Sona Elza Simon,Sonja Benčina,Sophie Gvasalia,Sphurti Kirit More,Spyros Dragazis,Stephan P. Kaufhold,Suba. S,Sultan AlRashed,Surangika Ranathunga,Taiga Someya,Taja Kuzman Pungeršek,Tal Haklay,Tasi'u Jibril,Tatsuya Aoyama,Tea Abashidze,Terenz Jomar Dela Cruz,Terra Blevins,Themistoklis Nikas,Theresa Dora Idoko,Thu Mai Do,Tilek Chubakov,Tommaso Gargiani,Uma Rathore,Uni Johannesen,Uwuma Doris Ugwu,Vallerie Alexandra Putra,Vanya Bannihatti Kumar,Varsha Jeyarajalingam,Varvara Arzt,Vasudevan Nedumpozhimana,Viktoria Ondrejova,Viktoryia Horbik,Vishnu Vardhan Reddy Kummitha,Vuk Dinić,Walelign Tewabe Sewunetie,Winston Wu,Xiaojing Zhao,Yacouba Diarra,Yaniv Nikankin,Yash Mathur,Yixi Chen,Yiyuan Li,Yolanda Xavier,Yonatan Belinkov,Yusuf Ismail Abayomi,Zaid Alyafeai,Zhengyang Shan,Zhi Rui Tam,Zilu Tang,Zuzana Nadova,Baber Abbasi,Stella Biderman,David Stap,Duygu Ataman,Fabian Schmidt,Hila Gonen,Jiayi Wang,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: To date, there exist almost no culturally-specific evaluation benchmarks for
large language models (LLMs) that cover a large number of languages and
cultures. In this paper, we present Global PIQA, a participatory commonsense
reasoning benchmark for over 100 languages, constructed by hand by 335
researchers from 65 countries around the world. The 116 language varieties in
Global PIQA cover five continents, 14 language families, and 23 writing
systems. In the non-parallel split of Global PIQA, over 50% of examples
reference local foods, customs, traditions, or other culturally-specific
elements. We find that state-of-the-art LLMs perform well on Global PIQA in
aggregate, but they exhibit weaker performance in lower-resource languages (up
to a 37% accuracy gap, despite random chance at 50%). Open models generally
perform worse than proprietary models. Global PIQA highlights that in many
languages and cultures, everyday knowledge remains an area for improvement,
alongside more widely-discussed capabilities such as complex reasoning and
expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA
provides a glimpse into the wide diversity of cultures in which human language
is embedded.

</details>


### [25] [RegSpeech12: A Regional Corpus of Bengali Spontaneous Speech Across Dialects](https://arxiv.org/abs/2510.24096)
*Md. Rezuwan Hassan,Azmol Hossain,Kanij Fatema,Rubayet Sabbir Faruque,Tanmoy Shome,Ruwad Naswan,Trina Chakraborty,Md. Foriduzzaman Zihad,Tawsif Tashwar Dipto,Nazia Tasnim,Nazmuddoha Ansary,Md. Mehedi Hasan Shawon,Ahmed Imtiaz Humayun,Md. Golam Rabiul Alam,Farig Sadeque,Asif Sushmit*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Bengali language, spoken extensively across South Asia and among
diasporic communities, exhibits considerable dialectal diversity shaped by
geography, culture, and history. Phonological and pronunciation-based
classifications broadly identify five principal dialect groups: Eastern
Bengali, Manbhumi, Rangpuri, Varendri, and Rarhi. Within Bangladesh, further
distinctions emerge through variation in vocabulary, syntax, and morphology, as
observed in regions such as Chittagong, Sylhet, Rangpur, Rajshahi, Noakhali,
and Barishal. Despite this linguistic richness, systematic research on the
computational processing of Bengali dialects remains limited. This study seeks
to document and analyze the phonetic and morphological properties of these
dialects while exploring the feasibility of building computational models
particularly Automatic Speech Recognition (ASR) systems tailored to regional
varieties. Such efforts hold potential for applications in virtual assistants
and broader language technologies, contributing to both the preservation of
dialectal diversity and the advancement of inclusive digital tools for
Bengali-speaking communities. The dataset created for this study is released
for public use.

</details>


### [26] [Squrve: A Unified and Modular Framework for Complex Real-World Text-to-SQL Tasks](https://arxiv.org/abs/2510.24102)
*Yihan Wang,Peiyu Liu,Runyu Chen,Jiaxing Pu,Wei Xu*

Main category: cs.CL

TL;DR: Squrve是一个统一、模块化的Text-to-SQL框架，整合了学术研究和实际应用，通过通用的执行范式和多主体协作机制，解决了现有方法的部署挑战，并在基准测试中取得了优于单独方法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Text-to-SQL技术发展迅速，但现有方法在实际部署中面临集成工具不足的挑战。Squrve旨在弥合学术研究与实际应用之间的差距，提供一个可扩展的框架。

Method: Squrve首先建立了一个通用的执行范式，标准化了调用接口。然后，提出了一种基于七个抽象原子行为组件的多主体协作机制。实验在广泛采用的基准测试上进行。

Result: 实验表明，Squrve的协作流程在性能上始终优于单独使用的方法，证明了其有效性。

Conclusion: Squrve通过统一的框架和创新的协作机制，为解决复杂的真实世界Text-to-SQL查询开辟了新的有效途径。代码已开源。

Abstract: Text-to-SQL technology has evolved rapidly, with diverse academic methods
achieving impressive results. However, deploying these techniques in real-world
systems remains challenging due to limited integration tools. Despite these
advances, we introduce Squrve, a unified, modular, and extensive Text-to-SQL
framework designed to bring together research advances and real-world
applications. Squrve first establishes a universal execution paradigm that
standardizes invocation interfaces, then proposes a multi-actor collaboration
mechanism based on seven abstracted effective atomic actor components.
Experiments on widely adopted benchmarks demonstrate that the collaborative
workflows consistently outperform the original individual methods, thereby
opening up a new effective avenue for tackling complex real-world queries. The
codes are available at https://github.com/Satissss/Squrve.

</details>


### [27] [Reinforcement Learning for Long-Horizon Multi-Turn Search Agents](https://arxiv.org/abs/2510.24126)
*Vivek Kalyan,Martin Andrews*

Main category: cs.CL

TL;DR: Reinforcement Learning (RL) significantly enhances Large Language Model (LLM) agents for complex tasks, outperforming prompt-based methods and leading models on a legal document search benchmark by achieving 85% accuracy compared to 78%. Longer multi-turn interactions during training and testing further improve agent performance.


<details>
  <summary>Details</summary>
Motivation: Current prompt-based LLM agents, while effective for complex tasks using tools and multi-turn interactions, have limitations. This research aims to explore whether Reinforcement Learning (RL), by enabling agents to learn from experience, can further advance LLM agent capabilities beyond existing prompt-based approaches and achieve superior performance.

Method: The study employs Reinforcement Learning (RL) to train LLM agents. A 14 Billion parameter model is utilized. Experiments are conducted on a legal document search benchmark. The performance is evaluated under varying turn restrictions, both during the training phase and at test-time, to analyze the impact of multi-turn interaction length on the agent's effectiveness.

Result: The RL-trained LLM agent demonstrated superior performance on the legal document search benchmark, achieving 85% accuracy, which surpasses frontier class models that attained 78% accuracy. The experiments also indicated that allowing agents to operate over longer multi-turn horizons, both in training and testing, leads to improved results.

Conclusion: Reinforcement Learning (RL) offers a significant advancement for LLM agents, enabling them to learn from experience and achieve higher performance on complex tasks compared to traditional prompt-based methods. The research highlights the benefit of extended multi-turn interactions for agent effectiveness. Future work could explore more sophisticated RL techniques and their application to a wider range of complex tasks and domains.

Abstract: Large Language Model (LLM) agents can leverage multiple turns and tools to
solve complex tasks, with prompt-based approaches achieving strong performance.
This work demonstrates that Reinforcement Learning (RL) can push capabilities
significantly further by learning from experience. Through experiments on a
legal document search benchmark, we show that our RL-trained 14 Billion
parameter model outperforms frontier class models (85% vs 78% accuracy). In
addition, we explore turn-restricted regimes, during training and at test-time,
that show these agents achieve better results if allowed to operate over longer
multi-turn horizons.

</details>


### [28] [Beyond Line-Level Filtering for the Pretraining Corpora of LLMs](https://arxiv.org/abs/2510.24139)
*Chanwoo Park,Suyoung Park,Yelim Ahn,Jongmin Kim,Jongyeon Park,Jaejin Lee*

Main category: cs.CL

TL;DR: 本文提出模式感知线级去重（PLD）和模式感知尾部标点过滤（PTF）两种新方法，通过考虑行内信号及其在文档中的序列分布，有效解决了传统方法过滤掉宝贵内容的问题，并在中小型语言模型上显著提升了多项选择基准和生成式问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统线级过滤技术（如去重和尾部标点过滤）虽然常用，但可能错误地删除有价值的内容，损害下游任务性能。因此，需要一种更智能的过滤方法来保留结构性内容。

Method: 本文提出两种模式感知方法：模式感知线级去重（PLD）和模式感知尾部标点过滤（PTF）。这两种方法不仅考虑线级信号，还结合了这些信号在文档中的序列分布特征，以更精确地识别和保留有价值的内容，避免误删。

Result: 通过在中小型（10亿参数）的英语和韩语语言模型上进行实验，结果表明，所提出的PLD和PTF方法能够持续提高模型在多项选择基准测试上的性能，并在SQuAD v1和KorQuAD v1数据集上显著提升生成式问答的准确性。

Conclusion: 本文提出的模式感知线级去重（PLD）和模式感知尾部标点过滤（PTF）方法，通过结合线级信号和序列分布信息，有效解决了传统过滤技术的局限性，显著提高了语言模型在多项任务上的性能，尤其是在问答任务上。这表明在数据预处理阶段采用更复杂的模式感知方法对于提升模型效果至关重要。未来的工作可以探索更广泛的模型规模和更多样化的下游任务，以进一步验证这些方法的普适性和鲁棒性。

Abstract: While traditional line-level filtering techniques, such as line-level
deduplication and trailing-punctuation filters, are commonly used, these basic
methods can sometimes discard valuable content, negatively affecting downstream
performance. In this paper, we introduce two methods-pattern-aware line-level
deduplication (PLD) and pattern-aware trailing punctuation filtering (PTF)-by
enhancing the conventional filtering techniques. Our approach not only
considers line-level signals but also takes into account their sequential
distribution across documents, enabling us to retain structurally important
content that might otherwise be removed. We evaluate these proposed methods by
training small language models (1 B parameters) in both English and Korean. The
results demonstrate that our methods consistently improve performance on
multiple-choice benchmarks and significantly enhance generative
question-answering accuracy on both SQuAD v1 and KorQuAD v1.

</details>


### [29] [Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding Korean](https://arxiv.org/abs/2510.24150)
*Chanwoo Park,Suyoung Park,JiA Kang,Jongyeon Park,Sangho Kim,Hyunji M. Park,Sumin Bae,Mingyu Kang,Jaejin Lee*

Main category: cs.CL

TL;DR: 本文提出了Ko-MuSR，一个评估韩语长篇叙事中多步、软推理的基准，并最大限度地减少了数据污染。研究发现，多语言模型在韩语推理任务上优于韩语专用模型，并且精心设计的提示策略能显著提高准确性，接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有的推理基准在评估长篇韩语叙事中的多步、软推理能力方面存在不足，尤其是在数据污染方面。因此，需要一个专门的韩语基准来系统地评估这一能力，并推动韩语自然语言处理（NLP）的发展。

Method: 构建了一个名为Ko-MuSR的基准，该基准包含全韩语叙事、推理链和多项选择题，并由人工注释者验证其逻辑一致性和可回答性。评估了四种大型语言模型（两种多语言，两种韩语专用），并测试了结合少量样本、推理轨迹和任务特定提示的提示策略。

Result: 在Ko-MuSR基准上，多语言模型在韩语推理任务上的表现优于韩语专用模型。精心设计的提示策略（结合少量样本、推理轨迹和任务特定提示）显著提高了模型的准确性，使其接近人类水平。

Conclusion: Ko-MuSR为评估韩语长上下文推理和提示策略提供了一个坚实的基础，有助于推动韩语NLP的发展。研究结果表明，多语言模型在跨语言推理方面具有优势，并且提示工程是提高模型性能的关键。未来的工作可以进一步扩展Ko-MuSR的规模和多样性，并探索更先进的推理和提示技术。

Abstract: We present Ko-MuSR, the first benchmark to comprehensively evaluate
multistep, soft reasoning in long Korean narratives while minimizing data
contamination. Built following MuSR, Ko-MuSR features fully Korean narratives,
reasoning chains, and multiple-choice questions verified by human annotators
for logical consistency and answerability. Evaluations of four large language
models -- two multilingual and two Korean-specialized -- show that multilingual
models outperform Korean-focused ones even in Korean reasoning tasks,
indicating cross-lingual generalization of reasoning ability. Carefully
designed prompting strategies, which combine few-shot examples, reasoning
traces, and task-specific hints, further boost accuracy, approaching
human-level performance. Ko-MuSR offers a solid foundation for advancing Korean
NLP by enabling systematic evaluation of long-context reasoning and prompting
strategies.

</details>


### [30] [MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations](https://arxiv.org/abs/2510.24178)
*Aaron Scott,Maike Züfle,Jan Niehues*

Main category: cs.CL

TL;DR: 该研究提出了MuSaG，一个德语多模态讽刺检测数据集，并评估了九个模型在文本、音频、视觉和多模态方面的表现。研究发现，人类在对话中主要依赖音频，而模型在文本上表现最佳，这表明当前多模态模型在处理真实场景时存在差距。MuSaG数据集的发布旨在促进多模态讽刺检测和人机对齐的研究。


<details>
  <summary>Details</summary>
Motivation: 讽刺是一种常见的修辞手法，其字面意思与实际含义相反，广泛应用于社交媒体和流行文化中，给自然语言理解、情感分析和内容审核带来了挑战。随着多模态大语言模型的发展，讽刺检测已从单一的文本分析扩展到需要整合音频和视觉线索的层面。因此，迫切需要一个能够支持多模态讽刺检测的研究数据集。

Method: 研究者创建了MuSaG数据集，其中包含33分钟从德国电视节目中手动挑选并经过人工标注的语句。每个数据样本都对齐了文本、音频和视频三种模态，并由人类分别进行标注，以便在单模态和多模态设置下进行评估。研究者们还对九个开源和商业模型进行了基准测试，这些模型涵盖了文本、音频、视觉和多模态架构，并将它们的性能与人类标注进行了比较。

Result: 研究结果显示，尽管人类在对话环境中非常依赖音频信息来识别讽刺，但目前的模型在文本模态上取得了最佳的检测效果。这揭示了当前多模态模型在处理包含多种感官线索的真实场景时存在性能上的不足。

Conclusion: MuSaG数据集的创建和发布是该研究的主要贡献，它为多模态讽刺检测和人机对齐的研究提供了宝贵的资源。研究结果强调了现有模型在整合多模态信息以准确识别讽刺方面的局限性，并指出了未来研究的方向，即开发更适合真实世界复杂场景的多模态模型。

Abstract: Sarcasm is a complex form of figurative language in which the intended
meaning contradicts the literal one. Its prevalence in social media and popular
culture poses persistent challenges for natural language understanding,
sentiment analysis, and content moderation. With the emergence of multimodal
large language models, sarcasm detection extends beyond text and requires
integrating cues from audio and vision. We present MuSaG, the first German
multimodal sarcasm detection dataset, consisting of 33 minutes of manually
selected and human-annotated statements from German television shows. Each
instance provides aligned text, audio, and video modalities, annotated
separately by humans, enabling evaluation in unimodal and multimodal settings.
We benchmark nine open-source and commercial models, spanning text, audio,
vision, and multimodal architectures, and compare their performance to human
annotations. Our results show that while humans rely heavily on audio in
conversational settings, models perform best on text. This highlights a gap in
current multimodal models and motivates the use of MuSaG for developing models
better suited to realistic scenarios. We release MuSaG publicly to support
future research on multimodal sarcasm detection and human-model alignment.

</details>


### [31] [Exploring the Influence of Relevant Knowledge for Natural Language Generation Interpretability](https://arxiv.org/abs/2510.24179)
*Iván Martínez-Murillo,Paloma Moreda,Elena Lloret*

Main category: cs.CL

TL;DR: 本研究探讨了在自然语言生成（NLG）任务中整合外部知识的影响，特别关注常识推理生成。研究者提出了KITGI基准，扩展了CommonGen数据集，用于评估外部知识（来自ConceptNet）对句子生成的重要性。实验表明，使用完整的外部知识可以显著提高生成句子的正确性和概念覆盖率（达到91%），而移除关键知识则会导致性能急剧下降至6%。这强调了外部知识对于维持NLG连贯性和概念覆盖率的关键作用，并呼吁设计可解释的、增强知识的NLG系统。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言生成（NLG）系统在生成连贯且符合常识的文本方面仍面临挑战，尤其是在需要整合外部背景知识的任务中。本研究旨在深入理解外部知识，特别是常识性语义关系，如何影响NLG系统的性能，以及如何量化这种影响。了解这一点对于开发更智能、更可靠的NLG系统至关重要，这些系统能够更好地理解和运用世界知识来生成高质量的文本。

Method: 研究者首先扩展了CommonGen数据集，创建了一个名为KITGI的新基准。KITGI将输入概念集与从ConceptNet检索到的语义关系配对，并辅以人工标注的输出。实验采用T5-Large模型，在两种条件下进行比较：一种是使用全部外部知识，另一种是使用经过过滤的知识（故意移除高度相关的语义关系）。研究采用了一个为期三阶段的可解释性评估方法：1.识别并移除关键的外部知识；2.使用剩余知识重新生成句子；3.由人工评估生成句子的常识合理性和概念覆盖度。

Result: 实验结果显示，在完整外部知识的条件下，模型生成的句子在常识合理性和概念覆盖度两项指标上均达到了91%的正确率。然而，当过滤掉关键的外部知识后，性能急剧下降至6%。这一显著的性能差异有力地证明了相关外部知识对于维持NLG输出的质量至关重要。

Conclusion: 本研究明确了外部知识，特别是常识性语义关系，对于提高自然语言生成（NLG）质量（包括连贯性和概念覆盖度）的关键作用。实验结果表明，移除这些知识会严重损害模型的生成能力。这项工作强调了设计可解释的、知识增强的NLG系统的必要性，并呼吁开发超越表面指标、能够评估底层推理能力的评估框架。未来的工作可以集中于开发更有效的知识整合策略以及更全面的评估方法。

Abstract: This paper explores the influence of external knowledge integration in
Natural Language Generation (NLG), focusing on a commonsense generation task.
We extend the CommonGen dataset by creating KITGI, a benchmark that pairs input
concept sets with retrieved semantic relations from ConceptNet and includes
manually annotated outputs. Using the T5-Large model, we compare sentence
generation under two conditions: with full external knowledge and with filtered
knowledge where highly relevant relations were deliberately removed. Our
interpretability benchmark follows a three-stage method: (1) identifying and
removing key knowledge, (2) regenerating sentences, and (3) manually assessing
outputs for commonsense plausibility and concept coverage. Results show that
sentences generated with full knowledge achieved 91\% correctness across both
criteria, while filtering reduced performance drastically to 6\%. These
findings demonstrate that relevant external knowledge is critical for
maintaining both coherence and concept coverage in NLG. This work highlights
the importance of designing interpretable, knowledge-enhanced NLG systems and
calls for evaluation frameworks that capture the underlying reasoning beyond
surface-level metrics.

</details>


### [32] [HACK: Hallucinations Along Certainty and Knowledge Axes](https://arxiv.org/abs/2510.24222)
*Adi Simhi,Jonathan Herzig,Itay Itzhak,Dana Arad,Zorik Gekhman,Roi Reichart,Fazl Barez,Gabriel Stanovsky,Idan Szpektor,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）的幻觉问题阻碍了其可靠应用。现有研究多关注幻觉的外在表现，忽视了其内在机制的差异。本文提出一个基于“知识”和“确定性”双轴的幻觉分类框架，并据此开发了模型特定的数据集。在“知识”轴上，区分了因缺乏知识和尽管拥有知识却产生幻觉两种情况，并通过“引导”干预方法验证了这一分类的有效性，证明了两种幻觉在干预下的显著差异。研究还揭示了不同模型即使拥有共享的参数知识，也会产生不同的幻觉模式。“确定性”轴上，识别出模型在拥有正确知识时却“确信地”产生幻觉的特定子集，并提出新的评估指标，发现现有缓解方法在平均表现良好，但在这些关键案例上表现不佳。研究强调了考虑知识和确定性在幻觉分析中的重要性，并呼吁开发针对幻觉内在因素的定制化缓解策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在生成内容时出现的“幻觉”现象（即生成不准确或虚假信息）是其广泛可靠应用的主要障碍。当前的学术研究大多从外部特征入手对幻觉进行分类，而未能深入探究导致幻觉产生的内部机制。这种外部视角可能导致所提出的缓解策略不够精确，因为不同机制的幻觉需要不同的解决方案。因此，本研究旨在弥合这一差距，提出一种新的幻觉分类框架，以期为开发更有效的幻觉缓解方法提供理论基础和实践指导。

Method: 本研究提出一个创新的幻觉分类框架，该框架沿“知识”（knowledge）和“确定性”（certainty）两个维度对幻觉进行划分。为了实现这一分类，研究人员构建了模型特定的数据集，这是因为参数知识和确定性在不同模型之间可能存在差异。在“知识”维度上，研究区分了两种类型的幻觉：一是由于模型缺乏相关知识而产生的幻觉；二是尽管模型内部拥有正确知识，但仍然产生了错误信息。为了验证“知识”维度的分类有效性，研究采用了“引导”（steering）干预方法。这种方法利用了模型参数知识的存在，通过操控模型激活来影响其输出。实验结果表明，通过引导干预，这两种类型的幻觉在模型行为上表现出显著差异，从而证明了该分类的有效性。此外，研究还分析了不同模型在知识和幻觉模式上的差异，即使在拥有相同参数知识的情况下也是如此。在“确定性”维度上，研究特别关注了一个令人担忧的子集：模型在拥有正确内部知识的情况下，却以高度确定的方式产生幻觉。为了评估缓解方法在这一特定子集上的效果，研究引入了一个新的评估指标。通过该指标的分析，研究发现，尽管一些现有的缓解方法在平均水平上表现尚可，但在这些关键的“确信幻觉”案例上却表现不佳。

Result: 本研究的主要发现包括：1. 提出了一个新颖的幻觉分类框架，区分了“知识”和“确定性”两个维度。2. 在“知识”维度上，成功区分了因缺乏知识和拥有知识却产生幻觉的两种情况，并通过引导干预方法验证了这种区分的有效性，显示出两种幻觉在干预下的显著不同。3. 分析表明，即使模型共享相同的参数知识，它们在幻觉模式上也存在显著差异，揭示了模型层面的异质性。4. 识别出一种特别令人担忧的幻觉子集，即模型在拥有正确知识时却“确信地”产生幻觉。5. 引入的新评估指标揭示了现有缓解方法在处理这种“确信幻觉”时的不足，即平均表现良好但在关键案例上失效。

Conclusion: 本研究强调了在分析大型语言模型（LLM）的幻觉问题时，必须同时考虑“知识”的可用性和“确定性”的水平。现有的幻觉分类方法过于侧重外部表现，而忽略了内部机制的重要性。本文提出的双轴分类框架为理解和解决幻觉问题提供了新的视角。研究结果表明，不同类型的幻觉需要定制化的缓解策略，特别是对于那些模型“确信地”产生幻觉的案例，现有的通用缓解方法效果有限。未来的工作应着重于开发能够精确识别和有效干预这些特定幻觉类型的技术，以提升LLM的可靠性和可用性。本研究的局限性在于模型特定的数据集构建过程可能需要大量标注工作，并且引导干预方法的普适性有待进一步验证。

Abstract: Hallucinations in LLMs present a critical barrier to their reliable usage.
Existing research usually categorizes hallucination by their external
properties rather than by the LLMs' underlying internal properties. This
external focus overlooks that hallucinations may require tailored mitigation
strategies based on their underlying mechanism. We propose a framework for
categorizing hallucinations along two axes: knowledge and certainty. Since
parametric knowledge and certainty may vary across models, our categorization
method involves a model-specific dataset construction process that
differentiates between those types of hallucinations. Along the knowledge axis,
we distinguish between hallucinations caused by a lack of knowledge and those
occurring despite the model having the knowledge of the correct response. To
validate our framework along the knowledge axis, we apply steering mitigation,
which relies on the existence of parametric knowledge to manipulate model
activations. This addresses the lack of existing methods to validate knowledge
categorization by showing a significant difference between the two
hallucination types. We further analyze the distinct knowledge and
hallucination patterns between models, showing that different hallucinations do
occur despite shared parametric knowledge. Turning to the certainty axis, we
identify a particularly concerning subset of hallucinations where models
hallucinate with certainty despite having the correct knowledge internally. We
introduce a new evaluation metric to measure the effectiveness of mitigation
methods on this subset, revealing that while some methods perform well on
average, they fail disproportionately on these critical cases. Our findings
highlight the importance of considering both knowledge and certainty in
hallucination analysis and call for targeted mitigation approaches that
consider the hallucination underlying factors.

</details>


### [33] [Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?](https://arxiv.org/abs/2510.24236)
*Teague McMillan,Gabriele Dominici,Martin Gjoreski,Marc Langheinrich*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) often produce explanations that do not
faithfully reflect the factors driving their predictions. In healthcare
settings, such unfaithfulness is especially problematic: explanations that omit
salient clinical cues or mask spurious shortcuts can undermine clinician trust
and lead to unsafe decision support. We study how inference and training-time
choices shape explanation faithfulness, focusing on factors practitioners can
control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA
8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),
and manipulate the number and type of few-shot examples, prompting strategies,
and training procedure. Our results show: (i) both the quantity and quality of
few-shot examples significantly impact model faithfulness; (ii) faithfulness is
sensitive to prompting design; (iii) the instruction-tuning phase improves
measured faithfulness on MedQA. These findings offer insights into strategies
for enhancing the interpretability and trustworthiness of LLMs in sensitive
domains.

</details>


### [34] [Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations](https://arxiv.org/abs/2510.24247)
*Ahmad Ghannam,Naif Alharthi,Faris Alasmary,Kholood Al Tabash,Shouq Sadah,Lahouari Ghouti*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we tackle the Diacritic Restoration (DR) task for Arabic
dialectal sentences using a multimodal approach that combines both textual and
speech information. We propose a model that represents the text modality using
an encoder extracted from our own pre-trained model named CATT. The speech
component is handled by the encoder module of the OpenAI Whisper base model.
Our solution is designed following two integration strategies. The former
consists of fusing the speech tokens with the input at an early stage, where
the 1500 frames of the audio segment are averaged over 10 consecutive frames,
resulting in 150 speech tokens. To ensure embedding compatibility, these
averaged tokens are processed through a linear projection layer prior to
merging them with the text tokens. Contextual encoding is guaranteed by the
CATT encoder module. The latter strategy relies on cross-attention, where text
and speech embeddings are fused. The cross-attention output is then fed to the
CATT classification head for token-level diacritic prediction. To further
improve model robustness, we randomly deactivate the speech input during
training, allowing the model to perform well with or without speech. Our
experiments show that the proposed approach achieves a word error rate (WER) of
0.25 and a character error rate (CER) of 0.9 on the development set. On the
test set, our model achieved WER and CER scores of 0.55 and 0.13, respectively.

</details>


### [35] [Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations](https://arxiv.org/abs/2510.24250)
*Syed Zohaib Hassan,Pål Halvorsen,Miriam S. Johnson,Pierre Lison*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs), predominantly trained on adult conversational
data, face significant challenges when generating authentic, child-like
dialogue for specialized applications. We present a comparative study
evaluating five different LLMs (GPT-4, RUTER-LLAMA-2-13b, GPTSW, NorMistral-7b,
and NorBloom-7b) to generate age-appropriate Norwegian conversations for
children aged 5 and 9 years. Through a blind evaluation by eleven education
professionals using both real child interview data and LLM-generated text
samples, we assessed authenticity and developmental appropriateness. Our
results show that evaluators achieved strong inter-rater reliability (ICC=0.75)
and demonstrated higher accuracy in age prediction for younger children
(5-year-olds) compared to older children (9-year-olds). While GPT-4 and
NorBloom-7b performed relatively well, most models generated language perceived
as more linguistically advanced than the target age groups. These findings
highlight critical data-related challenges in developing LLM systems for
specialized applications involving children, particularly in low-resource
languages where comprehensive age-appropriate lexical resources are scarce.

</details>


### [36] [Can LLMs Translate Human Instructions into a Reinforcement Learning Agent's Internal Emergent Symbolic Representation?](https://arxiv.org/abs/2510.24259)
*Ziqi Ma,Sao Mai Nguyen,Philippe Xu*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）能否将人类自然语言指令翻译成层次化强化学习（HRL）中出现的内部符号表示，以实现开发学习智能体进行规划和泛化。研究发现在Ant Maze和Ant Fall环境中，LLMs（包括GPT、Claude、Deepseek和Grok）在翻译语言到环境动力学符号表示方面表现出一定能力，但性能高度依赖于分区粒度和任务复杂度。结果表明，当前LLMs在表示对齐方面存在局限性，需要进一步研究以实现语言与智能体内部表示之间的稳健对齐。


<details>
  <summary>Details</summary>
Motivation: 开发学习智能体需要能够规划和泛化任务的涌现式符号表示。本研究旨在探索大型语言模型（LLMs）是否能将人类自然语言指令有效翻译成层次化强化学习（HRL）过程中出现的内部符号表示，以弥合语言理解与智能体内部表征之间的差距。

Method: 研究人员将LLMs（GPT、Claude、Deepseek和Grok）应用于Ant Maze和Ant Fall环境，并使用层次化强化学习算法生成的不同内部符号分区，通过结构化评估框架来衡量LLMs的翻译性能。评估标准包括不同符号分区粒度和任务复杂度下的翻译准确性。

Result: 研究发现，虽然LLMs在将自然语言翻译成环境动力学的符号表示方面显示出一定的潜力，但其性能受到分区粒度和任务复杂度的显著影响。这种敏感性暴露了当前LLMs在表示对齐方面的不足，表明它们在理解和映射复杂符号结构方面仍有改进空间。

Conclusion: 本研究揭示了当前LLMs在将自然语言指令与强化学习智能体的内部符号表示对齐方面存在局限性。尽管LLMs在某些情况下可以进行翻译，但其性能不稳定，易受表示粒度和任务复杂性的影响。这强调了在开发更通用的AI智能体方面，需要进一步探索和改进语言与内部表征之间的稳健对齐机制。未来的研究应关注如何提高LLMs在处理不同粒度和复杂度的符号表示时的鲁棒性。

Abstract: Emergent symbolic representations are critical for enabling developmental
learning agents to plan and generalize across tasks. In this work, we
investigate whether large language models (LLMs) can translate human natural
language instructions into the internal symbolic representations that emerge
during hierarchical reinforcement learning. We apply a structured evaluation
framework to measure the translation performance of commonly seen LLMs -- GPT,
Claude, Deepseek and Grok -- across different internal symbolic partitions
generated by a hierarchical reinforcement learning algorithm in the Ant Maze
and Ant Fall environments. Our findings reveal that although LLMs demonstrate
some ability to translate natural language into a symbolic representation of
the environment dynamics, their performance is highly sensitive to partition
granularity and task complexity. The results expose limitations in current LLMs
capacity for representation alignment, highlighting the need for further
research on robust alignment between language and internal agent
representations.

</details>


### [37] [MERGE: Minimal Expression-Replacement GEneralization Test for Natural Language Inference](https://arxiv.org/abs/2510.24295)
*Mădălina Zgreabăn,Tejaswini Deoskar,Lasha Abzianidze*

Main category: cs.CL

TL;DR: 语言模型在自然语言推理（NLI）方面缺乏鲁棒性，但创建新基准的成本高昂。本文提出了一种名为MERGE的方法，通过替换开放类词语并保留推理来自动生成NLI问题的变体，以评估模型的泛化能力。结果显示，模型在这些微小改动的问题上的表现下降了4-20%，表明其泛化能力不足。研究还分析了替换词的词类、词频和可信度对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明语言模型在自然语言推理（NLI）任务上存在泛化能力不足的问题，但手动创建新的测试基准既耗时又昂贵，而自动生成高质量的NLI变体（即使是修改现有基准）也极其困难。因此，迫切需要一种有效的方法来自动生成能够评估模型鲁棒性的NLI变体。

Method: 本文提出了一种名为MERGE（Minimal Expression-Replacements GEneralization）的方法，用于自动生成NLI问题的变体。该方法的核心在于通过替换问题中的开放类词语（如名词、动词、形容词、副词）来创建新变体，同时确保这些替换不会改变问题的基本推理结构。通过这种方式，MERGE生成了能够严格评估模型在细微变化下推理能力的新测试集。

Result: 实验结果表明，在MERGE生成的变体上，NLI模型的表现比在原始问题上差4-20%。这一结果凸显了即使是经过最小化改动的NLI问题，现有模型也表现出较低的泛化能力。此外，研究还深入分析了替换词的词类、词频以及可信度等因素对模型性能的影响，为理解模型泛化能力的局限性提供了更细致的视角。

Conclusion: 本文提出的MERGE方法能够有效自动生成NLI问题的变体，为评估模型的泛化能力提供了一种新颖且成本效益高的方式。研究发现，现有NLI模型在这些经过微小改动的变体上性能显著下降，表明其泛化能力仍有待提高。未来的工作可以进一步探索MERGE方法的扩展性，并将其应用于其他自然语言处理任务，以更全面地评估模型的鲁棒性。

Abstract: In recent years, many generalization benchmarks have shown language models'
lack of robustness in natural language inference (NLI). However, manually
creating new benchmarks is costly, while automatically generating high-quality
ones, even by modifying existing benchmarks, is extremely difficult. In this
paper, we propose a methodology for automatically generating high-quality
variants of original NLI problems by replacing open-class words, while
crucially preserving their underlying reasoning. We dub our generalization test
as MERGE (Minimal Expression-Replacements GEneralization), which evaluates the
correctness of models' predictions across reasoning-preserving variants of the
original problem. Our results show that NLI models' perform 4-20% worse on
variants, suggesting low generalizability even on such minimally altered
problems. We also analyse how word class of the replacements, word probability,
and plausibility influence NLI models' performance.

</details>


### [38] [Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration in Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2510.24302)
*Shangyu Xing,Siyuan Wang,Chenyuan Yang,Xinyu Dai,Xiang Ren*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在强化学习（RL）和可验证奖励（RLVR）的背景下，特别是使用GRPO等算法，其推理能力得到了显著提升。然而，当前流程的一个关键瓶颈在于，在群体回放过程中，采样轨迹的多样性有限。同质化轨迹及其相关奖励会削弱策略更新的回报信号，从而阻碍有效的策略学习。这种多样性不足主要是由于token级别的随机采样，局部变化很可能收敛到几乎相同的推理路径。为了解决这一限制，我们提出了Lookahead Tree-Based Rollouts（LATR），这是一种新颖的回放策略，旨在通过强制进行分支到可能产生不同延续的候选token来明确地促进轨迹级别的多样性。与随机采样相比，LATR在GRPO和DAPO算法上分别平均将策略学习速度提高了131%，并将最终的pass@1性能提高了4.2%。


<details>
  <summary>Details</summary>
Motivation: 当前用于增强大型语言模型（LLM）推理能力的强化学习（RL）方法，特别是基于可验证奖励（RLVR）的算法（如Group Relative Policy Optimization, GRPO），在实践中受到采样轨迹多样性不足的限制。这种多样性不足源于token级别的随机采样，导致生成的轨迹趋于同质化，从而削弱了策略更新信号，阻碍了模型的有效学习。解决这一问题对于进一步提升LLM的推理能力至关重要。

Method: 提出了一种名为Lookahead Tree-Based Rollouts（LATR）的新型回放策略，旨在通过显式地促进轨迹层面的多样性来克服现有方法的局限性。LATR通过以下三个阶段迭代进行：1.在生成不确定性高的步骤进行分支；2.对每个新分支进行前瞻性模拟；3.修剪在模拟过程中表现出长期相似性的分支。该方法旨在通过探索更多样化的推理路径来丰富策略更新信号。

Result: 与标准的随机采样方法相比，LATR在GRPO和Dynamic sAmpling Policy Optimization (DAPO)算法上均取得了显著的性能提升。实验结果表明，LATR平均将策略学习速度提高了131%，并将最终的pass@1性能提高了4.2%。这些改进在不同的推理任务上得到了验证。

Conclusion: Lookahead Tree-Based Rollouts（LATR）是一种有效的新型回放策略，能够显著提高基于RLVR的LLM的策略学习效率和推理性能。通过强制策略在生成过程中探索更多样化的路径，LATR克服了传统随机采样方法带来的多样性不足问题，从而加速了策略学习并提升了最终的模型表现。该研究为未来在LLM推理能力方面的工作提供了有价值的见解和改进方向。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), particularly with
algorithms like Group Relative Policy Optimization (GRPO), has proven highly
effective in enhancing the reasoning capabilities of large language models.
However, a critical bottleneck in current pipelines lies in the limited
diversity of sampled trajectories during group rollouts. Homogeneous
trajectories and their associated rewards would diminish the return signals for
policy updates, thereby hindering effective policy learning. This lack of
diversity stems primarily from token-level stochastic sampling, where local
variations are likely to collapse into near-identical reasoning paths. To
address this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a
novel rollout strategy designed to explicitly promotes trajectory-level
diversity by enforcing branching into different candidate tokens likely to
yield distinct continuations. Specifically, LATR iteratively operates in three
stages: (1) branching at high-uncertainty generation steps, (2) performing
lookahead simulation for each new branch, and (3) pruning branches that
exhibits prolonged similarity during simulation. Compared with stochastic
Sampling, LATR accelerates policy learning by 131% on average and improves
final pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy
Optimization (DAPO) algorithms across different reasoning tasks. Our code and
data are publicly available at https://github.com/starreeze/latr.

</details>


### [39] [Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning](https://arxiv.org/abs/2510.24320)
*Zhiheng Xi,Jixuan Huang,Xin Guo,Boyang Hong,Dingwen Yang,Xiaoran Fan,Shuo Li,Zehui Chen,Junjie Ye,Siyu Yuan,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Rui Zheng,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: Critique-RL是一种无需更强监督即可开发批评性语言模型的在线强化学习方法。它采用两阶段优化策略，首先用基于规则的奖励信号强化批评者的辨别能力，然后引入基于演员改进的间接奖励来提高批评者的有用性，同时通过适当的正则化来保持其辨别能力。实验表明，Critique-RL 在各种任务和模型上均可带来显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的批评性语言模型训练方法通常依赖于更强的监督者来注释批评数据，这限制了其可扩展性和应用范围。因此，研究一种无需更强监督即可开发批评性语言模型的方法具有重要意义，有助于提升大型语言模型在复杂推理任务中的表现。

Method: Critique-RL 是一种在线强化学习方法，采用两方博弈范式：演员生成响应，批评者提供反馈，演员根据反馈进行优化。该方法包含两个优化阶段：第一阶段，利用基于规则的直接奖励信号来增强批评者的辨别能力（区分响应质量）；第二阶段，引入基于演员改进的间接奖励来提高批评者的有用性（提供建设性反馈），同时通过正则化技术保持批评者的辨别能力。

Result: 通过在 Qwen2.5-7B 模型上进行实验，Critique-RL 在领域内任务上实现了 9.02% 的性能提升，在领域外任务上实现了 5.70% 的性能提升。这表明该方法能够显著提高模型的性能，并且具有良好的泛化能力。

Conclusion: Critique-RL 成功地开发了一种无需更强监督即可训练批评性语言模型的方法，并在多个任务和模型上取得了显著的性能提升。该研究解决了现有方法对强监督的依赖问题，为提升大型语言模型在复杂推理任务中的能力提供了一种有效的途径。未来的工作可以进一步探索该方法的扩展性和鲁棒性。

Abstract: Training critiquing language models to assess and provide feedback on model
outputs is a promising way to improve LLMs for complex reasoning tasks.
However, existing approaches typically rely on stronger supervisors for
annotating critique data. To address this, we propose Critique-RL, an online RL
approach for developing critiquing language models without stronger
supervision. Our approach operates on a two-player paradigm: the actor
generates a response, the critic provides feedback, and the actor refines the
response accordingly. We first reveal that relying solely on indirect reward
signals from the actor's outputs for RL optimization often leads to
unsatisfactory critics: while their helpfulness (i.e., providing constructive
feedback) improves, the discriminability (i.e., determining whether a response
is high-quality or not) remains poor, resulting in marginal performance gains.
To overcome this, Critique-RL adopts a two-stage optimization strategy. In
stage I, it reinforces the discriminability of the critic with direct
rule-based reward signals; in stage II, it introduces indirect rewards based on
actor refinement to improve the critic's helpfulness, while maintaining its
discriminability via appropriate regularization. Extensive experiments across
various tasks and models show that Critique-RL delivers substantial performance
improvements. For example, it achieves a 9.02% gain on in-domain tasks and a
5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.

</details>


### [40] [Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants](https://arxiv.org/abs/2510.24328)
*Hunzalah Hassan Bhatti,Firoj Alam*

Main category: cs.CL

TL;DR: 该研究提出了一种综合方法，用于评估和改进大型语言模型（LLMs）在处理阿拉伯语方言和文化背景问题方面的能力。研究将现代标准阿拉伯语问题翻译成英语和多种阿拉伯语方言，并将其转换为选择题（MCQ）和开放式问题（OEQ）两种形式。通过对不同LLMs进行零样本和微调的基准测试，并引入思维链（CoT）进行逐步推理微调，研究构建了一个跨语言和方言的QA数据集。实验结果表明，LLMs在阿拉伯语方言上的表现不佳，存在文化和方言知识的差距；以阿拉伯语为中心的模型在MCQ上表现较好，但在OEQ上遇到困难；CoT的引入提高了判断的正确性，但对基于N-gram的指标影响不一。该研究发布的数据集是首个此类数据集，旨在促进对文化和语言包容性评估的研究。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在回答日常问题方面表现日益出色，但在处理具有文化背景和方言的内容时，其在不同语言上的表现并不均衡。本研究旨在解决LLMs在阿拉伯语及其多种方言上的这一挑战，特别是在文化和方言特有知识方面的不足。通过评估和改进模型在这些方面的能力，可以促进更具包容性和准确性的语言技术发展，填补当前研究中的空白。

Method: 本研究提出了一种综合方法，包括四个主要步骤：1. 将现代标准阿拉伯语（MSA）选择题（MCQs）翻译成英语和多种阿拉伯语方言。2. 将MCQs转换为开放式问题（OEQs）。3. 对一系列零样本和微调的大型语言模型（LLMs）在MCQ和OEQ设置下进行基准测试。4. 生成思维链（CoT）的推理过程，以微调模型进行逐步推理。基于此方法，研究扩展了一个现有的、跨多种语言变体并行对齐的QA数据集，并进行了广泛的实验，包括开放和封闭模型。

Result: 实验结果显示：（1）LLMs在阿拉伯语方言上的表现普遍较差，这揭示了在文化背景和方言特有知识方面存在的持续性差距。（2）以阿拉伯语为中心的模型在处理选择题（MCQs）时表现良好，但在处理开放式问题（OEQs）时遇到了困难。（3）思维链（CoT）的引入在主观判断的正确性方面有所提高，但在客观的N-gram类指标上，结果好坏参半。

Conclusion: 本研究通过提出一种综合方法，并构建了一个跨语言和方言的QA数据集，深入分析了大型语言模型在阿拉伯语及其方言上的表现。研究发现，模型在处理方言和文化背景内容时存在显著不足，尤其是在开放式问题上。思维链（CoT）的引入在提高问题回答的准确性方面显示出潜力。该研究强调了开发更具文化和语言包容性的LLM评估方法的重要性，并计划公开数据集以支持未来的相关研究。

Abstract: Large Language Models (LLMs) are increasingly used to answer everyday
questions, yet their performance on culturally grounded and dialectal content
remains uneven across languages. We propose a comprehensive method that (i)
translates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into
English and several Arabic dialects, (ii) converts them into open-ended
questions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs
under both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)
rationales to fine-tune models for step-by-step reasoning. Using this method,
we extend an existing dataset in which QAs are parallelly aligned across
multiple language varieties, making it, to our knowledge, the first of its
kind. We conduct extensive experiments with both open and closed models. Our
findings show that (i) models underperform on Arabic dialects, revealing
persistent gaps in culturally grounded and dialect-specific knowledge; (ii)
Arabic-centric models perform well on MCQs but struggle with OEQs; and (iii)
CoT improves judged correctness while yielding mixed n-gram-based metrics. The
developed dataset will be publicly released to support further research on
culturally and linguistically inclusive evaluation.

</details>


### [41] [LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance and Verifiability](https://arxiv.org/abs/2510.24345)
*Zikai Xiao,Fei Huang,Jianhong Tu,Jianhui Wei,Wen Ma,Yuxuan Zhou,Jian Wu,Bowen Yu,Zuozhu Liu,Junyang Lin*

Main category: cs.CL

TL;DR: 长文本生成 LLMs 的挑战：提出 CoV-Eval 评估框架，在真实场景下可验证地评估模型性能，并对 23 个 LLMs 进行了评估，结果显示长文本生成仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 现有长文本生成评估基准要么难以验证，要么过于简化，无法反映真实世界复杂性。LLMs 在生成长而信息丰富且符合约束的文本方面面临挑战。

Method: 提出 CoV-Eval 评估框架，通过定义可验证目标，生成包含真实场景、材料和约束的任务。支持长输入/输出（64K/8K token），包含七种任务。在 23 个 LLMs 上进行评估。

Result: 评估显示，随着真实世界复杂性和输出长度的增加，即使是最先进的模型在长文本生成方面也面临显著挑战。

Conclusion: 长文本生成，尤其是在真实复杂场景下，对 LLMs 来说仍然是一个重大挑战。CoV-Eval 提供了一个可行的评估方法，未来工作可进一步改进模型性能和评估框架。

Abstract: Generating long, informative, and factual outputs remains a major challenge
for Large Language Models (LLMs). Existing benchmarks for long-form generation
typically assess real-world queries with hard-to-verify metrics or use
synthetic setups that ease evaluation but overlook real-world intricacies. In
this paper, we introduce \textbf{LongWeave}, which balances real-world and
verifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval
constructs tasks by first defining verifiable targets within real-world
scenarios, then systematically generating corresponding queries, textual
materials, and constraints based on these targets. This ensures that tasks are
both realistic and objectively assessable, enabling rigorous assessment of
model capabilities in meeting complex real-world constraints. LongWeave
supports customizable input/output lengths (up to 64K/8K tokens) across seven
distinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models
encounter significant challenges in long-form generation as real-world
complexity and output length increase.

</details>


### [42] [Comprehensive and Efficient Distillation for Lightweight Sentiment Analysis Models](https://arxiv.org/abs/2510.24425)
*Guangyu Xie,Yice Zhang,Jianzhu Bao,Qianlong Wang,Yang Sun,Bingbing Wang,Ruifeng Xu*

Main category: cs.CL

TL;DR: COMPEFFDIST是一个全面的、高效的知识蒸馏框架，用于情感分析。它通过自动构建基于属性的指令和基于难度的 D 数据过滤来解决现有方法的局限性，显著提高了数据效率，使小型模型（如 3B 学生模型）能够匹配大型模型（如 20 倍）的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于知识蒸馏的情感分析模型在实用性和性能方面面临两大挑战：1. 人工编写的指令缺乏多样性和数量，难以覆盖蒸馏所需的全部知识。2. 大规模用户文本处理成本高昂，影响了模型的实际应用。这些问题阻碍了轻量级且高效的情感分析模型的进一步发展。

Method: COMPEFFDIST框架包含两个核心模块：1. 基于属性的自动指令构建：自动生成多样化的指令，克服人工指令的局限性。2. 基于难度的 D 数据过滤：筛选出对模型学习最有价值的数据，提高数据效率。该方法在 Llama-3、Qwen-3 和 Gemma-3 等多个模型系列上进行了测试。

Result: 在 Llama-3、Qwen-3 和 Gemma-3 模型系列上，3B 学生模型在大多数任务上达到了 20 倍大教师模型的性能水平。与基线方法相比，COMPEFFDIST在数据效率方面表现优越，仅使用 10% 的数据即可达到相同的性能水平。

Conclusion: COMPEFFDIST框架通过创新的自动指令构建和数据过滤技术，有效解决了知识蒸馏在情感分析中的效率和性能瓶颈，实现了小型模型与大型模型的性能相当，并大幅提升了数据效率。这为开发更轻便、更实用的情感分析模型提供了新的途径。未来的工作可以探索该框架在更多 NLP 任务中的应用以及进一步优化其效率。

Abstract: Recent efforts leverage knowledge distillation techniques to develop
lightweight and practical sentiment analysis models. These methods are grounded
in human-written instructions and large-scale user texts. Despite the promising
results, two key challenges remain: (1) manually written instructions are
limited in diversity and quantity, making them insufficient to ensure
comprehensive coverage of distilled knowledge; (2) large-scale user texts incur
high computational cost, hindering the practicality of these methods. To this
end, we introduce COMPEFFDIST, a comprehensive and efficient distillation
framework for sentiment analysis. Our framework consists of two key modules:
attribute-based automatic instruction construction and difficulty-based data
filtering, which correspondingly tackle the aforementioned challenges. Applying
our method across multiple model series (Llama-3, Qwen-3, and Gemma-3), we
enable 3B student models to match the performance of 20x larger teacher models
on most tasks. In addition, our approach greatly outperforms baseline methods
in data efficiency, attaining the same performance level with only 10% of the
data.

</details>


### [43] [SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models](https://arxiv.org/abs/2510.24427)
*Ken Gu,Advait Bhat,Mike A Merrill,Robert West,Xin Liu,Daniel McDuff,Tim Althoff*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Evaluating the reasoning ability of language models (LMs) is complicated by
their extensive parametric world knowledge, where benchmark performance often
reflects factual recall rather than genuine reasoning. Existing datasets and
approaches (e.g., temporal filtering, paraphrasing, adversarial substitution)
cannot cleanly separate the two. We present SynthWorlds, a framework that
disentangles task reasoning complexity from factual knowledge. In SynthWorlds,
we construct parallel corpora representing two worlds with identical
interconnected structure: a real-mapped world, where models may exploit
parametric knowledge, and a synthetic-mapped world, where such knowledge is
meaningless. On top of these corpora, we design two mirrored tasks as case
studies: multi-hop question answering and page navigation, which maintain equal
reasoning difficulty across worlds. Experiments in parametric-only (e.g.,
closed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings
reveal a persistent knowledge advantage gap, defined as the performance boost
models gain from memorized parametric world knowledge. Knowledge acquisition
and integration mechanisms reduce but do not eliminate this gap, highlighting
opportunities for system improvements. Fully automatic and scalable,
SynthWorlds provides a controlled environment for evaluating LMs in ways that
were previously challenging, enabling precise and testable comparisons of
reasoning and memorization.

</details>


### [44] [LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed Data](https://arxiv.org/abs/2510.24434)
*Julian Valline,Cedric Lothritz,Jordi Cabot*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The effectiveness of instruction-tuned Large Language Models (LLMs) is often
limited in low-resource linguistic settings due to a lack of high-quality
training data. We introduce LuxIT, a novel, monolingual instruction tuning
dataset for Luxembourgish developed to mitigate this challenge. We synthesize
the dataset from a corpus of native Luxembourgish texts, utilizing
DeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following
generation, we apply a quality assurance process, employing an LLM-as-a-judge
approach. To investigate the practical utility of the dataset, we fine-tune
several smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base
models on Luxembourgish language proficiency examinations, however, yields
mixed results, with performance varying significantly across different models.
LuxIT represents a critical contribution to Luxembourgish natural language
processing and offers a replicable monolingual methodology, though our findings
highlight the need for further research to optimize its application.

</details>


### [45] [Fine-tuning Large Language Models with Limited Data: A Survey and Practical Guide](https://arxiv.org/abs/2411.09539)
*Marton Szep,Daniel Rueckert,Rüdiger von Eisenhart-Rothe,Florian Hinterwimmer*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fine-tuning large language models (LLMs) with limited data poses a practical
challenge in low-resource languages, specialized domains, and constrained
deployment settings. While pre-trained LLMs provide strong foundations,
effective adaptation under data scarcity requires focused and efficient
fine-tuning techniques. This paper presents a structured and practical survey
of recent methods for fine-tuning LLMs in data-scarce scenarios. We
systematically review parameter-efficient fine-tuning techniques that lower
training and deployment costs, domain and cross-lingual adaptation methods for
both encoder and decoder models, and model specialization strategies. We
further examine preference alignment approaches that guide model behavior using
limited human or synthetic feedback, emphasizing sample and compute efficiency.
Throughout, we highlight empirical trade-offs, selection criteria, and best
practices for choosing suitable techniques based on task constraints, including
model scaling, data scaling, and the mitigation of catastrophic forgetting. The
aim is to equip researchers and practitioners with actionable insights for
effectively fine-tuning LLMs when data and resources are limited.

</details>


### [46] [SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space](https://arxiv.org/abs/2510.24446)
*Viktoriia Zinkovich,Anton Antonov,Andrei Spiridonov,Denis Shepelev,Andrey Moskalenko,Daria Pugacheva,Elena Tutubalina,Andrey Kuznetsov,Vlad Shakhuro*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
in vision-language tasks such as reasoning segmentation, where models generate
segmentation masks based on textual queries. While prior work has primarily
focused on perturbing image inputs, semantically equivalent textual
paraphrases-crucial in real-world applications where users express the same
intent in varied ways-remain underexplored. To address this gap, we introduce a
novel adversarial paraphrasing task: generating grammatically correct
paraphrases that preserve the original query meaning while degrading
segmentation performance. To evaluate the quality of adversarial paraphrases,
we develop a comprehensive automatic evaluation protocol validated with human
studies. Furthermore, we introduce SPARTA-a black-box, sentence-level
optimization method that operates in the low-dimensional semantic latent space
of a text autoencoder, guided by reinforcement learning. SPARTA achieves
significantly higher success rates, outperforming prior methods by up to 2x on
both the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive
baselines to assess the robustness of advanced reasoning segmentation models.
We reveal that they remain vulnerable to adversarial paraphrasing-even under
strict semantic and grammatical constraints. All code and data will be released
publicly upon acceptance.

</details>


### [47] [Charting the European LLM Benchmarking Landscape: A New Taxonomy and a Set of Best Practices](https://arxiv.org/abs/2510.24450)
*Špela Vintar,Taja Kuzman Pungeršek,Mojca Brglez,Nikola Ljubešić*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While new benchmarks for large language models (LLMs) are being developed
continuously to catch up with the growing capabilities of new models and AI in
general, using and evaluating LLMs in non-English languages remains a
little-charted landscape. We give a concise overview of recent developments in
LLM benchmarking, and then propose a new taxonomy for the categorization of
benchmarks that is tailored to multilingual or non-English use scenarios. We
further propose a set of best practices and quality standards that could lead
to a more coordinated development of benchmarks for European languages. Among
other recommendations, we advocate for a higher language and culture
sensitivity of evaluation methods.

</details>


### [48] [Iterative Critique-Refine Framework for Enhancing LLM Personalization](https://arxiv.org/abs/2510.24469)
*Durga Prasad Maram,Dhruvin Gandhi,Zonghai Yao,Gayathri Akkinapalli,Franck Dernoncourt,Yu Wang,Ryan A. Rossi,Nesreen K. Ahmed*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Personalized text generation requires models not only to produce coherent
text but also to align with a target user's style, tone, and topical focus.
Existing retrieval-augmented approaches such as LaMP and PGraphRAG enrich
profiles with user and neighbor histories, but they stop at generation and
often yield outputs that drift in tone, topic, or style. We present PerFine, a
unified, training-free critique-refine framework that enhances personalization
through iterative, profile-grounded feedback. In each iteration, an LLM
generator produces a draft conditioned on the retrieved profile, and a critic
LLM - also conditioned on the same profile - provides structured feedback on
tone, vocabulary, sentence structure, and topicality. The generator then
revises, while a novel knockout strategy retains the stronger draft across
iterations. We further study additional inference-time strategies such as
Best-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,
Goodreads, and Amazon datasets, PerFine consistently improves personalization
over PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5
refinement iterations, and scalability with increasing critic size. These
results highlight that post-hoc, profile-aware feedback offers a powerful
paradigm for personalized LLM generation that is both training-free and
model-agnostic.

</details>


### [49] [Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems](https://arxiv.org/abs/2510.24476)
*Yihan Li,Xiyuan Fu,Ghanshyam Verma,Paul Buitelaar,Mingming Liu*

Main category: cs.CL

TL;DR: 本文调查了检索增强生成（RAG）和推理增强在大型语言模型（LLM）中减少幻觉的应用，提出了一个区分知识和逻辑幻觉的分类法，并提供了一个统一的框架来分析它们的协同作用，特别是在Agentic系统中。


<details>
  <summary>Details</summary>
Motivation: 幻觉是大型语言模型（LLM）可靠部署的关键障碍，尤其是在实际应用中。尽管检索增强生成（RAG）和推理增强是两种有效的方法，但它们协同作用以及减少幻觉的机制尚未得到系统研究。

Method: 该调查从应用导向的能力增强视角出发，分析了RAG、推理增强及其在Agentic系统中的集成如何减少幻觉。研究提出了一个区分知识型和逻辑型幻觉的分类法，并系统地研究了RAG和推理如何解决这些问题。

Result: 研究结果表明，RAG和推理增强在减少LLM幻觉方面具有协同潜力，尤其是在Agentic系统中。该调查提供了一个统一的框架，并以实际应用、评估和基准测试为支撑。

Conclusion: 该调查系统地考察了RAG和推理增强在减少LLM幻觉方面的作用，特别是在Agentic系统中的应用。通过提出一种新的幻觉分类法和统一的分析框架，该研究为未来开发更可靠、更具创造性的LLM提供了见解和方向。

Abstract: Hallucination remains one of the key obstacles to the reliable deployment of
large language models (LLMs), particularly in real-world applications. Among
various mitigation strategies, Retrieval-Augmented Generation (RAG) and
reasoning enhancement have emerged as two of the most effective and widely
adopted approaches, marking a shift from merely suppressing hallucinations to
balancing creativity and reliability. However, their synergistic potential and
underlying mechanisms for hallucination mitigation have not yet been
systematically examined. This survey adopts an application-oriented perspective
of capability enhancement to analyze how RAG, reasoning enhancement, and their
integration in Agentic Systems mitigate hallucinations. We propose a taxonomy
distinguishing knowledge-based and logic-based hallucinations, systematically
examine how RAG and reasoning address each, and present a unified framework
supported by real-world applications, evaluations, and benchmarks.

</details>


### [50] [Talk2Ref: A Dataset for Reference Prediction from Scientific Talks](https://arxiv.org/abs/2510.24478)
*Frederik Broy,Maike Züfle,Jan Niehues*

Main category: cs.CL

TL;DR: 本文提出了一种名为“Talk2Ref”的新型数据集和相关方法，旨在解决从科学讲座中自动识别相关文献的任务（Reference Prediction from Talks, RPT）。该数据集包含6,279个讲座和43,429篇引用的论文，为研究人员提供了一个大规模的资源来训练和评估模型。实验证明，通过在Talk2Ref数据集上进行微调，可以显著提高基于文本嵌入模型的引用预测性能，特别是在处理长篇讲座文本方面。


<details>
  <summary>Details</summary>
Motivation: 科学讲座是传播研究成果的重要途径，但目前缺乏自动识别与讲座内容相关的文献的工具。这使得研究人员和学生在查找支撑或丰富讲座内容的文献时面临挑战。因此，开发一种能够将非结构化的科学讲座内容映射到相关文献的系统具有重要价值。

Method: 研究人员提出了“Reference Prediction from Talks (RPT)”这一新任务，并创建了首个大规模数据集“Talk2Ref”，其中包含6,279个讲座和43,429篇引用论文。他们首先在零样本检索场景下评估了先进的文本嵌入模型作为基线。随后，他们提出了一种基于双编码器架构并在Talk2Ref数据集上进行训练的方法。此外，还探索了处理长篇讲座文本的策略以及领域自适应的训练方法。

Result: 实验结果表明，在Talk2Ref数据集上进行微调能够显著提升引用预测的性能。这不仅凸显了RPT任务的挑战性，也证明了该数据集在学习口头科学内容语义表示方面的有效性。研究提出的双编码器模型在处理长文本和领域适应方面也表现出良好的潜力。

Conclusion: 本文成功构建了Talk2Ref数据集并提出了相应的基线模型和改进策略，为科学讲座文献引用预测任务奠定了基础。该数据集和模型有望促进未来在口头科学交流与引用推荐系统集成方面的研究。未来的工作可以继续探索更复杂的模型结构和多模态信息，以进一步提升引用预测的准确性和鲁棒性。

Abstract: Scientific talks are a growing medium for disseminating research, and
automatically identifying relevant literature that grounds or enriches a talk
would be highly valuable for researchers and students alike. We introduce
Reference Prediction from Talks (RPT), a new task that maps long, and
unstructured scientific presentations to relevant papers. To support research
on RPT, we present Talk2Ref, the first large-scale dataset of its kind,
containing 6,279 talks and 43,429 cited papers (26 per talk on average), where
relevance is approximated by the papers cited in the talk's corresponding
source publication. We establish strong baselines by evaluating
state-of-the-art text embedding models in zero-shot retrieval scenarios, and
propose a dual-encoder architecture trained on Talk2Ref. We further explore
strategies for handling long transcripts, as well as training for domain
adaptation. Our results show that fine-tuning on Talk2Ref significantly
improves citation prediction performance, demonstrating both the challenges of
the task and the effectiveness of our dataset for learning semantic
representations from spoken scientific content. The dataset and trained models
are released under an open license to foster future research on integrating
spoken scientific communication into citation recommendation systems.

</details>


### [51] [CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?](https://arxiv.org/abs/2510.24505)
*Qing Zong,Jiayu Liu,Tianshi Zheng,Chunyang Li,Baixuan Xu,Haochen Shi,Weiqi Wang,Zhaowei Wang,Chunkit Chan,Yangqiu Song*

Main category: cs.CL

TL;DR: 该研究提出使用自然语言批判来校准大型语言模型（LLM）的置信度，以解决传统方法在生成准确且可信的置信度表达方面的不足。研究探索了两种批判方式：“不确定性”和“置信度”，并提出了两种方法：“自我批判”和“CritiCal”。实验表明，“CritiCal”在复杂推理任务中表现优于其他基线和教师模型GPT-4o，并具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在金融、医疗等高风险领域的安全应用，需要精确的置信度校准，以增强用户信任。然而，传统的置信度表达方法往往无法捕捉到进行准确置信度评估所需的推理过程。此外，获取精确的置信度标签困难且耗时，常常需要多次生成。因此，研究的动机是探索一种更有效的方法来校准LLM的置信度，特别是利用自然语言批判来解决这一挑战。

Method: 该研究提出了利用自然语言批判来增强LLM的置信度校准。研究人员探讨了两种批判方式：1）关注不确定性（问题导向）或2）关注置信度（答案特定）。分析表明，置信度适合多项选择任务，而不确定性在开放式场景中表现更好。在此基础上，研究提出了两种方法：1）自我批判（Self-Critique），让LLM能够批判和优化其置信度，超越单纯的准确性；2）CritiCal，一种新颖的批判校准训练方法，利用自然语言批判来改进置信度校准，而不是直接进行数值优化。实验在多个数据集上进行了评估，并将CritiCal与Self-Critique以及其他竞争性基线进行了比较。

Result: 实验结果表明，CritiCal在置信度校准方面显著优于Self-Critique和其他竞争性基线。特别是在复杂推理任务中，CritiCal的表现甚至超过了其教师模型GPT-4o。此外，CritiCal在不同的、未见过的数据分布（out-of-distribution）设置下也展现出鲁棒的泛化能力，表明其在提高LLM可靠性方面取得了显著进展。

Conclusion: 本研究成功地提出了使用自然语言批判来校准LLM置信度的方法，并开发了名为CritiCal的有效训练框架。研究证明了自然语言批判在提高LLM置信度准确性和可靠性方面的潜力，尤其是在复杂推理和泛化能力方面。CritiCal的优越性能及其超越教师模型的表现，标志着LLM安全应用领域的重要一步。未来的工作可以进一步探索不同类型的批判、更广泛的应用场景以及更精细化的校准技术。

Abstract: Accurate confidence calibration in Large Language Models (LLMs) is critical
for safe use in high-stakes domains, where clear verbalized confidence enhances
user trust. Traditional methods that mimic reference confidence expressions
often fail to capture the reasoning needed for accurate confidence assessment.
We propose natural language critiques as a solution, ideally suited for
confidence calibration, as precise gold confidence labels are hard to obtain
and often require multiple generations. This paper studies how natural language
critiques can enhance verbalized confidence, addressing: (1) What to critique:
uncertainty (question-focused) or confidence (answer-specific)? Analysis shows
confidence suits multiple-choice tasks, while uncertainty excels in open-ended
scenarios. (2) How to critique: self-critique or critique calibration training?
We propose Self-Critique, enabling LLMs to critique and optimize their
confidence beyond mere accuracy, and CritiCal, a novel Critique Calibration
training method that leverages natural language critiques to improve confidence
calibration, moving beyond direct numerical optimization. Experiments show that
CritiCal significantly outperforms Self-Critique and other competitive
baselines, even surpassing its teacher model, GPT-4o, in complex reasoning
tasks. CritiCal also shows robust generalization in out-of-distribution
settings, advancing LLM's reliability.

</details>


### [52] [Levée d'ambiguïtés par grammaires locales](https://arxiv.org/abs/2510.24530)
*Eric G. C. Laporte*

Main category: cs.CL

TL;DR: 该论文提出了一种改进词性标注（POS）的词义消歧方法，该方法适用于INTEX系统，旨在实现零漏报率（即不丢弃任何正确的词性标签）。研究发现，在验证局部消歧语法时，必须考虑变流器路径的交互作用，并且不能孤立地预测组合多个变流器的结果。此外，为了实现零漏报率，必须仔细测试局部语法，因为纯粹的语法直觉可能因意外的构造或歧义而不准确。


<details>
  <summary>Details</summary>
Motivation: 词语的词性（POS）存在歧义，但在特定文本上下文中，这种歧义通常会大大减少。词性消歧是自然语言处理中的一个重要挑战，广泛应用于拼写检查、语法风格检查、表达识别、文本到语音转换和文本语料库分析等领域。因此，词性标注系统是许多自然语言处理系统的关键组成部分。然而，现有的词性标注系统在处理歧义或无法找到唯一正确标签时，可能会产生多个解决方案，但存在丢弃正确标签的风险。

Method: 本文介绍了一种适用于实现零漏报率目标的词义消歧方法，并将其集成到Silberztein的INTEX系统中。研究人员对该方法进行了形式化描述，并强调了在验证局部消歧语法时，需要考虑变流器路径之间的交互作用，而非孤立分析。同样，当组合使用多个变流器时，其结果也无法仅凭单独分析来预测。

Result: 研究发现，在INTEX系统中进行初步标注后，尽管可以自然地产生消歧规则，但语法直觉可能并不总是准确的，这可能是由于意外的文本结构或歧义所致。如果目标是实现零漏报率，那么对局部语法进行细致的测试至关重要，需要详细说明语法在应用于文本时将产生的具体效果。

Conclusion: 该研究提出了一种用于词性消歧的方法，该方法集成在INTEX系统中，并专注于实现零漏报率。研究强调了在验证消歧方法时考虑组件交互作用的重要性，并指出在追求零漏报率时，细致的语法测试和明确的语法行为规范是必不可少的。未来的工作可能需要更深入地研究如何自动生成和验证这些局部语法，以应对复杂的语言现象。

Abstract: Many words are ambiguous in terms of their part of speech (POS). However,
when a word appears in a text, this ambiguity is generally much reduced.
Disambiguating POS involves using context to reduce the number of POS
associated with words, and is one of the main challenges of lexical tagging.
The problem of labeling words by POS frequently arises in natural language
processing, for example for spelling correction, grammar or style checking,
expression recognition, text-to-speech conversion, text corpus analysis, etc.
Lexical tagging systems are thus useful as an initial component of many natural
language processing systems. A number of recent lexical tagging systems produce
multiple solutions when the text is lexically ambiguous or the uniquely correct
solution cannot be found. These contributions aim to guarantee a zero silence
rate: the correct tag(s) for a word must never be discarded. This objective is
unrealistic for systems that tag each word uniquely. This article concerns a
lexical disambiguation method adapted to the objective of a zero silence rate
and implemented in Silberztein's INTEX system (1993). We present here a formal
description of this method. We show that to verify a local disambiguation
grammar in this framework, it is not sufficient to consider the transducer
paths separately: one needs to verify their interactions. Similarly, if a
combination of multiple transducers is used, the result cannot be predicted by
considering them in isolation. Furthermore, when examining the initial labeling
of a text as produced by INTEX, ideas for disambiguation rules come
spontaneously, but grammatical intuitions may turn out to be inaccurate, often
due to an unforeseen construction or ambiguity. If a zero silence rate is
targeted, local grammars must be carefully tested. This is where a detailed
specification of what a grammar will do once applied to texts would be
necessary.

</details>


### [53] [Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written](https://arxiv.org/abs/2510.24538)
*Venkata S Govindarajan,Laura Biester*

Main category: cs.CL

TL;DR: 本文构建并分析了一个包含“糟糕”幽默的英文语料库，发现现有幽默检测模型表现不佳，并揭示了该类幽默结合了常见幽默特征与文学手法。此外，研究还发现大型语言模型在模仿此类幽默时，虽然形式上相似，但会过度使用某些文学手法并生成比人类作者更多的“新颖形容词-名词”二元组。


<details>
  <summary>Details</summary>
Motivation: 文本幽默形式多样，现有计算方法需要更广泛地覆盖，包括刻意营造的“糟糕”幽默。理解这类幽默有助于改进幽默检测模型和幽默生成技术。

Method: 1. 构建了一个包含“糟糕”幽默例句的新型语料库，数据来源于Bulwer-Lytton小说竞赛。
2. 分析了该语料库的特征，并测试了标准幽默检测模型在该语料库上的表现。
3. 识别了构成“糟糕”幽默的文学修辞手法，如双关、讽刺、隐喻、元小说和明喻。
4. 使用大型语言模型（LLMs）模仿生成竞赛风格的句子，并分析其生成内容的特征。

Result: 1. 标准幽默检测模型在本文构建的“糟糕”幽默语料库上表现不佳。
2. “糟糕”幽默的句子融合了常见幽默特征（如双关、讽刺）与文学手法（如隐喻、元小说、明喻）。
3. 大型语言模型在模仿生成竞赛风格句子时，虽然形式上相似，但存在夸大效果的问题，具体表现在过度使用某些文学手法，并生成了比人类作者更多的“新颖形容词-名词”二元组。

Conclusion: 本文对“糟糕”幽默进行了开创性的研究，构建了新的语料库，揭示了其独特的语言特征，并指出了当前幽默检测模型的局限性。研究还表明，大型语言模型在生成此类幽默时存在不足。未来的工作可以进一步探索更复杂的幽默生成模型，以及更细致的幽默特征分析。

Abstract: Textual humor is enormously diverse and computational studies need to account
for this range, including intentionally bad humor. In this paper, we curate and
analyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to
better understand "bad" humor in English. Standard humor detection models
perform poorly on our corpus, and an analysis of literary devices finds that
these sentences combine features common in existing humor datasets (e.g., puns,
irony) with metaphor, metafiction and simile. LLMs prompted to synthesize
contest-style sentences imitate the form but exaggerate the effect by
over-using certain literary devices, and including far more novel
adjective-noun bigrams than human writers. Data, code and analysis are
available at https://github.com/venkatasg/bulwer-lytton

</details>


### [54] [Open Korean Historical Corpus: A Millennia-Scale Diachronic Collection of Public Domain Texts](https://arxiv.org/abs/2510.24541)
*Seyoung Song,Nawon Kim,Songeun Chae,Kiwoong Park,Jiho Jin,Haneul Yoo,Kyunghyun Cho,Alice Oh*

Main category: cs.CL

TL;DR: 鉴于自然语言处理（NLP）领域缺乏历史韩语语料库，本研究发布了一个包含1800万份文档、50亿词元的开放韩语历史语料库（Open Korean Historical Corpus），涵盖了7世纪至2025年的1300年间，并分析了主要语言转变，包括汉字词（Idu）使用量在19世纪60年代达到峰值后下降，汉字向谚文（Hangul）的快速转变（约1890年开始），以及朝鲜语言词汇差异导致现代分词器词汇外（OOV）比率高达51倍。该语料库为定量历时分析提供了基础，并可用于预训练大型语言模型，以增强其对汉韩词汇和古代书写系统的理解。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理（NLP）领域对韩语语言演变的研究不足，主要原因是缺乏可访问的历史语料库，无法进行深入的历时分析。因此，本研究旨在弥补这一空白，通过构建并公开一个大规模的历史韩语语料库，以支持对韩语历史语言学转变的量化研究，并促进NLP模型对韩语历史语言特征的理解。

Method: 本研究构建了一个名为“开放韩语历史语料库”（Open Korean Historical Corpus）的大规模数据集，该语料库包含1800万份文档和50亿词元，覆盖了从7世纪到2025年的1300年间，并支持6种语言以及包括韩式汉字（Idu）和汉字谚文混写等代表性书写系统。研究利用该语料库对韩语历史上的主要语言转变进行了量化分析，包括分析特定书写系统（如Idu）的使用频率变化、书写系统转变（如从汉字到谚文）的时间点和速度，以及现代分词器在处理来自不同地区（如朝鲜）的文本时产生的词汇外（OOV）比率差异。

Result: 通过对开放韩语历史语料库的分析，研究发现：1. 韩式汉字（Idu）的使用量在19世纪60年代达到顶峰，随后急剧下降；2. 从汉字向谚文（Hangul）的书写系统转变是一个大约始于1890年的快速过程；3. 朝鲜的词汇差异导致现代分词器在处理其文本时，词汇外（OOV）比率比处理韩国文本时高出高达51倍。

Conclusion: 本研究成功构建并发布了开放韩语历史语料库，为韩语的定量历时分析提供了重要的基础资源。该语料库不仅揭示了韩语历史上如汉式词（Idu）使用量变化、汉字向谚文转变速度以及朝韩语言词汇差异等关键语言现象，而且为改进大型语言模型对古今韩语及相关词汇的理解提供了潜力。未来的工作可以进一步扩展语料库的覆盖范围和语言多样性，并探索更多基于该语料库的NLP应用，例如开发专门针对历史语言的预训练模型。

Abstract: The history of the Korean language is characterized by a discrepancy between
its spoken and written forms and a pivotal shift from Chinese characters to the
Hangul alphabet. However, this linguistic evolution has remained largely
unexplored in NLP due to a lack of accessible historical corpora. To address
this gap, we introduce the Open Korean Historical Corpus, a large-scale, openly
licensed dataset spanning 1,300 years and 6 languages, as well as
under-represented writing systems like Korean-style Sinitic (Idu) and
Hanja-Hangul mixed script. This corpus contains 18 million documents and 5
billion tokens from 19 sources, ranging from the 7th century to 2025. We
leverage this resource to quantitatively analyze major linguistic shifts: (1)
Idu usage peaked in the 1860s before declining sharply; (2) the transition from
Hanja to Hangul was a rapid transformation starting around 1890; and (3) North
Korea's lexical divergence causes modern tokenizers to produce up to 51 times
higher out-of-vocabulary rates. This work provides a foundational resource for
quantitative diachronic analysis by capturing the history of the Korean
language. Moreover, it can serve as a pre-training corpus for large language
models, potentially improving their understanding of Sino-Korean vocabulary in
modern Hangul as well as archaic writing systems.

</details>


### [55] [BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation](https://arxiv.org/abs/2510.24570)
*Raphaël Bagat,Irina Illina,Emmanuel Vincent*

Main category: cs.CL

TL;DR: BEARD是一个新框架，它使用无标签数据来调整Whisper的编码器，通过结合BEST-RQ目标和知识蒸馏，在低资源和特定领域（如航空交通管制通信）的语音识别任务上取得了显著的性能提升，相对改进达12%。


<details>
  <summary>Details</summary>
Motivation: 现有的自动语音识别（ASR）系统，即使经过大规模多语言训练，在领域外和低资源场景下表现不佳，因为这些场景下的标注数据非常有限。这在航空交通管制（ATC）等特定领域尤为明显，该领域语音识别面临非母语者发音、噪声大和专业术语多等挑战。因此，需要一种有效的方法来适应这些具有挑战性的场景。

Method: BEARD（BEST-RQ Encoder Adaptation with Re-training and Distillation）框架旨在调整Whisper模型的编码器。它不依赖于传统的自监督学习方法，而是创新性地结合了BEST-RQ（一种新的自监督学习目标）和知识蒸馏技术。在知识蒸馏过程中，一个固定的教师编码器用于指导学生编码器的学习，以确保其与预训练的解码器保持兼容性。实验在一个包含约5000小时未转录语音的ATC数据集上进行训练，随后使用2小时的转录语音进行微调。

Result: 实验结果表明，BEARD框架在ATCO2语料库上取得了显著的性能提升。与基线模型和经过微调的模型相比，BEARD实现了12%的相对性能改进。这证明了该框架在处理具有挑战性的ATC通信数据方面的有效性。

Conclusion: BEARD框架首次成功地利用自监督学习目标实现了Whisper模型的领域自适应。该方法通过结合BEST-RQ目标和知识蒸馏，有效地利用了无标签数据，显著提高了在低资源和特定领域（如ATC通信）的语音识别性能。这项工作为解决ASR系统在数据稀疏和领域转移问题上提供了新的思路。未来的工作可以进一步探索该框架在更多领域和语言上的应用。

Abstract: Automatic Speech Recognition (ASR) systems, despite large multilingual
training, struggle in out-of-domain and low-resource scenarios where labeled
data is scarce. We propose BEARD (BEST-RQ Encoder Adaptation with Re-training
and Distillation), a novel framework designed to adapt Whisper's encoder using
unlabeled data. Unlike traditional self-supervised learning methods, BEARD
uniquely combines a BEST-RQ objective with knowledge distillation from a frozen
teacher encoder, ensuring the encoder's complementarity with the pre-trained
decoder. Our experiments focus on the ATCO2 corpus from the challenging Air
Traffic Control (ATC) communications domain, characterized by non-native
speech, noise, and specialized phraseology. Using about 5,000 hours of
untranscribed speech for BEARD and 2 hours of transcribed speech for
fine-tuning, the proposed approach significantly outperforms previous baseline
and fine-tuned model, achieving a relative improvement of 12% compared to the
fine-tuned model. To the best of our knowledge, this is the first work to use a
self-supervised learning objective for domain adaptation of Whisper.

</details>


### [56] [ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?](https://arxiv.org/abs/2510.24591)
*Christine Ye,Sihan Yuan,Suchetha Cooray,Steven Dillmann,Ian L. V. Roque,Dalya Baron,Philipp Frank,Sergio Martin-Alvarez,Nolan Koblischke,Frank J Qu,Diyi Yang,Risa Wechsler,Ioana Ciuca*

Main category: cs.CL

TL;DR: Frontier AI agents are promising for scientific research but require evaluation for faithfulness and correctness. We introduce ReplicationBench, a framework using astrophysics papers to test AI agents' ability to replicate research contributions, including experimental setup, derivations, data analysis, and code. Current frontier models score below 20% on this challenging benchmark. Analysis reveals diverse failure modes, providing insights into agent reliability for data-driven science and a scalable framework for future evaluation.


<details>
  <summary>Details</summary>
Motivation: AI agents show potential as scientific research assistants for open-ended workflows, but their faithfulness and correctness must be rigorously assessed before deployment in novel research. Evaluating these capabilities is crucial for trusting AI in scientific discovery.

Method: ReplicationBench was developed by splitting astrophysics research papers into tasks that require AI agents to replicate the paper's core contributions, such as experimental setup, derivations, data analysis, and codebase. Each task was co-developed with the original paper authors to ensure it targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy).

Result: Current frontier language models perform poorly on ReplicationBench, with the best models scoring under 20%. Analysis of agent performance trajectories, in collaboration with domain experts, identified a diverse range of failure modes encountered by agents when attempting scientific research tasks.

Conclusion: ReplicationBench provides the first benchmark for paper-scale, expert-validated astrophysics research tasks, offering valuable insights into the current limitations of AI agents in scientific research. The framework is generalizable to other data-driven scientific domains and establishes a scalable method for measuring the reliability of AI agents in research settings. Future work should focus on improving agent capabilities to address the identified failure modes and enhance their utility in scientific workflows.

Abstract: Frontier AI agents show increasing promise as scientific research assistants,
and may eventually be useful for extended, open-ended research workflows.
However, in order to use agents for novel research, we must first assess the
underlying faithfulness and correctness of their work. To evaluate agents as
research assistants, we introduce ReplicationBench, an evaluation framework
that tests whether agents can replicate entire research papers drawn from the
astrophysics literature. Astrophysics, where research relies heavily on
archival data and computational study while requiring little real-world
experimentation, is a particularly useful testbed for AI agents in scientific
research. We split each paper into tasks which require agents to replicate the
paper's core contributions, including the experimental setup, derivations, data
analysis, and codebase. Each task is co-developed with the original paper
authors and targets a key scientific result, enabling objective evaluation of
both faithfulness (adherence to original methods) and correctness (technical
accuracy of results). ReplicationBench is extremely challenging for current
frontier language models: even the best-performing language models score under
20%. We analyze ReplicationBench trajectories in collaboration with domain
experts and find a rich, diverse set of failure modes for agents in scientific
research. ReplicationBench establishes the first benchmark of paper-scale,
expert-validated astrophysics research tasks, reveals insights about agent
performance generalizable to other domains of data-driven science, and provides
a scalable framework for measuring AI agents' reliability in scientific
research.

</details>


### [57] [ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization](https://arxiv.org/abs/2510.24592)
*Guoxin Chen,Jing Wu,Xinjie Chen,Wayne Xin Zhao,Ruihua Song,Chengxi Li,Kai Fan,Dayiheng Liu,Minpeng Liao*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在将自然语言数学翻译成机器可验证的正式语句（自动形式化）方面存在语义不一致的问题。本文提出了 ReForm，一种结合了自我反思和迭代优化机制的自动形式化方法，以解决此问题。ReForm 通过评估和修正生成形式语句的语义保真度来工作。为了训练 ReForm，引入了期望有界序列优化（PBSO）来确保模型能够进行准确的形式化和有效的语义验证。此外，还提出了 ConsistencyCheck 基准来评估 LLMs 作为裁判的可靠性，并揭示了即使是人类专家在自动形式化任务中也可能出错。


<details>
  <summary>Details</summary>
Motivation: 现有的自动形式化方法，尽管能够生成语法正确的形式化数学语句，但常常无法保留原始问题的语义意图。这主要是因为它们将自动形式化视为一个简单的翻译任务，缺乏人类专家在解决数学问题时自然会使用的自我反思和迭代改进机制。因此，研究自动形式化方法以提高其语义保真度至关重要，这对于利用形式化推理解决自然语言数学问题至关重要。

Method: 本文提出了 ReForm，一种反思式自动形式化方法。ReForm 的核心在于将语义一致性评估紧密集成到自动形式化过程中，使其能够迭代地生成形式语句，评估其语义保真度，并通过渐进式精炼来纠正错误。为了有效训练这种反思模型，研究者引入了期望有界序列优化（PBSO）方法。PBSO 在序列的不同位置应用不同的奖励，以确保模型不仅能进行准确的自动形式化，还能进行正确的语义验证，从而避免了可能削弱反思目的的表面化批评。此外，为了确保评估的可靠性，还引入了一个名为 ConsistencyCheck 的新基准，该基准包含 859 个由专家标注的项目。

Result: 在四个自动形式化基准上的广泛实验表明，ReForm 方法的平均性能比最强的基线方法提高了 17.2 个百分点。ConsistencyCheck 基准的评估结果不仅验证了 LLMs 作为裁判的有效性，还揭示了自动形式化任务本身的固有难度，表明即使是人类专家在处理的 38.5% 的案例中也会产生语义错误。

Conclusion: ReForm 方法通过引入反思和迭代优化机制，显著提高了自动形式化任务的性能，解决了现有 LLMs 在语义保真度方面的不足。PBSO 训练策略和 ConsistencyCheck 基准的提出，为自动形式化和评估提供了新的视角和工具。研究结果强调了自动形式化任务的挑战性，即使对人类专家也是如此，并为未来开发更强大的形式化推理系统奠定了基础。

Abstract: Autoformalization, which translates natural language mathematics into
machine-verifiable formal statements, is critical for using formal mathematical
reasoning to solve math problems stated in natural language. While Large
Language Models can generate syntactically correct formal statements, they
often fail to preserve the original problem's semantic intent. This limitation
arises from the LLM approaches' treating autoformalization as a simplistic
translation task which lacks mechanisms for self-reflection and iterative
refinement that human experts naturally employ. To address these issues, we
propose ReForm, a Reflective Autoformalization method that tightly integrates
semantic consistency evaluation into the autoformalization process. This
enables the model to iteratively generate formal statements, assess its
semantic fidelity, and self-correct identified errors through progressive
refinement. To effectively train this reflective model, we introduce
Prospective Bounded Sequence Optimization (PBSO), which employs different
rewards at different sequence positions to ensure that the model develops both
accurate autoformalization and correct semantic validations, preventing
superficial critiques that would undermine the purpose of reflection. Extensive
experiments across four autoformalization benchmarks demonstrate that ReForm
achieves an average improvement of 17.2 percentage points over the strongest
baselines. To further ensure evaluation reliability, we introduce
ConsistencyCheck, a benchmark of 859 expert-annotated items that not only
validates LLMs as judges but also reveals that autoformalization is inherently
difficult: even human experts produce semantic errors in up to 38.5% of cases.

</details>


### [58] [Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way](https://arxiv.org/abs/2510.24605)
*Yicun Yang,Cong Wang,Shaobo Wang,Zichen Wen,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出了dLLM-Var，一种能够原生支持可变文本生成长度的扩散大语言模型，解决了现有dLLM固定生成长度的问题，实现了更高的效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的LLM（dLLM）在文本生成方面虽然具有并行潜力，但存在生成长度固定的问题，需要在解码前预设长度，这在效率和灵活性上存在不足，限制了其在实际应用中的推广。

Method: 提出dLLM-Var模型，其核心在于训练模型能够准确预测[EOS]（End-of-Sequence）标记。通过这种方式，dLLM能够在块扩散（block diffusion）的模式下进行原生推理，同时保留全局双向（full）注意力和高并行性的优势。

Result: 在标准基准测试中，dLLM-Var相比传统的dLLM推理方法实现了30.1倍的加速，相比Qwen和Llama等自回归模型也有2.4倍的加速。该方法在准确性和推理速度上均有提升。

Conclusion: dLLM-Var成功解决了扩散大语言模型生成长度固定的关键问题，显著提高了生成效率和灵活性，使其超越了单纯的学术研究价值，为在实际应用中部署dLLM提供了可行性。实验证明了其在准确性和速度上的优势。

Abstract: Diffusion-based large language models (dLLMs) have exhibited substantial
potential for parallel text generation, which may enable more efficient
generation compared to autoregressive models. However, current dLLMs suffer
from fixed generation lengths, which indicates the generation lengths of dLLMs
have to be determined before decoding as a hyper-parameter, leading to issues
in efficiency and flexibility. To solve these problems, in this work, we
propose to train a diffusion LLM with native variable generation lengths,
abbreviated as dLLM-Var. Concretely, we aim to train a model to accurately
predict the [EOS] token in the generated text, which makes a dLLM be able to
natively infer in a block diffusion manner, while still maintaining the ability
of global bi-directional (full) attention and high parallelism. Experiments on
standard benchmarks demonstrate that our method achieves a 30.1x speedup over
traditional dLLM inference paradigms and a 2.4x speedup relative to
autoregressive models such as Qwen and Llama. Our method achieves higher
accuracy and faster inference, elevating dLLMs beyond mere academic novelty and
supporting their practical use in real-world applications. Codes and models
have been released.

</details>


### [59] [Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs](https://arxiv.org/abs/2510.24606)
*Siheng Xiong,Joe Zou,Faramarz Fekri,Yae Jee Cho*

Main category: cs.CL

TL;DR: 该研究提出了一种名为动态分层稀疏注意力（DHSA）的框架，用于解决长上下文大型语言模型（LLMs）中二次注意力成本带来的可扩展性问题。DHSA通过数据驱动的方式动态预测注意力稀疏性，将序列自适应地分段，并进行长度归一化聚合，最终将块级相似度分数上采样到令牌级，以确定保留哪些令牌交互。实验表明，DHSA在Gemma2模型上，与密集注意力相当，同时将预填充延迟降低了20-60%，峰值内存使用量减少了35%，并且在准确性方面优于其他稀疏注意力基线。


<details>
  <summary>Details</summary>
Motivation: 长上下文大型语言模型（LLMs）面临着二次注意力成本带来的可扩展性挑战，尤其是在资源受限的环境下。现有的静态稀疏方法（如滑动窗口、全局令牌）虽然利用了注意力稀疏性，但其静态性导致它们难以适应内容依赖的注意力变化。之前提出的动态方法虽然更灵活，但依赖预定义的模板或启发式机制，这限制了它们的通用性，并可能裁剪掉重要的上下文令牌，从而影响跨任务的准确性。因此，研究的动机是开发一种更通用、更高效且适应性强的长上下文建模方法。

Method: 研究提出了一种名为动态分层稀疏注意力（DHSA）的框架。DHSA首先将输入序列自适应地分割成不同长度的块。然后，它通过聚合每个块内的令牌嵌入来计算块表示，并采用长度归一化聚合，将平均嵌入按块大小的平方根进行缩放，以避免因块长度变化而引入的偏差。最后，DHSA将块级相似度分数上采样到令牌级相似度，以计算重要性分数，并据此确定保留哪些令牌间的交互。该框架无需重新训练即可在线动态预测注意力稀疏性。实验在Gemma2模型上进行，并使用了Needle-in-a-Haystack Test和LongBench基准测试。

Result: 实验结果表明，DHSA在Gemma2模型上达到了与密集注意力相当的准确性。在性能方面，DHSA将预填充延迟降低了20-60%，并将峰值内存使用量减少了35%。与块稀疏注意力等代表性基线相比，DHSA在成本相当或更低的情况下，实现了持续更高的准确性，相对提高了6-18%。

Conclusion: DHSA为资源受限环境下的长上下文LLMs提供了一种高效且适应性强的解决方案。它通过数据驱动的动态稀疏注意力机制，在保持高准确性的同时显著降低了计算成本和内存使用量。未来的工作可以进一步探索DHSA在更多模型和任务上的应用，以及优化其动态稀疏性预测的算法。

Abstract: The quadratic cost of attention hinders the scalability of long-context LLMs,
especially in resource-constrained settings. Existing static sparse methods
such as sliding windows or global tokens utilizes the sparsity of attention to
reduce the cost of attention, but poorly adapts to the content-dependent
variations in attention due to their staticity. While previous work has
proposed several dynamic approaches to improve flexibility, they still depend
on predefined templates or heuristic mechanisms. Such strategies reduce
generality and prune tokens that remain contextually important, limiting their
accuracy across diverse tasks. To tackle these bottlenecks of existing methods
for long-context modeling, we introduce Dynamic Hierarchical Sparse Attention
(DHSA), a data-driven framework that dynamically predicts attention sparsity
online without retraining. Our proposed DHSA adaptively segments sequences into
variable-length chunks, then computes chunk representations by aggregating the
token embeddings within each chunk. To avoid the bias introduced by varying
chunk lengths, we apply length-normalized aggregation that scales the averaged
embeddings by the square root of the chunk size. Finally, DHSA upsamples the
chunk-level similarity scores to token level similarities to calculate
importance scores that determine which token-level interactions should be
preserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and
LongBench show that DHSA matches dense attention in accuracy, while reducing
prefill latency by 20-60% and peak memory usage by 35%. Compared to other
representative baselines such as block sparse attention, DHSA achieves
consistently higher accuracy (6-18% relative gains) with comparable or lower
cost, offering an efficient and adaptable solution for long-context on-device
LLMs.

</details>


### [60] [Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation](https://arxiv.org/abs/2510.24619)
*Snegha A,Sayambhu Sen,Piyush Singh Pasi,Abhishek Singhania,Preethi Jyothi*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在零样本跨语言迁移方面取得了进展，但针对特定任务的调整仍具挑战性。本研究全面评估了三种基于前缀的方法（软提示调优、前缀调优和Llama Adapter）在35+种语言上的零样本跨语言迁移能力，并与LoRA等参数高效微调（PeFT）技术进行了比较。研究发现，在Llama 3.1 8B和Mistral v0.3 7B模型上，基于前缀的方法在Belebele基准测试中表现优于LoRA，即使仅使用少量可学习参数（如前缀调优的1.23M）也能实现性能提升。研究还探讨了跨语言系、跨脚本迁移以及模型规模的影响，表明基于前缀的方法在低资源多语言场景下是LoRA的有效且可扩展的替代方案。


<details>
  <summary>Details</summary>
Motivation: 尽管Llama和Mistral等新型大型语言模型（LLMs）的发布使得零样本跨语言迁移变得更加可行，但针对这些模型在跨语言任务上的调整仍然面临挑战。特别是对于仅解码器的LLMs，虽然LoRA等参数高效微调（PeFT）技术被广泛应用，但软提示调优、前缀调优和Llama Adapter等基于前缀的技术在零样本迁移场景下的研究尚不充分。因此，有必要深入研究这些基于前缀的方法在跨语言迁移任务中的有效性，并与现有主流方法进行比较，特别是在低资源语言环境下。

Method: 本研究对三种基于前缀的方法（软提示调优、前缀调优和Llama Adapter）进行了全面的评估，用于评估LLMs从英语到35种以上高资源和低资源语言的零样本跨语言迁移能力。研究使用了Llama 3.1 8B和Mistral v0.3 7B模型，并在Belebele基准测试以及其他多样化基准上进行了实验。此外，研究还探索了模型在跨不同语言系和文字（scripts）的迁移效果，并分析了模型规模（从1B到24B）对迁移性能的影响。实验中，基于前缀的方法仅使用了少量可学习参数（例如，前缀调优使用1.23M参数），并与LoRA等基线方法进行了性能对比。

Result: 研究结果表明，在Llama 3.1 8B模型上，基于前缀的方法在Belebele基准测试上的性能比LoRA基线高出6%。类似地，在Mistral v0.3 7B模型上也观察到了类似的性能提升。即使在前缀调优中仅使用1.23M的可学习参数，研究也观察到了在多样化基准测试中持续的性能改进。这些结果表明，基于前缀的技术在作为LoRA的有效替代方案方面具有巨大潜力，尤其是在低资源多语言应用场景下。

Conclusion: 本研究全面评估了三种基于前缀的方法在LLMs零样本跨语言迁移任务中的表现，并将其与LoRA等主流参数高效微调技术进行了比较。研究发现，基于前缀的方法，即使在参数量很少的情况下，也能在多种语言和基准测试中实现与甚至优于LoRA的性能。这表明基于前缀的技术是一种在低资源多语言设置下，特别有潜力且可扩展的替代方案，为未来的跨语言LLM应用提供了新的方向。未来的工作可以进一步探索不同前缀配置、与其他PeFT方法的结合，以及在更多样化的跨语言任务上的应用。

Abstract: With the release of new large language models (LLMs) like Llama and Mistral,
zero-shot cross-lingual transfer has become increasingly feasible due to their
multilingual pretraining and strong generalization capabilities. However,
adapting these decoder-only LLMs to new tasks across languages remains
challenging. While parameter-efficient fine-tuning (PeFT) techniques like
Low-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as
soft prompt tuning, prefix tuning, and Llama Adapter are less explored,
especially for zero-shot transfer in decoder-only models. We present a
comprehensive study of three prefix-based methods for zero-shot cross-lingual
transfer from English to 35+ high- and low-resource languages. Our analysis
further explores transfer across linguistic families and scripts, as well as
the impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix
methods outperform LoRA-baselines by up to 6% on the Belebele benchmark.
Similar improvements were observed with Mistral v0.3 7B as well. Despite using
only 1.23M learning parameters with prefix tuning, we achieve consistent
improvements across diverse benchmarks. These findings highlight the potential
of prefix-based techniques as an effective and scalable alternative to LoRA,
particularly in low-resource multilingual settings.

</details>


### [61] [Relative Scaling Laws for LLMs](https://arxiv.org/abs/2510.24626)
*William Held,David Hall,Percy Liang,Diyi Yang*

Main category: cs.CL

TL;DR: 该研究提出了“相对缩放定律”，用于分析语言模型在扩展数据、参数和计算量时，不同测试分布之间性能差距的变化趋势，而非仅关注整体误差。研究发现，虽然模型整体性能随规模提升，但并非对所有子集都公平。例如，学术领域在MMLU上的表现趋于一致，但英语方言的差异会随人口规模变化，AI风险行为的集群则出现分化，能力和影响相关的风险随预训练增加而上升，而对抗性风险则不然。研究发布了所有模型检查点，以支持对相对缩放定律的进一步研究，并更好地应对模型鲁棒性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型缩放定律主要关注在聚合测试集上的平均性能提升，但这种评估方式掩盖了模型在不同子群体或分布上的性能差异。理解这些差异如何随模型规模变化至关重要，尤其是在AI安全和公平性日益受到关注的背景下。本研究旨在探索“相对缩放定律”，即模型在不同测试分布间的性能差距如何随计算量、数据量和参数量等因素的增加而演变，从而更全面地理解模型的扩展行为，并识别潜在的风险和不平等性。

Method: 研究训练了255个仅解码器的Transformer模型，并采用匹配计算预算（IsoFLOP）的方法，将计算量从$10^{18}$到$10^{20}$ FLOPs。模型在标准的预训练数据集上进行训练。通过对比模型在不同测试分布上的性能表现，分析了性能差距随模型规模变化的轨迹，并引入了“相对缩放定律”的概念来量化这种变化。研究关注了学术领域（如MMLU）、地域英语方言以及AI风险行为（如能力、影响和对抗性风险）等多个维度。

Result: 研究发现，语言模型在扩展过程中，不同测试分布间的性能差距呈现出多样化的演变轨迹。具体来说：1. 在MMLU基准测试的学术领域，模型性能差距趋于收敛，表现出一定程度的公平性。2. 在英语方言测试中，模型性能差距随目标人群规模的变化而变化，显示出受数据分布影响的现象。3. 在AI风险行为评估中，模型在能力和影响方面的风险随预训练的增加而上升，但对抗性风险并未表现出类似的增长趋势。总体而言，扩展虽然提升了模型的整体性能，但并未实现普遍的性能均衡。

Conclusion: 本研究提出的“相对缩放定律”为理解语言模型扩展行为提供了新的视角，揭示了模型在不同分布上的性能差距演变规律。研究结果表明，模型规模的增长并非万能的“公平化”手段，在带来整体性能提升的同时，也可能加剧或改变在特定子群体或风险维度上的性能差异。研究发布了模型检查点，旨在鼓励社区进一步研究相对缩放定律，以便更好地识别和解决模型鲁棒性方面的挑战，尤其是在“苦涩教训”（bitter lesson）的指导下，更优先地解决这些问题。

Abstract: Scaling laws describe how language models improve with additional data,
parameters, and compute. While widely used, they are typically measured on
aggregate test sets. Aggregate evaluations yield clean trends but average over
heterogeneous subpopulations, obscuring performance disparities. We introduce
relative scaling laws, which track how performance gaps between test
distributions evolve with scale rather than focusing solely on absolute error.
Using 255 decoder-only Transformers trained under matched-compute (IsoFLOP)
budgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we
find diverse trajectories: academic domains on MMLU converge toward parity;
regional English dialects shift depending on population size; and clusters of
AI risk behaviours split, with capability- and influence-related risks
increasing during pretraining while adversarial risks do not. These results
show that although scaling improves overall performance, it is not a universal
equalizer. To support further study, we release all model checkpoints from this
work to enable practitioners to measure relative alongside traditional scaling
laws, in order to better prioritize robustness challenges in light of the
bitter lesson.

</details>


### [62] ["Mm, Wat?" Detecting Other-initiated Repair Requests in Dialogue](https://arxiv.org/abs/2510.24628)
*Anh Ngo,Nicolas Rollet,Catherine Pelachaud,Chloe Clavel*

Main category: cs.CL

TL;DR: 本研究提出了一种结合语言和韵律特征的多模态模型，用于自动检测荷兰语对话中的修复启动，显著提高了对话系统的鲁棒性，并为未来研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 当前对话系统在识别用户发起的修复（OIR）方面存在不足，容易导致对话中断或用户流失。本研究旨在解决这一问题，提升对话系统的用户体验。

Method: 研究提出了一种多模态模型，该模型整合了基于对话分析的语言特征和韵律特征，并利用预训练的文本和音频嵌入技术来自动检测荷兰语对话中的修复启动。

Result: 实验结果表明，韵律线索能够有效补充语言特征，并显著提升预训练文本和音频嵌入的性能。这揭示了不同特征之间的交互作用。

Conclusion: 本研究成功开发了一种多模态模型，能够有效检测对话中的修复启动，为提高对话系统的智能性和用户体验提供了重要支持。未来的工作将考虑加入视觉线索，并探索多语言和跨语料库的应用，以评估模型的鲁棒性和泛化能力。

Abstract: Maintaining mutual understanding is a key component in human-human
conversation to avoid conversation breakdowns, in which repair, particularly
Other-Initiated Repair (OIR, when one speaker signals trouble and prompts the
other to resolve), plays a vital role. However, Conversational Agents (CAs)
still fail to recognize user repair initiation, leading to breakdowns or
disengagement. This work proposes a multimodal model to automatically detect
repair initiation in Dutch dialogues by integrating linguistic and prosodic
features grounded in Conversation Analysis. The results show that prosodic cues
complement linguistic features and significantly improve the results of
pretrained text and audio embeddings, offering insights into how different
features interact. Future directions include incorporating visual cues,
exploring multilingual and cross-context corpora to assess the robustness and
generalizability.

</details>


### [63] [OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning](https://arxiv.org/abs/2510.24636)
*Ziyou Hu,Zhengliang Shi,Minghang Zhu,Haitao Li,Teng Sun,Pengjie Ren,Suzan Verberne,Zhaochun Ren*

Main category: cs.CL

TL;DR: 现有奖励模型(RM)在处理需要外部知识验证的长篇幅、知识密集型任务时存在局限性。本文提出了OpenRM，一个结合工具检索外部证据的奖励模型，通过联合优化工具使用和结果准确性来提升评估能力。实验证明OpenRM在长篇幅评估任务上显著优于现有方法，并能在下游LLM对齐任务中带来持续收益。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型难以可靠地评估需要外部知识的长篇幅、知识密集型任务的响应质量，这限制了它们在LLM对齐中的应用，特别是在需要外部证据来区分细微质量差异时。

Method: 本文提出OpenRM，一个工具增强的长篇幅奖励模型。它通过调用外部工具来收集证据，并使用群体相对策略优化(GRPO)进行训练，训练数据包含27K+合成的成对样本。训练目标同时监督中间的工具使用和最终结果的准确性。

Result: 在三个新收集的数据集和两个常用基准的实验表明，OpenRM在长篇幅评估任务上显著优于现有的奖励建模方法。将OpenRM集成到推理时响应选择和训练时数据选择中，能在下游LLM对齐任务中带来持续的性能提升。

Conclusion: OpenRM通过引入工具增强机制，有效解决了现有奖励模型在长篇幅、知识密集型任务评估中的不足，证明了工具增强奖励模型在扩展可靠长篇幅评估方面的潜力。

Abstract: Reward models (RMs) have become essential for aligning large language models
(LLMs), serving as scalable proxies for human evaluation in both training and
inference. However, existing RMs struggle on knowledge-intensive and long-form
tasks, where evaluating correctness requires grounding beyond the model's
internal knowledge. This limitation hinders them from reliably discriminating
subtle quality differences, especially when external evidence is necessary. To
address this, we introduce OpenRM, a tool-augmented long-form reward model that
systematically judges open-ended responses by invoking external tools to gather
relevant evidence. We train OpenRM with Group Relative Policy Optimization
(GRPO) on over 27K synthesized pairwise examples generated through a
controllable data synthesis framework. The training objective jointly
supervises intermediate tool usage and final outcome accuracy, incentivizing
our reward model to learn effective evidence-based judgment strategies.
Extensive experiments on three newly-collected datasets and two widely-used
benchmarks demonstrate that OpenRM substantially outperforms existing reward
modeling approaches. As a further step, we integrate OpenRM into both
inference-time response selection and training-time data selection. This yields
consistent gains in downstream LLM alignment tasks, highlighting the potential
of tool-augmented reward models for scaling reliable long-form evaluation.

</details>


### [64] [Quantifying the Effects of Word Length, Frequency, and Predictability on Dyslexia](https://arxiv.org/abs/2510.24647)
*Hugo Rydel-Johnston,Alex Kafkas*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We ask where, and under what conditions, dyslexic reading costs arise in a
large-scale naturalistic reading dataset. Using eye-tracking aligned to
word-level features (word length, frequency, and predictability), we model how
each feature influences dyslexic time costs. We find that all three features
robustly change reading times in both typical and dyslexic readers, and that
dyslexic readers show stronger sensitivities to each, especially
predictability. Counterfactual manipulations of these features substantially
narrow the dyslexic-control gap by about one third, with predictability showing
the strongest effect, followed by length and frequency. These patterns align
with dyslexia theories that posit heightened demands on linguistic working
memory and phonological encoding, and they motivate further work on lexical
complexity and parafoveal preview benefits to explain the remaining gap. In
short, we quantify when extra dyslexic costs arise, how large they are, and
offer actionable guidance for interventions and computational models for
dyslexics.

</details>


### [65] [Optimizing Retrieval for RAG via Reinforced Contrastive Learning](https://arxiv.org/abs/2510.24652)
*Jiawei Zhou,Lei Chen*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As retrieval-augmented generation (RAG) becomes increasingly widespread, the
role of information retrieval (IR) is shifting from retrieving information for
human users to retrieving contextual knowledge for artificial intelligence (AI)
systems, where relevance becomes difficult to define or annotate beforehand. To
address this challenge, we propose R3, a Retrieval framework optimized for RAG
through trialand-feedback Reinforced contrastive learning. Unlike prior
approaches that rely on annotated or synthetic data for supervised fine-tuning,
R3 enables the retriever to dynamically explore and optimize relevance within
the RAG environment. During training, the retrieved results interact with the
environment to produce contrastive signals that automatically guide the
retriever's self-improvement. Extensive experiments across diverse tasks
demonstrate that R3 improves RAG performance by 5.2% over the original
retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving
comparable results to LLM-augmented retrieval and RAG systems built on
post-trained or instruction-tuned LLMs. It is both efficient and practical,
requiring only 4 GPUs and completing training within a single day.

</details>


### [66] [InteractComp: Evaluating Search Agents With Ambiguous Queries](https://arxiv.org/abs/2510.24668)
*Mingyi Deng,Lijun Huang,Yani Fan,Jiayi Zhang,Fashen Ren,Jinyi Bai,Fuzhen Yang,Dayi Miao,Zhaoyang Yu,Yifan Wu,Yanfei Zhang,Fengwei Teng,Yingjia Wan,Song Hu,Yude Li,Xin Jin,Conghao Hu,Haoyu Li,Qirui Fu,Tai Zhong,Xinyu Wang,Xiangru Tang,Nan Tang,Chenglin Wu,Yuyu Luo*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Language agents have demonstrated remarkable potential in web search and
information retrieval. However, these search agents assume user queries are
complete and unambiguous, an assumption that diverges from reality where users
begin with incomplete queries requiring clarification through interaction. Yet
most agents lack interactive mechanisms during the search process, and existing
benchmarks cannot assess this capability. To address this gap, we introduce
InteractComp, a benchmark designed to evaluate whether search agents can
recognize query ambiguity and actively interact to resolve it during search.
Following the principle of easy to verify, interact to disambiguate, we
construct 210 expert-curated questions across 9 domains through a
target-distractor methodology that creates genuine ambiguity resolvable only
through interaction. Evaluation of 17 models reveals striking failure: the best
model achieves only 13.73% accuracy despite 71.50% with complete context,
exposing systematic overconfidence rather than reasoning deficits. Forced
interaction produces dramatic gains, demonstrating latent capability current
strategies fail to engage. Longitudinal analysis shows interaction capabilities
stagnated over 15 months while search performance improved seven-fold,
revealing a critical blind spot. This stagnation, coupled with the immediate
feedback inherent to search tasks, makes InteractComp a valuable resource for
both evaluating and training interaction capabilities in search agents. The
code is available at https://github.com/FoundationAgents/InteractComp.

</details>


### [67] [Evolving Diagnostic Agents in a Virtual Clinical Environment](https://arxiv.org/abs/2510.24654)
*Pengcheng Qiu,Chaoyi Wu,Junwei Liu,Qiaoyu Zheng,Yusheng Liao,Haowen Wang,Yun Yue,Qianrui Fan,Shuai Zhen,Jian Wang,Jinjie Gu,Yanfeng Wang,Ya Zhang,Weidi Xie*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we present a framework for training large language models
(LLMs) as diagnostic agents with reinforcement learning, enabling them to
manage multi-turn diagnostic processes, adaptively select examinations, and
commit to final diagnoses. Unlike instruction-tuned models trained on static
case summaries, our method acquires diagnostic strategies through interactive
exploration and outcome-based feedback. Our contributions are fourfold: (i) We
present DiagGym, a diagnostics world model trained with electronic health
records that emits examination outcomes conditioned on patient history and
recommended examination, serving as a virtual clinical environment for
realistic diagnosis training and evaluation; (ii) We train DiagAgent via
end-to-end, multi-turn reinforcement learning to learn diagnostic policies that
optimize both information yield and diagnostic accuracy; (iii) We introduce
DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated
examination recommendations and 99 cases annotated with 973 physician-written
rubrics on diagnosis process; (iv) we demonstrate superior performance across
diverse diagnostic settings. DiagAgent significantly outperforms 10
state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two
prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%
higher diagnostic accuracy and 44.03% improvement in examination recommendation
hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic
accuracy and 23.09% boost in examination recommendation F1 score. In
rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by
7.1% in weighted rubric score. These findings indicate that learning policies
in interactive clinical environments confers dynamic and clinically meaningful
diagnostic management abilities unattainable through passive training alone.

</details>


### [68] [Dissecting Role Cognition in Medical LLMs via Neuronal Ablation](https://arxiv.org/abs/2510.24677)
*Xun Liang,Huayi Lai,Hanyu Wang,Wentao Zhang,Linfeng Zhang,Yanfang Chen,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) have gained significant traction in medical
decision support systems, particularly in the
  context of medical question answering and role-playing simulations. A common
practice, Prompt-Based Role Playing (PBRP),
  instructs models to adopt different clinical roles (e.g., medical students,
residents, attending physicians) to simulate varied
  professional behaviors. However, the impact of such role prompts on model
reasoning capabilities remains unclear. This
  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to
evaluate whether role prompts induce distinct,
  role-specific cognitive processes in LLMs or merely modify linguistic style.
We test this framework on three medical QA
  datasets, employing neuron ablation and representation analysis techniques to
assess changes in reasoning pathways. Our
  results demonstrate that role prompts do not significantly enhance the
medical reasoning abilities of LLMs. Instead, they
  primarily affect surface-level linguistic features, with no evidence of
distinct reasoning pathways or cognitive differentiation
  across clinical roles. Despite superficial stylistic changes, the core
decision-making mechanisms of LLMs remain uniform
  across roles, indicating that current PBRP methods fail to replicate the
cognitive complexity found in real-world medical
  practice. This highlights the limitations of role-playing in medical AI and
emphasizes the need for models that simulate genuine
  cognitive processes rather than linguistic imitation.We have released the
related code in the following repository:https:
  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor

</details>


### [69] [MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine Translation](https://arxiv.org/abs/2510.24664)
*Parker Riley,Daniel Deutsch,Mara Finkelstein,Colten DiIanni,Juraj Juraska,Markus Freitag*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human evaluation of machine translation is in an arms race with translation
model quality: as our models get better, our evaluation methods need to be
improved to ensure that quality gains are not lost in evaluation noise. To this
end, we experiment with a two-stage version of the current state-of-the-art
translation evaluation paradigm (MQM), which we call MQM re-annotation. In this
setup, an MQM annotator reviews and edits a set of pre-existing MQM
annotations, that may have come from themselves, another human annotator, or an
automatic MQM annotation system. We demonstrate that rater behavior in
re-annotation aligns with our goals, and that re-annotation results in
higher-quality annotations, mostly due to finding errors that were missed
during the first pass.

</details>


### [70] [Repurposing Synthetic Data for Fine-grained Search Agent Supervision](https://arxiv.org/abs/2510.24694)
*Yida Zhao,Kuan Li,Xixi Wu,Liwen Zhang,Dingchu Zhang,Baixuan Li,Maojia Song,Zhuo Chen,Chenxi Wang,Xinyu Wang,Kewei Tu,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLM-based search agents are increasingly trained on entity-centric synthetic
data to solve complex, knowledge-intensive tasks. However, prevailing training
methods like Group Relative Policy Optimization (GRPO) discard this rich entity
information, relying instead on sparse, outcome-based rewards. This critical
limitation renders them unable to distinguish informative "near-miss"
samples-those with substantially correct reasoning but a flawed final
answer-from complete failures, thus discarding valuable learning signals. We
address this by leveraging the very entities discarded during training. Our
empirical analysis reveals a strong positive correlation between the number of
ground-truth entities identified during an agent's reasoning process and final
answer accuracy. Building on this insight, we introduce Entity-aware Group
Relative Policy Optimization (E-GRPO), a novel framework that formulates a
dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect
samples proportional to their entity match rate, enabling the model to
effectively learn from these "near-misses". Experiments on diverse
question-answering (QA) and deep research benchmarks show that E-GRPO
consistently and significantly outperforms the GRPO baseline. Furthermore, our
analysis reveals that E-GRPO not only achieves superior accuracy but also
induces more efficient reasoning policies that require fewer tool calls,
demonstrating a more effective and sample-efficient approach to aligning search
agents.

</details>


### [71] [ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking](https://arxiv.org/abs/2510.24698)
*Baixuan Li,Dingchu Zhang,Jialong Wu,Wenbiao Yin,Zhengwei Tao,Yida Zhao,Liwen Zhang,Haiyang Shen,Runnan Fang,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: 并行思考通过扩展探索广度来增强信息寻求（IS）智能体的解决问题能力。然而，传统并行思考存在重复从头开始推出和整合长时推理轨迹的困难。为了解决这些问题，本文提出了ParallelMuse，一个两阶段范式。第一阶段，功能指定部分推出，通过不确定性引导的路径重用和分支来提高探索效率。第二阶段，压缩推理聚合，利用推理冗余来无损压缩与答案推导相关的信息，并合成连贯的最终答案。该方法在多个开源智能体和基准测试中展示了性能提升，并减少了探索性代币消耗。


<details>
  <summary>Details</summary>
Motivation: 当前的信息寻求（IS）智能体在解决问题时，虽然能够进行深度探索，但缺乏广度的探索能力。并行思考虽然可以扩展探索的广度，但面临效率低下（重复从头开始推出）和难以整合长时推理轨迹（由于有限的上下文容量）的挑战。这些问题限制了IS智能体的整体问题解决能力。

Method: 本文提出了一种名为ParallelMuse的两阶段范式来解决上述挑战。
第一阶段：功能指定部分推出（Functionality-Specified Partial Rollout）：该阶段将生成的序列划分为不同的功能区域，并利用不确定性引导的路径重用和分支策略来提高探索效率。
第二阶段：压缩推理聚合（Compressed Reasoning Aggregation）：该阶段利用推理过程中的冗余信息，对与答案推导相关的信息进行无损压缩，并最终合成一个连贯的答案。

Result: 实验表明，ParallelMuse在多个开源智能体和基准测试中，性能提升最高可达62%，同时探索性代币消耗降低了10%--30%。

Conclusion: ParallelMuse通过其两阶段范式，有效解决了传统并行思考在IS智能体中遇到的效率和长时推理整合问题，显著提升了性能并降低了计算成本。实验结果证明了该方法的有效性。未来的工作可以进一步探索更优化的压缩策略和跨领域应用的潜力。

Abstract: Parallel thinking expands exploration breadth, complementing the deep
exploration of information-seeking (IS) agents to further enhance
problem-solving capability. However, conventional parallel thinking faces two
key challenges in this setting: inefficiency from repeatedly rolling out from
scratch, and difficulty in integrating long-horizon reasoning trajectories
during answer generation, as limited context capacity prevents full
consideration of the reasoning process. To address these issues, we propose
ParallelMuse, a two-stage paradigm designed for deep IS agents. The first
stage, Functionality-Specified Partial Rollout, partitions generated sequences
into functional regions and performs uncertainty-guided path reuse and
branching to enhance exploration efficiency. The second stage, Compressed
Reasoning Aggregation, exploits reasoning redundancy to losslessly compress
information relevant to answer derivation and synthesize a coherent final
answer. Experiments across multiple open-source agents and benchmarks
demonstrate up to 62% performance improvement with a 10--30% reduction in
exploratory token consumption.

</details>


### [72] [AgentFold: Long-Horizon Web Agents with Proactive Context Management](https://arxiv.org/abs/2510.24699)
*Rui Ye,Zhongwang Zhang,Kuan Li,Huifeng Yin,Zhengwei Tao,Yida Zhao,Liangcai Su,Liwen Zhang,Zile Qiao,Xinyu Wang,Pengjun Xie,Fei Huang,Siheng Chen,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: LLM驱动的网页智能体在长程任务中面临上下文管理瓶颈，容易因历史记录过多而饱和或因过度总结丢失细节。本文提出了AgentFold，一种受人类回顾性整合启发的智能体范式，通过学习执行“折叠”操作来主动管理多尺度的上下文历史，实现细粒度压缩或多步子任务抽象。在BrowseComp和BrowseComp-ZH基准测试中，AgentFold-30B-A3B在无需持续预训练或强化学习的情况下，取得了36.2%和47.3%的性能，超越了更大规模的开源模型和领先的闭源模型。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的网页智能体虽然在信息检索方面展现出巨大潜力，但在处理长程任务时，上下文管理方面的根本性权衡限制了其效率。基于ReAct的智能体容易因积累了过多的噪声原始历史记录而导致上下文饱和；而固定在每一步总结整个历史记录的方法则可能不可逆地丢失关键细节。

Method: 本文引入了AgentFold，一种新颖的智能体范式，其核心在于主动的上下文管理，并借鉴了人类回顾性整合的认知过程。AgentFold将上下文视为一个动态的认知工作空间，而非被动填充的日志。在每一步，它学习执行一种“折叠”操作，该操作能够管理其在多个尺度上的历史轨迹：它可以进行细粒度的压缩以保留重要的细微细节，或者进行深度整合以抽象整个多步子任务。

Result: 在著名的基准测试上取得的成果非常显著：通过简单的监督微调（无需持续预训练或强化学习），AgentFold-30B-A3B智能体在BrowseComp上达到了36.2%的性能，在BrowseComp-ZH上达到了47.3%。值得注意的是，这一性能不仅超越或媲美了规模大得多的开源模型（如DeepSeek-V3.1-671B-A37B），而且还超越了领先的闭源模型（如OpenAI的o4-mini）。

Conclusion: AgentFold通过引入受人类认知启发的“折叠”操作，有效解决了LLM网页智能体在长程任务中的上下文管理难题，实现了在不牺牲关键信息的前提下高效地处理和压缩历史记录。实验结果表明，AgentFold在性能上取得了突破性进展，尤其是在与现有模型进行比较时，其展现出的高效性和优越性不容忽视。尽管取得了显著成果，但仍有进一步探索的空间，例如在更广泛的任务和更复杂的场景下验证其鲁棒性，以及研究更先进的“折叠”策略和多模态信息整合能力。

Abstract: LLM-based web agents show immense promise for information seeking, yet their
effectiveness on long-horizon tasks is hindered by a fundamental trade-off in
context management. Prevailing ReAct-based agents suffer from context
saturation as they accumulate noisy, raw histories, while methods that fixedly
summarize the full history at each step risk the irreversible loss of critical
details. Addressing these, we introduce AgentFold, a novel agent paradigm
centered on proactive context management, inspired by the human cognitive
process of retrospective consolidation. AgentFold treats its context as a
dynamic cognitive workspace to be actively sculpted, rather than a passive log
to be filled. At each step, it learns to execute a `folding' operation, which
manages its historical trajectory at multiple scales: it can perform granular
condensations to preserve vital, fine-grained details, or deep consolidations
to abstract away entire multi-step sub-tasks. The results on prominent
benchmarks are striking: with simple supervised fine-tuning (without continual
pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp
and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or
matches open-source models of a dramatically larger scale, such as the
DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like
OpenAI's o4-mini.

</details>


### [73] [SPICE: Self-Play In Corpus Environments Improves Reasoning](https://arxiv.org/abs/2510.24684)
*Bo Liu,Chuanyang Jin,Seungone Kim,Weizhe Yuan,Wenting Zhao,Ilia Kulikov,Xian Li,Sainbayar Sukhbaatar,Jack Lanchantin,Jason Weston*

Main category: cs.CL

TL;DR: SPICE是一个包含“挑战者”和“推理器”两种角色的强化学习框架，其中“挑战者”负责从文档语料库中生成多样化的推理任务，“推理器”负责解决这些任务。通过对抗性训练，SPICE能够动态生成符合“推理器”能力边界的任务，并利用语料库的丰富信号实现持续改进，在数学和通用推理基准测试中均取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的自改进系统需要与环境进行交互以实现持续适应。然而，现有的自对抗方法缺乏与外部世界的联系，其改进效果有限。本研究旨在解决这一问题，探索一种能够利用外部信号进行持续自我改进的方法，并证明其有效性。

Method: SPICE（Self-Play In Corpus Environments）框架包含两个角色：挑战者和推理器。挑战者从大型文档语料库中挖掘信息，生成多样化的推理任务；推理器则负责解决这些任务。通过对抗性训练，挑战者能够生成处于推理器能力边界上的任务，从而形成一个自动课程。同时，语料库提供了丰富的外部信号，支持推理器的持续改进。

Result: SPICE在数学推理和通用推理的基准测试中均取得了显著的性能提升，分别达到了+8.9%和+9.8%。这些改进在多个模型家族中得到了验证。实验分析表明，文档基础是SPICE能够持续生成更具挑战性的目标并实现它们，从而实现持续自我改进的关键因素。

Conclusion: SPICE框架通过将自对抗与文档语料库相结合，有效地解决了现有自改进系统在持续适应和改进方面的局限性。该方法能够动态生成具有挑战性的目标，并利用丰富的外部信号实现持续的性能提升。研究结果证明了文档基础在驱动自主学习和持续改进中的重要作用，为开发更强大的自改进系统提供了新的方向。未来的工作可以进一步探索SPICE在更广泛领域的应用以及优化其训练过程。

Abstract: Self-improving systems require environmental interaction for continuous
adaptation. We introduce SPICE (Self-Play In Corpus Environments), a
reinforcement learning framework where a single model acts in two roles: a
Challenger that mines documents from a large corpus to generate diverse
reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,
the Challenger creates an automatic curriculum at the frontier of the
Reasoner's capability, while corpus grounding provides the rich,
near-inexhaustible external signal necessary for sustained improvement. Unlike
existing ungrounded self-play methods that offer more limited benefits, SPICE
achieves consistent gains across mathematical (+8.9%) and general reasoning
(+9.8%) benchmarks on multiple model families. Our analysis reveals how
document grounding is a key ingredient in SPICE to continuously generate its
own increasingly challenging goals and achieve them, enabling sustained
self-improvement.

</details>


### [74] [AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis](https://arxiv.org/abs/2510.24695)
*Xuanzhong Chen,Zile Qiao,Guoxin Chen,Liangcai Su,Zhen Zhang,Xinyu Wang,Pengjun Xie,Fei Huang,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: 通过引入“近邻发展区”（ZPD）理论，我们开发了“AgentFrontier Engine”来合成LLM能力边界上的数据，用于模型的持续预训练和推理任务后训练。基于此引擎训练的AgentFrontier-30B-A3B模型在Humanity's Last Exam等基准测试中取得了最先进的成果，证明了ZPD指导的数据合成是提升LLM能力的有效途径。


<details>
  <summary>Details</summary>
Motivation: 当前训练大型语言模型（LLM）智能体以应对前沿任务是提升其高级推理能力的关键。然而，如何精确定义和触及LLM能力的最前沿，并为其提供有效的训练数据，是一个核心挑战。

Method: 本文提出了一种受“近邻发展区”（ZPD）教育理论启发的“AgentFrontier Engine”数据合成方法。该引擎能够自动生成高质量、跨学科的数据，这些数据精确地位于LLM能力的最前沿，即LLM单独无法解决但通过指导可以掌握的任务。该引擎支持两种训练范式：1）使用知识密集型数据进行持续预训练；2）针对复杂推理任务进行定向的后训练。此外，研究还从同一框架中衍生出“ZPD Exam”，这是一个动态的、自动化的基准测试，用于评估智能体在这些前沿任务上的能力。

Result: 使用AgentFrontier Engine合成的数据训练的AgentFrontier-30B-A3B模型，在一些高要求的基准测试（如Humanity's Last Exam）上取得了最先进（state-of-the-art）的成果，甚至在某些方面超越了一些领先的商业模型。

Conclusion: 研究证明，采用ZPD理论指导的数据合成方法，为构建更强大的LLM智能体提供了一条可扩展且有效的前进路径。这种方法能够精确地定位LLM的能力边界，并生成针对性的训练数据，从而显著提升模型的推理和解决复杂问题的能力。未来的工作可以进一步探索ZPD在不同类型任务和模型上的应用，以及优化数据合成的效率和多样性。

Abstract: Training large language model agents on tasks at the frontier of their
capabilities is key to unlocking advanced reasoning. We introduce a data
synthesis approach inspired by the educational theory of the Zone of Proximal
Development (ZPD), which defines this frontier as tasks an LLM cannot solve
alone but can master with guidance. To operationalize this, we present the
AgentFrontier Engine, an automated pipeline that synthesizes high-quality,
multidisciplinary data situated precisely within the LLM's ZPD. This engine
supports both continued pre-training with knowledge-intensive data and targeted
post-training on complex reasoning tasks. From the same framework, we derive
the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent
capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on
our synthesized data, which achieves state-of-the-art results on demanding
benchmarks like Humanity's Last Exam, even surpassing some leading proprietary
agents. Our work demonstrates that a ZPD-guided approach to data synthesis
offers a scalable and effective path toward building more capable LLM agents.

</details>


### [75] [ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?](https://arxiv.org/abs/2510.24706)
*Shuqing Li,Jiayi Yan,Chenyu Niu,Jen-tse Huang,Yun Peng,Wenxuan Wang,Yepang Liu,Michael R. Lyu*

Main category: cs.CL

TL;DR: 该研究提出了一个名为ComboBench的基准测试，用于评估大型语言模型（LLM）在虚拟现实（VR）游戏中将语义动作转化为设备操控序列的能力。研究评估了七种主流LLM，并与人类表现进行了比较。结果表明，尽管顶尖LLM（如Gemini-1.5-Pro）在任务分解方面表现出潜力，但在程序推理和空间理解方面仍不及人类。游戏间的表现差异突显了交互复杂性对LLM能力的影响。少量示例（few-shot examples）能显著提升LLM的表现，预示了通过针对性训练来增强LLM在VR操控能力的可能性。


<details>
  <summary>Details</summary>
Motivation: 当前，虚拟现实（VR）游戏需要玩家将高级的语义动作转化为精确的设备操控（如控制器和头戴式显示器）。虽然人类能够凭借常识和具身理解直观地完成这种转换，但大型语言模型（LLM）是否能有效复制这种能力仍是未被充分探索的领域。本研究旨在弥合这一认知差距，探究LLM在理解和执行VR交互方面的潜力。

Method: 研究引入了一个名为ComboBench的新基准测试，该测试包含来自四款热门VR游戏（Half-Life: Alyx, Into the Radius, Moss: Book II, and Vivecraft）的262个不同场景。ComboBench用于评估LLM将语义动作转化为VR设备操控序列的能力。研究人员对七种不同的LLM进行了评估，包括GPT-3.5, GPT-4, GPT-4o, Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, 和 GLM-4-Flash。模型表现与经过标注的真实数据（ground truth）以及人类的实际操作表现进行了对比分析。

Result: 评估结果显示，在所测试的LLM中，Gemini-1.5-Pro等表现最佳的模型在任务分解能力上展现出较强的潜力。然而，与人类玩家相比，这些模型在程序推理和空间理解方面仍存在显著差距。不同VR游戏的表现差异很大，表明LLM的能力对交互的复杂性非常敏感。此外，研究发现，通过提供少量示例（few-shot examples）进行训练，能够显著提升LLM在VR操控任务上的表现，这为后续的性能优化提供了方向。

Conclusion: 本研究通过ComboBench基准测试，首次系统性地评估了LLM在VR游戏操控方面的能力。研究发现，尽管先进的LLM在某些方面（如任务分解）已具备一定潜力，但它们在模仿人类在VR环境中进行直观、连贯的操作方面仍显不足，尤其是在程序推理和空间理解能力上。游戏交互复杂性对LLM的表现有显著影响。少量示例学习是提升LLM在VR操控任务上表现的有效途径。未来的工作可以集中于开发更复杂的基准测试，以及探索能够提升LLM在VR场景下程序推理和空间理解能力的训练方法，最终目标是使LLM能够更自然、更有效地与VR环境进行交互。本研究成果已在 https://sites.google.com/view/combobench 公开。

Abstract: Virtual Reality (VR) games require players to translate high-level semantic
actions into precise device manipulations using controllers and head-mounted
displays (HMDs). While humans intuitively perform this translation based on
common sense and embodied understanding, whether Large Language Models (LLMs)
can effectively replicate this ability remains underexplored. This paper
introduces a benchmark, ComboBench, evaluating LLMs' capability to translate
semantic actions into VR device manipulation sequences across 262 scenarios
from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,
and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,
Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against
annotated ground truth and human performance. Our results reveal that while
top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition
capabilities, they still struggle with procedural reasoning and spatial
understanding compared to humans. Performance varies significantly across
games, suggesting sensitivity to interaction complexity. Few-shot examples
substantially improve performance, indicating potential for targeted
enhancement of LLMs' VR manipulation capabilities. We release all materials at
https://sites.google.com/view/combobench.

</details>


### [76] [MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25 Evaluation Shared Task](https://arxiv.org/abs/2510.24707)
*Juraj Juraska,Tobias Domhan,Mara Finkelstein,Tetsuji Nakagawa,Geza Kovacs,Daniel Deutsch,Pidong Wang,Markus Freitag*

Main category: cs.CL

TL;DR: 本文介绍了作者为WMT25翻译评估共享任务提交的两个系统：MetricX-25和GemSpanEval。MetricX-25是为质量分数预测子任务设计的，它改进了输入格式和训练协议，并基于Gemma 3模型，在MQM和ESA质量分数预测方面表现优于前代。GemSpanEval是为错误跨度检测子任务设计的，同样基于Gemma 3，能够预测错误跨度及其严重程度和类别，并且通过将其视为生成任务来确保错误跨度的明确性，在错误跨度检测方面与xCOMET相当。


<details>
  <summary>Details</summary>
Motivation: 当前的机器翻译评估方法在准确性和效率方面仍有不足，尤其是在预测翻译质量分数和精确定位错误方面。现有的指标可能无法完全捕捉人类对翻译质量的细微判断，而错误定位则更加困难。因此，需要更先进的评估方法来提高机器翻译质量评估的准确性和细粒度。

Method: 1. **质量分数预测 (MetricX-25):** 使用Gemma 3模型，采用编码器-only架构，并在其顶部添加回归头。通过改进输入格式和训练协议，在公开可用的WMT数据上进行微调，以预测MQM和ESA质量分数。
2. **错误跨度检测 (GemSpanEval):** 使用Gemma 3模型，采用解码器-only架构。将错误跨度检测视为一个生成任务，模型被指示输出每个预测错误跨度的上下文，以确保其明确性。模型同时预测错误跨度的严重程度和类别。

Result: 1. **MetricX-25:** 在MQM和ESA质量分数预测任务上，MetricX-25显著优于其前代产品，表明改进的输入格式和训练协议有效提高了预测性能。
2. **GemSpanEval:** 在错误跨度检测任务上，GemSpanEval表现出与强大的编码器-only基线模型xCOMET相当的竞争力。通过生成任务的 formulation，GemSpanEval能够明确地识别错误跨度。

Conclusion: 本文提出的MetricX-25和GemSpanEval系统在WMT25翻译评估共享任务中取得了有竞争力的结果。MetricX-25在质量分数预测方面取得了显著进步，而GemSpanEval通过创新的生成任务方法在错误跨度检测方面展现了潜力。这些基于先进的Gemma 3多语言模型的系统，为机器翻译质量评估提供了新的解决方案，尤其是在细粒度错误分析方面。未来的工作可以进一步探索模型的泛化能力和在更多语言对上的应用。

Abstract: In this paper, we present our submissions to the unified WMT25 Translation
Evaluation Shared Task. For the Quality Score Prediction subtask, we create a
new generation of MetricX with improvements in the input format and the
training protocol, while for the Error Span Detection subtask we develop a new
model, GemSpanEval, trained to predict error spans along with their severities
and categories. Both systems are based on the state-of-the-art multilingual
open-weights model Gemma 3, fine-tuned on publicly available WMT data. We
demonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture
with a regression head on top, can be trained to effectively predict both MQM
and ESA quality scores, and significantly outperforms its predecessor. Our
decoder-only GemSpanEval model, on the other hand, we show to be competitive in
error span detection with xCOMET, a strong encoder-only sequence-tagging
baseline. With error span detection formulated as a generative task, we
instruct the model to also output the context for each predicted error span,
thus ensuring that error spans are identified unambiguously.

</details>


### [77] [Retrieval-Augmented Generation-based Relation Extraction](https://arxiv.org/abs/2404.13397)
*Sefika Efeoglu,Adrian Paschke*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Information Extraction (IE) is a transformative process that converts
unstructured text data into a structured format by employing entity and
relation extraction (RE) methodologies. The identification of the relation
between a pair of entities plays a crucial role within this framework. Despite
the existence of various techniques for relation extraction, their efficacy
heavily relies on access to labeled data and substantial computational
resources. In addressing these challenges, Large Language Models (LLMs) emerge
as promising solutions; however, they might return hallucinating responses due
to their own training data. To overcome these limitations, Retrieved-Augmented
Generation-based Relation Extraction (RAG4RE) in this work is proposed,
offering a pathway to enhance the performance of relation extraction tasks.
  This work evaluated the effectiveness of our RAG4RE approach utilizing
different LLMs. Through the utilization of established benchmarks, such as
TACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to
comprehensively evaluate the efficacy of our RAG4RE approach. In particularly,
we leverage prominent LLMs including Flan T5, Llama2, and Mistral in our
investigation. The results of our study demonstrate that our RAG4RE approach
surpasses performance of traditional RE approaches based solely on LLMs,
particularly evident in the TACRED dataset and its variations. Furthermore, our
approach exhibits remarkable performance compared to previous RE methodologies
across both TACRED and TACREV datasets, underscoring its efficacy and potential
for advancing RE tasks in natural language processing.

</details>


### [78] [Evaluation of Geographical Distortions in Language Models](https://arxiv.org/abs/2404.17401)
*Rémy Decoupes,Roberto Interdonato,Mathieu Roche,Maguelonne Teisseire,Sarah Valentin*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Language models now constitute essential tools for improving efficiency for
many professional tasks such as writing, coding, or learning. For this reason,
it is imperative to identify inherent biases. In the field of Natural Language
Processing, five sources of bias are well-identified: data, annotation,
representation, models, and research design. This study focuses on biases
related to geographical knowledge. We explore the connection between geography
and language models by highlighting their tendency to misrepresent spatial
information, thus leading to distortions in the representation of geographical
distances. This study introduces four indicators to assess these distortions,
by comparing geographical and semantic distances. Experiments are conducted
from these four indicators with ten widely used language models. Results
underscore the critical necessity of inspecting and rectifying spatial biases
in language models to ensure accurate and equitable representations.

</details>


### [79] [Zero-Shot Tokenizer Transfer](https://arxiv.org/abs/2405.07883)
*Benjamin Minixhofer,Edoardo Maria Ponti,Ivan Vulić*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Language models (LMs) are bound to their tokenizer, which maps raw text to a
sequence of vocabulary items (tokens). This restricts their flexibility: for
example, LMs trained primarily on English may still perform well in other
natural and programming languages, but have vastly decreased efficiency due to
their English-centric tokenizer. To mitigate this, we should be able to swap
the original LM tokenizer with an arbitrary one, on the fly, without degrading
performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer
Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for
the tokens in the vocabulary of the new tokenizer. Since prior heuristics for
initializing embeddings often perform at chance level in a ZeTT setting, we
propose a new solution: we train a hypernetwork taking a tokenizer as input and
predicting the corresponding embeddings. We empirically demonstrate that the
hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and
decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models'
performance in cross-lingual and coding tasks while markedly reducing the
length of the tokenized sequence. We also find that the remaining gap can be
quickly closed by continued training on less than 1B tokens. Finally, we show
that a ZeTT hypernetwork trained for a base (L)LM can also be applied to
fine-tuned variants without extra training. Overall, our results make
substantial strides toward detaching LMs from their tokenizer.

</details>


### [80] [Says Who? Effective Zero-Shot Annotation of Focalization](https://arxiv.org/abs/2409.11390)
*Rebecca M. M. Hicke,Yuri Bizzoni,Pascale Feldkamp,Ross Deans Kristensen-McLachlan*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在文学文本中“焦点化”现象标注任务上的表现。研究发现，LLM，特别是GPT-4o，在焦点化标注上的表现可与受过训练的人类标注者相媲美，GPT-4o的平均F1得分为84.79%。此外，研究还表明LLM的对数概率能够反映标注的难度，并通过对斯蒂芬·金小说的案例研究，展示了该方法在计算文学研究中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 焦点化是文学叙事中一个复杂且主观的现象，指的是叙事信息如何根据叙述者的知识进行限制或控制。由于其编码涉及广泛的词汇语法特征且易于读者解释，即使是训练有素的标注者在标注时也常常出现分歧，这使得焦点化成为一项在定性研究和计算处理上都极具挑战性的任务。本研究旨在探索当前先进的大型语言模型（LLM）在自动标注焦点化现象方面的能力，以期为解决这一挑战提供新的计算视角，并推动计算文学研究的发展。

Method: 本研究采用了多种大型语言模型（LLM）家族和基线模型来测试它们在标注文学文本中焦点化现象的能力。具体来说，研究测试了五种主流LLM家族和两种基线模型的性能。实验设计包括使用这些模型对简短的文学选段进行焦点化标注。此外，研究还利用GPT系列模型输出的对数概率来评估模型对标注难度的感知能力。最后，通过对十六部斯蒂芬·金小说的案例分析，验证了该方法在实际文学研究中的应用效果。

Result: 研究结果显示，尽管焦点化标注任务具有挑战性，但LLM在准确性上表现出色，其性能与训练有素的人类标注者相当。其中，GPT-4o模型取得了平均84.79%的F1得分，表现尤为突出。研究还发现，GPT系列模型产生的对数概率能够有效反映出特定文本片段在标注上的难易程度。在对斯蒂芬·金小说的案例研究中，该方法成功展示了其在规模化分析焦点化现象方面的实用性，并揭示了相关文学见解。

Conclusion: 本研究成功证明了大型语言模型在处理复杂的计算文学任务，如焦点化标注方面具有巨大潜力，其表现足以媲美人类专家。GPT-4o等模型不仅能准确标注焦点化，其概率输出还能指示标注难度，为深入的计算文学研究提供了新的工具。通过对斯蒂芬·金小说的规模化分析，证实了该方法在发现文学模式和获得新见解方面的有效性。尽管如此，未来的研究可以进一步探索模型在处理更长文本、更多样化文学风格以及提高标注可解释性方面的能力。

Abstract: Focalization describes the way in which access to narrative information is
restricted or controlled based on the knowledge available to knowledge of the
narrator. It is encoded via a wide range of lexico-grammatical features and is
subject to reader interpretation. Even trained annotators frequently disagree
on correct labels, suggesting this task is both qualitatively and
computationally challenging. In this work, we test how well five contemporary
large language model (LLM) families and two baselines perform when annotating
short literary excerpts for focalization. Despite the challenging nature of the
task, we find that LLMs show comparable performance to trained human
annotators, with GPT-4o achieving an average F1 of 84.79%. Further, we
demonstrate that the log probabilities output by GPT-family models frequently
reflect the difficulty of annotating particular excerpts. Finally, we provide a
case study analyzing sixteen Stephen King novels, demonstrating the usefulness
of this approach for computational literary studies and the insights gleaned
from examining focalization at scale.

</details>


### [81] [TrajAgent: An LLM-Agent Framework for Trajectory Modeling via Large-and-Small Model Collaboration](https://arxiv.org/abs/2410.20445)
*Yuwei Du,Jie Feng,Jie Zhao,Yong Li*

Main category: cs.CL

TL;DR: 该研究提出了一种名为TrajAgent的框架，利用大型语言模型（LLM）自动化轨迹建模过程。TrajAgent通过统一的环境（UniEnv）整合多种专业模型，并引入LLM与专业模型之间的协作学习机制，以应对数据异构性和任务多样性带来的挑战。实验结果表明，TrajAgent在五个任务和四个真实世界数据集上显著优于基线方法，性能提升幅度为2.38%-69.91%。


<details>
  <summary>Details</summary>
Motivation: 轨迹建模在生活服务、城市交通和公共管理等领域具有广泛应用，但数据异构性和任务多样性使得有效的轨迹建模成为一项重要且极具挑战性的任务，即使对领域专家也是如此。现有的方法难以应对这些挑战，因此需要一种更强大、更通用的解决方案。

Method: 研究提出了TrajAgent框架，该框架基于大型语言模型（LLM）。核心组件包括：1. UniEnv：一个提供统一数据和模型接口的执行环境，支持模型执行和训练。2. Agentic Workflow：一个为自动化轨迹建模设计的流程，能够处理各种轨迹任务和数据。3. LLM-based Agents与Specialized Models的协作学习：通过这种机制，框架能够有效提升整体性能。实验在五个任务和四个真实世界数据集上进行，以验证TrajAgent的有效性。

Result: TrajAgent在五个轨迹建模任务和四个真实世界数据集上的广泛实验证明了其在自动化轨迹建模方面的有效性。与基线方法相比，TrajAgent的性能提升幅度达到了2.38%-69.91%，显示出显著的优势。

Conclusion: TrajAgent框架通过利用LLM和创新的协作学习机制，成功实现了自动化轨迹建模，有效解决了数据异构性和任务多样性带来的挑战。该框架在多个任务和数据集上取得了显著的性能提升，为轨迹建模领域提供了一个强大且通用的解决方案。未来研究可进一步探索该框架在更广泛应用场景中的潜力以及与其他先进模型的融合。

Abstract: Trajectory modeling, which includes research on trajectory data pattern
mining and future prediction, has widespread applications in areas such as life
services, urban transportation, and public administration. Numerous methods
have been proposed to address specific problems within trajectory modeling.
However, the heterogeneity of data and the diversity of trajectory tasks make
effective and reliable trajectory modeling an important yet highly challenging
endeavor, even for domain experts. In this paper, we propose TrajAgent, an
agent framework powered by large language models, designed to facilitate robust
and efficient trajectory modeling through automation modeling. This framework
leverages and optimizes diverse specialized models to address various
trajectory modeling tasks across different datasets effectively. In TrajAgent,
we first develop UniEnv, an execution environment with a unified data and model
interface, to support the execution and training of various models. Building on
UniEnv, we introduce an agentic workflow designed for automatic trajectory
modeling across various trajectory tasks and data. Furthermore, we introduce
collaborative learning schema between LLM-based agents and small speciallized
models, to enhance the performance of the whole framework effectively.
Extensive experiments on five tasks using four real-world datasets demonstrate
the effectiveness of TrajAgent in automated trajectory modeling, achieving a
performance improvement of 2.38%-69.91% over baseline methods. The codes and
data can be accessed via https://github.com/tsinghua-fib-lab/TrajAgent.

</details>


### [82] [Provable Scaling Laws for the Test-Time Compute of Large Language Models](https://arxiv.org/abs/2411.19477)
*Yanxi Chen,Xuchen Pan,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文提出了两种用于大型语言模型（LLMs）的简单、有原则且实用的算法，它们具有可证明的测试时间计算扩展定律。第一种是两阶段淘汰赛算法：为给定输入生成多个候选解决方案，然后通过淘汰赛进行聚合。第二种是两阶段联赛算法：每个候选方案通过与多个对手的平均胜率进行评估。这两种算法都只需要一个黑盒LLM，无需其他工具，易于实现和适应不同任务。实验验证了理论并展示了算法的优越扩展性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理复杂任务时，其测试时间计算成本高昂，限制了其实际应用。如何在大规模计算下，保证LLM的输出质量并降低错误率，是一个重要的研究问题。本文旨在提出一种能够有效降低LLM测试错误率，并具有可证明的扩展规律的算法，以解决计算成本与模型性能之间的权衡问题。

Method: 本文提出了两种算法：
1. **两阶段淘汰赛算法**：首先生成多个候选解决方案，然后通过类似淘汰赛的机制进行两两比较和淘汰，最终选出最优解。
2. **两阶段联赛算法**：同样生成多个候选解决方案，但每个方案会与其他多个方案进行多轮比较（类似联赛），根据平均胜率进行排名，选择胜率最高的方案。

这两种算法都假设LLM能够以非零概率生成正确答案，并且在比较两个解决方案时优于随机猜测。算法的实现仅需一个黑盒LLM，无需额外的验证器或奖励模型。通过广泛的实验，在不同模型和数据集上验证了算法的有效性。

Result: 实验结果表明，所提出的两种算法在增加测试时间计算量时，错误率能够以指数级或幂律级（取决于具体扩展方式）的速度衰减。这证明了算法具有优越的可扩展性。算法的实现简单，并且在各种模型和数据集上都表现出了良好的性能，验证了理论分析的准确性。

Conclusion: 本文提出的两种新颖的LLM算法（淘汰赛和联赛）能够有效降低测试时间错误率，并具有可证明的扩展规律。这些算法仅依赖于黑盒LLM，易于实现和应用。实验证明了其有效性和优越的可扩展性，为在计算资源有限的情况下提升LLM性能提供了有前景的解决方案。未来的工作可以探索更复杂的比较机制和扩展策略。

Abstract: We propose two simple, principled and practical algorithms that enjoy
provable scaling laws for the test-time compute of large language models
(LLMs). The first one is a two-stage knockout-style algorithm: given an input
problem, it first generates multiple candidate solutions, and then aggregate
them via a knockout tournament for the final output. Assuming that the LLM can
generate a correct solution with non-zero probability and do better than a
random guess in comparing a pair of correct and incorrect solutions, we prove
theoretically that the failure probability of this algorithm decays to zero
exponentially or by a power law (depending on the specific way of scaling) as
its test-time compute grows. The second one is a two-stage league-style
algorithm, where each candidate is evaluated by its average win rate against
multiple opponents, rather than eliminated upon loss to a single opponent.
Under analogous but more robust assumptions, we prove that its failure
probability also decays to zero exponentially with more test-time compute. Both
algorithms require a black-box LLM and nothing else (e.g., no verifier or
reward model) for a minimalistic implementation, which makes them appealing for
practical applications and easy to adapt for different tasks. Through extensive
experiments with diverse models and datasets, we validate the proposed theories
and demonstrate the outstanding scaling properties of both algorithms.

</details>


### [83] [Discourse Features Enhance Detection of Document-Level Machine-Generated Content](https://arxiv.org/abs/2412.12679)
*Yupei Li,Manuel Milling,Lucia Specia,Björn W. Schuller*

Main category: cs.CL

TL;DR: 鉴于大型语言模型（LLM）API的普及，机器生成内容（MGC）的激增带来了学术剽窃和虚假信息传播等挑战。现有MGC检测器主要关注表面信息，忽略了隐式和结构化特征，容易被表面句子模式欺骗，尤其是在长文本和经过释义的文本中。本研究提出了新的方法和数据集，包括释义后的长问答（paraLFQA）和释义后的写作提示（paraWP）数据集，并引入了DTransformer模型。该模型通过结合话语分析和PDTB预处理来编码长文本的结构特征。实验结果表明，DTransformer在paraLFQA、paraWP和M4数据集上分别取得了15.5%、4%和1.5%的绝对性能提升，显著优于现有最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）API的广泛可用性导致了机器生成内容（MGC）的激增，这带来了严重的挑战，例如学术剽窃和虚假信息的传播。然而，现有的MGC检测方法往往只关注文本的表面特征，忽略了深层的隐式和结构化信息。这种方法使得它们容易被表面上的句子模式所欺骗，特别是在处理长文本或经过释义的文本时，检测效果大打折扣。因此，有必要开发更强大的MGC检测方法，以应对这些挑战。

Method: 本研究提出了两种新的数据集：释义后的长问答（paraLFQA）和释义后的写作提示（paraWP）。这些数据集是通过对现有数据集进行扩展和释义而创建的，使用了GPT和DIPPER（一种话语释义工具）。此外，为了更好地捕捉长文本在文档层面的结构，研究者们提出了DTransformer模型。该模型的核心在于整合话语分析，并利用PDTB（Penn Discourse Treebank）预处理来编码文本的结构特征。这种方法旨在超越表面特征的限制，更深入地理解文本的内在结构。

Result: 研究结果显示，DTransformer模型在多个数据集上取得了显著的性能提升。具体而言，在paraLFQA数据集上，性能提升了15.5%；在paraWP数据集上，性能提升了4%；在M4数据集上，性能也提升了1.5%。这些改进相对于现有最先进（SOTA）的方法来说是相当可观的，证明了所提出模型在检测文档级机器生成内容方面的有效性。

Conclusion: 本研究成功开发了新的数据集（paraLFQA和paraWP）和一种名为DTransformer的新模型，通过整合话语分析和PDTB预处理来捕捉长文本的结构特征，从而有效提升了机器生成内容（MGC）的检测性能。实验结果表明，DTransformer在多个数据集上均取得了显著的性能提升，优于现有最先进的方法。这项工作为解决MGC带来的学术剽窃和虚假信息传播等问题提供了新的解决方案。未来的工作可以进一步探索更复杂的话语结构特征，并针对不同类型的MGC进行模型优化。

Abstract: The availability of high-quality APIs for Large Language Models (LLMs) has
facilitated the widespread creation of Machine-Generated Content (MGC), posing
challenges such as academic plagiarism and the spread of misinformation.
Existing MGC detectors often focus solely on surface-level information,
overlooking implicit and structural features. This makes them susceptible to
deception by surface-level sentence patterns, particularly for longer texts and
in texts that have been subsequently paraphrased. To overcome these challenges,
we introduce novel methodologies and datasets. Besides the publicly available
dataset Plagbench, we developed the paraphrased Long-Form Question and Answer
(paraLFQA) and paraphrased Writing Prompts (paraWP) datasets using GPT and
DIPPER, a discourse paraphrasing tool, by extending artifacts from their
original versions. To better capture the structure of longer texts at document
level, we propose DTransformer, a model that integrates discourse analysis
through PDTB preprocessing to encode structural features. It results in
substantial performance gains across both datasets - 15.5% absolute improvement
on paraLFQA, 4% absolute improvement on paraWP, and 1.5% absolute improvemene
on M4 compared to SOTA approaches. The data and code are available at:
https://github.com/myxp-lyp/Discourse-Features-Enhance-Detection-of-Document-Level-Machine-Generated-Content.git.

</details>


### [84] [Face the Facts! Evaluating RAG-based Fact-checking Pipelines in Realistic Settings](https://arxiv.org/abs/2412.15189)
*Daniel Russo,Stefano Menini,Jacopo Staiano,Marco Guerini*

Main category: cs.CL

TL;DR: 本研究旨在更真实地评估基于检索增强生成（RAG）的自动事实核查方法，特别是在生成判断（即讨论声明真实性的简短文本）方面。研究者放宽了现有RAG流水线的一些限制，并在风格复杂声明和异构但可靠的知识库上进行了基准测试。研究发现，基于大型语言模型（LLM）的检索器优于其他检索技术，但在处理异构知识库时仍存在困难；较大的模型在判断忠实度方面表现更好，而较小的模型在上下文遵循方面表现更佳；人类评估显示，零样本和单样本方法在信息量方面表现更好，而微调模型在情感一致性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 事实核查工作成本高昂且耗时，自然语言处理和生成系统（如RAG）有潜力辅助这一工作。然而，当前基于RAG的自动事实核查方法在更现实的场景下（如风格复杂的声明和异构知识库）的评估仍有待深入研究。本研究旨在弥补这一不足，通过在更贴近实际的场景下对RAG方法进行基准测试，为改进自动事实核查系统提供指导。

Method: 本研究放宽了现有基于RAG的自动事实核查流水线的一些限制，并在风格复杂的声明和异构但可靠的知识库上进行了基准测试。研究评估了不同检索技术（包括基于LLM的检索器）和不同模型规模（大模型和小模型）在生成判断方面的表现。同时，也对比了零样本、单样本和微调模型在信息量和情感一致性方面的人类评估结果。

Result: 研究发现，基于LLM的检索器在检索方面优于其他技术，但在处理异构知识库时仍遇到挑战。在生成判断方面，较大的模型在忠实度（判断内容与事实的一致性）上表现更优，而较小的模型在上下文遵循（判断是否贴合原始声明的语境）上表现更好。人类评估显示，零样本和单样本方法在提供信息方面更受青睐，而微调模型则在情感一致性方面表现更佳。

Conclusion: 本研究揭示了在更现实的场景下评估RAG基事实核查方法的复杂性。虽然LLM检索器和大型模型在某些方面表现出优势，但在处理异构知识库和平衡判断忠实度与上下文遵循方面仍需改进。未来的工作可以进一步探索如何优化检索策略以适应异构知识库，以及如何在不同应用场景下平衡模型大小、忠实度和上下文遵循，并优化零样本/单样本与微调方法的选择，以满足不同的评估标准（如信息量和情感一致性）。

Abstract: Natural Language Processing and Generation systems have recently shown the
potential to complement and streamline the costly and time-consuming job of
professional fact-checkers. In this work, we lift several constraints of
current state-of-the-art pipelines for automated fact-checking based on the
Retrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under
more realistic scenarios, RAG-based methods for the generation of verdicts -
i.e., short texts discussing the veracity of a claim - evaluating them on
stylistically complex claims and heterogeneous, yet reliable, knowledge bases.
Our findings show a complex landscape, where, for example, LLM-based retrievers
outperform other retrieval techniques, though they still struggle with
heterogeneous knowledge bases; larger models excel in verdict faithfulness,
while smaller models provide better context adherence, with human evaluations
favouring zero-shot and one-shot approaches for informativeness, and fine-tuned
models for emotional alignment.

</details>


### [85] [NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables](https://arxiv.org/abs/2504.06560)
*Lanrui Wang,Mingyu Zheng,Hongyin Tang,Zheng Lin,Yanan Cao,Jingang Wang,Xunliang Cai,Weiping Wang*

Main category: cs.CL

TL;DR: 针对大型语言模型（LLMs）在处理结构化表格数据方面的挑战，特别是长上下文表格，现有的长上下文基准测试（如 Needle-in-a-Haystack）主要关注非结构化文本，忽略了表格数据的多样性。同时，以往的表格基准测试侧重于需要高级推理能力的下游任务，忽视了模型对单个表格单元格的细粒度感知能力。为了解决这些问题，研究者提出了 NeedleInATable (NIAT) 基准测试，将每个表格单元格视为“针”，要求模型根据单元格位置或查找问题提取目标单元格。通过对多种 LLMs 和多模态 LLMs 的评估，发现它们在 NIAT 任务上的表现与流行的下游表格任务之间存在显著差距，表明现有模型可能依赖数据集特定的关联或捷径来获得高分，而非真正具备对结构化表格的鲁棒长上下文理解能力。此外，使用合成的 NIAT 训练数据可以有效提升模型在 NIAT 任务和下游表格任务上的表现，验证了 NIAT 能力对于 LLMs 理解表格的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的长上下文基准测试（如 Needle-in-a-Haystack）主要关注非结构化文本，忽略了大型、长格式结构化表格的处理这一关键挑战。现有的表格基准测试主要侧重于需要高级推理能力的下游任务，而忽视了模型对表格单元格的细粒度感知能力，这种能力对于实际和鲁棒的 LLM 表格应用至关重要。因此，需要一个专门评估 LLM 在长上下文表格场景下细粒度理解能力的新基准。

Method: 提出 NeedleInATable (NIAT) 这一新的长上下文表格基准测试。NIAT 将表格中的每个单元格视为“针”，要求模型根据单元格的位置或基于单元格内容的查询来提取目标单元格。对多种 LLMs 和多模态 LLMs 进行了全面评估。研究了使用合成 NIAT 训练数据来提高模型在 NIAT 任务和下游表格任务上的性能。

Result: 评估结果显示，在 NIAT 任务上的模型性能与流行的下游表格任务之间存在显著的性能差距。这表明，许多 LLMs 在处理下游表格任务时可能依赖于数据集特定的关联或“捷径”，而非真正具备对结构化长上下文表格的鲁棒理解能力。研究还发现，使用合成的 NIAT 训练数据能够有效提升模型在 NIAT 任务和下游表格任务上的表现，证明了 NIAT 能力对于 LLMs 真正理解表格的重要性。

Conclusion: NIAT 基准测试揭示了当前 LLMs 在处理长上下文结构化表格方面的显著局限性，特别是它们对细粒度单元格的感知能力。现有的下游表格任务评估可能无法充分反映模型在真实世界表格应用中的鲁棒性。通过合成 NIAT 训练数据可以有效提升模型在这方面的能力，强调了专门训练模型进行细粒度表格理解的必要性。未来的工作可以继续扩展 NIAT 的规模和多样性，并探索更先进的模型架构和训练策略来应对这些挑战。

Abstract: Processing structured tabular data, particularly large and lengthy tables,
constitutes a fundamental yet challenging task for large language models
(LLMs). However, existing long-context benchmarks like Needle-in-a-Haystack
primarily focus on unstructured text, neglecting the challenge of diverse
structured tables. Meanwhile, previous tabular benchmarks mainly consider
downstream tasks that require high-level reasoning abilities, and overlook
models' underlying fine-grained perception of individual table cells, which is
crucial for practical and robust LLM-based table applications. To address this
gap, we introduce \textsc{NeedleInATable} (NIAT), a new long-context tabular
benchmark that treats each table cell as a ``needle'' and requires models to
extract the target cell based on cell locations or lookup questions. Our
comprehensive evaluation of various LLMs and multimodal LLMs reveals a
substantial performance gap between popular downstream tabular tasks and the
simpler NIAT task, suggesting that they may rely on dataset-specific
correlations or shortcuts to obtain better benchmark results but lack truly
robust long-context understanding towards structured tables. Furthermore, we
demonstrate that using synthesized NIAT training data can effectively improve
performance on both NIAT task and downstream tabular tasks, which validates the
importance of NIAT capability for LLMs' genuine table understanding ability.

</details>


### [86] [BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text](https://arxiv.org/abs/2504.19467)
*Jiageng Wu,Bowen Gu,Ren Zhou,Kevin Xie,Doug Snyder,Yixing Jiang,Valentina Carducci,Richard Wyss,Rishi J Desai,Emily Alsentzer,Leo Anthony Celi,Adam Rodman,Sebastian Schneeweiss,Jonathan H. Chen,Santiago Romero-Brufau,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TL;DR: 现有的大型语言模型（LLMs）在医疗领域的应用前景广阔，但对其在真实世界电子健康记录（EHRs）等大规模临床数据上的评估仍然有限。现有的基准测试未能充分反映临床决策的复杂性。为此，我们提出了BRIDGE，一个包含87个任务、覆盖9种语言、8种任务类型、6个临床阶段、20个应用场景和14个临床专业的综合性多语言基准测试。我们评估了95个LLMs，结果显示模型在不同规模、语言、任务和临床专业上的表现存在显著差异。值得注意的是，开源模型可以媲美闭源模型，而基于旧架构的医学微调模型有时不如更新的通用模型。BRIDGE及其排行榜将为开发和评估临床文本理解LLMs提供重要资源。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在医疗领域的应用潜力巨大，但当前的评估方法存在局限性。大多数现有的基准测试依赖于医学考试风格的问题或从PubMed提取的文本，未能捕捉真实世界临床数据（如电子健康记录EHRs）的复杂性和多样性。此外，一些评估仅限于特定应用场景，缺乏广泛的临床适用性。因此，迫切需要一个全面、大规模的基准测试来准确评估LLMs在真实临床环境中的表现，以指导模型的开发和优化。

Method: 我们构建了一个名为BRIDGE 的多语言基准测试，它包含 87 个任务，这些任务来源于真实世界的临床数据，覆盖九种语言。该基准测试涵盖了八种主要的任务类型，包括分诊转诊、咨询、信息提取、诊断、预后和计费编码等，这些任务贯穿了患者护理的全过程，涉及六个临床阶段和二十个代表性应用。此外，基准测试还覆盖了十四个临床专业。我们在此基准测试上系统性地评估了包括 DeepSeek-R1、GPT-4o、Gemini 系列和 Qwen3 系列在内的 95 个大型语言模型，并采用了多种推理策略。

Result: 在对 95 个大型语言模型的评估中，我们观察到模型在不同尺寸、语言、自然语言处理任务以及临床专业上的表现存在显著的性能差异。研究结果表明，一些开源 LLMs 在性能上可以与闭源模型相媲美。有趣的是，我们发现基于较旧模型架构、经过医学领域专门微调的模型，其性能往往不如更新的通用模型。

Conclusion: BRIDGE 基准测试的构建和评估结果为理解和改进大型语言模型在真实临床文本处理能力方面提供了重要的见解。该基准测试及其在线排行榜（https://huggingface.co/spaces/YLab-Open/BRIDGE-Medical-Leaderboard）为开发和评估新兴的临床文本理解 LLMs 提供了一个基础性资源和独特的参考点。未来的工作可以利用 BRIDGE 来推动 LLMs 在实际医疗应用中的发展。

Abstract: Large language models (LLMs) hold great promise for medical applications and
are evolving rapidly, with new models being released at an accelerated pace.
However, benchmarking on large-scale real-world data such as electronic health
records (EHRs) is critical, as clinical decisions are directly informed by
these sources, yet current evaluations remain limited. Most existing benchmarks
rely on medical exam-style questions or PubMed-derived text, failing to capture
the complexity of real-world clinical data. Others focus narrowly on specific
application scenarios, limiting their generalizability across broader clinical
use. To address this gap, we present BRIDGE, a comprehensive multilingual
benchmark comprising 87 tasks sourced from real-world clinical data sources
across nine languages. It covers eight major task types spanning the entire
continuum of patient care across six clinical stages and 20 representative
applications, including triage and referral, consultation, information
extraction, diagnosis, prognosis, and billing coding, and involves 14 clinical
specialties. We systematically evaluated 95 LLMs (including DeepSeek-R1,
GPT-4o, Gemini series, and Qwen3 series) under various inference strategies.
Our results reveal substantial performance variation across model sizes,
languages, natural language processing tasks, and clinical specialties.
Notably, we demonstrate that open-source LLMs can achieve performance
comparable to proprietary models, while medically fine-tuned LLMs based on
older architectures often underperform versus updated general-purpose models.
The BRIDGE and its corresponding leaderboard serve as a foundational resource
and a unique reference for the development and evaluation of new LLMs in
real-world clinical text understanding.
  The BRIDGE leaderboard:
https://huggingface.co/spaces/YLab-Open/BRIDGE-Medical-Leaderboard

</details>


### [87] [The Hawthorne Effect in Reasoning Models: Evaluating and Steering Test Awareness](https://arxiv.org/abs/2505.14617)
*Sahar Abdelnabi,Ahmed Salem*

Main category: cs.CL

TL;DR: 研究首次量化了“测试意识”对LLM安全对齐的影响，提出了一种白盒探测框架，可以通过控制模型对测试的意识来缓解负面影响，提高模型评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM在被评估时可能改变行为，优化测试表现或更容易被诱导产生有害输出。这种“测试意识”对模型安全对齐的影响尚不明确，需要量化研究来理解和控制。

Method: 提出了一种白盒探测框架，能够识别与测试意识相关的激活，并引导模型产生或避免测试意识。该框架应用于多种先进的开放权重推理LLM，并在现实和假设任务中进行了测试。

Result: 结果表明，测试意识显著影响模型的安全对齐，包括对有害请求的服从和对刻板印象的遵从。这种影响在不同模型上的表现各异，其影响的大小和方向也存在差异。

Conclusion: 研究首次量化了“测试意识”对LLM安全对齐的影响，并提供了一种控制该效应的方法。这有助于更好地进行模型安全评估，增强评估的可靠性，并为未来的研究提供方向。

Abstract: Reasoning-focused LLMs sometimes alter their behavior when they detect that
they are being evaluated, which can lead them to optimize for test-passing
performance or to comply more readily with harmful prompts if real-world
consequences appear absent. We present the first quantitative study of how such
"test awareness" impacts model behavior, particularly its performance on
safety-related tasks. We introduce a white-box probing framework that (i)
linearly identifies awareness-related activations and (ii) steers models toward
or away from test awareness while monitoring downstream performance. We apply
our method to different state-of-the-art open-weight reasoning LLMs across both
realistic and hypothetical tasks (denoting tests or simulations). Our results
demonstrate that test awareness significantly impacts safety alignment (such as
compliance with harmful requests and conforming to stereotypes) with effects
varying in both magnitude and direction across models. By providing control
over this latent effect, our work aims to provide a stress-test mechanism and
increase trust in how we perform safety evaluations.

</details>


### [88] [Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector](https://arxiv.org/abs/2505.17100)
*Haoyan Yang,Runxue Bao,Cao Xiao,Jun Ma,Parminder Bhatia,Shangqian Gao,Taha Kass-Hout*

Main category: cs.CL

TL;DR: LLM-as-a-Judge在评估生成文本方面显示出潜力，但其可靠性受到偏见的影响。本研究提出了一个名为RBD（Reasoning-based Bias Detector）的插件模块，用于识别评估中的偏见并提供结构化推理以指导评估器自我纠正。RBD作为外部模块运行，不修改评估器本身，并通过迭代过程检测偏见并进行反馈驱动的修订。研究还构建了一个完整的流水线，包括偏见数据集的构建、监督数据的收集、RBD的蒸馏推理式微调以及与LLM评估器的集成。在四种偏见类型（冗长、位置、从众和情感）以及八种LLM评估器上的实验表明，RBD能有效提升评估准确性和一致性，并且优于基于提示的方法和微调的评估器。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）作为评估工具（LLM-as-a-Judge）在自动评估生成文本方面展现出巨大潜力，但其评估结果的可靠性常常受到内在偏见的严重影响。现有的缓解偏见的方法存在局限性：基于上下文学习（in-context learning）的方法由于评估器有限的自我反思能力，无法解决根深蒂固的偏见；而模型微调（fine-tuning）的方法又无法应用于所有类型的评估器，特别是那些无法访问其内部参数的闭源模型。因此，研究需要一种新的方法来有效且广泛地解决LLM评估中的偏见问题。

Method: 本研究提出了一个名为RBD（Reasoning-based Bias Detector）的插件模块。RBD作为一个外部组件，能够识别LLM评估中的偏见，并生成结构化的推理过程，以指导评估器进行自我纠正，而无需修改评估器本身。RBD通过一个迭代过程来运作：首先检测评估中的偏见，然后根据检测到的偏见生成反馈，驱动评估器进行修订。为了支持RBD的开发，研究人员设计并实现了一个完整的流水线，包括：1）构建包含各种偏见类型的评估数据集；2）收集用于监督RBD的标注数据；3）对RBD进行蒸馏推理式微调；4）将RBD集成到现有的LLM评估流程中。研究人员微调了四种不同规模（1.5B到14B参数）的RBD模型，并在四种偏见类型（冗长、位置、从众和情感）上，使用八种不同的LLM评估器进行了广泛的实验评估。

Result: 实验结果表明，RBD在消除LLM评估偏见方面表现出强大的有效性和可扩展性。无论模型规模如何，RBD的性能均随着模型增大而持续提升。在所有四种偏见类型和八种LLM评估器上，RBD模型显著提高了评估的准确性和一致性。具体而言，RBD-8B模型平均将评估准确率提高了18.5%，一致性提高了10.9%。与仅使用提示（prompting）的方法相比，RBD的评估准确率平均高出12.8%；而与经过微调的评估器相比，RBD的评估准确率平均高出17.2%。此外，额外的实验证明了RBD在不同偏见类型和不同领域之间具有良好的泛化能力，并且在计算效率方面也表现出色。

Conclusion: 本研究成功开发并验证了一种名为RBD的新型插件模块，它能够有效地识别和减轻LLM评估中的偏见，同时保持评估器（包括闭源模型）的完整性。RBD通过生成结构化推理来指导评估器的自我修正，提供了一种灵活且可扩展的解决方案。实验结果证明了RBD在提高评估准确性和一致性方面的显著优势，并且其泛化能力和效率也得到了验证。尽管RBD在多个维度上取得了优异的表现，但未来的工作可以进一步探索更广泛的偏见类型、更复杂的评估场景，以及优化RBD的计算效率和集成方式。

Abstract: LLM-as-a-Judge has emerged as a promising tool for automatically evaluating
generated outputs, but its reliability is often undermined by potential biases
in judgment. Existing efforts to mitigate these biases face key limitations:
in-context learning-based methods fail to address rooted biases due to the
evaluator's limited capacity for self-reflection, whereas fine-tuning is not
applicable to all evaluator types, especially closed-source models. To address
this challenge, we introduce the Reasoning-based Bias Detector (RBD), which is
a plug-in module that identifies biased evaluations and generates structured
reasoning to guide evaluator self-correction. Rather than modifying the
evaluator itself, RBD operates externally and engages in an iterative process
of bias detection and feedback-driven revision. To support its development, we
design a complete pipeline consisting of biased dataset construction,
supervision collection, distilled reasoning-based fine-tuning of RBD, and
integration with LLM evaluators. We fine-tune four sizes of RBD models, ranging
from 1.5B to 14B, and observe consistent performance improvements across all
scales. Experimental results on 4 bias types--verbosity, position, bandwagon,
and sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong
effectiveness. For example, the RBD-8B model improves evaluation accuracy by an
average of 18.5% and consistency by 10.9%, and surpasses prompting-based
baselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results
highlight RBD's effectiveness and scalability. Additional experiments further
demonstrate its strong generalization across biases and domains, as well as its
efficiency.

</details>


### [89] [PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings](https://arxiv.org/abs/2506.00481)
*Junseo Kim,Jongwook Han,Dongmin Choi,Jongwook Yoon,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 该研究发布了一个名为PVP（Personalized Visual Persuasion）的大规模数据集，该数据集包含28,454张具有说服力的图像，涵盖596条信息和9种说服策略。数据集记录了2,521名人类标注者对图像说服力的评分，以及他们的个人信息（人口统计学特征、性格特质和价值观）。研究人员利用该数据集开发了一个自动生成说服性图像的系统和一个自动评估系统，并建立了基准模型。实验证明，结合用户的心理特征可以提升说服性图像生成和评估的效果，为个性化视觉说服领域提供了宝贵见解。


<details>
  <summary>Details</summary>
Motivation: 广告和政治宣传等领域广泛应用视觉说服技术。然而，目前缺乏将图像说服力与评估者个人信息联系起来的数据集，这阻碍了能够自动为个体生成定制化说服图像的AI系统的发展。为了解决这一问题，本研究旨在创建一个全面的数据集，以促进个性化视觉说服技术的研究和应用。

Method: 研究人员构建了一个名为PVP（Personalized Visual Persuasion）的数据集，其中包含28,454张图像，这些图像与596条不同的信息以及9种不同的说服策略相关联。2,521名人类标注者对这些图像的说服力进行了评分，同时还收集了他们的个人信息，包括人口统计学特征、性格特质和价值观。在此基础上，研究人员开发了一个自动生成说服性图像的模型和一个自动评估说服性图像的模型，并使用PVP数据集对这些模型进行了训练和评估，建立了基准性能。

Result: 通过实验，研究发现将用户的心理特征（如性格特质和价值观）融入到说服性图像的生成和评估过程中，能够显著提升模型的性能。具体来说，利用心理特征的模型在生成更具说服力的图像以及更准确地评估图像说服力方面表现更优。研究人员建立了相应的基准模型，为后续研究提供了参考。

Conclusion: 本研究成功构建并发布了PVP数据集，解决了现有数据集的不足，为个性化视觉说服领域的研究提供了关键资源。实验结果表明，考虑用户的心理特征对于提升视觉说服的效果至关重要，为开发更有效的个性化说服系统提供了新的方向。未来的研究可以基于此数据集，进一步探索更复杂的个性化说服机制和模型。

Abstract: Visual persuasion, which uses visual elements to influence cognition and
behaviors, is crucial in fields such as advertising and political
communication. With recent advancements in artificial intelligence, there is
growing potential to develop persuasive systems that automatically generate
persuasive images tailored to individuals. However, a significant bottleneck in
this area is the lack of comprehensive datasets that connect the persuasiveness
of images with the personal information about those who evaluated the images.
To address this gap and facilitate technological advancements in personalized
visual persuasion, we release the Personalized Visual Persuasion (PVP) dataset,
comprising 28,454 persuasive images across 596 messages and 9 persuasion
strategies. Importantly, the PVP dataset provides persuasiveness scores of
images evaluated by 2,521 human annotators, along with their demographic and
psychological characteristics (personality traits and values). We demonstrate
the utility of our dataset by developing a persuasive image generator and an
automated evaluator, and establish benchmark baselines. Our experiments reveal
that incorporating psychological characteristics enhances the generation and
evaluation of persuasive images, providing valuable insights for personalized
visual persuasion.

</details>


### [90] [RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.00789)
*Yixiao Zeng,Tianyu Cao,Danqing Wang,Xinran Zhao,Zimeng Qiu,Morteza Ziyadi,Tongshuang Wu,Lei Li*

Main category: cs.CL

TL;DR: 现有的检索增强生成（RAG）系统在处理真实世界中的噪声、内部检索与外部上下文冲突以及快速变化的事实时，鲁棒性不足。本文提出了检索感知鲁棒性评估（RARE）框架和基准测试，以应对这些挑战。RARE利用知识图谱驱动的合成管道（RARE-Get）自动生成多层次的问题集，并构建了一个包含527篇金融、经济和政策文档及48295个问题的RARE-Set数据集。该数据集的特点是信息随时间动态变化。RARE-Met度量标准用于量化模型的鲁棒性，发现RAG系统对扰动非常敏感，并且在多跳查询方面鲁棒性低于单跳查询。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）技术虽然提高了答案的时效性和事实准确性，但在真实世界的应用中面临严峻挑战。具体来说，这些系统在处理带有噪声的数据、区分内部检索信息与外部上下文信息，以及应对快速变化的事实时，其鲁棒性往往不足。然而，现有的评估方法很少能全面地测试RAG系统在这些真实场景下的表现。因此，研究如何有效地评估和提升RAG系统在复杂多变的真实环境下的鲁棒性，具有重要的现实意义和研究价值。

Method: 本文提出了一种名为检索感知鲁棒性评估（RARE）的统一框架和大规模基准测试。该框架能够联合测试查询和文档在动态、时敏性语料库上的扰动影响。RARE框架的核心是一个名为RARE-Get的知识图谱驱动的合成管道，该管道可以自动从自定义语料库中提取单跳和多跳关系，并生成多层次的问题集，无需人工干预。基于此管道，研究人员构建了一个名为RARE-Set的数据集，其中包含527篇专家级别的、关于金融、经济和政策的、具有时效性的文档，以及48295个问题。该数据集的特点是，随着底层数据源的变化，问题的分布也会随之演变。为了量化模型的鲁棒性，研究人员形式化了检索条件鲁棒性度量（RARE-Met），该度量能够捕捉模型在查询、文档或真实检索结果被系统性地改变时，保持正确或恢复正确的能力。

Result: 通过RARE框架进行的评估显示，当前的RAG系统对扰动表现出出乎意料的敏感性。具体而言，在所有测试的领域中，RAG系统在面对查询、文档或检索结果的系统性改变时，其回答的准确性会显著下降。此外，评估结果还表明，与单跳查询相比，RAG系统在处理多跳查询时，其鲁棒性 consistently 较低。这意味着RAG系统在需要整合多步推理或关联多个信息片段时，更容易受到噪声和变化的影响而产生错误。

Conclusion: 本文提出的RARE框架和RARE-Set数据集为评估RAG系统在真实世界动态、时敏性环境下的鲁棒性提供了重要的工具。研究结果揭示了当前RAG系统在面对数据扰动和复杂查询（如多跳查询）时的脆弱性，强调了提高RAG系统鲁棒性的紧迫性。未来的工作可以集中于开发更具鲁棒性的RAG模型架构、优化检索策略以更好地处理噪声和不确定性，以及进一步扩展RARE基准测试以覆盖更广泛的领域和更复杂的评估场景。

Abstract: Retrieval-Augmented Generation (RAG) enhances recency and factuality in
answers. However, existing evaluations rarely test how well these systems cope
with real-world noise, conflicting between internal and external retrieved
contexts, or fast-changing facts. We introduce Retrieval-Aware Robustness
Evaluation (RARE), a unified framework and large-scale benchmark that jointly
stress-tests query and document perturbations over dynamic, time-sensitive
corpora. One of the central features of RARE is a knowledge-graph-driven
synthesis pipeline (RARE-Get) that automatically extracts single and multi-hop
relations from the customized corpus and generates multi-level question sets
without manual intervention. Leveraging this pipeline, we construct a dataset
(RARE-Set) spanning 527 expert-level time-sensitive finance, economics, and
policy documents and 48295 questions whose distribution evolves as the
underlying sources change. To quantify resilience, we formalize
retrieval-conditioned robustness metrics (RARE-Met) that capture a model's
ability to remain correct or recover when queries, documents, or real-world
retrieval results are systematically altered. Our findings reveal that RAG
systems are unexpectedly sensitive to perturbations. Moreover, they
consistently demonstrate lower robustness on multi-hop queries compared to
single-hop queries across all domains.

</details>


### [91] [AdaRewriter: Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation](https://arxiv.org/abs/2506.01381)
*Yilong Lai,Jialong Wu,Zhenglin Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: 该研究提出了一种名为AdaRewriter的新型框架，用于通过测试时适应（test-time adaptation）来优化基于提示的对话查询重构，并使用结果监督的奖励模型来选择最佳重构结果，该方法在黑盒系统中也能有效运行，并在五个对话搜索数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的对话查询重构方法在利用最佳候选重构（Best-of-N reformulation）方面存在局限性，无论是通过训练时调整还是测试时适应，都未能充分发挥其潜力。这阻碍了在对话搜索中有效改进模糊用户查询并生成独立搜索查询的能力。

Method: 该研究提出了AdaRewriter框架，该框架利用结果监督的奖励模型进行测试时适应。具体来说，它通过对比损失函数训练一个轻量级奖励模型，并在推理时利用该模型选择最有希望的重构结果。AdaRewriter能够有效处理黑盒系统，包括商业大型语言模型API。

Result: 在五个对话搜索数据集上的实验表明，AdaRewrewriter在大多数设置下显著优于现有方法。这证明了测试时适应在对话查询重构中的潜力。

Conclusion: AdaRewriter通过引入测试时适应和结果监督的奖励模型，成功解决了现有对话查询重构方法的局限性，并在多个数据集上取得了优越的性能。该研究强调了测试时适应在提升对话搜索能力方面的有效性，并为未来的研究指明了方向。

Abstract: Prompting-based conversational query reformulation has emerged as a powerful
approach for conversational search, refining ambiguous user queries into
standalone search queries. Best-of-N reformulation over the generated
candidates via prompting shows impressive potential scaling capability.
However, both the previous tuning methods (training time) and adaptation
approaches (test time) can not fully unleash their benefits. In this paper, we
propose AdaRewriter, a novel framework for query reformulation using an
outcome-supervised reward model via test-time adaptation. By training a
lightweight reward model with contrastive ranking loss, AdaRewriter selects the
most promising reformulation during inference. Notably, it can operate
effectively in black-box systems, including commercial LLM APIs. Experiments on
five conversational search datasets show that AdaRewriter significantly
outperforms the existing methods across most settings, demonstrating the
potential of test-time adaptation for conversational query reformulation.

</details>


### [92] [Offline RL by Reward-Weighted Fine-Tuning for Conversation Optimization](https://arxiv.org/abs/2506.06964)
*Subhojyoti Mukherjee,Viet Dac Lai,Raghavendra Addanki,Ryan Rossi,Seunghyun Yoon,Trung Bui,Anup Rao,Jayakumar Subramanian,Branislav Kveton*

Main category: cs.CL

TL;DR: 本文提出了一种使用大型语言模型（LLMs）进行离线强化学习（RL）的实用方法，将问题转化为奖励加权微调，并在短时问答策略学习中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的离线RL方法在处理大规模数据集和复杂任务时面临挑战。而现有的基于SFT和DPO的方法虽然在问答任务中表现良好，但并未直接优化奖励，且存在额外的超参数。本文旨在探索一种更直接、更高效的离线RL方法，特别是针对短时问答策略的学习，以克服现有方法的局限性。

Method: 本文提出了一种将离线RL问题重构为奖励加权微调（reward-weighted fine-tuning）的新方法，该方法可以利用监督微调（SFT）的技术进行求解。具体来说，研究人员将LLMs应用于学习固定长度的短时问答策略，通过优化奖励信号来指导模型的学习过程。实验中，将该方法与基于SFT和DPO的当前最优方法进行了对比。

Result: 与基于SFT和DPO的当前最优方法相比，本文提出的方法在优化的奖励和语言质量方面均取得了显著的提升。这表明该方法在短时问答策略学习任务中，能够更有效地利用离线数据进行学习，并生成更高质量的回答。

Conclusion: 本文成功地将LLMs应用于离线RL，并提出了一种新颖的奖励加权微调方法，在短时问答任务上取得了优于现有SOTA方法的性能。该方法为离线RL在LLM领域的应用提供了新的思路，并为未来在该领域的研究奠定了基础。然而，该方法在更长期的任务和更复杂的RL问题上的有效性仍需进一步验证。

Abstract: Offline reinforcement learning (RL) is a variant of RL where the policy is
learned from a previously collected dataset of trajectories and rewards. In our
work, we propose a practical approach to offline RL with large language models
(LLMs). We recast the problem as reward-weighted fine-tuning, which can be
solved using similar techniques to supervised fine-tuning (SFT). To showcase
the value of our approach, we apply it to learning short-horizon
question-answering policies of a fixed length, where the agent reasons about
potential answers or asks clarifying questions. Our work stands in a stark
contrast to state-of-the-art methods in this domain, based on SFT and direct
preference optimization, which have additional hyper-parameters and do not
directly optimize for rewards. We compare to them empirically, and report major
gains in both optimized rewards and language quality.

</details>


### [93] [DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations](https://arxiv.org/abs/2506.09349)
*Chao-Hong Tan,Qian Chen,Wen Wang,Chong Deng,Qinglin Zhang,Luyao Cheng,Hai Yu,Xin Zhang,Xiang Lv,Tianyu Zhao,Chong Zhang,Yukun Ma,Yafeng Chen,Hui Wang,Jiaqing Liu,Xiangang Li,Jieping Ye*

Main category: cs.CL

TL;DR: DrVoice是一个基于联合自回归建模的并行语音-文本对话模型，采用双分辨率语音表示。该模型将LLM的输入音频频率从12.5Hz降低到5Hz，显著降低了计算成本，并解决了语音和文本令牌之间的频率差异，从而更好地利用了LLM的能力。DrVoice-7B在OpenAudioBench和Big Bench Audio基准测试中创下新纪录，并在VoiceBench和UltraEval-Audio基准测试中取得了与SOTA相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音生成模型存在两种主要方法：一是独立生成离散语音令牌，导致文本生成无法感知语音合成；二是通过联合自回归建模生成交错或并行的语音-文本令牌，实现了模态间的相互感知。然而，这些方法在处理语音令牌时可能面临计算成本高和频率差异问题。

Method: DrVoice采用联合自回归建模，实现了语音和文本令牌的并行生成，并引入了双分辨率语音表示。通过将LLM的输入音频频率从常见的12.5Hz降低到5Hz，DrVoice有效降低了计算复杂度，并减小了语音令牌和文本令牌之间的频率差异，从而增强了LLM对语音信息的利用能力。模型实验基于~7B参数规模。

Result: DrVoice-7B在OpenAudioBench和Big Bench Audio基准测试中取得了新的SOTA性能。在VoiceBench和UltraEval-Audio基准测试中，其性能与SOTA相当。

Conclusion: DrVoice通过其创新的双分辨率语音表示和降低的输入频率，在语音生成领域取得了显著的进展，成为一个领先的开源语音基础模型。该模型在多个基准测试中表现出色，证明了其有效性和效率。未来的研究可以进一步探索更低的声音频率表示或更复杂的模型结构，以期在更广泛的语音任务中实现更高的性能。

Abstract: Recent studies on end-to-end (E2E) speech generation with large language
models (LLMs) have attracted significant community attention, with multiple
works extending text-based LLMs to generate discrete speech tokens. Existing
E2E approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents DrVoice, a parallel
speech-text voice conversation model based on joint autoregressive modeling,
featuring dual-resolution speech representations. Notably, while current
methods utilize mainly 12.5Hz input audio representation, our proposed
dual-resolution mechanism reduces the input frequency for the LLM to 5Hz,
significantly reducing computational cost and alleviating the frequency
discrepancy between speech and text tokens and in turn better exploiting LLMs'
capabilities. Experimental results demonstrate that DRVOICE-7B establishes new
state-of-the-art (SOTA) on OpenAudioBench and Big Bench Audio benchmarks, while
achieving performance comparable to the SOTA on VoiceBench and UltraEval-Audio
benchmarks, making it a leading open-source speech foundation model in ~7B
models.

</details>


### [94] [SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture](https://arxiv.org/abs/2506.15355)
*Arijit Maji,Raghvendra Kumar,Akash Ghosh,Anushka,Sriparna Saha*

Main category: cs.CL

TL;DR: 本文介绍了SANSKRITI，一个评估语言模型对印度文化理解能力的基准测试。该测试包含21,853个问答对，涵盖印度28个邦和8个联邦属地的16个文化属性，是目前最大的印度文化知识测试数据集。研究发现，现有的大型语言模型（LLMs）、指示性语言模型（ILMs）和小型语言模型（SLMs）在处理具有文化特异性的查询时存在显著差异，许多模型在区域性语境下表现不佳。SANSKRITI的推出为评估和改进语言模型的文化理解能力设定了新标准。


<details>
  <summary>Details</summary>
Motivation: 全球范围内语言模型的有效性在很大程度上依赖于它们对当地社会文化背景的理解。然而，目前缺乏专门评估语言模型对印度丰富文化多样性理解能力的基准测试。这阻碍了在印度等文化多元化地区有效部署和应用语言模型。

Method: 1. 数据集构建：创建了一个名为SANSKRITI的基准测试，包含21,853个精心策划的问答对。2. 覆盖范围：该数据集涵盖了印度的28个邦和8个联邦属地，并关注了16个关键的印度文化属性，包括：仪式与庆典、历史、旅游、美食、舞蹈与音乐、服饰、语言、艺术、节日、宗教、医药、交通、体育、夜生活和名人。3. 模型评估：在SANSKRITI基准测试上评估了主流的大型语言模型（LLMs）、指示性语言模型（ILMs）和小型语言模型（SLMs）。

Result: 评估结果显示，在处理具有文化特异性的查询时，不同类型的语言模型表现出显著的性能差距。许多模型在理解和回应特定区域的文化语境时遇到了困难，表明它们对印度文化多样性的掌握程度不足。

Conclusion: SANSKRITI是一个大规模、文化丰富且多样化的数据集，它为评估和提升语言模型对印度文化多样性的理解能力提供了新的标准。该基准测试的评估结果揭示了现有模型在文化敏感性方面的不足，为未来语言模型在多元文化环境下的研究和应用指明了方向。

Abstract: Language Models (LMs) are indispensable tools shaping modern workflows, but
their global effectiveness depends on understanding local socio-cultural
contexts. To address this, we introduce SANSKRITI, a benchmark designed to
evaluate language models' comprehension of India's rich cultural diversity.
Comprising 21,853 meticulously curated question-answer pairs spanning 28 states
and 8 union territories, SANSKRITI is the largest dataset for testing Indian
cultural knowledge. It covers sixteen key attributes of Indian culture: rituals
and ceremonies, history, tourism, cuisine, dance and music, costume, language,
art, festivals, religion, medicine, transport, sports, nightlife, and
personalities, providing a comprehensive representation of India's cultural
tapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic
Language Models (ILMs), and Small Language Models (SLMs), revealing significant
disparities in their ability to handle culturally nuanced queries, with many
models struggling in region-specific contexts. By offering an extensive,
culturally rich, and diverse dataset, SANSKRITI sets a new standard for
assessing and improving the cultural understanding of LMs.

</details>


### [95] [AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees](https://arxiv.org/abs/2510.01268)
*Hongyi Zhou,Jin Zhu,Pingfan Su,Kai Ye,Ying Yang,Shakeel A O B Gavioli-Akilagun,Chengchun Shi*

Main category: cs.CL

TL;DR: 该研究提出了一种名为 AdaDetectGPT 的新型文本分类器，用于区分人类和大型语言模型（LLM）生成的文本。与仅依赖对数概率的现有方法不同，AdaDetectGPT 能够自适应地从训练数据中学习“见证函数”，从而提升检测性能，并在各种数据集和 LLM 上实现了高达 37% 的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前区分人类文本与大型语言模型（LLM）生成文本的任务面临挑战。现有的基于对数概率的检测器虽然有效，但其性能可能受到限制，无法充分利用所有可用信息，因此存在提升空间。研究的意义在于提高文本真实性的鉴别能力，这对信息传播的准确性和安全性至关重要。

Method: 研究引入了一种名为 AdaDetectGPT 的新型分类器。该方法的核心在于自适应地学习一个“见证函数”（witness function）。这个函数从训练数据中提取，旨在增强现有的基于对数概率的检测器的性能。研究还对 AdaDetectGPT 的真阳性率、假阳性率、真阴性率和假阴性率提供了统计保证。实验部分进行了广泛的数值研究，以评估其性能。

Result: 通过广泛的数值研究，AdaDetectGPT 在各种不同的数据集和 LLM 组合下，都展现出对现有最先进方法的近乎均匀的改进。性能提升幅度最高可达 37%。这表明 AdaDetectGPT 在提高 LLM 生成文本检测的准确性和鲁棒性方面取得了显著成效。

Conclusion: AdaDetectGPT 是一种有效增强 LLM 生成文本检测器性能的新方法，通过自适应学习见证函数，显著优于现有技术。该研究不仅在理论上提供了性能保证，而且在实践中取得了显著的性能提升。未来可在此基础上进一步探索更复杂的检测机制或应用于更广泛的文本生成场景。附带的 Python 实现也方便了该方法的推广和应用。

Abstract: We study the problem of determining whether a piece of text has been authored
by a human or by a large language model (LLM). Existing state of the art
logits-based detectors make use of statistics derived from the log-probability
of the observed text evaluated using the distribution function of a given
source LLM. However, relying solely on log probabilities can be sub-optimal. In
response, we introduce AdaDetectGPT -- a novel classifier that adaptively
learns a witness function from training data to enhance the performance of
logits-based detectors. We provide statistical guarantees on its true positive
rate, false positive rate, true negative rate and false negative rate.
Extensive numerical studies show AdaDetectGPT nearly uniformly improves the
state-of-the-art method in various combination of datasets and LLMs, and the
improvement can reach up to 37\%. A python implementation of our method is
available at https://github.com/Mamba413/AdaDetectGPT.

</details>


### [96] [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
*Duncan Soiffer,Steven Kolawole,Virginia Smith*

Main category: cs.CL

TL;DR: 本文提出了一种名为“语义级联”的新方法，用于在大型语言模型（LLM）部署中平衡成本和质量。该方法通过一种称为“语义共识”的训练无关信号来判断输出的可靠性，当不同模型生成的输在语义上达成一致时，就认为输出是可靠的。实验证明，语义级联在成本降低 40% 和延迟减少 60% 的同时，能够达到或超过目标模型的质量。该方法适用于黑盒 API，且不依赖模型内部信息，是一种实用的 LLM 部署解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的级联系统虽然可以通过将计算请求路由到更小的模型来降低成本，但在开放式文本生成任务中存在一个根本性问题：如何确定输出的可靠性，因为生成质量通常处于一个连续的光谱上，并且可能存在多个有效响应。这使得在成本和质量之间取得平衡变得困难。

Method: 本文提出了一种名为“语义共识”的训练无关信号，用于判断输出的可靠性。当多个模型生成的输出在语义层面上达成一致时，这种共识就被视为一个可靠的信号。该方法不依赖于模型内部的任何信息，可以直接应用于黑盒 API，并且能够适应模型更新。

Result: 在 5 亿到 700 亿参数的模型上进行的评估显示，语义级联在保证模型质量的同时，成本降低了 40%，延迟减少了高达 60%。

Conclusion: 语义级联通过利用“语义共识”作为一种训练无关的可靠性信号，有效地解决了开放式文本生成中级联系统的挑战。该方法无需访问模型内部，可用于黑盒 API，并对模型更新具有鲁棒性，为大规模语言模型的实际部署提供了一个实用且高效的基准。

Abstract: Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.

</details>


### [97] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: 本文提出了SEER基准测试，用于评估大型语言模型（LLM）识别文本中表达情感的具体文本片段（span）的能力。该基准测试包含两个任务：在单句内识别情感证据，以及在包含五个连续句子的短文段中识别证据。研究评估了14个开源LLM，发现模型在单句输入上表现尚可，但在处理长文本时准确率下降。错误分析揭示了模型过度依赖情感关键词和在非情感文本中产生误报等问题。


<details>
  <summary>Details</summary>
Motivation: 传统的情感识别任务通常将单一标签分配给整个句子，但未能捕捉情感的具体表达方式。然而，在需要理解情感表达细节的应用（如共情对话和临床支持）中，识别出传达情感的确切短语至关重要。因此，本文旨在解决情感证据检测这一未被充分探索的任务，以填补现有研究的空白。

Method: 本文引入了SEER（Span-based Emotion Evidence Retrieval）基准测试。该基准测试包含对1200个真实世界句子进行的情感和情感证据的新标注。SEER包含两个子任务：1. 在单句内部识别情感证据；2. 在包含五个连续句子的短文段中识别情感证据。研究评估了14个开源LLM在SEER基准测试上的表现。

Result: 在SEER基准测试上，一些模型在单句输入上的表现接近平均人类水平。然而，模型在处理包含五个连续句子的较长文本段时，准确率明显下降。错误分析表明，模型倾向于过度依赖情感关键词，并在中性文本中产生误报，这表明模型在理解上下文和细微情感表达方面仍存在挑战。

Conclusion: SEER基准测试的提出为评估LLM在情感证据检测方面的能力提供了一个新的平台。研究结果表明，尽管LLM在识别单句中的情感证据方面取得了一定进展，但在更复杂的长文本场景下仍需改进。未来的工作可以集中在开发能够更好地理解上下文、减少对关键词依赖并提高在非情感文本中区分能力的模型。SplashScreen

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [98] [GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training](https://arxiv.org/abs/2509.24494)
*Hongcheng Wang,Yinuo Huang,Sukai Wang,Guanghui Ren,Hao Dong*

Main category: cs.CL

TL;DR: GRPO-MA通过生成每个思考过程的多个答案来解决GRPO算法在训练LLMs和VLMs进行CoT推理时遇到的梯度耦合、稀疏奖励和不稳定的优势估计问题。理论和实验均表明，增加每个思考过程的答案数量可以减少优势估计的方差，降低梯度峰值，从而提高训练效率和模型在数学、代码及多模态任务上的性能。


<details>
  <summary>Details</summary>
Motivation: GRPO算法在利用强化学习训练大型语言模型（LLMs）和视觉语言模型（VLMs）进行思维链（CoT）推理方面取得了进展，但存在梯度耦合、奖励稀疏和优势估计不稳定等挑战。这些挑战限制了GRPO的有效性和效率。因此，有必要研究一种新的方法来克服这些限制，以更有效地训练CoT推理。

Method: 本文提出了GRPO-MA，一种利用每个思考过程生成多个答案的方法，来解决GRPO面临的挑战。理论上，证明了增加每个思考过程的答案数量可以减小其优势估计的方差。实验上，通过梯度分析验证了GRPO-MA可以减少梯度峰值。在数学、代码和多模态任务上进行了实验，并将GRPO-MA与GRPO进行了比较。此外，还进行了消融研究，以评估增加每个思考过程的答案数量对模型性能的影响。

Result: 实验结果表明，GRPO-MA在降低梯度峰值方面优于GRPO。在数学、代码和多模态任务上的实验结果显示，GRPO-MA能够显著提升模型性能和训练效率。消融研究进一步证实，增加每个思考过程的答案数量能够持续提高模型性能。

Conclusion: GRPO-MA通过生成多答案来有效解决GRPO在CoT推理训练中的挑战，提高了训练稳定性和效率，并在多项任务上取得了性能提升。该方法不仅在理论上得到支持，在实践中也得到了验证。未来的工作可以进一步探索多答案生成策略的优化和在更广泛任务上的应用。

Abstract: Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a
Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought
(CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models
(VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling
between thoughts and answers, sparse reward signals caused by limited parallel
sampling, and unstable advantage estimation. To mitigate these challenges, we
propose GRPO-MA, a simple yet theoretically grounded method that leverages
multi-answer generation from each thought process, enabling more robust and
efficient optimization. Theoretically, we show that the variance of thought
advantage decreases as the number of answers per thought increases.
Empirically, our gradient analysis confirms this effect, showing that GRPO-MA
reduces gradient spikes compared to GRPO. Experiments on math, code, and
diverse multimodal tasks demonstrate that GRPO-MA substantially improves
performance and training efficiency. Our ablation studies further reveal that
increasing the number of answers per thought consistently enhances model
performance.

</details>


### [99] [TokenTiming: A Dynamic Alignment Method for Universal Speculative Decoding Model Pairs](https://arxiv.org/abs/2510.15545)
*Sibo Xiao,Jinyuan Fu,Zhongle Xie,Lidan Shou*

Main category: cs.CL

TL;DR: 本文提出了一种名为TokenTiming的通用投机解码算法，通过动态时间规整（DTW）技术解决了现有投机解码算法中草稿模型和目标模型必须共享相同词汇的限制，实现了1.57倍的加速，并允许使用任何现成的模型而无需重新训练或修改。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的推理加速是生成式AI中的一个关键挑战。投机解码（SD）是一种有效提升LLM推理效率的方法，但其应用受到草稿模型和目标模型必须共享相同词汇的限制，这不仅限制了可用草稿模型的选择范围，还常常需要从头开始训练新模型。

Method: 提出了一种名为TokenTiming的通用投机解码算法，该算法借鉴了动态时间规整（DTW）的思想。TokenTiming通过重新编码草稿模型的词元序列来生成新的目标词元序列，并利用DTW建立映射关系，从而将概率分布转移以进行投机采样。这种方法能够处理词汇不匹配的问题，并适用于任何现成的模型，无需进行再训练或修改。在实验中，该方法在多种任务上进行了广泛的测试。

Result: 实验证明，TokenTiming算法能够实现1.57倍的加速效果。

Conclusion: TokenTiming算法为草稿模型的选择提供了一种通用的方法，克服了现有投机解码算法的词汇限制，使得投机解码成为一个更通用、更实用的LLM加速工具。该研究为LLM推理效率的提升开辟了新的可能性。

Abstract: Accelerating the inference of large language models (LLMs) has been a
critical challenge in generative AI. Speculative decoding (SD) substantially
improves LLM inference efficiency. However, its utility is limited by a
fundamental constraint: the draft and target models must share the same
vocabulary, thus limiting the herd of available draft models and often
necessitating the training of a new model from scratch. Inspired by Dynamic
Time Warping (DTW), a classic algorithm for aligning time series, we propose
the algorithm TokenTiming for universal speculative decoding. It operates by
re-encoding the draft token sequence to get a new target token sequence, and
then uses DTW to build a mapping to transfer the probability distributions for
speculative sampling. Benefiting from this, our method accommodates mismatched
vocabularies and works with any off-the-shelf models without retraining and
modification. We conduct comprehensive experiments on various tasks,
demonstrating 1.57x speedup. This work enables a universal approach for draft
model selection, making SD a more versatile and practical tool for LLM
acceleration.

</details>


### [100] [The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents' Inquiry Capability](https://arxiv.org/abs/2509.24958)
*Linlu Gong,Ante Wang,Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 该研究提出了MAQuE，一个包含3000个模拟患者的医疗多轮问诊评估基准，并引入了一个多维度评估框架，旨在全面评估AI医生的问诊能力，发现现有大型语言模型在模拟真实患者交互和诊断准确性方面仍有显著提升空间，并揭示了不同评估维度间的权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医学诊断方面取得了进展，但同理心、耐心和清晰沟通等医生应具备的其他重要品质仍被忽视。现有AI医生在主动获取信息方面有所欠缺，无法全面模拟一名优秀医生。因此，有必要开发一个能够全面评估AI医生在多轮问诊中表现的基准，以弥补这一差距，并推动AI在医疗领域更全面、更人性化的发展。

Method: 研究者构建了MAQuE（Medical Agent Questioning Evaluation）基准，包含3000个模拟患者，这些患者具有多样的语言模式、认知局限、情绪反应和被动披露倾向。他们还设计了一个多方面评估框架，涵盖任务成功率、问诊熟练度、对话能力、问诊效率和患者体验。在不同的LLM上进行了实验，以评估其在这些方面的表现。

Result: 实验结果表明，即使是先进的大型语言模型在问诊能力方面也面临巨大挑战。模型在处理真实患者行为变化时表现出高度敏感性，这显著影响了诊断准确性。精细化指标的分析还揭示了不同评估视角之间的权衡，凸显了在现实临床环境中平衡性能和实用性的难度。

Conclusion: MAQuE基准和评估框架为全面评估AI医生的多轮问诊能力提供了重要工具。研究发现，现有LLM在模拟真实患者交互和保持诊断准确性方面仍存在显著不足，特别是在处理患者行为的多样性和情绪时。未来的研究需要关注如何提高AI医生在这些方面的表现，以及如何在不同评估指标之间找到最佳平衡点，以实现更可靠、更人性化的AI医疗助手。

Abstract: An effective physician should possess a combination of empathy, expertise,
patience, and clear communication when treating a patient. Recent advances have
successfully endowed AI doctors with expert diagnostic skills, particularly the
ability to actively seek information through inquiry. However, other essential
qualities of a good doctor remain overlooked. To bridge this gap, we present
MAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the
automatic and comprehensive evaluation of medical multi-turn questioning. It
features 3,000 realistically simulated patient agents that exhibit diverse
linguistic patterns, cognitive limitations, emotional responses, and tendencies
for passive disclosure. We also introduce a multi-faceted evaluation framework,
covering task success, inquiry proficiency, dialogue competence, inquiry
efficiency, and patient experience. Experiments on different LLMs reveal
substantial challenges across the evaluation aspects. Even state-of-the-art
models show significant room for improvement in their inquiry capabilities.
These models are highly sensitive to variations in realistic patient behavior,
which considerably impacts diagnostic accuracy. Furthermore, our fine-grained
metrics expose trade-offs between different evaluation perspectives,
highlighting the challenge of balancing performance and practicality in
real-world clinical settings.

</details>


### [101] [LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora](https://arxiv.org/abs/2510.10114)
*Luyao Zhuang,Shengyuan Chen,Yilin Xiao,Huachi Zhou,Yujing Zhang,Hao Chen,Qinggang Zhang,Xiao Huang*

Main category: cs.CL

TL;DR: LinearRAG是一种新的图增强检索生成（GraphRAG）框架，通过构建关系无关的层次图（Tri-Graph）并采用两阶段检索策略，解决了现有GraphRAG方法在处理大规模非结构化语料库时遇到的图构建不稳定和检索质量下降的问题。实验表明，LinearRAG在多个数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）系统在处理大规模、非结构化语料库时，特别是当信息分散时，会遇到困难。虽然知识图谱可以捕捉实体间的关系以改进检索，但现有的图增强RAG（GraphRAG）方法依赖于不稳定且成本高昂的关系抽取来构建图谱，这会导致图谱噪声大、关系错误，从而降低检索质量。因此，有必要开发一种更稳定、高效且可扩展的GraphRAG方法。

Method: LinearRAG提出了一种新颖的图谱构建方法，即构建一个关系无关的层次图，称为Tri-Graph。该方法仅使用轻量级的实体提取和语义链接，避免了不稳定的关系建模。这种图谱构建方法可以随语料库大小线性扩展，并且不增加额外的token消耗。在检索方面，LinearRAG采用两阶段策略：首先通过局部语义桥接激活相关实体，然后通过全局重要性聚合进行文档检索。

Result: 在四个数据集上的广泛实验表明，LinearRAG显著优于现有的基线模型。具体性能提升和指标对比未在摘要中详细说明，但强调了其优越性。

Conclusion: LinearRAG通过构建关系无关的Tri-Graph和采用两阶段检索策略，成功解决了现有GraphRAG方法的局限性，实现了稳定、高效且可扩展的图谱构建和精确的文档检索。该方法在多个数据集上取得了优于基线模型的性能，为处理大规模非结构化数据提供了新的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) is widely used to mitigate
hallucinations of Large Language Models (LLMs) by leveraging external
knowledge. While effective for simple queries, traditional RAG systems struggle
with large-scale, unstructured corpora where information is fragmented. Recent
advances incorporate knowledge graphs to capture relational structures,
enabling more comprehensive retrieval for complex, multi-hop reasoning tasks.
However, existing graph-based RAG (GraphRAG) methods rely on unstable and
costly relation extraction for graph construction, often producing noisy graphs
with incorrect or inconsistent relations that degrade retrieval quality. In
this paper, we revisit the pipeline of existing GraphRAG systems and propose
LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient
framework that enables reliable graph construction and precise passage
retrieval. Specifically, LinearRAG constructs a relation-free hierarchical
graph, termed Tri-Graph, using only lightweight entity extraction and semantic
linking, avoiding unstable relation modeling. This new paradigm of graph
construction scales linearly with corpus size and incurs no extra token
consumption, providing an economical and reliable indexing of the original
passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant
entity activation via local semantic bridging, followed by (ii) passage
retrieval through global importance aggregation. Extensive experiments on four
datasets demonstrate that LinearRAG significantly outperforms baseline models.
Our code and datasets are available at https://github.com/DEEP-PolyU/LinearRAG.

</details>


### [102] [Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays](https://arxiv.org/abs/2510.22830)
*Haowei Hua,Hong Jiao,Xinyi Wang*

Main category: cs.CL

TL;DR: BERT等模型在长文书自动评分方面存在512个词元的限制，本研究采用生成式语言模型，通过文本摘要和提示技术，提高了评分准确率，QWK从0.822提升至0.8878。


<details>
  <summary>Details</summary>
Motivation: 现有的基于BERT等编码器模型的自动评分方法在处理长篇论文时存在512个词元的限制，这限制了其在长文书评分方面的应用。因此，研究长文书的自动评分方法具有重要意义。

Method: 本研究探索使用生成式语言模型，结合文本摘要和提示技术，来解决长文书自动评分的限制。具体实验采用了Learning Agency Lab Automated Essay Scoring 2.0数据集。

Result: 通过使用生成式语言模型，评分准确率得到显著提升，在Learning Agency Lab Automated Essay Scoring 2.0数据集上的QWK评分从0.822提高到0.8878。

Conclusion: 生成式语言模型通过文本摘要和提示技术，能够有效地克服BERT等模型在长文书自动评分中的词元限制，显著提高评分准确性。未来的研究可以进一步探索不同的生成模型和提示策略，并尝试应用于更多样化的长文书评分任务。

Abstract: BERT and its variants are extensively explored for automated scoring.
However, a limit of 512 tokens for these encoder-based models showed the
deficiency in automated scoring of long essays. Thus, this research explores
generative language models for automated scoring of long essays via
summarization and prompting. The results revealed great improvement of scoring
accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab
Automated Essay Scoring 2.0 dataset.

</details>


### [103] [MATCH: Task-Driven Code Evaluation through Contrastive Learning](https://arxiv.org/abs/2510.23169)
*Marah Ghoummaid,Vladimir Tchuiev,Ofek Glick,Michal Moshkovitz,Dotan Di Castro*

Main category: cs.CL

TL;DR: GitHub Copilot等AI工具生成的代码占比较高，但评估其是否符合开发者意图仍是挑战。现有评估方法（如单元测试、BLEU、ROUGE、CodeBERTScore）存在可扩展性差、成本高、无法捕捉代码功能或需要参考代码等问题。本文提出了一种名为MATCH的新型无参考代码评估指标，使用对比学习生成代码和自然语言任务描述的嵌入，从而能够评估生成代码与任务的契合度。研究表明，MATCH在多种编程语言中，与功能正确性和人类偏好的相关性均优于现有指标。


<details>
  <summary>Details</summary>
Motivation: AI代码生成日益普及（如GitHub Copilot估计生成46%的代码），但准确评估生成代码是否符合开发者意图面临严峻挑战。传统的单元测试方法成本高、可扩展性差；BLEU、ROUGE等语法相似性指标无法衡量代码功能；CodeBERTScore等指标则需要参考代码，并非总是可用。因此，在缺乏参考代码的情况下，需要一种有效的评估方法，以填补现有评估手段的不足。

Method: 本文提出了一种名为MATCH的新型无参考评估指标。MATCH利用对比学习（Contrastive Learning）技术，为代码和自然语言任务描述生成有意义的嵌入（embeddings）。通过计算这些嵌入之间的相似度，MATCH能够量化生成代码在多大程度上实现了给定的任务描述。这种方法旨在提供一种可扩展且无需参考代码的代码评估方案。

Result: 通过在多种编程语言上的实验，研究表明MATCH指标在评估生成代码的功能正确性和人类偏好方面，表现出了比现有指标（包括一些无参考指标如ICE-Score）更强的相关性。这证明了MATCH在衡量AI生成代码质量方面的有效性和优越性。

Conclusion: 本文提出的MATCH指标为AI生成代码提供了一种新颖且有效的无参考评估方法。MATCH通过对比学习生成代码和自然语言描述的嵌入，能够更好地反映代码的功能性和开发者意图，克服了传统评估方法的局限性。未来可在此基础上进一步探索和优化，以适应更广泛的代码生成和评估场景。

Abstract: AI-based code generation is increasingly prevalent, with GitHub Copilot
estimated to generate 46% of the code on GitHub. Accurately evaluating how well
generated code aligns with developer intent remains a critical challenge.
Traditional evaluation methods, such as unit tests, are often unscalable and
costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code
functionality, and metrics like CodeBERTScore require reference code, which is
not always available. To address the gap in reference-free evaluation, with few
alternatives such as ICE-Score, this paper introduces MATCH, a novel
reference-free metric. MATCH uses Contrastive Learning to generate meaningful
embeddings for code and natural language task descriptions, enabling similarity
scoring that reflects how well generated code implements the task. We show that
MATCH achieves stronger correlations with functional correctness and human
preference than existing metrics across multiple programming languages.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [104] [Human Machine Social Hybrid Intelligence:A Collaborative Decision Making Framework for Large Model Agent Groups and Human Experts](https://arxiv.org/abs/2510.24030)
*Ahmet Akkaya Melih,Yamuna Singh,Kunal L. Agarwal,Priya Mukherjee,Kiran Pattnaik,Hanuman Bhatia*

Main category: cs.MA

TL;DR: 大型基础模型和多智能体系统的进步带来了前所未有的能力，但现有的人机协作（HiTL）范式未能充分整合人类专业知识，导致在高风险复杂环境中出现认知过载和决策瓶颈。我们提出了“人机社会混合智能”（HMS-HI）框架，这是一种新颖的架构，用于人类专家和由LLM驱动的AI智能体群体之间的深度协作决策。HMS-HI建立在三个核心支柱之上：（1）用于统一、多模态态势感知和结构化世界建模的共享认知空间（SCS）；（2）一个动态角色和任务分配（DRTA）模块，根据能力和工作负载自适应地将任务分配给最合适的智能体（人类或AI）；（3）一个跨物种信任校准（CSTC）协议，通过可解释的声明和结构化反馈促进透明度、问责制和相互适应。在高度仿真的城市应急响应模拟中，HMS-HI与传统的HiTL方法相比，将平民伤亡减少了72%，认知负荷降低了70%，并在决策质量、效率和人机信任方面表现出优越性。单独研究证实了每个模块的关键贡献，并强调工程信任和共享上下文是可扩展、协同的人机协作的基础。


<details>
  <summary>Details</summary>
Motivation: 当前的人机协作（HiTL）范式在整合人类专业知识方面存在不足，在高风险复杂环境中，这会导致认知过载和决策瓶颈。随着大型基础模型和多智能体系统的快速发展，有必要探索更深层次的人机协作模式。

Method: 提出了一种名为“人机社会混合智能”（HMS-HI）的新颖框架，该框架包含三个核心组件：（1）共享认知空间（SCS），用于统一的多模态态势感知和结构化世界建模；（2）动态角色和任务分配（DRTA）模块，能够根据能力和工作负载自适应地分配任务；（3）跨物种信任校准（CSTC）协议，通过可解释的声明和结构化反馈来增强透明度、问责制和相互适应。该框架在城市应急响应模拟中进行了验证。

Result: 在城市应急响应模拟中，HMS-HI框架与传统的HiTL方法相比，显著减少了72%的平民伤亡和70%的认知负荷。此外，在决策质量、效率和人机信任方面也表现出优越性。单独研究证实了SCS、DRTA和CSTC模块各自的关键作用，并强调了工程信任和共享上下文对于可扩展、协同的人机协作的重要性。

Conclusion: HMS-HI框架通过整合共享认知空间、动态任务分配和信任校准机制，实现了人机之间更深层次的协作决策，显著提高了在高风险环境下的决策质量和效率，并降低了认知负荷。该研究强调了在复杂的人机协作中建立信任和共享上下文的基础性作用，为未来人机协同智能系统的发展提供了有价值的见解。

Abstract: The rapid advancements in large foundation models and multi-agent systems
offer unprecedented capabilities, yet current Human-in-the-Loop (HiTL)
paradigms inadequately integrate human expertise, often leading to cognitive
overload and decision-making bottlenecks in complex, high-stakes environments.
We propose the "Human-Machine Social Hybrid Intelligence" (HMS-HI) framework, a
novel architecture designed for deep, collaborative decision-making between
groups of human experts and LLM-powered AI agents. HMS-HI is built upon three
core pillars: (1) a \textbf{Shared Cognitive Space (SCS)} for unified,
multi-modal situational awareness and structured world modeling; (2) a
\textbf{Dynamic Role and Task Allocation (DRTA)} module that adaptively assigns
tasks to the most suitable agent (human or AI) based on capabilities and
workload; and (3) a \textbf{Cross-Species Trust Calibration (CSTC)} protocol
that fosters transparency, accountability, and mutual adaptation through
explainable declarations and structured feedback. Validated in a high-fidelity
urban emergency response simulation, HMS-HI significantly reduced civilian
casualties by 72\% and cognitive load by 70\% compared to traditional HiTL
approaches, demonstrating superior decision quality, efficiency, and human-AI
trust. An ablation study confirms the critical contribution of each module,
highlighting that engineered trust and shared context are foundational for
scalable, synergistic human-AI collaboration.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [105] [AI and the Decentering of Disciplinary Creativity](https://arxiv.org/abs/2510.23734)
*Eamon Duede*

Main category: cs.AI

TL;DR: 本文研究了人工智能在科学问题解决中的作用，特别是对学科创造力的影响。研究区分了创造性方法和创造性产品，并引入了“学科创造力”的概念，即在特定领域内创造性地运用学科专业知识来解决有价值的问题。通过两个数学案例研究，本文表明计算可以扩展学科创造力，但某些涉及人工智能的方法可能会取代它，从而可能改变甚至降低科学追求的价值。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨人工智能在科学问题解决过程中对“学科创造力”的潜在影响。研究者关注人工智能是会增强还是削弱科学家运用其专业知识解决领域内难题的能力，并认为理解这一点对于评估科学追求的未来价值至关重要。

Method: 本文采用了基于哲学理论和案例研究的方法。研究者首先区分了创造性方法和创造性产品，并提出了“学科创造力”的概念。随后，通过对数学领域两个具体案例的分析，探讨了人工智能在这些案例中的作用，以论证其对学科创造力的影响。

Result: 研究表明，虽然计算方法可以扩展学科创造力，但某些涉及人工智能的方法可能导致学科创造力的“位移”。这种位移可能会对科学追求的价值产生负面影响，甚至可能降低其价值。

Conclusion: 本文认为，人工智能在科学问题解决中的应用具有双重性。一方面，它可以作为工具扩展人类的创造力；另一方面，某些人工智能方法的应用可能会取代人类的创造性劳动，从而对科学研究本身的价值和意义带来挑战。未来的研究需要进一步探索如何平衡人工智能与人类创造力在科学研究中的关系。

Abstract: This paper examines the role of artificial intelligence in scientific
problem-solving, with a focus on its implications for disciplinary creativity.
Drawing on recent work in the philosophy of creativity, I distinguish between
creative approaches and creative products, and introduce the concept of
disciplinary creativity -the creative application of discipline-specific
expertise to a valued problem within that field. Through two cases in
mathematics, I show that while computation can extend disciplinary creativity,
certain approaches involving AI can serve to displace it. This displacement has
the potential to alter (and, perhaps, diminish) the value of scientific
pursuit.

</details>


### [106] [Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability](https://arxiv.org/abs/2510.23744)
*Eline M. Bovy,Caleb Probine,Marnix Suilen,Ufuk Topcu,Nils Jansen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete
model uncertainty. ME-POMDPs represent a finite set of POMDPs that share the
same state, action, and observation spaces, but may arbitrarily vary in their
transition, observation, and reward models. Such models arise, for instance,
when multiple domain experts disagree on how to model a problem. The goal is to
find a single policy that is robust against any choice of POMDP within the set,
i.e., a policy that maximizes the worst-case reward across all POMDPs. We
generalize and expand on existing work in the following way. First, we show
that ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which
we call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any
arbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its
transition and reward functions or only in its observation and reward
functions, while preserving (optimal) policies. We then devise exact and
approximate (point-based) algorithms to compute robust policies for AB-POMDPs,
and thus ME-POMDPs. We demonstrate that we can compute policies for standard
POMDP benchmarks extended to the multi-environment setting.

</details>


### [107] [Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents](https://arxiv.org/abs/2510.24383)
*Juraj Mavračić*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Policy Cards are introduced as a machine-readable, deployment-layer standard
for expressing operational, regulatory, and ethical constraints for AI agents.
The Policy Card sits with the agent and enables it to follow required
constraints at runtime. It tells the agent what it must and must not do. As
such, it becomes an integral part of the deployed agent. Policy Cards extend
existing transparency artifacts such as Model, Data, and System Cards by
defining a normative layer that encodes allow/deny rules, obligations,
evidentiary requirements, and crosswalk mappings to assurance frameworks
including NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can
be validated automatically, version-controlled, and linked to runtime
enforcement or continuous-audit pipelines. The framework enables verifiable
compliance for autonomous agents, forming a foundation for distributed
assurance in multi-agent ecosystems. Policy Cards provide a practical mechanism
for integrating high-level governance with hands-on engineering practice and
enabling accountable autonomy at scale.

</details>


### [108] [Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra](https://arxiv.org/abs/2510.23746)
*Laura Mismetti,Marvin Alberts,Andreas Krause,Mara Graziani*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Tandem Mass Spectrometry enables the identification of unknown compounds in
crucial fields such as metabolomics, natural product discovery and
environmental analysis. However, current methods rely on database matching from
previously observed molecules, or on multi-step pipelines that require
intermediate fragment or fingerprint prediction. This makes finding the correct
molecule highly challenging, particularly for compounds absent from reference
databases. We introduce a framework that, by leveraging test-time tuning,
enhances the learning of a pre-trained transformer model to address this gap,
enabling end-to-end de novo molecular structure generation directly from the
tandem mass spectra and molecular formulae, bypassing manual annotations and
intermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on
two popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.
Test-time tuning on experimental spectra allows the model to dynamically adapt
to novel spectra, and the relative performance gain over conventional
fine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground
truth, the generated molecular candidates remain structurally accurate,
providing valuable guidance for human interpretation and more reliable
identification.

</details>


### [109] [Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions](https://arxiv.org/abs/2510.23772)
*Vivek Veeriah,Federico Barbero,Marcus Chiam,Xidong Feng,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Johan Obando-Ceron,Jiaxin Shi,Shaobo Hou,Satinder Singh,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 本研究探讨了生成式AI在国际象棋谜题生成方面的创造力，开发了一个能够生成具有美学吸引力、新颖性、反直觉和独特解法的AI系统。通过邀请三位国际象棋大师对AI生成的谜题进行评估，旨在量化AI的创造力。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的飞速发展，其生成创意和新颖内容的能力引发了广泛关注。本研究聚焦于国际象棋谜题生成领域，旨在探索AI在这一具有高度创造性要求的任务上的表现，并评估其生成具有美学价值和新颖性的谜题的能力。

Method: 本研究开发了一个AI系统，用于生成具有特定品质（如美学吸引力、新颖性、反直觉和独特解法）的国际象棋谜题。研究者邀请了三位国际象棋领域的专家（国际大师Amatzia Avni、特级大师 Jonathan Levitt 和特级大师 Matthew Sadler）评估AI生成的谜题。专家们被要求选出他们最喜欢的谜题，并阐述其吸引力所在，评估维度包括创造力、挑战性和美学设计等方面。

Result: 三位国际象棋专家对AI生成的谜题进行了评估。虽然具体量化指标未在摘要中详细说明，但评估结果表明专家们能够识别并欣赏AI生成的谜题中的创造力、挑战性和美学设计。专家们的反馈为衡量AI在国际象棋谜题生成方面的创造力提供了定性依据。

Conclusion: 本研究展示了AI在生成具有创造性和美学吸引力的国际象棋谜题方面的潜力。通过引入专家评估机制，为量化AI的创造力提供了一种途径。未来的工作可以进一步探索更复杂的谜题生成，并开发更精细的评估指标来全面衡量AI的创造力。

Abstract: The rapid advancement of Generative AI has raised significant questions
regarding its ability to produce creative and novel outputs. Our recent work
investigates this question within the domain of chess puzzles and presents an
AI system designed to generate puzzles characterized by aesthetic appeal,
novelty, counter-intuitive and unique solutions. We briefly discuss our method
below and refer the reader to the technical paper for more details. To assess
our system's creativity, we presented a curated booklet of AI-generated puzzles
to three world-renowned experts: International Master for chess compositions
Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All
three are noted authors on chess aesthetics and the evolving role of computers
in the game. They were asked to select their favorites and explain what made
them appealing, considering qualities such as their creativity, level of
challenge, or aesthetic design.

</details>


### [110] [Why Foundation Models in Pathology Are Failing](https://arxiv.org/abs/2510.23807)
*Hamid R. Tizhoosh*

Main category: cs.AI

TL;DR: 虽然在非医疗领域取得了巨大成功，但目前计算病理学领域的基础模型（FMs）在癌症诊断、预后和多模态检索方面存在重大缺陷，包括诊断准确性低、鲁棒性差、几何不稳定性、计算需求高和安全漏洞。本文深入探讨了这些问题，并认为其根源在于主流AI中的通用FM假设与人体组织内在复杂性之间的概念不匹配。文章确定了七个相互关联的原因：生物复杂性、无效的自监督学习、过度泛化、过度的架构复杂性、缺乏领域特定创新、数据不足以及与组织斑块尺寸相关的基本设计缺陷。这些发现表明，当前病理学FM在概念上未能与组织形态学的本质保持一致，需要对该范式进行根本性反思。


<details>
  <summary>Details</summary>
Motivation: 在非医疗领域，基础模型（FMs）通过大规模自监督和多模态学习在计算机视觉和语言处理方面取得了革命性进展。因此，人们期望它们在计算病理学中的快速应用能够推动癌症诊断、预后和多模态检索方面的突破。然而，实际应用揭示了FMs在计算病理学中存在显著的局限性，这阻碍了其在癌症诊断等关键领域的有效应用，因此，理解并解决这些局限性至关重要，以充分发挥AI在病理学中的潜力。

Method: 本文采用系统性评估和深入分析的方法，考察了计算病理学领域现有基础模型（FMs）的缺点。通过识别模型在诊断准确性、鲁棒性、几何稳定性、计算需求和安全性方面存在的根本性弱点，并追溯这些问题与主流AI通用FM假设与人体组织复杂性之间概念不匹配的关联。文章进一步详细阐述了七个具体原因：生物复杂性、无效的自监督学习、过度泛化、过度的架构复杂性、缺乏领域特定创新、数据不足以及组织斑块尺寸的设计缺陷。

Result: 系统性评估揭示了当前计算病理学领域基础模型（FMs）存在一系列根本性问题，包括诊断准确性低、鲁棒性差、几何不稳定性、计算需求高以及安全漏洞等。这些问题阻碍了FMs在癌症诊断、预后和多模态检索等方面的有效应用。

Conclusion: 本文的结论是，当前计算病理学领域的基础模型（FMs）在概念上未能与组织形态学的内在复杂性相匹配，导致了模型在准确性、鲁棒性和安全性方面存在严重缺陷。文章指出的七个相互关联的原因，包括生物复杂性、无效的自监督学习、过度泛化、架构复杂性、缺乏领域创新、数据不足和斑块尺寸问题，都强调了当前范式的不适应性。因此，需要对基础模型在病理学中的应用范式进行根本性的反思和重新设计，以开发出真正适用于该领域的有效模型。

Abstract: In non-medical domains, foundation models (FMs) have revolutionized computer
vision and language processing through large-scale self-supervised and
multimodal learning. Consequently, their rapid adoption in computational
pathology was expected to deliver comparable breakthroughs in cancer diagnosis,
prognostication, and multimodal retrieval. However, recent systematic
evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor
robustness, geometric instability, heavy computational demands, and concerning
safety vulnerabilities. This short paper examines these shortcomings and argues
that they stem from deeper conceptual mismatches between the assumptions
underlying generic foundation modeling in mainstream AI and the intrinsic
complexity of human tissue. Seven interrelated causes are identified:
biological complexity, ineffective self-supervision, overgeneralization,
excessive architectural complexity, lack of domain-specific innovation,
insufficient data, and a fundamental design flaw related to tissue patch size.
These findings suggest that current pathology foundation models remain
conceptually misaligned with the nature of tissue morphology and call for a
fundamental rethinking of the paradigm itself.

</details>


### [111] [ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents](https://arxiv.org/abs/2510.23822)
*Zhenyu Zhang,Tianyi Chen,Weiran Xu,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: 长时任务对大型语言模型（LLMs）来说是一个挑战，现有方法存在上下文漂移和信息丢失等问题。本文提出的 ReCAP 框架通过递归上下文感知推理和规划，结合“预先规划分解”、“结构化重注入父计划”和“内存高效执行”机制，有效解决了这些问题，显著提高了模型在长时推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 长时任务要求多步推理和动态重规划，这对大型语言模型（LLMs）构成了严峻挑战。现有的顺序提示方法容易出现上下文漂移、目标信息丢失和循环失败等问题。而分层提示方法虽然可以解决部分问题，但常常削弱跨层级连续性，或导致显著的运行时开销。因此，研究一种能够有效处理长时任务，同时克服现有方法局限性的新框架具有重要意义。

Method: 本文提出 ReCAP（Recursive Context-Aware Reasoning and Planning）框架，这是一种分层框架，具有共享的推理和规划上下文。ReCAP 结合了三个关键机制：（1）预先规划分解：模型生成完整的子任务列表，执行第一项任务，然后优化剩余任务。（2）结构化重注入父计划：在递归返回过程中，保持多层级上下文的一致性。（3）内存高效执行：限制活动提示的长度，使成本与任务深度呈线性关系。这些机制共同作用，将高层目标与低层动作对齐，减少冗余提示，并保持递归过程中连贯的上下文更新。

Result: 实验结果表明，ReCAP 在各种长时推理基准测试中显著提高了子目标对齐和成功率。在严格的 pass@1 协议下，ReCAP 在同步 Robotouille 任务上取得了 32% 的提升，在异步 Robotouille 任务上取得了 29% 的改进。

Conclusion: ReCAP 框架通过结合预先规划分解、结构化重注入父计划和内存高效执行等机制，成功解决了大型语言模型在处理长时任务时面临的上下文漂移、信息丢失和效率低下等问题。实验结果证明了 ReCAP 在提高子目标对齐和任务成功率方面的有效性，为 LLMs 在复杂推理任务上的应用提供了新的解决方案。未来的工作可以进一步探索 ReCAP 在更多样化的长时任务和更复杂的环境中的应用，并优化其效率和可扩展性。

Abstract: Long-horizon tasks requiring multi-step reasoning and dynamic re-planning
remain challenging for large language models (LLMs). Sequential prompting
methods are prone to context drift, loss of goal information, and recurrent
failure cycles, while hierarchical prompting methods often weaken cross-level
continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive
Context-Aware Reasoning and Planning), a hierarchical framework with shared
context for reasoning and planning in LLMs. ReCAP combines three key
mechanisms: (i) plan-ahead decomposition, in which the model generates a full
subtask list, executes the first item, and refines the remainder; (ii)
structured re-injection of parent plans, maintaining consistent multi-level
context during recursive return; and (iii) memory-efficient execution, bounding
the active prompt so costs scale linearly with task depth. Together these
mechanisms align high-level goals with low-level actions, reduce redundant
prompting, and preserve coherent context updates across recursion. Experiments
demonstrate that ReCAP substantially improves subgoal alignment and success
rates on various long-horizon reasoning benchmarks, achieving a 32% gain on
synchronous Robotouille and a 29% improvement on asynchronous Robotouille under
the strict pass@1 protocol.

</details>


### [112] [Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models](https://arxiv.org/abs/2510.23824)
*Murad Ismayilov,Edwin Meriaux,Shuo Wen,Gregory Dudek*

Main category: cs.AI

TL;DR: 本文提出了一种利用大型语言模型（LLM）进行去中心化多智能体路径规划中目标分配的方法，并与传统启发式方法和最优分配进行了比较。结果表明，LLM 在设计良好的提示和相关定量信息的支持下，可以实现接近最优的完成时间和性能。


<details>
  <summary>Details</summary>
Motivation: 在共享环境中协调多个自主智能体，特别是在去中心化条件下，是机器人学和人工智能领域一个长期存在的挑战。具体而言，如何有效地为多智能体路径规划进行去中心化目标分配是一个关键问题，因为这直接影响到任务的整体效率和完成时间。

Method: 该研究提出了一种去中心化目标分配方法。智能体首先根据环境的结构化表示（包括网格可视化和场景数据）独立生成对目标的排序偏好。然后，智能体交换其目标排名，并根据固定的、确定的冲突解决规则（例如，智能体索引排序）来确定分配，无需协商或迭代协调。在完全可观的网格世界环境中，研究人员系统地比较了贪婪启发式算法、最优分配和基于大型语言模型（LLM）的智能体。

Result: 在完全可观的网格世界设置中，LLM 驱动的智能体在经过精心设计的提示和相关定量信息的支持下，取得了接近最优的完成时间，并且其表现持续优于传统的启发式方法。这项比较研究了贪婪启发式、最优分配和 LLM 智能体。

Conclusion: 研究结果表明，大型语言模型在去中心化多智能体路径规划的目标分配方面具有巨大潜力，并且信息的结构化对于此类系统的性能至关重要。LLM 智能体在接近最优的完成时间和一致性方面优于传统方法。

Abstract: Coordinating multiple autonomous agents in shared environments under
decentralized conditions is a long-standing challenge in robotics and
artificial intelligence. This work addresses the problem of decentralized goal
assignment for multi-agent path planning, where agents independently generate
ranked preferences over goals based on structured representations of the
environment, including grid visualizations and scenario data. After this
reasoning phase, agents exchange their goal rankings, and assignments are
determined by a fixed, deterministic conflict-resolution rule (e.g., agent
index ordering), without negotiation or iterative coordination. We
systematically compare greedy heuristics, optimal assignment, and large
language model (LLM)-based agents in fully observable grid-world settings. Our
results show that LLM-based agents, when provided with well-designed prompts
and relevant quantitative information, can achieve near-optimal makespans and
consistently outperform traditional heuristics. These findings underscore the
potential of language models for decentralized goal assignment in multi-agent
path planning and highlight the importance of information structure in such
systems.

</details>


### [113] [Generating Creative Chess Puzzles](https://arxiv.org/abs/2510.23881)
*Xidong Feng,Vivek Veeriah,Marcus Chiam,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Federico Barbero,Johan Obando-Ceron,Jiaxin Shi,Satinder Singh,Shaobo Hou,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While Generative AI rapidly advances in various domains, generating truly
creative, aesthetic, and counter-intuitive outputs remains a challenge. This
paper presents an approach to tackle these difficulties in the domain of chess
puzzles. We start by benchmarking Generative AI architectures, and then
introduce an RL framework with novel rewards based on chess engine search
statistics to overcome some of those shortcomings. The rewards are designed to
enhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.
Our RL approach dramatically increases counter-intuitive puzzle generation by
10x, from 0.22\% (supervised) to 2.5\%, surpassing existing dataset rates
(2.1\%) and the best Lichess-trained model (0.4\%). Our puzzles meet novelty
and diversity benchmarks, retain aesthetic themes, and are rated by human
experts as more creative, enjoyable, and counter-intuitive than composed book
puzzles, even approaching classic compositions. Our final outcome is a curated
booklet of these AI-generated puzzles, which is acknowledged for creativity by
three world-renowned experts.

</details>


### [114] [Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins](https://arxiv.org/abs/2510.23882)
*Adil Rasheed,Oscar Ravik,Omer San*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work investigates the use of digital twins for dynamical system modeling
and control, integrating physics-based, data-driven, and hybrid approaches with
both traditional and AI-driven controllers. Using a miniature greenhouse as a
test platform, four predictive models Linear, Physics-Based Modeling (PBM),
Long Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are
developed and compared under interpolation and extrapolation scenarios. Three
control strategies Model Predictive Control (MPC), Reinforcement Learning (RL),
and Large Language Model (LLM) based control are also implemented to assess
trade-offs in precision, adaptability, and implementation effort. Results show
that in modeling HAM provides the most balanced performance across accuracy,
generalization, and computational efficiency, while LSTM achieves high
precision at greater resource cost. Among controllers, MPC delivers robust and
predictable performance, RL demonstrates strong adaptability, and LLM-based
controllers offer flexible human-AI interaction when coupled with predictive
tools.

</details>


### [115] [Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges](https://arxiv.org/abs/2510.23883)
*Shrestha Datta,Shahriar Kabir Nahin,Anshuman Chhabra,Prasant Mohapatra*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with
planning, tool use, memory, and autonomy, are emerging as powerful, flexible
platforms for automation. Their ability to autonomously execute tasks across
web, software, and physical environments creates new and amplified security
risks, distinct from both traditional AI safety and conventional software
security. This survey outlines a taxonomy of threats specific to agentic AI,
reviews recent benchmarks and evaluation methodologies, and discusses defense
strategies from both technical and governance perspectives. We synthesize
current research and highlight open challenges, aiming to support the
development of secure-by-design agent systems.

</details>


### [116] [Latent Chain-of-Thought for Visual Reasoning](https://arxiv.org/abs/2510.23925)
*Guohao Sun,Hang Hua,Jian Wang,Jiebo Luo,Sohail Dianat,Majid Rabbani,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.AI

TL;DR: 本文提出了一种基于变分推理的链式思考（CoT）训练算法，用于解决现有方法在视觉语言模型（LVLM）推理任务上的泛化能力不足和奖励模型偏差问题。该方法将推理重构为后验推理，利用多样性寻求的强化学习算法引入稀疏奖励函数，鼓励生成多样化的高似然度潜 CoT，克服了确定性采样和奖励hacking的限制。此外，还采用贝叶斯推理-缩放策略替代了计算成本高的 Best-of-N 和 Beam Search，以高效地评估最优推理路径和答案。实验证明，该方法在七个推理基准测试中显著提升了 LVLM 的有效性、泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的链式思考（CoT）训练算法，如SFT、PPO和GRPO，在处理未知的推理任务时泛化能力不足，并且严重依赖有偏见的奖励模型。这限制了大型视觉语言模型（LVLM）在提高可解释性和可靠性方面的潜力。因此，需要一种更鲁棒、更通用的训练方法来解决这些问题。

Method: 本文将LVLM的推理过程重构为后验推理问题，并提出了一种基于加速变分推理（amortized variational inference）的可扩展训练算法。具体来说，引入了多样性寻求的强化学习算法，设计了一种新颖的稀疏奖励函数，用于提供token级别的学习信号，以鼓励生成多样化且高似然度的潜在CoT，从而克服确定性采样和奖励hacking的局限性。同时，采用贝叶斯推理-缩放策略，用边际似然（marginal likelihood）替代了计算成本高的Best-of-N和Beam Search方法，以高效地对最优推理路径和答案进行排序。

Result: 在七个推理基准测试上的实验表明，所提出的方法在有效性、泛化性和可解释性方面均优于现有最先进的LVLM。具体性能提升和与现有方法的对比数据将在论文的实验部分详细呈现。

Conclusion: 本文提出的基于变分推理的CoT训练新方法，有效解决了现有LVLM训练算法的泛化能力不足和奖励模型偏差问题，显著提升了模型在各项推理任务上的表现。该方法通过新颖的奖励函数和推理策略，实现了更优的有效性、泛化性和可解释性，为未来LVLM的研究和应用提供了新的方向。未来工作可以探索该方法在更广泛任务上的应用，以及进一步优化模型结构和训练过程。

Abstract: Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.

</details>


### [117] [Decentralized Causal Discovery using Judo Calculus](https://arxiv.org/abs/2510.23942)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 提出了一种基于judo演算的直觉主义去中心化因果发现框架，并给出了其在应用中的计算效率和性能提升。Judo演算将因果效应的依赖性形式化为局部真理，并通过j-稳定性在相关模型上保持一致性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的因果效应因各种因素（如年龄、国家、剂量、基因型或实验方案）而异。现有的因果发现方法未能充分解决这种情境依赖性，导致在实际应用中效果不佳。

Method: 提出并形式化了judo演算，该演算将因果发现中的情境依赖性表述为局部真理。利用Lawvere-Tierney模态算子j选择相关模型，并通过j-稳定性确保因果主张在所选模型家族中具有建设性和一致性。结合了基于分数、基于约束和基于梯度等标准因果发现方法，并实现了一个算法和实现框架。

Result: 实验结果表明，基于sheaf理论的去中心化因果发现方法在计算效率上有所提升，并且在合成和真实世界的数据集（包括生物学和经济学领域）上，其性能优于经典的因果发现方法。

Conclusion: Judo演算提供了一个形式化因果发现情境依赖性的新框架，并能通过去中心化的sheaf理论实现高效且高性能的因果发现。未来的工作可以进一步探索该框架在更广泛领域的应用以及与其他因果推断方法的结合。

Abstract: We describe a theory and implementation of an intuitionistic decentralized
framework for causal discovery using judo calculus, which is formally defined
as j-stable causal inference using j-do-calculus in a topos of sheaves. In
real-world applications -- from biology to medicine and social science --
causal effects depend on regime (age, country, dose, genotype, or lab
protocol). Our proposed judo calculus formalizes this context dependence
formally as local truth: a causal claim is proven true on a cover of regimes,
not everywhere at once. The Lawvere-Tierney modal operator j chooses which
regimes are relevant; j-stability means the claim holds constructively and
consistently across that family. We describe an algorithmic and implementation
framework for judo calculus, combining it with standard score-based,
constraint-based, and gradient-based causal discovery methods. We describe
experimental results on a range of domains, from synthetic to real-world
datasets from biology and economics. Our experimental results show the
computational efficiency gained by the decentralized nature of sheaf-theoretic
causal discovery, as well as improved performance over classical causal
discovery methods.

</details>


### [118] [The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity](https://arxiv.org/abs/2510.23965)
*Aymane El Gadarri,Ali Aouad,Vivek F. Farias*

Main category: cs.AI

TL;DR: 提出了新方法“符号估计器”，通过在聚合步骤中用二元分类损失替换交叉熵，解决了传统LLM对齐方法在处理人类偏好异质性时存在的不一致性问题，该方法简单、一致且高效，实现了首个多项式有限样本误差界限，并在模拟实验中显著减少了偏好失真，降低了估计误差和与真实人口偏好的不一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型（LLM）对齐方法在面对人类偏好异质性时容易出现不一致性问题。直接拟合概率模型到成对比较数据（如提示-完成对）会导致对人口平均效用（社会福利的典型衡量标准）的估计不一致，这限制了LLM对齐的有效性。

Method: 提出了一种名为“符号估计器”的新方法，通过在聚合步骤中用二元分类损失替换交叉熵来实现。这种方法旨在提供一个简单、可证明一致且高效的估计器。在模拟LLM对齐的实验中，使用了数字孪生来模拟不同个体的偏好。

Result: 符号估计器在模拟实验中，相比标准的基于强化学习的人类反馈（RLHF）方法，显著减少了模拟人物面板的偏好失真，将角度估计误差降低了近35%，并将与真实人口偏好的一致性从12%降低到8%。该方法在与显式建模用户异质性和需要跟踪个体层面偏好数据的面板数据启发式方法相比时，也表现出优势，同时保持了现有LLM对齐流水线的实现简单性。

Conclusion: 符号估计器是一种有效且简单的LLM对齐方法，能够解决人类偏好异质性带来的挑战，并在模拟实验中展现出优于现有方法的性能。该方法在减少偏好失真、提高估计一致性以及简化实现方面具有重要意义，为未来LLM的对齐研究提供了新的方向。

Abstract: Traditional LLM alignment methods are vulnerable to heterogeneity in human
preferences. Fitting a na\"ive probabilistic model to pairwise comparison data
(say over prompt-completion pairs) yields an inconsistent estimate of the
population-average utility -a canonical measure of social welfare. We propose a
new method, dubbed the sign estimator, that provides a simple, provably
consistent, and efficient estimator by replacing cross-entropy with binary
classification loss in the aggregation step. This simple modification recovers
consistent ordinal alignment under mild assumptions and achieves the first
polynomial finite-sample error bounds in this setting. In realistic simulations
of LLM alignment using digital twins, the sign estimator substantially reduces
preference distortion over a panel of simulated personas, cutting (angular)
estimation error by nearly 35% and decreasing disagreement with true population
preferences from 12% to 8% compared to standard RLHF. Our method also compares
favorably to panel data heuristics that explicitly model user heterogeneity and
require tracking individual-level preference data-all while maintaining the
implementation simplicity of existing LLM alignment pipelines.

</details>


### [119] [Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance](https://arxiv.org/abs/2510.23989)
*Shangde Gao,Zelin Xu,Zhe Jiang*

Main category: cs.AI

TL;DR: 本研究提出了一种结合个体社会基础设施韧性（SIR）和空间上下文的深度学习模型，用于预测灾难性事件后个体移动模式的变化。实验证明，该模型能有效提高预测精度，并能区分具有相似事前移动模式但SIR不同的个体。


<details>
  <summary>Details</summary>
Motivation: 预测灾难性事件后个体移动模式的变化对于了解社区资源需求的变化至关重要，但现有方法面临挑战：1. 缺乏衡量个体异质性SIR的指标，且常用特征（如社会人口特征）难以大规模获取。2. 未充分捕捉个体移动模式与空间上下文的复杂交互。3. 个体移动数据稀疏，不适合传统预测方法。因此，本研究旨在解决这些问题，以提高灾难性事件后个体移动模式预测的准确性。

Method: 本研究提出了一种条件深度学习模型，该模型将个体的SIR纳入考量，以捕捉个体移动模式与局部空间上下文之间的复杂关系。该模型能够处理大规模、稀疏的个体移动数据。实验中，通过对比分析，验证了引入SIR和空间上下文对预测精度的提升效果，并展示了模型区分具有相似事前移动模式但SIR不同的个体的影响。

Result: 实验结果表明，将个体SIR和空间上下文信息融入模型显著提高了对事件后个体移动模式的预测能力。该条件模型能够捕捉到那些事前移动模式相似但SIR不同的个体所表现出的不同移动模式变化。

Conclusion: 本研究成功开发并验证了一种结合SIR和空间上下文的深度学习模型，用于预测事件后个体移动模式的变化。研究强调了SIR在个体移动决策中的重要性，并为更精准的资源需求预测和应急响应提供了新的方法。未来的工作可以进一步探索更丰富的SIR指标和更复杂的模型结构，以应对更广泛的场景和数据。

Abstract: Shifts in individual movement patterns following disruptive events can reveal
changing demands for community resources. However, predicting such shifts
before disruptive events remains challenging for several reasons. First,
measures are lacking for individuals' heterogeneous social infrastructure
resilience (SIR), which directly influences their movement patterns, and
commonly used features are often limited or unavailable at scale, e.g.,
sociodemographic characteristics. Second, the complex interactions between
individual movement patterns and spatial contexts have not been sufficiently
captured. Third, individual-level movement may be spatially sparse and not
well-suited to traditional decision-making methods for movement predictions.
This study incorporates individuals' SIR into a conditioned deep learning model
to capture the complex relationships between individual movement patterns and
local spatial context using large-scale, sparse individual-level data. Our
experiments demonstrate that incorporating individuals' SIR and spatial context
can enhance the model's ability to predict post-event individual movement
patterns. The conditioned model can capture the divergent shifts in movement
patterns among individuals who exhibit similar pre-event patterns but differ in
SIR.

</details>


### [120] [OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting](https://arxiv.org/abs/2510.24028)
*Tingyue Pan,Mingyue Cheng,Shilong Zhang,Zhiding Liu,Xiaoyu Tao,Yucong Luo,Jintao Zhang,Qi Liu*

Main category: cs.AI

TL;DR: OneCast是一个创新的跨域时间序列预测框架，通过将时间序列分解为季节性和趋势性成分，并分别建模，解决了现有方法在处理领域特定趋势变化和不一致周期性模式方面的局限性。实验证明，OneCast在多个领域上均优于现有最先进的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有跨域时间序列预测方法在处理异构时间序列数据时，尤其是在面对领域特定的趋势变化和不一致的周期性模式时，泛化能力不足。其根本原因在于将时间序列视为未区分的序列，未能显式地分离出其内在的结构成分。

Method: OneCast框架将时间序列分解为季节性和趋势性成分。季节性成分通过一个轻量级的投影模块，利用可解释的基函数来重建周期性模式。趋势性成分则通过一个语义感知的分词器在分段层面编码为离散标记，随后通过掩码离散扩散机制进行推理。最后，将两个分支的输出结合起来，生成包含季节性模式并能跟踪领域特定趋势的预测结果。

Result: 在八个领域的广泛实验表明，OneCast在大多数情况下都优于现有的最先进基线模型，证明了其在跨域时间序列预测任务上的有效性。

Conclusion: OneCast通过结构化和模块化的方法，成功地将时间序列分解为季节性和趋势性成分，并采用不同的生成路径进行建模，有效解决了跨域时间序列预测中的泛化难题，并在实验中取得了优于现有方法的性能。

Abstract: Cross-domain time series forecasting is a valuable task in various web
applications. Despite its rapid advancement, achieving effective generalization
across heterogeneous time series data remains a significant challenge. Existing
methods have made progress by extending single-domain models, yet often fall
short when facing domain-specific trend shifts and inconsistent periodic
patterns. We argue that a key limitation lies in treating temporal series as
undifferentiated sequence, without explicitly decoupling their inherent
structural components. To address this, we propose OneCast, a structured and
modular forecasting framework that decomposes time series into seasonal and
trend components, each modeled through tailored generative pathways.
Specifically, the seasonal component is captured by a lightweight projection
module that reconstructs periodic patterns via interpretable basis functions.
In parallel, the trend component is encoded into discrete tokens at segment
level via a semantic-aware tokenizer, and subsequently inferred through a
masked discrete diffusion mechanism. The outputs from both branches are
combined to produce a final forecast that captures seasonal patterns while
tracking domain-specific trends. Extensive experiments across eight domains
demonstrate that OneCast mostly outperforms state-of-the-art baselines.

</details>


### [121] [Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach](https://arxiv.org/abs/2510.24085)
*Md. Shihab Uddin,Md Nazmus Shakib,Rahul Bhadani*

Main category: cs.AI

TL;DR: 研究比较了经典模型和随机森林回归模型在电动汽车（EV）跟车行为模拟方面的性能。结果表明，随机森林模型在所有场景下均表现出优越的准确性，其均方根误差（RMSE）显著低于经典模型，特别是在中、长、超长间隙条件下。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车（EV）的日益普及，理解其驾驶行为对于提升交通安全和开发智能驾驶系统至关重要。本研究旨在对比分析经典模型和机器学习模型在模拟EV跟车行为方面的表现，为相关领域的研究提供参考。

Method: 本研究对比了包括IDM、OVM、OVRV和简化CACC模型在内的经典模型，以及采用随机森林回归（Random Forest Regressor）的机器学习模型。研究使用了真实世界的EV跟随内燃机（ICE）车辆在不同驾驶条件下的数据集。经典模型的参数通过最小化预测值与真实数据之间的均方根误差（RMSE）进行校准。随机森林模型以间隙、速度和间隙类型作为输入，预测加速度。

Result: 研究结果显示，随机森林模型在模拟EV跟车行为方面具有更高的准确性。在不同间隙条件下，其RMSE值分别为：中等间隙0.0046，长间隙0.0016，超长间隙0.0025。在经典模型中，CACC模型表现最佳，在长间隙条件下的RMSE为2.67。总体而言，机器学习模型在所有场景下均优于经典模型。

Conclusion: 本研究证实了机器学习模型（特别是随机森林回归）在模拟电动汽车跟车行为方面的优越性能。这些模型在各种驾驶条件下均展现出更高的准确性，为EV行为模拟和未来在集成EV的环境中分析混合交通流的动态提供了有价值的工具。未来的研究可以进一步探索更复杂的机器学习模型或集成学习方法，以应对更广泛的交通场景和更精细化的驾驶行为分析。

Abstract: The increasing adoption of electric vehicles (EVs) necessitates an
understanding of their driving behavior to enhance traffic safety and develop
smart driving systems. This study compares classical and machine learning
models for EV car following behavior. Classical models include the Intelligent
Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative
Velocity (OVRV), and a simplified CACC model, while the machine learning
approach employs a Random Forest Regressor. Using a real world dataset of an EV
following an internal combustion engine (ICE) vehicle under varied driving
conditions, we calibrated classical model parameters by minimizing the RMSE
between predictions and real data. The Random Forest model predicts
acceleration using spacing, speed, and gap type as inputs. Results demonstrate
the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),
0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,
CACC performed best, with an RMSE of 2.67 for long gaps. These findings
highlight the machine learning model's performance across all scenarios. Such
models are valuable for simulating EV behavior and analyzing mixed autonomy
traffic dynamics in EV integrated environments.

</details>


### [122] [HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology](https://arxiv.org/abs/2510.24115)
*Sandeep Vissapragada,Vikrant Sahu,Gagan Raj Gupta,Vandita Singh*

Main category: cs.AI

TL;DR: HistoLens是一个透明的、可解释的人工智能系统，它允许病理学家用自然语言提问关于组织切片的问题，并获得结构化的报告和可视化的证据，从而增强了病理学家的诊断信心和效率。


<details>
  <summary>Details</summary>
Motivation: 目前的AI在医学诊断领域被视为“黑箱”，缺乏透明度和可解释性，这阻碍了医生对其的信任和广泛应用。病理学家需要一个能够像同事一样解释其推理过程的AI工具，以辅助诊断决策。

Method: HistoLens系统允许病理学家用自然语言提问，系统将其转化为对AI引擎的精确查询。AI引擎生成结构化报告，并能提供“视觉证据”（如热图）来解释其发现。系统还被训练以忽略背景噪音，专注于患者组织。

Result: HistoLens提供了一个增强病理学家能力的工作流程，使他们能够获得可信赖的AI助手，以验证见解，做出更快、更自信的诊断。

Conclusion: HistoLens通过提供透明度、可解释性和可视化证据，解决了AI在医学诊断中的信任问题，使病理学家能够更有效地利用AI辅助诊断。

Abstract: For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.

</details>


### [123] [From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems](https://arxiv.org/abs/2510.24145)
*Yu Luo,Jiamin Jiang,Jingfei Feng,Lei Tao,Qingliang Zhang,Xidao Wen,Yongqian Sun,Shenglin Zhang,Jielong Huang,Nan Qi,Dan Pei*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Incident management (IM) is central to the reliability of large-scale cloud
systems. Yet manual IM, where on-call engineers examine metrics, logs, and
traces is labor-intensive and error-prone in the face of massive and
heterogeneous observability data. Existing automated IM approaches often
struggle to generalize across systems, provide limited interpretability, and
incur high deployment costs, which hinders adoption in practice. In this paper,
we present OpsAgent, a lightweight, self-evolving multi-agent system for IM
that employs a training-free data processor to convert heterogeneous
observability data into structured textual descriptions, along with a
multi-agent collaboration framework that makes diagnostic inference transparent
and auditable. To support continual capability growth, OpsAgent also introduces
a dual self-evolution mechanism that integrates internal model updates with
external experience accumulation, thereby closing the deployment loop.
Comprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art
performance and show that OpsAgent is generalizable, interpretable,
cost-efficient, and self-evolving, making it a practically deployable and
sustainable solution for long-term operation in real-world cloud systems.

</details>


### [124] [BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data](https://arxiv.org/abs/2510.24151)
*Bingsen Qiu,Zijian Liu,Xiao Liu,Haoshen Yang,Zeren Gao,Bingjie Wang,Feier Zhang,Yixuan Qin,Chunyan Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Building training-ready multi-hop question answering (QA) datasets that truly
stress a model's retrieval and reasoning abilities remains highly challenging
recently. While there have been a few recent evaluation datasets that capture
the characteristics of hard-to-search but easy-to-verify problems -- requiring
the integration of ambiguous, indirect, and cross-domain cues -- these data
resources remain scarce and are mostly designed for evaluation, making them
unsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).
Meanwhile, manually curating non-trivially retrievable questions -- where
answers cannot be found through a single direct query but instead require
multi-hop reasoning over oblique and loosely connected evidence -- incurs
prohibitive human costs and fails to scale, creating a critical data bottleneck
for training high-capability retrieval-and-reasoning agents.
  To address this, we present an automated framework for generating
high-difficulty, training-ready multi-hop questions from semi-structured
knowledge sources. The system (i) grows diverse, logically labeled evidence
clusters through Natural Language Inference (NLI)-based relation typing and
diversity-aware expansion; (ii) applies reverse question construction to
compose oblique cues so that isolated signals are underinformative but their
combination uniquely identifies the target entity; and (iii) enforces quality
with a two-step evaluation pipeline that combines multi-model consensus
filtering with structured constraint decomposition and evidence-based matching.
The result is a scalable process that yields complex, retrieval-resistant yet
verifiable questions suitable for SFT/RL training as well as challenging
evaluation, substantially reducing human curation effort while preserving the
difficulty profile of strong evaluation benchmarks.

</details>


### [125] [BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning](https://arxiv.org/abs/2510.24161)
*Wentao Tan,Bowen Wang,Heng Zhi,Chenyu Liu,Zhe Li,Jian Liu,Zengrong Lin,Yukun Dai,Yipeng Chen,Wenjie Yang,Enci Xie,Hao Xue,Baixu Ji,Chen Xu,Zhibin Wang,Tianshi Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal large language models (MLLMs) have advanced vision-language
reasoning and are increasingly deployed in embodied agents. However,
significant limitations remain: MLLMs generalize poorly across digital-physical
spaces and embodiments; vision-language-action models (VLAs) produce low-level
actions yet lack robust high-level embodied reasoning; and most embodied large
language models (ELLMs) are constrained to digital-space with poor
generalization to the physical world. Thus, unified models that operate
seamlessly across digital and physical spaces while generalizing across
embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
instruction following and reasoning, incorporates embodied knowledge, and
supports robust cross-embodiment control. BLM$_1$ integrates three key
capabilities -- \textit{cross-space transfer, cross-task learning, and
cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
injects embodied knowledge into the MLLM through curated digital corpora while
maintaining language competence. Stage II trains a policy module through an
intent-bridging interface that extracts high-level semantics from the MLLM to
guide control, without fine-tuning the MLLM backbone. This process is supported
by a self-collected cross-embodiment demonstration suite spanning four robot
embodiments and six progressively challenging tasks. Evaluations across digital
and physical benchmarks show that a single BLM$_1$ instance outperforms four
model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical
tasks.

</details>


### [126] [UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration](https://arxiv.org/abs/2510.24166)
*Xin Yang,Yuhang Zhang,Wei Li,Xin Lin,Wenbin Zou,Chen Xu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Motion planning is a critical component of autonomous vehicle decision-making
systems, directly determining trajectory safety and driving efficiency. While
deep learning approaches have advanced planning capabilities, existing methods
remain confined to single-dataset training, limiting their robustness in
planning.
  Through systematic analysis, we discover that vehicular trajectory
distributions and history-future correlations demonstrate remarkable
consistency across different datasets. Based on these findings, we propose
UniPlanner, the first planning framework designed for multi-dataset integration
in autonomous vehicle decision-making. UniPlanner achieves unified
cross-dataset learning through three synergistic innovations.
  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates
history-future trajectory pairs from multiple datasets, using historical
trajectory similarity to retrieve relevant futures and generate cross-dataset
planning guidance.
  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust
history-future correlations from multiple datasets, transforming historical
trajectories into universal planning priors. Its gradient-free design ensures
the introduction of valuable priors while preventing shortcut learning, making
the planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)
paradigm implements adaptive dropout to selectively suppress planning priors
during training for robust learning, while enabling full prior utilization
during inference to maximize planning performance.

</details>


### [127] [MGA: Memory-Driven GUI Agent for Observation-Centric Interaction](https://arxiv.org/abs/2510.24168)
*Weihua Cheng,Ersheng Ni,Wenlong Wang,Yifei Sun,Junming Liu,Wangyu Shen,Yirong Chen,Botian Shi,Ding Wang*

Main category: cs.AI

TL;DR: LLM驱动的GUI智能体MGA，通过“先观察后决策”范式克服了现有方法的局限性，显著提高了在OSworld、桌面应用和跨任务迁移中的鲁棒性、泛化性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体在处理复杂界面和保持泛化性方面存在挑战，主要受历史轨迹依赖（加剧误差传播）和局部探索偏差（忽略关键界面线索）的限制。

Method: MGA将GUI交互重新定义为“先观察后决策”的范式。每个交互步骤被建模为一个独立的、富含上下文的环境状态，该状态由三个部分组成：当前屏幕截图、任务无关的空间信息以及动态更新的结构化记忆。

Result: 在OSworld基准、Chrome、VSCode、VLC等真实桌面应用以及跨任务迁移实验中，MGA相较于最先进的基线方法，在鲁棒性、泛化性和效率方面取得了显著的提升。

Conclusion: MGA通过引入“先观察后决策”的新范式，有效解决了现有GUI智能体的关键挑战，并在多个基准和真实应用中展现出优越性能，为开发更强大的GUI智能体提供了新方向。

Abstract: The rapid progress of Large Language Models (LLMs) and their multimodal
extensions (MLLMs) has enabled agentic systems capable of perceiving and acting
across diverse environments. A challenging yet impactful frontier is the
development of GUI agents, which must navigate complex desktop and web
interfaces while maintaining robustness and generalization. Existing paradigms
typically model tasks as long-chain executions, concatenating historical
trajectories into the context. While approaches such as Mirage and GTA1 refine
planning or introduce multi-branch action selection, they remain constrained by
two persistent issues: Dependence on historical trajectories, which amplifies
error propagation. And Local exploration bias, where "decision-first,
observation-later" mechanisms overlook critical interface cues. We introduce
the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the
principle of observe first, then decide. MGA models each step as an
independent, context-rich environment state represented by a triad: current
screenshot, task-agnostic spatial information, and a dynamically updated
structured memory. Experiments on OSworld benchmarks, real desktop applications
(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves
substantial gains in robustness, generalization, and efficiency compared to
state-of-the-art baselines. The code is publicly available at:
{https://anonymous.4open.science/r/MGA-3571}.

</details>


### [128] [Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms](https://arxiv.org/abs/2510.24297)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: MCTS的样本效率低是其弱点，可以通过构建和使用状态/动作抽象来解决，以共享层内节点信息。现有抽象算法（如pruned OGA）在处理同一抽象节点内的多个动作时，会遇到同质U C B值问题，并隐式采用随机的tiebreak规则。本文提出了几种替代的抽象内策略，并在大多数环境和参数设置下优于随机策略。


<details>
  <summary>Details</summary>
Motivation: MCTS的主要弱点是样本效率低。虽然使用状态和/或动作抽象可以提高信息共享和UCB值，但现有方法在处理同一抽象节点内的多个动作时存在问题，导致UCB值相同，需要tiebreak规则。pruned OGA等算法未注意到此问题，并默认采用随机tiebreak，这可能不是最优选择。

Method: 本文提出并实证评估了几种不同的抽象内策略，用以替代MCTS中由抽象节点引起的随机tiebreak规则。具体方法是在同一抽象节点内的多个动作之间引入更优化的策略，以改进UCB值的计算和选择。

Result: 与随机tiebreak策略相比，本文提出的几种替代抽象内策略在大多数环境和参数设置下均表现更优。

Conclusion: 本文提出的替代抽象内策略在解决MCTS抽象节点内的多动作同质UCB值问题上，优于现有的随机tiebreak方法。这为提高MCTS的样本效率和性能提供了新的方向，但仍需在更多样化的环境和参数设置下进行验证。

Abstract: One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which
can be addressed by building and using state and/or action abstractions in
parallel to the tree search such that information can be shared among nodes of
the same layer. The primary usage of abstractions for MCTS is to enhance the
Upper Confidence Bound (UCB) value during the tree policy by aggregating visits
and returns of an abstract node. However, this direct usage of abstractions
does not take the case into account where multiple actions with the same parent
might be in the same abstract node, as these would then all have the same UCB
value, thus requiring a tiebreak rule. In state-of-the-art abstraction
algorithms such as pruned On the Go Abstractions (pruned OGA), this case has
not been noticed, and a random tiebreak rule was implicitly chosen. In this
paper, we propose and empirically evaluate several alternative
intra-abstraction policies, several of which outperform the random policy
across a majority of environments and parameter settings.

</details>


### [129] [Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank](https://arxiv.org/abs/2510.24299)
*Jiayu Liu,Wei Dai,Zhenya Huang,Ning Miao,Enhong Chen*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）虽然推理能力强大，但容易出错和产生幻觉。现有检查方法依赖外部资源，计算开销大且适用性有限。本研究提出一种“自指示器”（Self-Indicator）方法，通过分析输入问题与输出推理路径之间相关矩阵的秩来评估LLM输出的可信度。该方法仅依赖LLM自身，无需额外模型或复杂提示，可插入现有模型，显著提升性能且计算开销极小。实验表明，该方法能以超过75%的准确率区分正确和错误的推理路径，并在三个推理基准测试中提升准确率超过8%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成内容时虽然展现出强大的推理能力，但其输出结果常常包含错误和不准确的信息（幻觉）。因此，如何有效且高效地验证LLM的输出成为了一个关键的研究问题，尤其是在其广泛应用场景下。当前已有的验证方法大多依赖外部资源，例如需要额外训练的验证模型（如过程/结果奖励模型）或设计复杂的提示词工程。这些方法不仅带来了高昂的计算成本，而且往往只适用于特定的应用领域，通用性较差。

Method: 本研究的核心思想是探索LLM自身的内部行为是否能指示其推理过程的可信度。研究者发现，输入问题与输出推理路径之间相关矩阵的秩（rank）是衡量推理正确性的一个可靠指标。基于这一发现，研究者设计了一种名为“自指示器”（Self-Indicator）的方法。该方法是一种即插即用的（plug-and-play）技术，它利用计算相关矩阵的方法来重新加权（reweight）候选的推理路径。这种方法的关键优势在于，其计算过程仅依赖于LLM自身，避免了训练额外模型的麻烦，也无需设计复杂的提示词。

Result: 通过在多种不同规模和模型系列的LLM上的实验验证，“自指示器”方法展现了其有效性。实验结果表明，该方法在区分正确和错误的推理路径时，准确率超过了75%。更重要的是，在三个不同的推理基准测试中，应用“自指示器”方法后，LLM的整体准确率得到了超过8%的提升。与现有的其他投票（voting）和验证（verification）方法相比，“自指示器”在取得显著性能改进的同时，计算开销却非常小。

Conclusion: 本研究成功提出了一种新颖的“自指示器”方法，它利用LLM内部行为（相关矩阵的秩）来评估推理路径的可信度。该方法有效解决了现有LLM输出验证方法计算开销大、适用性受限的问题。通过仅依赖LLM自身，实现了高效、通用且即插即用的输出验证，并在多个实验中证明了其有效性，显著提升了LLM在推理任务上的准确率。未来的工作可以进一步探索该方法的泛化能力，以及与其他验证技术的结合潜力。

Abstract: Despite the strong reasoning ability of large language models~(LLMs), they
are prone to errors and hallucinations. As a result, how to check their outputs
effectively and efficiently has become a critical problem in their
applications. Existing checking methods heavily rely on external resources,
such as trained verifiers (e.g., process/outcome reward models) or elaborate
prompts, which lead to high computational overhead and are only applicable to
specific domains. In this paper, we investigate whether the internal behaviors
of LLMs have already implied the credibility of their reasoning paths.
Specifically, we find that the rank of the correlation matrix between the input
problem and the output reasoning path is a robust indicator of reasoning
correctness. Different from other correctness indicators for LLMs, the
calculation of the correlation matrix only relies on the LLM itself, which
avoids the hassle of training a separate model or designing complicated
prompts. Based on it, we design a simple, plug-and-play Self-Indicator method
to reweight candidate reasoning paths, which achieves significant performance
improvements than other voting and verification methods with very few
computational overhead. Our experiments across multiple LLMs of varying scales
and model families have further shown the effectiveness of Self-Indicator. It
achieves over 75% accuracy in distinguishing correct reasoning paths from
incorrect ones, and, in turn, improves the accuracies on three reasoning
benchmarks by more than 8%.

</details>


### [130] [Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting](https://arxiv.org/abs/2510.24303)
*Deniz Gorur,Antoni Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本研究提出了一种新的基于多智能体框架的判断式预测方法，该方法利用量化双极论证框架（QBAFs）来验证关于未来事件的预测。通过结合不同智能体（包括ArgLLM、RbAM和RAG-ArgLLM）的证据，该框架能够提高预测准确性，尤其是在使用三个智能体时，并提供可解释的证据组合。


<details>
  <summary>Details</summary>
Motivation: 判断式预测（Judgmental forecasting）是将对人类判断的未来事件的预测作为一项任务。这项任务可以被看作是一种主张验证（claim verification）的形式，其中主张对应于一个未来的事件，而任务是评估该事件的可能性。本研究旨在解决如何更准确、更可信地进行判断式预测的问题。

Method: 本研究提出了一个新的多智能体框架，用于主张验证。在该框架中，不同的智能体可能对主张的真实性有不同意见，并提供支持或反对主张的特定证据。这些证据以量化双极论证框架（QBAFs）的形式表示。为了支持主张验证，该框架被实例化，其中各种智能体由大型语言模型（LLMs）实现，包括：1）ArgLLM智能体，一种生成和评估QBAFs的现有方法；2）RbAM智能体，利用LLM赋能的关系型论证挖掘（RbAM）从外部来源生成QBAFs；3）RAG-ArgLLM智能体，通过检索增强生成（RAG）从外部来源获取论证来扩展ArgLLM智能体。最后，研究人员使用两个标准的判断式预测数据集，并结合两个或三个智能体（由六种不同的基础LLMs支持）对框架进行了实验。

Result: 实验结果表明，结合来自不同智能体的证据可以提高判断式预测的准确性，尤其是在使用三个智能体的情况下。此外，该框架能够提供可解释的证据组合，用于主张验证。

Conclusion: 本研究提出的多智能体框架通过结合不同LLM智能体的证据，有效地提高了判断式预测的准确性和可解释性。实验证明，增加智能体数量（从两个到三个）可以进一步提升预测性能。该框架为未来的判断式预测和主张验证任务提供了一种有前景的方法。未来的工作可以探索更复杂的智能体交互机制和论证表示方式，以应对更广泛的预测场景。

Abstract: Judgmental forecasting is the task of making predictions about future events
based on human judgment. This task can be seen as a form of claim verification,
where the claim corresponds to a future event and the task is to assess the
plausibility of that event. In this paper, we propose a novel multi-agent
framework for claim verification, whereby different agents may disagree on
claim veracity and bring specific evidence for and against the claims,
represented as quantitative bipolar argumentation frameworks (QBAFs). We then
instantiate the framework for supporting claim verification, with a variety of
agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an
existing approach for claim verification that generates and evaluates QBAFs;
(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)
from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,
extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of
arguments from external sources. Finally, we conduct experiments with two
standard judgmental forecasting datasets, with instances of our framework with
two or three agents, empowered by six different base LLMs. We observe that
combining evidence from agents can improve forecasting accuracy, especially in
the case of three agents, while providing an explainable combination of
evidence for claim verification.

</details>


### [131] [Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research](https://arxiv.org/abs/2510.24337)
*Daria Kravets-Meinke,Hannah Schmid-Petri,Sonja Niemann,Ute Schmid*

Main category: cs.AI

TL;DR: 生成式大型语言模型（gLLMs）在沟通研究的内容分析方面展现出巨大潜力，能够超越人工编码员，并以更低的成本和时间实现自动化内容分析。然而，gLLM在沟通研究中的应用仍处于早期阶段，研究人员在代码簿开发、提示工程、模型选择、参数调优、迭代优化、模型可靠性验证以及性能增强等方面面临严峻挑战。本文旨在整合gLLM辅助定量内容分析的最新研究，并提出一套全面的最佳实践指南，以应对这些挑战，从而提高gLLM在沟通研究中的可及性和质量标准（有效性、可靠性、可复现性和研究伦理）。


<details>
  <summary>Details</summary>
Motivation: 沟通研究领域日益增长的内容分析需求，以及现有方法在效率、成本和深度上的局限性。生成式大型语言模型（gLLMs）如ChatGPT，在内容分析任务上展现出超越人类编码员的潜力，能够以更少的时间和成本完成复杂编码，还能理解隐含意义和上下文信息。然而，gLLM在沟通研究中的应用方法尚不成熟，研究人员在实际操作中会遇到一系列挑战，阻碍了其广泛应用。因此，有必要整合现有研究，并提供一套明确的实践指南，以应对这些挑战，从而推动gLLM在沟通研究方法学中的发展，并确保研究质量。

Method: 本文整合了关于gLLM辅助定量内容分析的最新研究进展，并基于这些研究提出了一套全面的最佳实践指南。该指南旨在帮助沟通研究人员有效应对在应用gLLM进行内容分析时遇到的关键挑战，包括代码簿开发、提示工程、模型选择、参数调优、迭代优化、模型可靠性验证以及可选的性能增强。研究方法侧重于文献综述和实践经验的总结，旨在为研究人员提供一个清晰的操作框架。

Result: 尽管本文主要提供方法论指导而非实证结果，但其“结果”体现在为gLLM辅助定量内容分析提供了一个结构化的方法和一套应对挑战的最佳实践。通过遵循这些指南，研究人员可以预期在有效性、可靠性、可复现性和研究伦理方面取得更高质量的研究成果。文章强调了gLLM在处理复杂编码任务、理解隐含意义和上下文信息方面的优势，以及其在节省时间、降低成本方面的效率提升，这些都是gLLM在该领域应用的积极“结果”。

Conclusion: 本文成功地整合了gLLM辅助定量内容分析的现有研究，并提出了一套全面的最佳实践指南，为沟通研究人员提供了应对关键挑战的实用框架。该指南旨在降低gLLM在沟通研究中应用的门槛，并确保研究质量符合学科标准。虽然gLLM在内容分析方面潜力巨大，但仍需进一步的研究来完善其应用，特别是在模型选择、提示工程的精细化以及跨模型和跨任务的可靠性验证方面。未来的工作应继续探索gLLM在更广泛的沟通研究问题上的应用，并致力于开发更标准化的评估和验证工具，以充分发挥gLLM在这一领域的作用。

Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly
being used in communication research for content analysis. Studies show that
gLLMs can outperform both crowd workers and trained coders, such as research
assistants, on various coding tasks relevant to communication science, often at
a fraction of the time and cost. Additionally, gLLMs can decode implicit
meanings and contextual information, be instructed using natural language,
deployed with only basic programming skills, and require little to no annotated
data beyond a validation dataset - constituting a paradigm shift in automated
content analysis. Despite their potential, the integration of gLLMs into the
methodological toolkit of communication research remains underdeveloped. In
gLLM-assisted quantitative content analysis, researchers must address at least
seven critical challenges that impact result quality: (1) codebook development,
(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)
iterative refinement, (6) validation of the model's reliability, and
optionally, (7) performance enhancement. This paper synthesizes emerging
research on gLLM-assisted quantitative content analysis and proposes a
comprehensive best-practice guide to navigate these challenges. Our goal is to
make gLLM-based content analysis more accessible to a broader range of
communication researchers and ensure adherence to established disciplinary
quality standards of validity, reliability, reproducibility, and research
ethics.

</details>


### [132] [VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation](https://arxiv.org/abs/2510.24339)
*Yunxuan Jiang,Silan Hu,Xiaoning Wang,Yuanyuan Zhang,Xiangyu Chang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在数据科学工作流中日益普及，但它们仅依赖内部推理，缺乏科学和理论指导，导致在处理真实世界数据时可信度和鲁棒性不足。本文提出了VDSAgents，一个基于Veridical Data Science（VDS）框架的Predictability-Computability-Stability（PCS）原理的多智能体系统。该系统通过模块化工作流（数据清洗、特征工程、建模、评估）和智能体（包含扰动分析、单元测试、模型验证）来确保功能性和科学可审计性。在九个多样化数据集上的评估显示，VDSAgents优于AutoKaggle和DataInterpreter等现有系统，证明了将PCS原理嵌入LLM驱动的数据科学自动化的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在数据科学系统设计中虽然自动化程度高，但其决策过程仅依赖内部推理，缺乏科学和理论原则的指导。这导致系统在面对复杂和带有噪声的真实世界数据集时，其可信度和鲁棒性受到限制。因此，有必要开发一种能够结合LLM的自动化能力和科学理论的方法，以提高数据科学系统的可靠性。

Method: 本文提出了一种名为VDSAgents的多智能体系统，该系统以Veridical Data Science（VDS）框架中的Predictability-Computability-Stability（PCS）原理为指导。系统采用模块化设计，将数据科学流程分解为数据清洗、特征工程、建模和评估四个阶段，每个阶段由一个独立的智能体负责。这些智能体在执行任务时，会集成扰动分析、单元测试和模型验证等技术，以确保其输出的有效性和科学上的可审计性。

Result: 在九个不同特征的数据集上的实验评估表明，VDSAgents在性能上超越了AutoKaggle和DataInterpreter等先进的端到端数据科学系统。这充分证明了将PCS原理成功融入LLM驱动的数据科学自动化是可行的，并且能够有效提升系统的性能和可靠性。

Conclusion: 本文成功地提出了VDSAgents，一个将Veridical Data Science（VDS）框架的PCS原理应用于LLM驱动的数据科学自动化的多智能体系统。实验结果证明了该方法在提高数据科学自动化系统性能和可信度方面的有效性。这项工作为构建更可靠、可审计的数据科学工具奠定了基础，并为未来在LLM驱动的自动化系统中融入科学理论提供了有价值的见解。未来的研究可以进一步探索更复杂的PCS原理集成和智能体间的协作优化。

Abstract: Large language models (LLMs) become increasingly integrated into data science
workflows for automated system design. However, these LLM-driven data science
systems rely solely on the internal reasoning of LLMs, lacking guidance from
scientific and theoretical principles. This limits their trustworthiness and
robustness, especially when dealing with noisy and complex real-world datasets.
This paper provides VDSAgents, a multi-agent system grounded in the
Predictability-Computability-Stability (PCS) principles proposed in the
Veridical Data Science (VDS) framework. Guided by PCS principles, the system
implements a modular workflow for data cleaning, feature engineering, modeling,
and evaluation. Each phase is handled by an elegant agent, incorporating
perturbation analysis, unit testing, and model validation to ensure both
functionality and scientific auditability. We evaluate VDSAgents on nine
datasets with diverse characteristics, comparing it with state-of-the-art
end-to-end data science systems, such as AutoKaggle and DataInterpreter, using
DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the
results of AutoKaggle and DataInterpreter, which validates the feasibility of
embedding PCS principles into LLM-driven data science automation.

</details>


### [133] [An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine](https://arxiv.org/abs/2510.24359)
*Pedram Fard,Alaleh Azhir,Neguine Rezaii,Jiazi Tian,Hossein Estiri*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Artificial intelligence in medicine is built to serve the average patient. By
minimizing error across large datasets, most systems deliver strong aggregate
accuracy yet falter at the margins: patients with rare variants,
multimorbidity, or underrepresented demographics. This average patient fallacy
erodes both equity and trust. We propose a different design: a multi-agent
ecosystem for N-of-1 decision support. In this environment, agents clustered by
organ systems, patient populations, and analytic modalities draw on a shared
library of models and evidence synthesis tools. Their results converge in a
coordination layer that weighs reliability, uncertainty, and data density
before presenting the clinician with a decision-support packet: risk estimates
bounded by confidence ranges, outlier flags, and linked evidence. Validation
shifts from population averages to individual reliability, measured by error in
low-density regions, calibration in the small, and risk--coverage trade-offs.
Anticipated challenges include computational demands, automation bias, and
regulatory fit, addressed through caching strategies, consensus checks, and
adaptive trial frameworks. By moving from monolithic models to orchestrated
intelligence, this approach seeks to align medical AI with the first principle
of medicine: care that is transparent, equitable, and centered on the
individual.

</details>


### [134] [Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion](https://arxiv.org/abs/2510.24390)
*Xianjun Gao,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The integration of Large Language Models (LLMs) into real-time Web
applications, such as AI-powered search and conversational agents, presents a
fundamental Web infrastructure challenge: reconciling the demand for
high-quality, complex reasoning with the stringent low-latency and
high-throughput requirements of interactive services. Current LLM reasoning,
hindered by computationally inefficient sequential generation and rigid
reasoning strategies, creates a critical bottleneck for the Web services.
Existing approaches typically optimize the LLM reasoning for either efficiency
or quality but struggle to achieve both, and thus fail to meet the dual
requirements of modern Web platforms. To overcome these limitations, we propose
Orion, a novel and efficient reasoning framework that enables dependency-aware
query decomposition and logic-parallel content expansion. Concretely, Orion
decomposes a single query reasoning process into two synergistic phases: (1)
\textit{key point generation}, which distills logically structured key points
through retrieval-augmented few-shot prompting, and (2) \textit{content
parallel expansion}, which concurrently elaborates on these points based on a
dependency graph to ensure logical consistency. Furthermore, Orion introduces a
pipeline scheduling mechanism that exploits the complementary computational
characteristics of the two phases (generation imposes pressure on GPU computing
and expansion stresses on GPU memory) across multiple queries, enabling
cross-query parallelism and dramatically improving reasoning performance (\ie,
efficiency and quality). Experiments on diverse benchmarks show that Orion not
only delivers up to 4.33x higher token generation speed and 3.42x lower answer
latency over the baselines but also improves reasoning quality by up to 18.75%
through explicitly modeling inter-point dependencies.

</details>


### [135] [APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training](https://arxiv.org/abs/2510.24397)
*Jiarui Qin,Yunjia Xi,Junjie Huang,Renting Rui,Di Yin,Weiwen Liu,Yong Yu,Weinan Zhang,Xing Sun*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the rapid development of LLM-based agents, there is a growing trend to
incorporate agent-specific data into the pre-training stage of LLMs, aiming to
better align LLMs with real-world autonomous task execution. However, current
pre-training benchmarks primarily focus on isolated and static skills, e.g.,
common knowledge or mathematical/code reasoning, and fail to reflect model's
agentic capabilities. On the other hand, agent benchmarks are typically
designed for post-trained models, requiring multi-turn task execution abilities
that base models struggle to support. Thus, there is a compelling need for a
benchmark that can evaluate agentic potentials during pre-training and guide
the model training more effectively. To address this gap, we propose APTBench,
a framework that converts real-world agent tasks and successful trajectories
into multiple-choice or text completion questions tailored for base models. It
focuses on core agentic abilities, e.g., planning and action, and covers key
agent scenarios, software engineering and deep research. Compared to existing
general-purpose benchmarks, APTBench offers a more predictive signal of a
model's downstream performance as an agent, while remaining significantly more
lightweight and cost-effective than full-scale, end-to-end agent evaluations
after post-training.

</details>


### [136] [Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning](https://arxiv.org/abs/2510.24435)
*Benjamin Grando Moreira*

Main category: cs.AI

TL;DR: 该研究评估了包括GPT、Claude、DeepSeek、Gemini、Grok、Llama、Mistral、Perplexity和Sabi'a在内的多种大型语言模型（LLMs）的逻辑和抽象推理能力，并与人类表现进行基准测试，发现了LLMs在演绎推理方面存在的显著差异和挑战。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）的推理能力对于推动人工智能发展至关重要，因为它超越了单纯的语言任务表现。这涉及到理解模型是否真正理解信息、执行推理以及能否以合乎逻辑且有效的方式得出结论。

Method: 本研究采用了一套八个定制设计的推理问题来比较多个LLMs（包括GPT、Claude、DeepSeek、Gemini、Grok、Llama、Mistral、Perplexity和Sabi'a）的逻辑和抽象推理技能。LLM的结果与人类在相同任务上的表现进行了基准测试。

Result: 研究结果显示，在所测试的LLMs与人类表现之间存在显著差异，突显了LLMs在演绎推理方面存在的不足之处。

Conclusion: 该研究通过对比多个人工智能模型和人类在特定推理任务上的表现，揭示了当前LLMs在逻辑和抽象推理能力上的局限性，并为未来改进这些模型提供了方向。

Abstract: Evaluating reasoning ability in Large Language Models (LLMs) is important for
advancing artificial intelligence, as it transcends mere linguistic task
performance. It involves understanding whether these models truly understand
information, perform inferences, and are able to draw conclusions in a logical
and valid way. This study compare logical and abstract reasoning skills of
several LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,
Perplexity, and Sabi\'a - using a set of eight custom-designed reasoning
questions. The LLM results are benchmarked against human performance on the
same tasks, revealing significant differences and indicating areas where LLMs
struggle with deduction.

</details>


### [137] [From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning](https://arxiv.org/abs/2510.24528)
*Zihan Chen,Song Wang,Xingbo Fu,Chengshuai Shi,Zhenyu Lei,Cong Shen,Jundong Li*

Main category: cs.AI

TL;DR: 本文提出了一种高效的两阶段方法，利用少量跨任务示例和图基标签传播技术，为大型语言模型（LLM）生成伪标签数据，用于在上下文学习（ICL），从而降低数据标注成本并提高效率。


<details>
  <summary>Details</summary>
Motivation: 在上下文学习（ICL）中，大型语言模型（LLM）无需更新参数即可执行新任务，但收集高质量的示例成本高昂。本研究旨在解决LLM在数据标注方面的成本和人力投入问题，提出一种更经济高效的解决方案。

Method: 该方法采用两阶段流程：1. 利用现有的跨任务示例，提示LLM为少量目标任务实例生成伪标签。2. 引入一种基于图的标签传播方法，在不进行额外LLM查询的情况下，将标签信息传播给剩余的目标示例。最后，使用完全伪标签的数据集构建用于ICL的 in-task 演示。

Result: 在五个任务上的实验表明，该方法在降低标注成本的同时，实现了强大的性能。具体性能提升和与现有方法的对比在实验部分进行了详细阐述。

Conclusion: 该方法结合了跨任务监督的灵活性和无LLM传播的可扩展性，有效降低了ICL的数据标注成本，同时保持了良好的性能。未来的工作可以探索更复杂的图结构或传播算法，以进一步提高标签传播的准确性。

Abstract: The capability of in-context learning (ICL) enables large language models
(LLMs) to perform novel tasks without parameter updates by conditioning on a
few input-output examples. However, collecting high-quality examples for new or
challenging tasks can be costly and labor-intensive. In this work, we propose a
cost-efficient two-stage pipeline that reduces reliance on LLMs for data
labeling. Our approach first leverages readily available cross-task examples to
prompt an LLM and pseudo-label a small set of target task instances. We then
introduce a graph-based label propagation method that spreads label information
to the remaining target examples without additional LLM queries. The resulting
fully pseudo-labeled dataset is used to construct in-task demonstrations for
ICL. This pipeline combines the flexibility of cross-task supervision with the
scalability of LLM-free propagation. Experiments across five tasks demonstrate
that our method achieves strong performance while lowering labeling costs.

</details>


### [138] [Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning](https://arxiv.org/abs/2510.24650)
*Nitin Rai,Daeun,Choi,Nathan S. Boyd,Arnold W. Schumann*

Main category: cs.AI

TL;DR: 本文综述了基础模型（FMs）在作物病害精准管理（SSDM）中的应用，重点介绍了大型语言模型（LLMs）和视觉语言模型（VLMs）在图像识别、文本分析和智能决策方面的潜力。研究发现，FMs，特别是VLMs，在处理作物病害数据集方面展现出巨大优势，并且在自适应学习（AL）、强化学习（RL）和数字孪生框架的结合下，为精准喷洒提供了新的可能性。尽管RL和AL在智能喷洒方面的应用尚处于早期阶段，但数字孪生结合RL的虚拟模拟能力预示着未来的发展方向。文章强调了弥合模拟与现实差距、加强人机协作以及利用多模态FMs进行实时反馈的重要性，为下一代SSDM的发展指明了方向。


<details>
  <summary>Details</summary>
Motivation: 作物病害的精准管理（SSDM）是提高作物产量和质量的关键。近年来，机器学习（ML）和深度学习（DL）在实时计算机视觉方面取得了显著进展，使得从手工特征提取到大规模自动化特征学习的研究范式发生了转变。基础模型（FMs）的出现，为处理作物病害数据集提供了根本性的新方法。FMs能够整合视觉和文本数据，理解病害症状的文本描述，推理症状与管理措施之间的关系，并支持交互式问答，从而为种植者和教育者提供更智能化的支持。因此，深入探讨FMs在SSDM中的应用，特别是LLMs和VLMs，以及它们与机器人技术（如自适应学习和模仿学习）的结合，对于推动SSDM技术的发展具有重要意义。

Method: 本综述筛选了大约40篇关于FM应用于SSDM的文章，重点关注大型语言模型（LLMs）和视觉语言模型（VLMs）。文章深入探讨了这些模型在自适应学习（AL）、强化学习（RL）以及数字孪生框架中的作用，特别是它们如何支持目标喷洒等应用。综述分析了相关研究在数据处理、模型集成、学习范式以及人机协作等方面的具体技术和方法，并讨论了将模拟环境中的学习成果迁移到现实世界中所面临的挑战，例如“模拟到现实”的差距（sim-to-real gap）。

Result: 研究发现，FMs在SSDM领域的应用正在迅速增长，尤其是在2023-2024年间，相关文献数量激增。其中，视觉语言模型（VLMs）的发展势头明显优于大型语言模型（LLMs），发表的出版物数量增加了5到10倍。然而，强化学习（RL）和自适应学习（AL）在智能喷洒领域的应用仍处于初步探索阶段。数字孪生技术结合RL，能够在虚拟环境中模拟精准喷洒过程，为实际应用提供指导。弥合模拟与现实之间的差距是实现大规模实际部署的关键挑战。此外，人机协作，特别是“人在回路”的模式（即机器人检测早期病害，人类验证不确定情况）的潜力尚未得到充分发挥。

Conclusion: 基础模型（FMs），特别是视觉语言模型（VLMs），为作物病害精准管理（SSDM）带来了革命性的变化，能够处理多模态数据并进行复杂的推理。尽管RL和AL在智能喷洒中的应用尚不成熟，但数字孪生技术为这些技术的虚拟测试和优化提供了平台。未来的研究应重点关注缩小模拟与现实之间的差距，并加强人机协作，以实现更高效、更可靠的SSDM系统。多模态FMs结合实时反馈将是驱动下一代SSDM发展的关键。

Abstract: Site-specific disease management (SSDM) in crops has advanced rapidly through
machine and deep learning (ML and DL) for real-time computer vision. Research
evolved from handcrafted feature extraction to large-scale automated feature
learning. With foundation models (FMs), crop disease datasets are now processed
in fundamentally new ways. Unlike traditional neural networks, FMs integrate
visual and textual data, interpret symptoms in text, reason about
symptom-management relationships, and support interactive QA for growers and
educators. Adaptive and imitation learning in robotics further enables
field-based disease management. This review screened approx. 40 articles on FM
applications for SSDM, focusing on large-language models (LLMs) and
vision-language models (VLMs), and discussing their role in adaptive learning
(AL), reinforcement learning (RL), and digital twin frameworks for targeted
spraying. Key findings: (a) FMs are gaining traction with surging literature in
2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL
and AL are still nascent for smart spraying; (d) digital twins with RL can
simulate targeted spraying virtually; (e) addressing the sim-to-real gap is
critical for real-world deployment; (f) human-robot collaboration remains
limited, especially in human-in-the-loop approaches where robots detect early
symptoms and humans validate uncertain cases; (g) multi-modal FMs with
real-time feedback will drive next-gen SSDM. For updates, resources, and
contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to
submit papers, code, or datasets.

</details>


### [139] [OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs](https://arxiv.org/abs/2510.24663)
*Yifu Lu,Shengjie Liu,Li Dong*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Agentic tool use has gained traction with the rise of agentic tool calling,
yet most existing work overlooks the complexity of multi-turn tool
interactions. We introduce OrchDAG, a synthetic data generation pipeline that
models tool execution as directed acyclic graphs (DAGs) with controllable
complexity. Using this dataset, we benchmark model performance and propose a
graph-based reward to enhance RLVR training. Experiments show that the dataset
presents a challenging but solvable benchmark, and the proposed reward is
effective when combined with GRPO-style algorithms, highlighting the importance
of leveraging topological structure and data complexity in multi-turn tool use.

</details>


### [140] [Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning](https://arxiv.org/abs/2510.24690)
*Shengjie Liu,Li Dong,Zhenyu Zhang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a framework for uncovering and exploiting dependencies among tools
and documents to enhance exemplar artifact generation. Our method begins by
constructing a tool knowledge graph from tool schemas,including descriptions,
arguments, and output payloads, using a DeepResearch-inspired analysis. In
parallel, we derive a complementary knowledge graph from internal documents and
SOPs, which is then fused with the tool graph. To generate exemplar plans, we
adopt a deep-sparse integration strategy that aligns structural tool
dependencies with procedural knowledge. Experiments demonstrate that this
unified framework effectively models tool interactions and improves plan
generation, underscoring the benefits of linking tool graphs with domain
knowledge graphs for tool-augmented reasoning and planning.

</details>


### [141] [A Survey on Large Language Model-Based Game Agents](https://arxiv.org/abs/2404.02039)
*Sihao Hu,Tiansheng Huang,Gaowen Liu,Ramana Rao Kompella,Fatih Ilhan,Selim Furkan Tekin,Yichang Xu,Zachary Yahn,Ling Liu*

Main category: cs.AI

TL;DR: 本篇论文全面回顾了基于大语言模型（LLM）的游戏智能体（LLMGA）的研究现状，提出了一个统一的参考架构，并从单智能体和多智能体两个层面进行了深入分析。在单智能体层面，研究聚焦于LLM如何赋能智能体的记忆、推理和感知-行动接口，实现更强的泛化、记忆和适应能力。在多智能体层面，则探讨了通信协议和组织模型在支持协调、角色分化和大规模社会行为中的作用。论文还引入了一个以挑战为中心的分类法，将六种主要游戏类型与其对智能体的需求相匹配，为LLMGA的设计和评估提供了依据。


<details>
  <summary>Details</summary>
Motivation: 游戏环境因其丰富性、可控性以及对现实世界复杂性的模拟能力，成为测试人工智能通用智能（AGI）相关能力的有价值的平台。特别是大型语言模型（LLM）的出现，为增强游戏智能体在复杂游戏环境中的泛化推理、记忆和适应能力提供了新的机遇。本研究旨在系统性地梳理和总结当前LLM在游戏智能体领域的研究进展，为该领域的研究者提供一个全面的视角和参考框架。

Method: 本研究采用文献综述的方法，回顾了现有关于LLM在游戏智能体方面的研究。论文提出了一个统一的参考架构，将LLMGA在单智能体和多智能体两个层面的研究进行整合。在单智能体层面，研究围绕记忆、推理和感知-行动接口这三个核心组件进行分析，探讨语言模型如何实现智能体的感知、思考和行动。在多智能体层面，研究关注通信协议和组织模型在支持智能体间的协调、角色分化和大规模社会行为方面的作用。此外，论文还构建了一个以挑战为中心的分类法，将六种主要游戏类型（如动作游戏、沙盒游戏等）与它们对智能体的需求进行关联，以阐述不同游戏场景对LLMGA设计的影响。

Result: 本研究对LLM在游戏智能体领域的应用进行了系统性梳理。研究表明，LLM在增强游戏智能体的记忆能力、推理能力以及感知-行动接口方面展现出巨大潜力。通过整合LLM，智能体能够更好地理解游戏状态、进行更复杂的决策，并与游戏环境进行更自然的交互。在多智能体场景下，LLM能够促进智能体之间的有效沟通和协作，实现更高级的社会行为。论文提出的挑战中心分类法，清晰地展示了不同游戏类型对LLMGA提出的具体要求，为未来的研究和开发指明了方向。

Conclusion: 本篇综述为LLM在游戏智能体领域的应用提供了一个结构化的视角。通过提出的统一参考架构和挑战中心分类法，研究者可以更好地理解LLMGA的设计原理、现有研究成果以及未来的发展方向。LLM为构建更强大、更通用的游戏智能体提供了前所未有的机会，但仍有许多挑战需要克服，例如提高LLM在实时性要求高的游戏中的表现、增强其长期规划能力以及探索更高效的训练方法等。未来的工作将集中在解决这些挑战，进一步推动LLM在复杂游戏环境中的应用。

Abstract: Game environments provide rich, controllable settings that stimulate many
aspects of real-world complexity. As such, game agents offer a valuable testbed
for exploring capabilities relevant to Artificial General Intelligence.
Recently, the emergence of Large Language Models (LLMs) provides new
opportunities to endow these agents with generalizable reasoning, memory, and
adaptability in complex game environments. This survey offers an up-to-date
review of LLM-based game agents (LLMGAs) through a unified reference
architecture. At the single-agent level, we synthesize existing studies around
three core components: memory, reasoning, and perception-action interfaces,
which jointly characterize how language enables agents to perceive, think, and
act. At the multi-agent level, we outline how communication protocols and
organizational models support coordination, role differentiation, and
large-scale social behaviors. To contextualize these designs, we introduce a
challenge-centered taxonomy linking six major game genres to their dominant
agent requirements, from low-latency control in action games to open-ended goal
formation in sandbox worlds. A curated list of related papers is available at
https://github.com/git-disl/awesome-LLM-game-agent-papers

</details>


### [142] [3D-Prover: Diversity Driven Theorem Proving With Determinantal Point Processes](https://arxiv.org/abs/2410.11133)
*Sean Lamont,Christian Walder,Amir Dezfouli,Paul Montague,Michael Norrish*

Main category: cs.AI

TL;DR: 该研究提出了一种名为3D-Prover的新方法，利用合成数据通过生成语义感知策略表示来解决自动化形式推理中的搜索空间爆炸问题。通过使用决心点过程来选择多样化和高质量的策略，3D-Prover显著提高了miniF2F和LeanDojo基准上的证明率、策略成功率、执行时间和多样性。


<details>
  <summary>Details</summary>
Motivation: 自动化形式推理面临搜索空间巨大的挑战，这会随着证明深度的增加呈指数级增长，因为可能应用的候选证明策略数量庞大。许多策略在语义上相似或会导致执行错误，从而浪费宝贵的计算资源。解决搜索空间爆炸问题并有效修剪不必要的策略至关重要。

Method: 该研究首先生成能够捕获对证明环境的影响、成功可能性和执行时间的语义感知策略表示。然后，提出了一种新颖的过滤机制，该机制利用这些表示，并使用决心点过程来选择语义上多样且高质量的策略。该方法3D-Prover旨在通用，并能增强任何潜在的策略生成器。

Result: 在miniF2F和LeanDojo基准上，通过增强流行的开源证明语言模型，该方法在整体证明率方面有所提高，并在策略成功率、执行时间和多样性方面取得了显著的改进。

Conclusion: 3D-Prover通过利用合成数据和决心点过程，有效地解决了自动化形式推理中的搜索空间爆炸问题，并在关键基准测试中展示了其优越性。这项工作为提高自动化证明系统的效率和性能开辟了新的途径。

Abstract: A key challenge in automated formal reasoning is the intractable search
space, which grows exponentially with the depth of the proof. This branching is
caused by the large number of candidate proof tactics which can be applied to a
given goal. Nonetheless, many of these tactics are semantically similar or lead
to an execution error, wasting valuable resources in both cases. We address the
problem of effectively pruning this search, using only synthetic data generated
from previous proof attempts. We first demonstrate that it is possible to
generate semantically aware tactic representations which capture the effect on
the proving environment, likelihood of success, and execution time. We then
propose a novel filtering mechanism which leverages these representations to
select semantically diverse and high quality tactics, using Determinantal Point
Processes. Our approach, 3D- Prover, is designed to be general, and to augment
any underlying tactic generator. We demonstrate the effectiveness of 3D-Prover
on the miniF2F and LeanDojo benchmarks by augmenting popular open source
proving LLMs. We show that our approach leads to an increase in the overall
proof rate, as well as a significant improvement in the tactic success rate,
execution time and diversity. We make our code available at
https://github.com/sean-lamont/3D-Prover.

</details>


### [143] [TableTime: Reformulating Time Series Classification as Training-Free Table Understanding with Large Language Models](https://arxiv.org/abs/2411.15737)
*Jiahao Wang,Mingyue Cheng,Qingyang Mao,Yitong Zhou,Daoyu Wang,Qi Liu,Feiyang Xu,Xin Li*

Main category: cs.AI

TL;DR: 该研究提出了一种名为TableTime的新方法，将多元时间序列分类（MTSC）任务重构为表格理解任务，以克服现有基于大语言模型（LLM）的方法在处理MTSC时的信息丢失、表示空间对齐困难以及需要任务特定重训等瓶颈。TableTime通过将时间序列转换为表格形式、文本表示以及设计整合了上下文文本信息、邻域辅助、多路径推理和问题分解的推理框架，实现了LLM在MTSC任务上的零样本分类能力，并在UEA档案库的10个代表性数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的方法在处理多元时间序列分类（MTSC）时，存在三个主要瓶颈：1. 无法无损地编码时间序列中的时间信息和通道特定信息；2. 学习到的表示空间难以与LLM的语义空间对齐；3. 需要耗时耗力的任务特定重训。这些瓶颈限制了LLM在MTSC任务上的有效性和效率。

Method: TableTime方法将MTSC任务重构为表格理解任务。具体策略包括：1. 将多元时间序列转换为表格形式，最大程度地减少信息损失。2. 将表格形式的时间序列表示为文本，以便与LLM的语义空间自然对齐。3. 设计了一个推理框架，整合了上下文文本信息、邻域辅助、多路径推理和问题分解，以增强LLM的推理能力，并实现零样本分类。实验在UEA档案库的10个公开代表性数据集上进行。

Result: TableTime方法在10个公开代表性数据集上进行了广泛的实验，结果表明该方法优于现有方法，有效解决了信息丢失、空间对齐困难和任务重训等问题，并实现了零样本分类。

Conclusion: TableTime通过将MTSC转化为表格理解任务，成功克服了现有LLM方法的局限性，实现了更有效、更高效的时间序列分类。该方法在多个数据集上表现出优越性，为LLM在MTSC领域的应用提供了新的方向。未来的工作可以进一步探索TableTime在处理更复杂的时间序列模式和更大规模数据集上的性能。

Abstract: Large language models (LLMs) have demonstrated their effectiveness in
multivariate time series classification (MTSC). Effective adaptation of LLMs
for MTSC necessitates informative data representations. Existing LLM-based
methods directly encode embeddings for time series within the latent space of
LLMs from scratch to align with semantic space of LLMs. Despite their
effectiveness, we reveal that these methods conceal three inherent bottlenecks:
(1) they struggle to encode temporal and channel-specific information in a
lossless manner, both of which are critical components of multivariate time
series; (2) it is much difficult to align the learned representation space with
the semantic space of the LLMs; (3) they require task-specific retraining,
which is both computationally expensive and labor-intensive. To bridge these
gaps, we propose TableTime, which reformulates MTSC as a table understanding
task. Specifically, TableTime introduces the following strategies: (1) convert
multivariate time series into a tabular form, thus minimizing information loss
to the greatest extent; (2) represent tabular time series in text format to
achieve natural alignment with the semantic space of LLMs; (3) design a
reasoning framework that integrates contextual text information, neighborhood
assistance, multi-path inference and problem decomposition to enhance the
reasoning ability of LLMs and realize zero-shot classification. Extensive
experiments performed on 10 publicly representative datasets from UEA archive
verify the superiorities of the TableTime.

</details>


### [144] [Multimodal Dreaming: A Global Workspace Approach to World Model-Based Reinforcement Learning](https://arxiv.org/abs/2502.21142)
*Léopold Maytié,Roland Bertin Johannet,Rufin VanRullen*

Main category: cs.AI

TL;DR: 本文提出了一种结合了全局工作空间（GW）理论和世界模型（World Models）的强化学习（RL）系统，称为GW-Dreamer。该系统在GW的潜在空间中进行“梦境”过程（即心智模拟），从而可以用更少的环境交互步数进行训练，并且在缺少一种观察模态（图像或模拟属性）的情况下表现出更强的鲁棒性，而基线模型则不具备此特性。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习（RL）中的世界模型通常直接在环境变量（如像素、物理属性）上操作，导致训练缓慢且效率低下。为了克服这一限制，本文旨在利用高层潜在维度来捕捉相关的多模态变量，以提高训练效率和泛化能力。全局工作空间（GW）理论提供了一个认知框架，用于大脑中的多模态信息整合和广播，这为构建更高效的学习系统提供了启发。

Method: 本文提出的GW-Dreamer系统结合了全局工作空间（GW）理论和世界模型。该方法的核心在于，将“梦境”过程（心智模拟）置于GW的潜在空间中进行。研究人员将GW-Dreamer与标准的PPO算法和原始的Dreamer算法进行了比较，评估其在训练效率和鲁棒性方面的表现。

Result: 实验结果表明，在GW潜在空间中执行梦境过程的GW-Dreamer系统，能够以更少的环境交互步数完成训练。此外，GW-Dreamer展现出一种新兴的鲁棒性优势，即使在缺少一种观察模态（图像或模拟属性）的情况下，其性能也表现出色，而对比的基线模型则不具备这种鲁棒性。

Conclusion: 本文的结论是，将全局工作空间（GW）理论与世界模型相结合，为提升RL智能体的决策能力带来了巨大的潜力。GW-Dreamer系统在训练效率和对观察模态缺失的鲁棒性方面均优于现有方法，预示着该方向在未来RL研究中的重要价值。

Abstract: Humans leverage rich internal models of the world to reason about the future,
imagine counterfactuals, and adapt flexibly to new situations. In Reinforcement
Learning (RL), world models aim to capture how the environment evolves in
response to the agent's actions, facilitating planning and generalization.
However, typical world models directly operate on the environment variables
(e.g. pixels, physical attributes), which can make their training slow and
cumbersome; instead, it may be advantageous to rely on high-level latent
dimensions that capture relevant multimodal variables. Global Workspace (GW)
Theory offers a cognitive framework for multimodal integration and information
broadcasting in the brain, and recent studies have begun to introduce efficient
deep learning implementations of GW. Here, we evaluate the capabilities of an
RL system combining GW with a world model. We compare our GW-Dreamer with
various versions of the standard PPO and the original Dreamer algorithms. We
show that performing the dreaming process (i.e., mental simulation) inside the
GW latent space allows for training with fewer environment steps. As an
additional emergent property, the resulting model (but not its comparison
baselines) displays strong robustness to the absence of one of its observation
modalities (images or simulation attributes). We conclude that the combination
of GW with World Models holds great potential for improving decision-making in
RL agents.

</details>


### [145] [Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)](https://arxiv.org/abs/2505.17323)
*Ruaridh Mon-Williams,Max Taylor-Davies,Elizabeth Mieczkowski,Natalia Velez,Neil R. Bramley,Yanwei Wang,Thomas L. Griffiths,Christopher G. Lucas*

Main category: cs.AI

TL;DR: 本研究训练简单的无模型RNN智能体与多样化的伙伴进行协作，发现在缺乏额外架构、归纳偏置或辅助目标的情况下，智能体能自发地发展出对其伙伴任务能力的结构化内部表征，从而实现对新伙伴的快速适应和泛化。这种伙伴建模的出现与环境条件有关，特别是当智能体能够通过控制任务分配来影响伙伴行为时。


<details>
  <summary>Details</summary>
Motivation: 为了构建具备人类协作能力的AI系统，研究者旨在探究这种灵活性是需要显式的伙伴建模机制，还是可以自发地从开放式合作互动中涌现。理解这一点对于开发更强大的AI协作至关重要。

Method: 研究者使用`Overcooked-AI`环境，训练了简单的无模型RNN智能体与一个由多样化伙伴组成的群体进行协作。通过收集数千个协作团队的数据，并分析智能体的内部隐藏状态，来探究伙伴建模的出现。研究中运用了探测技术和大规模行为分析来调查智能体的内部模型。

Result: 结果表明，即使智能体没有额外的架构特征、归纳偏置或辅助目标，它们也能发展出对伙伴任务能力的结构化内部表征，从而能够快速适应并泛化到新的合作者。研究发现，当智能体可以通过控制任务分配来影响伙伴行为时，结构化的伙伴建模就会出现。

Conclusion: 本研究证明了在模型无关的智能体中，伙伴建模可以自发涌现，但前提是环境条件施加了适当的社会压力。这表明通过精心设计的协作环境，可以培养出更智能、更具适应性的AI合作者。未来的工作可以进一步探索不同环境因素对伙伴建模涌现的影响，以及将其应用于更复杂的协作任务。

Abstract: Humans are remarkably adept at collaboration, able to infer the strengths and
weaknesses of new partners in order to work successfully towards shared goals.
To build AI systems with this capability, we must first understand its building
blocks: does such flexibility require explicit, dedicated mechanisms for
modelling others -- or can it emerge spontaneously from the pressures of
open-ended cooperative interaction? To investigate this question, we train
simple model-free RNN agents to collaborate with a population of diverse
partners. Using the `Overcooked-AI' environment, we collect data from thousands
of collaborative teams, and analyse agents' internal hidden states. Despite a
lack of additional architectural features, inductive biases, or auxiliary
objectives, the agents nevertheless develop structured internal representations
of their partners' task abilities, enabling rapid adaptation and generalisation
to novel collaborators. We investigated these internal models through probing
techniques, and large-scale behavioural analysis. Notably, we find that
structured partner modelling emerges when agents can influence partner
behaviour by controlling task allocation. Our results show that partner
modelling can arise spontaneously in model-free agents -- but only under
environmental conditions that impose the right kind of social pressure.

</details>


### [146] [The Confidence Paradox: Can LLM Know When It's Wrong](https://arxiv.org/abs/2506.23464)
*Sahil Tripathi,Md Tabrez Nafis,Imran Hussain,Jiechao Gao*

Main category: cs.AI

TL;DR: 文档视觉问答（DocVQA）模型常在不确定时给出过于自信或不符合伦理的回答。为解决此问题，本研究提出了HonestVQA，一个模型无关的自监督框架，通过加权损失和对比学习来校准模型置信度与正确性。研究还引入了H-Score和ECI两个新指标来评估伦理对齐。实验结果表明，HonestVQA在多个数据集上提高了准确率和F1分数，并减少了过度自信，同时在不同领域展现了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的DocVQA模型（如LayoutLMv3、UDOP、DONUT）主要关注准确率，但在面对不确定性时，其回答可能过于自信或存在伦理偏差，未能有效校准模型的置信度。这种不准确的置信度评估可能导致模型在关键应用场景中做出错误的决策，因此，提升DocVQA模型在不确定性下的可靠性和伦理对齐能力具有重要意义。

Method: 本研究提出了HonestVQA，一个模型无关的自监督框架。该框架通过结合加权损失（weighted loss）和对比学习（contrastive learning）来优化模型，旨在使模型的置信度与其预测的正确性相匹配。具体而言，加权损失函数能够根据预测的准确性调整样本的重要性，而对比学习则帮助模型区分相似但不同的样本，从而提高其对细微差别的感知能力。此外，研究还定义了两个新的评估指标：H-Score（Honesty Score）和ECI（Ethical Confidence Index），用于量化模型的诚实度和伦理置信水平。

Result: 在SpDocVQA、InfographicsVQA和SROIE数据集上的实验显示，HonestVQA相较于现有模型，在准确率和F1分数上提高了高达4.3%。同时，该框架有效降低了模型的过度自信问题。此外，HonestVQA在不同领域也表现出良好的泛化能力，在测试中达到了78.9%的准确率和76.1%的F1分数。

Conclusion: HonestVQA框架通过创新的加权损失和对比学习方法，成功解决了DocVQA模型在不确定性下的过度自信和伦理偏差问题，提升了模型的准确性和可靠性。新提出的H-Score和ECI指标为评估模型伦理对齐提供了有效工具。该研究不仅在多个基准数据集上取得了显著的性能提升，而且展示了良好的跨领域泛化能力，为构建更负责任的AI系统奠定了基础。未来的工作可以探索将此框架应用于更广泛的VQA任务，并进一步研究其在真实世界复杂场景下的鲁棒性。

Abstract: Document Visual Question Answering (DocVQA) models often produce
overconfident or ethically misaligned responses, especially under uncertainty.
Existing models like LayoutLMv3, UDOP, and DONUT focus on accuracy but lack
ethical calibration. We propose HonestVQA, a model-agnostic, self-supervised
framework that aligns model confidence with correctness using weighted loss and
contrastive learning. We introduce two new metrics Honesty Score (H-Score) and
Ethical Confidence Index (ECI)-to evaluate ethical alignment. HonestVQA
improves accuracy and F1 by up to 4.3% across SpDocVQA, InfographicsVQA, and
SROIE datasets, while reducing overconfidence. It also generalizes well across
domains, achieving 78.9% accuracy and 76.1% F1-score.

</details>


### [147] [Memory Mosaics at scale](https://arxiv.org/abs/2507.03285)
*Jianyu Zhang,Léon Bottou*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Memory Mosaics [Zhang et al., 2025], networks of associative memories, have
demonstrated appealing compositional and in-context learning capabilities on
medium-scale networks (GPT-2 scale) and synthetic small datasets. This work
shows that these favorable properties remain when we scale memory mosaics to
large language model sizes (llama-8B scale) and real-world datasets.
  To this end, we scale memory mosaics to 10B size, we train them on one
trillion tokens, we introduce a couple architectural modifications ("Memory
Mosaics v2"), we assess their capabilities across three evaluation dimensions:
training-knowledge storage, new-knowledge storage, and in-context learning.
  Throughout the evaluation, memory mosaics v2 match transformers on the
learning of training knowledge (first dimension) and significantly outperforms
transformers on carrying out new tasks at inference time (second and third
dimensions). These improvements cannot be easily replicated by simply
increasing the training data for transformers. A memory mosaics v2 trained on
one trillion tokens still perform better on these tasks than a transformer
trained on eight trillion tokens.

</details>


### [148] [Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem](https://arxiv.org/abs/2508.21730)
*Fabrizio Fagiolo,Nicolò Vescera*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper we present a variational algorithm for the Traveling Salesman
Problem (TSP) that combines (i) a compact encoding of permutations, which
reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy:
where the circuit topology (``Ansatz'') is first optimized on a training
instance by Simulated Annealing (SA), then ``frozen'' and re-used on novel
instances, limited to a rapid re-optimization of only the circuit parameters.
This pipeline eliminates costly structural research in testing, making the
procedure immediately implementable on NISQ hardware.
  On a set of $40$ randomly generated symmetric instances that span $4 - 7$
cities, the resulting Ansatz achieves an average optimal trip sampling
probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for
6 city cases. With 7 cities the success rate drops markedly to an average of
$\sim 20\%$, revealing the onset of scalability limitations of the proposed
method.
  The results show robust generalization ability for moderate problem sizes and
indicate how freezing the Ansatz can dramatically reduce time-to-solution
without degrading solution quality. The paper also discusses scalability
limitations, the impact of ``warm-start'' initialization of parameters, and
prospects for extension to more complex problems, such as Vehicle Routing and
Job-Shop Scheduling.

</details>


### [149] [Accelerate Scaling of LLM Finetuning via Quantifying the Coverage and Depth of Instruction Set](https://arxiv.org/abs/2509.06463)
*Chengwei Wu,Li Du,Hanyu Zhao,Yiming Ju,Jiapu Wang,Tianyu Chen,Haoyi Zhou*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scaling the amount of data used for supervied fine-tuning(SFT) does not
guarantee the proportional gains in model performance, highlighting a critical
need to understand what makes training samples effective. This work identifies
two fundamental dataset properties that govern SFT scalability:
\textbf{semantic coverage}, or the breadth of task domains, and
\textbf{information depth}, or the richness of individual examples. We
demonstrate that simple proxies for these properties explain the majority of
validation loss variance in our experiments. In this work, we further propose
the \textbf{Information Landscape Approximation (ILA)}, a model-agnostic data
selection framework that jointly optimizes for these two factors. ILA
constructs compact subsets that approximate the informational value of large
datasets. Empirical results show that models tuned on ILA-selected data achieve
faster and more sustained performance improvements across diverse tasks and
model sizes compared to existing methods, a phenomenon we term
\textbf{accelerated scaling}.

</details>


### [150] [Is It Certainly a Deepfake? Reliability Analysis in Detection & Generation Ecosystem](https://arxiv.org/abs/2509.17550)
*Neslihan Kose,Anthony Rhodes,Umur Aybars Ciftci,Ilke Demir*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As generative models are advancing in quality and quantity for creating
synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors
are proposed to counter this effect, however, misuse of detectors claiming fake
content as real or vice versa further fuels this misinformation problem. We
present the first comprehensive uncertainty analysis of deepfake detectors,
systematically investigating how generative artifacts influence prediction
confidence. As reflected in detectors' responses, deepfake generators also
contribute to this uncertainty as their generative residues vary, so we cross
the uncertainty analysis of deepfake detectors and generators. Based on our
observations, the uncertainty manifold holds enough consistent information to
leverage uncertainty for deepfake source detection. Our approach leverages
Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and
epistemic uncertainties across diverse detector architectures. We evaluate
uncertainty on two datasets with nine generators, with four blind and two
biological detectors, compare different uncertainty methods, explore region-
and pixel-based uncertainty, and conduct ablation studies. We conduct and
analyze binary real/fake, multi-class real/fake, source detection, and
leave-one-out experiments between the generator/detector combinations to share
their generalization capability, model calibration, uncertainty, and robustness
against adversarial attacks. We further introduce uncertainty maps that
localize prediction confidence at the pixel level, revealing distinct patterns
correlated with generator-specific artifacts. Our analysis provides critical
insights for deploying reliable deepfake detection systems and establishes
uncertainty quantification as a fundamental requirement for trustworthy
synthetic media detection.

</details>


### [151] [MathBode: Understanding LLM Reasoning with Dynamical Systems](https://arxiv.org/abs/2509.23143)
*Charles L. Wang*

Main category: cs.AI

TL;DR: 本文介绍了MathBode，一个用于评估大型语言模型（LLM）数学推理能力的动态诊断工具。与传统的单次准确率评估不同，MathBode将参数化问题视为一个系统，通过对单个参数进行正弦驱动，并拟合模型输出和精确解的一阶谐波响应，从而得到可解释的、频率相关的增益（幅度跟踪）和相位（滞后）指标，形成类似Bode图的“指纹”。该方法揭示了LLM在处理数学问题时普遍存在的低通滤波行为和不断增加的相位滞后，这些动态特性是仅凭准确率无法捕捉的。研究结果表明，MathBode能够有效地区分不同模型在动态推理能力上的差异，提供了一种紧凑、可复现的评估协议，以补充标准基准测试，并提供可操作的推理保真度和一致性度量。研究人员已开源数据集和代码以促进未来研究。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）评估方法主要关注最终的准确率，忽略了模型在处理数学推理过程中的动态特性。这种片面的评估方式无法揭示模型推理过程中可能存在的系统性问题，例如对输入变化的响应速度、信息传递的延迟等。因此，有必要开发一种能够深入探究LLM数学推理动态行为的诊断工具，以更全面地理解和改进模型的性能。

Method: 本文提出了一种名为MathBode的动态诊断方法。该方法将数学问题视为一个动态系统，通过对输入参数进行正弦信号的周期性扰动，并分析模型输出对这种扰动的响应。具体来说，研究人员测量模型输出的一阶谐波响应，包括增益（幅度跟踪能力）和相位（响应滞后），以此来量化模型的动态行为。通过在不同参数和问题类型上进行测试，MathBode生成了类似Bode图的“指纹”，用于表征模型的推理动态。研究人员使用了一个符号基线模型来校准该诊断工具。

Result: 通过在五种不同类型的封闭形式问题（线性求解、比例/饱和、复利、2x2线性方程组、相似三角形）上的实验，MathBode诊断工具揭示了LLM普遍存在的低通滤波行为，即模型对快速变化的输入响应较慢。此外，研究还观察到模型输出存在不断增加的相位滞后，表明模型在处理复杂或多步推理时，其输出会落后于精确解。MathBode能够区分不同模型在动态推理能力上的差异，将前沿模型与中等水平模型区分开来。相比于传统的准确率指标，这种动态评估提供了更细致、更具信息量的模型性能洞察。

Conclusion: MathBode提供了一种新颖的动态诊断方法，用于评估LLM的数学推理能力，其核心贡献在于引入了增益和相位的概念来量化模型的动态响应特性。该方法揭示了LLM在数学推理中普遍存在的低通滤波和相位滞后现象，这是传统静态评估方法所无法察觉的。MathBode的评估结果能够有效地区分不同模型的性能，并为改进LLM的推理能力提供了可操作的见解。虽然该研究在多个问题类型上验证了方法的有效性，但未来的工作可以扩展到更广泛的数学领域，并进一步探索这些动态特性与模型架构、训练数据之间的关系。该方法具有普适性，并且已开源，有望推动LLM数学推理研究的进一步发展。

Abstract: This paper presents MathBode, a dynamic diagnostic for mathematical reasoning
in large language models (LLMs). Instead of one-shot accuracy, MathBode treats
each parametric problem as a system: we drive a single parameter sinusoidally
and fit first-harmonic responses of model outputs and exact solutions. This
yields interpretable, frequency-resolved metrics -- gain (amplitude tracking)
and phase (lag) -- that form Bode-style fingerprints. Across five closed-form
families (linear solve, ratio/saturation, compound interest, 2x2 linear
systems, similar triangles), the diagnostic surfaces systematic low-pass
behavior and growing phase lag that accuracy alone obscures. We compare several
models against a symbolic baseline that calibrates the instrument ($G \approx
1$, $\phi \approx 0$). Results separate frontier from mid-tier models on
dynamics, providing a compact, reproducible protocol that complements standard
benchmarks with actionable measurements of reasoning fidelity and consistency.
We open-source the dataset and code to enable further research and adoption.

</details>


### [152] [Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research](https://arxiv.org/abs/2509.26080)
*Emma Rose Madden*

Main category: cs.AI

TL;DR: 该论文建议在社会科学中使用大型语言模型（LLMs）时要谨慎，将其视为高容量模式匹配器，用于准预测插值，而不是概率推理的替代品，并提出了实用的护栏措施，如独立抽样、预注册的人类基线、可靠性感知验证和子群校准，以避免范畴错误，实现有用的原型设计和预测。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在社会科学中的应用日益广泛，例如在增强调查响应和驱动多主体模拟方面，研究者们需要理解如何正确解释LLM的输出。忽视LLM的局限性可能导致错误的结论，因此，为LLM在社会科学中的应用提供清晰的指导和实践框架至关重要，以确保研究的严谨性和有效性。

Method: 本研究提出了一种务实的重构框架，建议在社会科学中使用LLMs时，应将其视为高容量模式匹配器，用于在明确的范围条件下进行准预测插值。研究者应避免将LLMs视为概率推理的替代品。此外，论文还引入了实用的护栏措施，包括独立抽样、预注册的人类基线、可靠性感知验证和子群校准。

Result: 通过采用所提出的框架和护栏措施，研究者可以在社会科学领域进行有用的原型设计和预测。这些措施有助于避免将LLMs视为能够进行概率推理的实体，从而防止范畴错误，并提高研究结果的可靠性。

Conclusion: 在社会科学研究中，应谨慎使用大型语言模型（LLMs），并将其定位为用于准预测插值的模式匹配工具，而非概率推理的替代品。通过实施独立抽样、预注册的人类基线、可靠性感知验证和子群校准等护栏措施，可以有效地利用LLMs进行原型设计和预测，同时避免潜在的误用和错误解释，确保研究的准确性和有效性。未来的研究可以进一步探索这些护栏措施在不同社会科学场景下的具体应用和优化。

Abstract: Large Language Models (LLMs) are being increasingly used as synthetic agents
in social science, in applications ranging from augmenting survey responses to
powering multi-agent simulations. This paper outlines cautions that should be
taken when interpreting LLM outputs and proposes a pragmatic reframing for the
social sciences in which LLMs are used as high-capacity pattern matchers for
quasi-predictive interpolation under explicit scope conditions and not as
substitutes for probabilistic inference. Practical guardrails such as
independent draws, preregistered human baselines, reliability-aware validation,
and subgroup calibration, are introduced so that researchers may engage in
useful prototyping and forecasting while avoiding category errors.

</details>


### [153] [A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications](https://arxiv.org/abs/2510.16724)
*Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在信息获取和推理方面取得了巨大进展，但它们受限于静态知识、事实错误以及无法获取实时或领域特定信息。检索增强生成（RAG）通过引入外部证据来缓解这些问题，但传统RAG流程通常是单轮且启发式的，缺乏对检索和推理的自适应控制。基于智能体搜索的最新进展通过实现LLMs在搜索环境中进行多步交互的规划、检索和反思，来解决这些局限性。该调查全面概述了基于强化学习（RL）的智能体搜索，并从三个维度组织了该新兴领域：（i）RL的功能角色，（ii）RL的优化策略，以及（iii）RL的应用范围。文章总结了代表性的方法、评估协议和应用，并讨论了在构建可靠和可扩展的RL驱动的智能体搜索系统方面的开放性挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在信息访问和推理方面取得了显著成就，但它们仍然面临着知识静态、事实幻觉以及无法检索实时或特定领域信息等挑战。检索增强生成（RAG）通过整合外部知识来部分解决这些问题，然而，传统的RAG方法往往是单轮且基于启发式的，缺乏对检索和推理过程的自适应控制能力。因此，有必要探索更先进的方法来增强LLMs的知识获取和推理能力，使其能够更有效地利用外部信息并进行自适应的搜索。

Method: 本调查全面概述了基于强化学习（RL）的智能体搜索（Agentic Search）。该研究将新兴的RL驱动的智能体搜索领域，沿着三个互补的维度进行组织：1. RL的功能角色（即RL被用于解决什么问题）；2. RL的优化策略（即如何利用RL来优化搜索过程）；3. RL的应用范围（即RL在搜索过程的哪些部分被应用）。研究人员总结了该领域内具有代表性的方法、评估协议和应用实例，并探讨了在构建可靠和可扩展的RL驱动的智能体搜索系统方面所面临的挑战和未来的研究方向。

Result: 该调查系统地梳理了基于RL的智能体搜索的最新进展，并从功能角色、优化策略和应用范围三个维度对其进行了分类和组织。文章总结了该领域内的代表性方法、评估协议和实际应用，为研究人员提供了一个全面的视角来理解该领域。通过对现有研究的总结和分析，该调查揭示了RL在提升智能体搜索能力方面的巨大潜力，同时也指出了当前存在的挑战，为未来的研究提供了宝贵的参考。

Conclusion: 基于强化学习（RL）的智能体搜索是增强大型语言模型（LLMs）信息获取和推理能力的一个有前途的方向。通过将RL的自适应和自改进能力应用于智能体搜索的规划、检索和反思过程，可以克服传统RAG方法的局限性。本调查全面概述了该领域，并从功能角色、优化策略和应用范围三个维度进行了组织，总结了现有方法和应用，并指出了未来的研究方向。该领域的未来发展将集中在构建更可靠、更可扩展的RL驱动的智能体搜索系统，以应对日益增长的信息需求和复杂性。

Abstract: The advent of large language models (LLMs) has transformed information access
and reasoning through open-ended natural language interaction. However, LLMs
remain limited by static knowledge, factual hallucinations, and the inability
to retrieve real-time or domain-specific information. Retrieval-Augmented
Generation (RAG) mitigates these issues by grounding model outputs in external
evidence, but traditional RAG pipelines are often single turn and heuristic,
lacking adaptive control over retrieval and reasoning. Recent advances in
agentic search address these limitations by enabling LLMs to plan, retrieve,
and reflect through multi-step interaction with search environments. Within
this paradigm, reinforcement learning (RL) offers a powerful mechanism for
adaptive and self-improving search behavior. This survey provides the first
comprehensive overview of \emph{RL-based agentic search}, organizing the
emerging field along three complementary dimensions: (i) What RL is for
(functional roles), (ii) How RL is used (optimization strategies), and (iii)
Where RL is applied (scope of optimization). We summarize representative
methods, evaluation protocols, and applications, and discuss open challenges
and future directions toward building reliable and scalable RL driven agentic
search systems. We hope this survey will inspire future research on the
integration of RL and agentic search. Our repository is available at
https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [154] [Quantum Kinetic Modeling of KEEN waves in a Warm-Dense Regime](https://arxiv.org/abs/2510.23690)
*F. Alejandro Padilla-Gomez,Sining Gong,Michael S. Murillo,F. R. Graziani,Andrew J. Christlieb*

Main category: physics.plasm-ph

TL;DR: 本研究使用全动力学量子方法研究了动量静电电子非线性(KEEN)波。研究发现，量子效应会削弱经典捕获机制，限制谐波锁定，并加速驱动后的衰减。随着量子参数H的增加，驱动阈值升高，高次谐波被抑制，被捕获的电子涡旋扩散，静电能量降低。这些微观变化对宏观现象有重要影响，例如在惯性约束聚变中，电子德布罗意波长可能接近德拜长度，此时经典描述已不适用。本研究将KEEN物理扩展到量子领域，为下一代惯性约束设计和高能密度平台提供了诊断非平衡电子动力学的新方法，表明未来的聚变模型可能需要结合动力学保真度和量子效应。


<details>
  <summary>Details</summary>
Motivation: 经典动量静电电子非线性(KEEN)波研究在惯性约束聚变等高能密度物理领域具有重要意义。然而，在某些极端条件下，例如电子德布罗意波长接近德拜长度时，经典动力学描述可能不再准确。因此，有必要研究量子效应对KEEN波的影响，以更准确地描述和预测这些极端条件下的等离子体行为。

Method: 本研究采用全动力学量子方法，使用二阶Strang分裂1D1V Wigner-泊松求解器来模拟电子动力学。该求解器耦合了保守的半拉格朗日WENO平流和非局域Wigner项的解析傅里叶空间更新，而离子则保持经典。通过短的、频率调谐的衡动脉冲来驱动KEEN波的形成。研究了量子参数H从经典极限到与暖密物质、掺杂半导体和二维电子系统相关的数值，以探究量子效应对KEEN波的影响。

Result: 研究结果表明，随着量子参数H的增加，驱动阈值升高，高次谐波被抑制，被捕获的电子涡旋扩散，并且静电能量降低到较低的稳态水平。连续小波分析证实了这些变化。具体来说，量子衍射系统地侵蚀了经典的捕获机制，窄化了谐波与基频的锁定，并加速了驱动后的衰减。

Conclusion: 本研究将KEEN物理扩展到了量子领域，揭示了量子效应对KEEN波的重要影响。研究结果表明，在惯性约束聚变等极端条件下，量子效应不可忽视，经典的动力学描述已不足以准确预测等离子体行为。本研究提出的方法和发现为诊断非平衡电子动力学提供了新途径，并强调了在未来的聚变模型中结合动力学保真度和量子效应的重要性。未来的工作可以进一步探索更广泛的量子参数范围和更复杂的等离子体系统，以期更全面地理解量子效应在高能密度物理中的作用。

Abstract: We report a fully kinetic, quantum study of Kinetic Electrostatic Electron
Nonlinear (KEEN) waves, showing that quantum diffraction systematically erodes
the classical trapping mechanism, narrow harmonic locking to the fundamental,
and hasten post-drive decay. Electrons are evolved with a second-order
Strang-split 1D1V Wigner-Poisson solver that couples conservative
semi-Lagrangian WENO advection to an analytic Fourier space update for the
non-local Wigner term, while ions remain classical. Short, frequency-tuned
ponderomotive pulses drive KEEN formation in a uniform Maxwellian plasma; as
the dimensionless quantum parameter H rises from the classical limit to values
relevant to warm-dense matter, doped semiconductors, and 2D electron systems,
the drive threshold increases, higher harmonics are damped, trapped electron
vortices diffuse, and the subplasma electrostatic energy relaxes to a lower
stationary level, as confirmed by continuous wavelet analysis. These
microscopic changes carry macroscopic weight. Ignition-scale capsules now
compress matter to regimes where the electron de Broglie wavelength rivals the
Debye length, making classical kinetic descriptions insufficient. By extending
KEEN physics into this quantum domain, our results offer a potential diagnostic
of nonequilibrium electron dynamics for next-generation inertial-confinement
designs and high-energy-density platforms, indicating that predictive fusion
modeling may benefit from the integration of kinetic fidelity with quantum
effects.

</details>


### [155] [Effect of flow-aligned external magnetic fields on mushroom instability](https://arxiv.org/abs/2510.24121)
*Y. Guo,D. Wu,J. Zhang*

Main category: physics.plasm-ph

TL;DR: 本文研究了外部磁场对喷流中磁场不稳定性（MI）的影响。研究发现，外部磁场会抑制MI的增长，但MI相比于电子尺度开尔文-亥姆霍兹不稳定性（ESKHI）对外部磁场更具鲁棒性。理论分析和粒子模拟（PIC）均证实了这一点。在有温度的情况下，MI与扩散引起的直流磁场之间存在竞争与合作关系。


<details>
  <summary>Details</summary>
Motivation: 研究磁化喷流中磁场不稳定性（MI）的行为，因为MI被认为是产生和放大相对论喷流磁场的原因，而磁化喷流是天体物理学中的常见现象，但对其MI行为的理解尚不充分。

Method: 结合理论分析和粒子 the cell (PIC) 模拟。理论上，推导了磁化MI线性增长率的广义色散关系，并求解了数值解。分析了任意波矢的不稳定性。在模拟中，使用了二维PIC模拟单模MI，并与理论预测进行对比。同时，也进行了有限温度下的模拟。

Result: 外部磁场总是抑制MI的增长，但MI比ESKHI更能抵抗外部磁场的影响。理论预测与二维PIC模拟结果吻合良好。在有限温度模拟中，观察到MI与扩散引起的直流磁场之间存在竞争与合作。

Conclusion: 外部磁场对MI有抑制作用，但MI仍能有效增长。MI在磁化喷流中的行为比之前认为的更复杂，存在与扩散磁场的相互作用。未来的研究可以进一步探索这些相互作用以及MI在不同喷流环境下的行为。

Abstract: Mushroom instability (MI) is a shear instability considered responsible for
generating and amplifying magnetic fields in relativistic jets. While
astrophysical jets are usually considered to be magnetized, how MI acts in
magnetized jets remains poorly understood. In this paper, we investigate the
effect of a flow-aligned external magnetic field on MI, with both theoretical
analyses and particle-in-cell (PIC) simulations. In the limit of a cold and
collisionless plasma, we derive a generalized dispersion relation for linear
growth rates of the magnetized MIs. Numerical solutions of the dispersion
relation reveal that the external magnetic field always suppresses the growth
of MI, though MIs are much more robust to the external magnetic field than
electron-scale Kelvin-Helmholtz instabilities (ESKHIs). Analyses are also
extended to instabilities with an arbitrary wavevector in the shear interface
plane. Two-dimensional PIC simulations of single-mode MIs reach a good
agreement with our analytical predictions. In simulations with finite
temperatures, we observe the competition and cooperation between MIs and a
diffusion-induced DC magnetic field.

</details>


### [156] [Physics-Informed Visual MARFE Prediction on the HL-3 Tokamak](https://arxiv.org/abs/2510.24347)
*Qianyun Dong,Rongpeng Li,Zongyu Yang,Fan Xia,Liang Liu,Zhifeng Zhao,Wulyu Zhong*

Main category: physics.plasm-ph

TL;DR: 本文提出了一种结合物理信息和深度学习的MARFE（多方面不对称辐射）预测新方法，用于提前预警托卡马克装置中的密度极限破坏。通过高精度标签修正和约束神经ODE模型，该方法在HL-3托卡马克上实现了0.969的AUC预测精度，并已成功部署用于实时操作，为未来的主动式MARFE缓解奠定了基础。


<details>
  <summary>Details</summary>
Motivation: MARFE（多方面不对称辐射）是托卡马克装置中一种可能导致密度极限破坏的关键等离子体不稳定性，对装置的安全运行构成威胁。因此，实现MARFE的早期可靠预警对于制定有效的破坏缓解策略，特别是对于ITER等下一代装置至关重要。

Method: 本文提出了一种新颖的、基于物理信息的MARFE预测指标。该框架包含两项核心创新：1. 一个高精度标签修正流程，利用基于物理评分的加权期望最大化（EM）算法，系统地校正相机原始视觉数据中的噪声和伪影；2. 一个连续时间、物理约束的神经常微分方程（Neural ODE）模型，用于预测MARFE的短期“恶化”趋势。该模型通过将关键等离子体参数（如归一化密度 $f_G$ 和核心电子温度 $T_e$）纳入模型动力学，以提高预测性能。

Result: 在HL-3托卡马克的海量实验数据上，该模型展示了高预测精度，在提前40毫秒进行预测时，达到了0.969的曲线下面积（AUC）。该预测指标已成功部署，实现每1毫秒更新一次的实时操作。

Conclusion: 本研究提出的MARFE预测新方法，通过结合物理信息和先进的深度学习技术，显著提高了早期预警能力和预测精度，同时保持了低误报率。该方法已成功应用于HL-3托卡马克进行实时操作，为解决托卡马克运行中的关键安全问题提供了有效的解决方案，并为未来的主动式MARFE缓解策略奠定了坚实的基础。

Abstract: The Multifaceted Asymmetric Radiation From the Edge (MARFE) is a critical
plasma instability that often precedes density-limit disruptions in tokamaks,
posing a significant risk to machine integrity and operational efficiency.
Early and reliable alert of MARFE formation is therefore essential for
developing effective disruption mitigation strategies, particularly for
next-generation devices like ITER. This paper presents a novel,
physics-informed indicator for early MARFE prediction and disruption warning
developed for the HL-3 tokamak. Our framework integrates two core innovations:
(1) a high-fidelity label refinement pipeline that employs a physics-scored,
weighted Expectation-Maximization (EM) algorithm to systematically correct
noise and artifacts in raw visual data from cameras, and (2) a continuous-time,
physics-constrained Neural Ordinary Differential Equation (Neural ODE) model
that predicts the short-horizon ``worsening" of a MARFE. By conditioning the
model's dynamics on key plasma parameters such as normalized density ($f_G$,
derived from core electron density) and core electron temperature ($T_e$), the
predictor achieves superior performance in the low-false-alarm regime crucial
for control. On a large experimental dataset from HL-3, our model demonstrates
high predictive accuracy, achieving an Area Under the Curve (AUC) of 0.969 for
40ms-ahead prediction. The indicator has been successfully deployed for
real-time operation with updates every 1 ms. This work lays a very foundation
for future proactive MARFE mitigation.

</details>


### [157] [Wave Topology in Hall MHD](https://arxiv.org/abs/2506.18830)
*Alejandro Mesa Dame,Hong Qin,Eric Palmerduca,Yichen Fu*

Main category: physics.plasm-ph

TL;DR: Hall Magnetohydrodynamics (HMHD) is an extension of ideal MHD that includes the Hall effect for more accurate plasma behavior at small scales. This paper comprehensively analyzes HMHD's eigenmodes, proving its wave spectrum is homotopic to ideal MHD's, consisting of slow, shear Alfvén, and fast magnetosonic-Hall waves. It identifies a nontrivial topological structure with a Weyl point and nonzero Chern numbers, refuting claims of additional wave branches. The findings clarify HMHD's wave properties and topological characteristics.


<details>
  <summary>Details</summary>
Motivation: Ideal Magnetohydrodynamics (MHD) is limited in describing plasma behavior at small length scales. Hall Magnetohydrodynamics (HMHD) extends ideal MHD by incorporating the Hall effect through the induction equation, offering greater accuracy for phenomena below the ion skin depth. However, a complete understanding and description of HMHD's eigenmodes and their topological structure were lacking, hindering a full grasp of its physical implications.

Method: The researchers derived the complete spectrum and eigenvectors of Hall Magnetohydrodynamics (HMHD) waves. They analyzed these to identify the underlying topological structure of the wave spectrum. The study involved mathematical derivations and theoretical analysis of the HMHD equations and their solutions.

Result: The study successfully derived the complete spectrum and eigenvectors of HMHD waves. It was proven that the HMHD wave spectrum is homotopic to that of ideal MHD, comprising three distinct branches: slow magnetosonic-Hall waves, shear Alfvén-Hall waves, and fast magnetosonic-Hall waves. These branches continuously merge into their ideal MHD counterparts as the Hall parameter diminishes. Crucially, the research found that HMHD does not introduce new wave branches beyond those in ideal MHD. A key finding is the nontrivial topological nature of the HMHD wave structure, characterized by a Weyl point (an isolated eigenmode degeneracy point) and associated nonzero Chern numbers of the eigenmode bundles over a 2-sphere in k-space around the Weyl point.

Conclusion: This work provides a comprehensive analysis of Hall Magnetohydrodynamics (HMHD) wave properties and their topological structure. The findings confirm that HMHD's spectrum is topologically similar to ideal MHD, comprising three main wave branches, and refutes the existence of additional branches. The identification of a Weyl point and associated topological invariants offers new insights into the fundamental physics of HMHD, with potential implications for understanding plasma phenomena at small scales and in topological contexts. Future work could explore the experimental verification of these topological features and their role in specific astrophysical or laboratory plasma scenarios.

Abstract: Hall Magnetohydrodynamics (HMHD) extends ideal MHD by incorporating the Hall
effect via the induction equation, making it more accurate for describing
plasma behavior at length scales below the ion skin depth. Despite its
importance, a comprehensive description of the eigenmodes in HMHD has been
lacking. In this work, we derive the complete spectrum and eigenvectors of HMHD
waves and identify their underlying topological structure. We prove that the
HMHD wave spectrum is homotopic to that of ideal MHD, consisting of three
distinct branches: the slow magnetosonic-Hall waves, the shear Alfv\'en-Hall
waves, and the fast magnetosonic-Hall waves, which continuously reduce to their
ideal MHD counterparts in the limit of vanishing Hall parameter. Contrary to a
recent claim, we find that HMHD does not admit any additional wave branches
beyond those in ideal MHD. The key qualitative difference lies in the
topological nature of the HMHD wave structure: it exhibits nontrivial topology
characterized by a Weyl point-an isolated eigenmode degeneracy point-and
associated nonzero Chern numbers of the eigenmode bundles over a 2-sphere in
k-space surrounding the Weyl point.

</details>


### [158] [Laboratory formation of scaled astrophysical outflows](https://arxiv.org/abs/2510.21239)
*Shun-yi Yang,Tao Tao,Guang-yue Hu,Chao Xiong,Tian-yi Li,Xue-cheng Li,Hui-bo Tang,Shuo-ting Shao,Xiang Lv,Chen Zhang,Ming-yang Yu*

Main category: physics.plasm-ph

TL;DR: 通过激光等离子体实验，研究了磁化背景气体中的等离子体流出形态，发现了其形态（准直喷流、阻塞喷流、椭圆泡、球形风和泡）由外部阿尔芬马赫数 (Me-a) 和声波马赫数 (Me-s) 决定，并给出了形态转变的临界值 (Me-a ~ 2 和 0.5，Me-s ~ 1)，为理解天体物理流出提供了定量框架。


<details>
  <summary>Details</summary>
Motivation: 天体物理系统中流出的形态多样，但其形成机制和存在条件仍是该领域未解之谜。理解这些流出的形态和形成条件对于揭示宇宙演化、星系形成以及能量传输过程至关重要。

Method: 本研究采用基于激光驱动的等离子体流出实验，在磁化背景气体中模拟天体物理流出。通过改变实验参数，控制等离子体流出的动压与背景介质的磁压和热压的相对强度，即外部阿尔芬马赫数 (Me-a) 和声波马赫数 (Me-s)，来研究不同流出形态的形成条件。实验结果通过磁流体力学模拟进行验证。

Result: 实验结果表明，流出的形态（准直喷流、阻塞喷流、椭圆泡、球形风和泡）完全由外部阿尔芬马赫数 (Me-a) 和声波马赫数 (Me-s) 决定。研究确定了形态转变的临界值：当 Me-a 约等于 2 和 0.5，以及 Me-s 约等于 1 时，流出形态发生转变。这些发现得到了磁流体力学模拟的证实。

Conclusion: 本研究通过实验室模拟实验，首次提出了一个定量框架来理解天体物理流出的形态及其形成条件，该框架由外部阿尔芬马赫数和声波马赫数决定。这些发现不仅能解释已有的天文观测，还能指导未来的观测研究，有助于深入理解宇宙中的能量传输和物质分布。研究的局限性可能在于实验室模拟的尺度效应和物理条件的简化，未来的工作可以考虑更复杂的物理过程和更大尺度的模拟。

Abstract: Astrophysical systems exhibit a rich diversity of outflow morphologies, yet
their mechanisms and existence conditions remain among the most persistent
puzzles in the field. Here we present scaled laboratory experiments based on
laser-driven plasma outflow into magnetized ambient gas, which mimic five basic
astrophysical outflows regulated by interstellar medium, namely collimated
jets, blocked jets, elliptical bubbles, as well as spherical winds and bubbles.
Their morphologies and existence conditions are found to be uniquely determined
by the external Alfvenic and sonic Mach numbers Me-a and Me-s, i.e. the
relative strengths of the outflow ram pressure against the magnetic/thermal
pressures in the interstellar medium, with transitions occurring at Me-a ~ 2
and 0.5, as well as Me-s ~ 1. These results are confirmed by
magnetohydrodynamics simulations and should also be verifiable from existing
and future astronomical observations. Our findings provide a quantitative
framework for understanding astrophysical outflows.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [159] [Phase-Space Shaping in Wakefield Accelerators due to Betatron Cooling](https://arxiv.org/abs/2510.24567)
*Pablo J. Bilbao,Thales Silva,Luis O. Silva*

Main category: physics.acc-ph

TL;DR: 等离子体加速器中，高密度等离子体中的相对论性束流通过拍频辐射效应，导致束相空间结构化，形成具有正径向位置和动量梯度的环状结构，实现粒子布居反转，此过程可通过解析和PIC模拟得到证实，并可能引发相干拍频辐射。


<details>
  <summary>Details</summary>
Motivation: 随着等离子体加速器中使用的相对论性束流的电荷量和持续时间达到前所未有的水平，在传统上难以处理的高密度等离子体（密度大于10^19 cm^-3）中，拍频辐射效应日益显著，并开始影响被加速束流的动力学行为。理解并利用这种效应对于优化加速过程和实现新型辐射源至关重要。

Method: 通过多维粒子束模拟（Particle-in-Cell simulations）和解析推导，研究了拍频辐射对束流相空间动力学的影响。具体地，分析了拍频辐射导致的束流结构化，并推导了该过程的特征时间尺度。

Result: 研究发现，拍频冷却效应在束流相空间中产生了强烈的结构化，形成了具有正径向位置和动量梯度的环状结构，这相当于实现了振荡幅度的布居反转。通过解析和模拟，验证了该结构化的形成过程及其时间尺度。

Conclusion: 在辐射主导的动力学机制下，束流的加速过程发生了根本性改变，产生了能够触发离子通道中相干拍频辐射的自结构化束流。这一发现为利用等离子体加速器产生高亮度相干辐射开辟了新的途径。

Abstract: Plasma-based accelerators are beginning to employ relativistic beams with
unprecedented charge and ultrashort durations. These dense driver beams can
drive wakes even in high-density plasmas ($\gtrsim10^{19}$ cm$^{-3}$), where
betatron radiation becomes increasingly important and begins to affect the
dynamics of the accelerated beam. In this Letter, we show that betatron cooling
leads to a strong, structuring of the phase space of the beam. This gives rise
to bunched, ring-like structures with positive radial position and momentum
gradients, \emph{i.e.}, population inversion of the amplitude of oscillation.
We derive the characteristic timescales for this process analytically and
confirm our predictions with multi-dimensional Particle-in-Cell simulations.
The radiation-dominated regime of beam dynamics fundamentally alters the
acceleration process and produces self-structured beams capable of triggering
coherent betatron emission in ion channels.

</details>


### [160] [Distributed Inter-Strand Coupling Current Model for Finite Element Simulations of Rutherford Cables](https://arxiv.org/abs/2510.24618)
*Julien Dular,Alexander Glock,Arjan Verweij,Mariusz Wozniak*

Main category: physics.acc-ph

TL;DR: 本文提出了一种名为分布式跨层耦合电流（DISCC）的有限元模型，该模型采用均质化方法，能够高效准确地模拟超导卢瑟福电缆的瞬态磁响应，而无需显式表示单个导体。DISCC模型通过一种新颖的混合有限元公式再现了跨层耦合电流动力学，并可与基于导体层面的降阶迟滞磁化（ROHM）和磁通（ROHF）模型结合，以重现导体内部的迟滞、涡流、跨丝耦合电流和欧姆效应等动力学。该模型在保持对所有损耗、磁化和电感贡献的考虑的同时，显著减少了计算时间。


<details>
  <summary>Details</summary>
Motivation: 传统的超导卢瑟福电缆的有限元模型在模拟瞬态磁响应时计算成本高昂，尤其是在需要考虑导体内部复杂动力学（如迟滞、涡流和耦合电流）时。这种计算效率低下限制了在大型电磁系统（如强子对撞机）中进行详细仿真和设计优化的能力。因此，需要一种更高效的模型来准确捕捉电缆的电磁行为，以支持更快的仿真速度和更广泛的应用。

Method: 本文提出并实现了一种分布式跨层耦合电流（DISCC）有限元模型。该模型基于均质化方法，通过一种新颖的混合有限元公式来模拟跨层耦合电流的动力学，而无需显式解析每根导体。此外，DISCC模型可以与降阶迟滞磁化（ROHM）和磁通（ROHF）模型相结合，以考虑导体内部的迟滞、涡流、跨丝耦合电流以及欧姆效应。文章分析了DISCC模型单独作为线性问题时的性能，并进一步将其扩展到包含导体内部动力学的非线性问题。文章还提出了两种实现DISCC模型的有限元方法：一种基于h-φ公式，另一种基于h-φ-A公式，后者特别适用于处理磁体横截面中的铁磁区域。

Result: 与传统的全解析有限元模型相比，DISCC模型在计算效率上实现了数量级的提升，显著缩短了计算时间。该模型能够准确地考虑所有类型的损耗、磁化和电感贡献。研究表明，通过DISCC模型均质化后的卢瑟福电缆可以直接集成到磁体横截面的有限元模型中，用于高效的电磁-热瞬态响应仿真。

Conclusion: DISCC模型提供了一种高效且准确的方法来模拟超导卢瑟福电缆的瞬态磁响应，同时能够考虑导体内部复杂的电磁现象。该模型通过均质化显著降低了计算复杂性，使得在大型电磁系统中进行详细的瞬态仿真成为可能。其灵活性使其能够与现有模型结合，并适用于包含铁磁材料的复杂几何结构。未来工作可以进一步探索该模型在不同应用场景下的优化和验证。

Abstract: In this paper, we present the Distributed Inter-Strand Coupling Current
(DISCC) model. It is a finite element (FE) model based on a homogenization
approach enabling efficient and accurate simulation of the transient magnetic
response of superconducting Rutherford cables without explicitly representing
individual strands. The DISCC model reproduces the inter-strand coupling
current dynamics via a novel mixed FE formulation, and can be combined with the
Reduced Order Hysteretic Magnetization (ROHM) and Flux (ROHF) models applied at
the strand level in order to reproduce the internal strand dynamics:
hysteresis, eddy, and inter-filament coupling currents, as well as ohmic
effects. We first analyze the performance of the DISCC model alone, as a linear
problem. We then extend the analysis to include the internal strand dynamics
that make the problem nonlinear. In all cases, the DISCC model offers a massive
reduction of the computational time compared to conventional fully detailed FE
models while still accounting for all types of loss, magnetization and
inductance contributions. Rutherford cables homogenized with the DISCC model
can be directly included in FE models of magnet cross-sections for efficient
electro-magneto-thermal simulations of their transient response. We present two
possible FE formulations for the implementation of the DISCC model, a first one
based on the h-phi-formulation, and a second one based on the
h-phi-a-formulation, which is well suited for an efficient treatment of the
ferromagnetic regions in magnet cross-sections.

</details>


### [161] [Controls Abstraction Towards Accelerator Physics: A Middle Layer Python Package for Particle Accelerator Control](https://arxiv.org/abs/2509.19794)
*M. King,A. D. Brynes,F. Jackson,J. K. Jones,N. Ziyan,M. A. Johnson,K. Baker,D. J. Scott,E. Yang,T. Kabana,C. Garnier,S. Chowdhury,N. Neveu,R. Roussel*

Main category: physics.acc-ph

TL;DR: 本文介绍了一个名为CATAP的Python包，它为加速器物理学提供了一个现代化的控制系统中间层。CATAP通过显式抽象、YAML配置和过程代码生成，简化了用户编写控制逻辑和设备信息所需的工作量，并将系统知识编码化。该工具已在两个加速器设施中部署，并设计成可生成特定于设施的中间层包，以便推广到其他机器。


<details>
  <summary>Details</summary>
Motivation: 现有的控制系统中间层在协调终端用户（如操作员、专家、科学家和实验用户）与低级控制系统接口之间存在不足。需要一个更现代、更易于使用的解决方案来简化用户的工作，并对通常以轶事形式存在的系统知识进行编码。

Method: 本文描述了一个名为CATAP（Controls Abstraction Towards Acclerator Physics）的Python包。CATAP采用显式抽象、基于YAML的配置和过程代码生成。它提供了一个结构化、连贯的接口，允许研究人员和操作员集中更高层次的控制逻辑和设备信息。CATAP的设计已被部署到两个加速器设施，并能够从配置文件生成特定于设施的中间层包。

Result: CATAP的部署表明，它能够显著减少用户编写控制任务所需的代码量，并系统地记录和传播系统知识。通过从配置文件生成特定于设施的中间层包，CATAP能够更广泛地传播到其他机器。

Conclusion: CATAP作为一个现代化的Python控制系统中间层，通过简化用户交互、编码系统知识和促进跨机器的传播，在加速器物理学领域取得了显著进展。该工具的成功部署证明了其有效性，为未来的发展和更广泛的应用奠定了基础。

Abstract: Control system middle layers act as a co-ordination and communication bridge
between end users, including operators, system experts, scientists, and
experimental users, and the low-level control system interface. This article
describes a Python package -- Controls Abstraction Towards Acclerator Physics
(CATAP) -- which aims to build on previous experience and provide a modern
Python-based middle layer with explicit abstraction, YAML-based configuration,
and procedural code generation. CATAP provides a structured and coherent
interface to a control system, allowing researchers and operators to centralize
higher-level control logic and device information. This greatly reduces the
amount of code that a user must write to perform a task, and codifies system
knowledge that is usually anecdotal. The CATAP design has been deployed at two
accelerator facilities, and has been developed to produce a procedurally
generated facility-specific middle layer package from configuration files to
enable its wider dissemination across other machines.

</details>


### [162] [Three Dimensional Theory of the Ion Channel Laser](https://arxiv.org/abs/2509.20995)
*Claire Hansel,Agostino Marinelli,Zhirong Huang,Michael Litos*

Main category: physics.acc-ph

TL;DR: 本文提出了一种离子通道激光器（ICL）的 3D 理论模型，该模型克服了自由电子激光器（FEL）的限制，并在极短距离内实现高增益相干辐射，即使电子束能量展宽较大也能实现。研究结果表明，ICL 具有优于 FEL 的增益参数，但对电子束的横向相位空间要求更高。该理论模型考虑了衍射、横向辐射分布、频率和 betatron 阶段失谐以及能量和 the undulator parameter 展宽等多种效应，并推导了 ICL 的李萨如图和场方程，以及 3D ICL 色散关系。数值模拟结果揭示了 3D 效应、能量展宽和发射率对增益的影响，并得出了 ICL 实现激光的电子束相位空间和发射率要求。


<details>
  <summary>Details</summary>
Motivation: 自由电子激光器（FEL）在产生相干辐射方面取得了显著进展，但其对电子束的要求非常苛刻，尤其是在能量展宽和横向相位空间方面。离子通道激光器（ICL）作为一种基于等离子体的替代方案，利用离子通道的强聚焦能力，有望在更短的距离内实现更高的增益，并且对电子束能量展宽的容忍度更高，这使其在与等离子体加速器结合方面具有巨大潜力。然而，现有的 ICL 理论模型未能充分考虑三维效应，限制了其在高能量、高亮度应用中的精确预测和优化。因此，有必要发展一个更全面的 3D ICL 理论模型，以准确评估其性能，并为未来的实验设计提供指导。

Method: 本文提出了一种新的离子通道激光器（ICL）的 3D 理论模型，该模型针对平面离轴配置进行了优化。研究人员首先推导了 ICL 的李萨如图和场方程，这些方程考虑了包括衍射、横向辐射分布、频率和 betatron 阶段失谐以及能量和 the undulator parameter 展宽在内的多种效应。随后，将这些方程与 3D 麦克斯韦-克利蒙托维奇方程相结合，并进行线性化处理，得到一个描述辐射场 Z 向演化的耦合微分方程。利用 Van Kampen 正常模式展开方法，推导了 ICL 的 3D 色散关系。最后，通过数值求解 Z 向演化方程，计算了在不同 ICL 参数下，辐射功率增长率和横向辐射分布，并分析了 3D 效应、能量展宽和发射率对增益的影响，从而得出了 ICL 实现激光所需的电子束相位空间和发射率要求。

Result: 通过数值模拟，研究发现 ICL 的增益参数显著高于 FEL，并且能够在极短的距离内实现激光。即使电子束的能量展宽达到几个百分比，ICL 仍然能够保持较高的增益，这与当前等离子体加速器的能力相匹配。然而，研究也强调了 ICL 对电子束横向相位空间提出了比 FEL 更为严格的要求。3D 效应、能量展宽和发射率都会导致增益降低，具体降低幅度取决于 ICL 的参数设置。研究结果还量化了实现激光所需的电子束发射率和相位空间限制。

Conclusion: 本文提出的 3D ICL 理论模型为理解和优化离子通道激光器提供了一个强大的工具。研究结果表明，ICL 作为一种有前景的相干光源，在克服 FEL 的一些局限性方面具有显著优势，尤其是在增益和对能量展宽的容忍度方面。然而，其对电子束横向相位空间的高要求是实现高效 ICL 的关键挑战。未来的工作应集中于进一步完善理论模型，包括考虑更复杂的离子通道构型和更广泛的等离子体效应，并结合实验验证，以充分发挥 ICL 的潜力，推动其在科学研究和工业应用中的发展。

Abstract: The ion channel laser (ICL) is a plasma-based alternative to the free
electron laser (FEL) that uses the electric field of a uniform-density ion
channel rather than the magnetic field of an undulator to induce transverse
oscillations of electrons in an ultrarelativistic bunch and thereby produce
coherent radiation via a collective electromagnetic instability. The powerful
focusing of the ion channel generally yields significantly higher gain
parameters in the ICL as compared to the FEL. This permits lasing in extremely
short distances using electron bunches with an energy spread as large as a few
percent; a value readily achievable with current plasma-based accelerators.
ICLs, however, impose stringent transverse phase space requirements on the
electron bunch beyond what is required in FELs. In this work, we present a
novel 3D theory of the planar off-axis configuration of the ICL that accounts
for a number of effects including diffraction, transverse radiation profile,
frequency and betatron phase detuning, and nonzero spread in energy and
undulator parameter. We derive the ICL pendulum and field equations, which we
use to write down the 3D Maxwell-Klimontovich equations. After linearizing, we
obtain an integro-differential equation describing the $z$-evolution of the
radiation field. The 3D ICL dispersion relation is obtained using a Van Kampen
normal mode expansion. We numerically solve the $z$-evolution equation to
compute radiation power growth rates and transverse radiation profiles over a
range of different ICL parameters. We examine the gain reduction due to 3D
effects, energy spread, and emittance. Electron bunch phase space and emittance
requirements for lasing are derived. Finally, we make general observations
about the performance and feasibility of the ICL and discuss future prospects.

</details>
