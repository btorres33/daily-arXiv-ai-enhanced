<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training](https://arxiv.org/abs/2510.20059)
*Mehrdad Ghassabi,Sadra Hakim,Hamidreza Baradaran Kashani,Pedram Rostami*

Main category: cs.CL

TL;DR: 本研究旨在使用AI反馈强化学习（RLAIF）和直接偏好优化（DPO）技术，通过构建和训练一个包含200万个词元的偏好答案和250万个词元的拒绝答案的数据集，显著提升小型波斯语语言模型在医学问答领域的推理能力。结果表明，经过少量数据训练的模型在推理能力上超越了使用更大数据集训练的先前模型，证明了在数据受限情况下进行推理导向训练的有效性。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在医学问答等专业领域的推理能力提升至关重要，尤其是在波斯语等代表性不足的语言中。现有模型在处理复杂推理任务时能力有限，特别是在低资源语言环境下，需要有效的方法来增强其推理能力，以满足特定应用的需求。

Method: 本研究采用了RLAIF和DPO技术来提升一个通用的波斯语语言模型的推理能力。首先，将一个多项选择的医学问答数据集翻译成波斯语。然后，利用RLAIF生成偏好-拒绝答案对，用于DPO训练。通过提示教师模型和学生模型生成思维链（CoT）推理过程，构建了一个包含正确和错误推理路径的数据集。该数据集包含200万个词元的偏好答案和250万个词元的拒绝答案，用于训练基线模型。

Result: 经过RLAIF和DPO训练后，基线模型在医学推理任务上的能力得到了显著提升。与在约5700万词元上训练的先前模型gaokerena-V相比，本研究提出的模型在数据量少得多的情况下，在波斯语医学问答的推理能力上表现更优。这表明该方法能够有效地利用有限的数据进行模型优化。

Conclusion: 本研究证明了在数据量有限的情况下，通过RLAIF和DPO进行推理导向的训练，能够高效且有效地提升小型波斯语语言模型在医学问答等专业领域的推理能力。研究结果突显了这种方法在开发特定领域语言模型方面的潜力，为低资源语言的AI应用提供了有价值的见解，并为未来的研究指明了方向。

Abstract: Enhancing reasoning capabilities in small language models is critical for
specialized applications such as medical question answering, particularly in
underrepresented languages like Persian. In this study, we employ Reinforcement
Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to
improve the reasoning skills of a general-purpose Persian language model. To
achieve this, we translated a multiple-choice medical question-answering
dataset into Persian and used RLAIF to generate rejected-preferred answer
pairs, which are essential for DPO training. By prompting both teacher and
student models to produce Chain-of-Thought (CoT) reasoning responses, we
compiled a dataset containing correct and incorrect reasoning trajectories.
This dataset, comprising 2 million tokens in preferred answers and 2.5 million
tokens in rejected ones, was used to train a baseline model, significantly
enhancing its medical reasoning capabilities in Persian. Remarkably, the
resulting model outperformed its predecessor, gaokerena-V, which was trained on
approximately 57 million tokens, despite leveraging a much smaller dataset.
These results highlight the efficiency and effectiveness of reasoning-focused
training approaches in developing domain-specific language models with limited
data availability.

</details>
